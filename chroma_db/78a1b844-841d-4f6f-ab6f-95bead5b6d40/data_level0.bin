18.657: Mathematics of Machine Learning
Lecturer: Philippe Rigollet Lecture 1
Scribe:Philippe Rigollet Sep. 9, 2015
1. WHAT IS MACHINE LEARNING (IN THIS COURSE) ?
This course focuses on statistical learning theory , which roughly means understanding the
amount of data required to achieve a certain prediction accuracy. To better understand
what this means, we ï¬rst focus on stating some diï¬€erences between statistics andmachine
learning since the two ï¬elds share common goals.
Indeed,bothseemtotrytousedatatoimprovedecisions. Whiletheseï¬eldshaveevolved
in the same direction and currently share a lot of aspects, they were at the beginning quite
diï¬€erent. Statistics was around much before machine learning and statistics was already
a fully developed scientiï¬c discipline by 1920, most notably thanks to the contributions of
R. Fisher, who popularized maximum likelihood estimation (MLE) as a systematic tool for
statistical inference. However, MLErequiresessentially knowingtheprobabilitydistribution
fromwhichthedataisdraw,uptosomeunknownparameterofinterest. Often, theunknown
parameter has a physical meaning and its estimation is key in better understanding some
phenomena. Enabling MLE thus requires knowing a lot about the data generating process:
this is known as modeling . Modeling can be driven by physics or prior knowledge of the
problem. In any case, it requires quite a bit of domain knowledge.
More recently (examples go back to the 1960â€™s) new types of datasets (demographics,
social, medical,...) have become available. However, modeling the data that they contain
is much more hazardous since we do not understand very well the input/output process
thus requiring a distribution free approach. A typical example is image classiï¬cation where
the goal is to label an image simply from a digitalization of this image. Understanding
what makes an image a cat or a dog for example is a very complicated process. However,
for the classiï¬cation task, one does not need to understand the labelling process but rather
to replicate it. In that sense, machine learning favors a blackbox approach (see Figure 1).
inputX outputYblackbox
 y=f(x)+ÎµinputX outputY
Figure 1: The machine learning blackbox (left) where the goal is to replicate input/output
pairs from past observations, versus the statistical approach that opens the blackbox and
models the relationship.
These diï¬€erences between statistics and machine learning have receded over the last
couple of decades. Indeed, on the one hand, statistics is more and more concerned with
ï¬nite sample analysis, model misspeciï¬cation and computational considerations. On the
other hand, probabilistic modeling is now inherent to machine learning. At the intersection
of the two ï¬elds, lies statistical learning theory , a ï¬eld which is primarily concerned with
sample complexity questions, some of which will be the focus of this class.
1
2. STATISTICAL LEARNING THEORY
2.1 Binary classiï¬cation
A large partof this class will bedevoted tooneof thesimplest problemof statistical learning
theory: binary classiï¬cation (aka pattern recognition [DGL96]). In this problem, weobserve
(X1,Y1),...,(Xn,Yn) that are nindependent random copies of ( X,Y)âˆˆ XÃ—{0,1}. Denote
byPX,Ythe joint distribution of ( X,Y). The so-called featureXlives in some abstract
spaceX(think IRd) andYâˆˆ {0,1}is called label. For example, Xcan be a collection of
gene expression levels measured on a patient and Yindicates if this person suï¬€ers from
obesity. The goal of binary classiï¬cation is to build a rule to predict YgivenXusing
only the data at hand. Such a rule is a function h:X â†’ {0,1}called a classiï¬er . Some
classiï¬ers are better than others and we will favor ones that have low classiï¬cation error
R(h) = IP(h(X) =Y). Let us make some important remarks.
Fist of all, since Yâˆˆ {0,1}thenYhas a Bernoulli distribution: so much for distribution
free assumptions! However, we will not make assumptions on the marginal distribution of
Xor, what matters for prediction, the conditional distribution of YgivenX. We write,
Y|Xâˆ¼Ber(Î·(X)), where Î·(X) = IP(Y= 1|X) = IE[Y|X] is called the regression function
ofYontoX.
Next, note that we did not write Y=Î·(X). Actually we have Y=Î·(X) +Îµ, where
Îµ=Yâˆ’Î·(X) is aâ€œnoiseâ€ randomvariablethat satisï¬es IE[ Îµ|X] = 0. Inparticular, this noise
accounts for the fact that Xmay not contain enough information to predict Yperfectly.
This is clearly the case in our genomic example above: it not whether there is even any
information about obesity contained in a patientâ€™s genotype. The noise vanishes if and only
ifÎ·(x)âˆˆ {0,1}for allxâˆˆ X. Figure 2.1 illustrates the case where there is no noise and the
the more realistic case where there is noise. When Î·(x) is close to .5, there is essentially no
information about YinXas theYis determined essentially by a toss up. In this case, it
is clear that even with an inï¬nite amount of data to learn from, we cannot predict Ywell
since there is nothing to learn. We will see what the eï¬€ect of the noise also appears in the
sample complexity./ne}ationslash
Figure 2: The thick black curve corresponds to the noiseless case where Y=Î·(X)âˆˆ {0,1}
and the thin red curve corresponds to the more realistic case where Î·âˆˆ[0,1]. In the latter
case, even full knowledge of Î·does not guarantee a perfect prediction of Y.
In the presence of noise, since we cannot predict Yaccurately, we cannot drive the
classiï¬cation error R(h) to zero, regardless of what classiï¬er hwe use. What is the smallest
value that can beachieved? As a thought experiment, assume to begin with that we have all
xÎ·(x)
1
.5
2
the information that we may ever hope to get, namely we know the regression function Î·(Â·).
For a given Xto classify, if Î·(X) = 1/2 we may just toss a coin to decide our prediction
and discard Xsince it contains no information about Y. However, if Î·(X) = 1/2, we have
an edge over random guessing: if Î·(X)>1/2, it means that IP( Y= 1|X)>IP(Y= 0|X)
or, in words, that 1 is more likely to be the correct label. We will see that the classiï¬er
hâˆ—(X) = 1I(Î·(X)>1/2) (called Bayes classiï¬er ) is actually the best possible classiï¬er in
the sense that
R(hâˆ—) = infR(h),
h(Â·)
where the inï¬mum is taken over all classiï¬ers, i.e. functions from Xto{0,1}. Note that
unlessÎ·(x)âˆˆ {0,1}for allxâˆˆ X(noiseless case), we have R(hâˆ—) = 0. However, we can
always look at the excess risk E(h) of a classiï¬er hdeï¬ned by
E(h) =R(h)âˆ’R(hâˆ—)â‰¥0.
In particular, we can hope to drive the excess risk to zero with enough observations by
mimicking hâˆ—accurately.
2.2 Empirical risk
The Bayes classiï¬er hâˆ—, while optimal, presents a major drawback: we cannot compute it
because we do not know the regression function Î·. Instead, we have access to the data
(X1,Y1),...,(Xn,Yn), which contains some (but not all) information about Î·and thus hâˆ—.
In order to mimic the properties of hâˆ—recall that it minimizes R(h) over all h. But the
function R(Â·) is unknown since it depends on the unknown distribution PX,Yof (X,Y). We
Ë† estimate it by the empirical classiï¬cation error, or simply empirical risk Rn(Â·) deï¬ned for
any classiï¬er hby
n1Ë†Rn(h) =/summationdisplay
1I(h(Xi) =Yi).ni=1
Ë† Ë† Since IE[1I( h(Xi) =Yi)] = IP(h(Xi) =Yi) =R(h), we have IE[ Rn(h)] =R(h) soRn(h) is
anunbiased estimator of R(h). Moreover, for any h, by the law of large numbers, we have
Ë† Ë† Rn(h)â†’R(h) asnâ†’ âˆ, almost surely. This indicates that if nis large enough, Rn(h)
should be close to R(h).
As a result, in order to mimic the performance of hâˆ—, let us use the empirical risk
Ë† Ë† minimizer (ERM) hdeï¬ned to minimize Rn(h) over all classiï¬ers h. This is an easy enough
Ë† Ë† task: deï¬ne hsuchh(Xi) =Yifor alli= 1,...,nandh(x) = 0 ifxâˆˆ/{X1,...,X n}. We
Ë†Ë† haveRn(h) = 0, which is clearly minimal. The problem with this classiï¬er is obvious: it
does not generalize outside the data. Rather, it predicts the label 0 for any xthat is not in
Ë†Ë† the data. We could have predicted 1 or any combination of 0 and 1 and still get Rn(h) = 0.
Ë† In particular it is unlikely that IE[ R(h)] will be small./ne}ationslash
/ne}ationslash
/ne}ationslash
/ne}ationslash /ne}ationslash
3
Important Remark :Recall that R(h) = IP(h(X)/ne}ationslash=Y).
Ë† Ë† Ë† Ifh(Â·) =h({(X1,Y1),...,(Xn,Yn)};Â·) is c
onstructed from the data, R(h) denotes
theconditional probability
Ë† Ë†R(h) = IP(h(X)/ne}ationslash=Y|(X1,Y1),...,(Xn,Yn)).
Ë† Ë† rathert
hanIP(h(X)/ne}ationslash=Y). As aresult R(h)is arandomva r iablesinceit dependsonthe
randomness of the data ( X1,Y1),...,(Xn,Yn). One way to view this is to observe that
Ë† we compute the deterministic function R(Â·) and then plug in the random classiï¬er h.
This problem is inherent to any method if we are not willing to make any assumption
on the distribution of ( X,Y) (again, so much for distribution freeness!). This can actually
be formalized in theorems, known as no-free-lunch theorems.
Theorem: Ë† For any integer nâ‰¥1, any classiï¬er hbuilt from ( X1,Y1),...,(Xn,Yn) and
anyÎµ >0, there exists a distribution PX,Yfor (X,Y) such that R(hâˆ—) = 0 and
Ë†IER(hn)â‰¥1/2âˆ’Îµ.
To be fair, note that here the distribution of the pair ( X,Y) is allowed to depend on
nwhich is cheating a bit but there are weaker versions of the no-free-lunch theorem that
essentially imply that it is impossible to learn without further assumptions. One such
theorem is the following.
Theorem: Ë† For any classiï¬er hbuilt from ( X1,Y1),...,(Xn,Yn) and any sequence
{an}n>0 that converges to 0, there exists a distribution PX,Yfor (X,Y) such that
R(hâˆ—) = 0 and
Ë†IER(hn)â‰¥an,for allnâ‰¥1
In the above theorem, the distribution of ( X,Y) is allowed to depend on the whole sequence
{an}n>0 but not on a speciï¬c n. The above result implies that the convergence to zero of
the classiï¬cation error may be arbitrarily slow.
2.3 Generative vs discriminative approaches
Both theorems above imply that we need to restrict the distribution PX,Yof (X,Y). But
isnâ€™t that exactly what statistical modeling is? The is answer is not so clear depending on
how we perform this restriction. There are essentially two schools: generative which is the
statistical modeling approach and discriminative which is the machine learning approach.
Generative: This approach consists in restricting the set of candidate distributions PX,Y.
This is what is done in discriminant analysis1where it is assumed that the condition dis-
1Amusingly, the generative approach is called discriminant analysis but donâ€™t let the terminology fool
you.
4
tributions of XgivenY(there are only two of them: one for Y= 0 and one for Y= 1) are
Gaussians on X= IRd(see for example [HTF09] for an overview of this approach).
Discriminative: This is the machine learning approach. Rather than making assumptions
directly on the distribution, one makes assumptions on what classiï¬ers are likely to perform
correctly. In turn, this allows to eliminate classiï¬ers such as the one described above and
that does not generalize well.
While it is important to understand both, we will focus on the discriminative approach
in this class. Speciï¬cally we are going to assume that we are given a class Hof classiï¬ers
such that R(h) is small for some hâˆˆ H.
2.4 Estimation vs. approximation
Assumethat we aregiven a class Hin which weexpect to ï¬nda classiï¬er that performswell.
Thisclassmaybeconstructedfromdomainknowledgeorsimplycomputational convenience.
Ë† We will see some examples in the class. For any candidate classiï¬er hnbuilt from the data,
we can decompose its excess risk as follows:
Ë† Ë† Ë† E(hn) =R(hn)âˆ’R(hâˆ—) =R(hn)âˆ’infR(h)+ infR(h)âˆ’R(hâˆ—).
hâˆˆH hâˆˆH/bracehtipupleft
estimat/bracehtipdownright
io/bracehtipdownleft
n error/bracehtipupright /bracehtipupleft
approxim/bracehtipdownright
a/bracehtipdownleft
tion error/bracehtipupright
On the one hand, estimation error accounts for the fact that we only have a ï¬nite
amount of observations and thus a partial knowledge of the distribution PX,Y. Hopefully
we can drive this error to zero as nâ†’ âˆ. But we already know from the no-free-lunch
theorem that this will not happen if His the set of all classiï¬ers. Therefore, we need to
takeHsmall enough. On the other hand, if His too small, it is unlikely that we will
ï¬nd classiï¬er with performance close to that of hâˆ—. A tradeoï¬€ between estimation and
approximation can be made by letting H=Hngrow (but not too fast) with n.
For now, assume that His ï¬xed. The goal of statistical learning theory is to understand
how the estimation error drops to zero as a function not only of nbut also of H. For the
ï¬rst argument, we will use concentration inequalities such as Hoeï¬€dingâ€™s and Bernsteinâ€™s
inequalities that allow us to control how close the empirical risk is to the classiï¬cation error
by bounding the random variable
/vextendsinglen1/vextendsingle/summationdisplay
1I(h(X (h /vextendsingle i) =Yi)âˆ’IP (X) =Y)/vextendsingle
n/vextendsingle
i=1/vextendsingle
with high probability. More generally we will be interested in results that allow to quantify
how close the average of independent and identically distributed (i.i.d) random variables is
to their common expected value.
Ë†Ë†Ë† Indeed, since by deï¬nition, we have Rn(h)â‰¤Rn(h) for allhâˆˆ H, the estimation error
Â¯ can be controlled as follows. Deï¬ne hâˆˆ Hto be any classiï¬er that minimizes R(Â·) overH
(assuming that such a classiï¬er exist).
Ë† Ë† Â¯ R(hn)âˆ’infR(h) =R(hn)âˆ’R(h)
hâˆˆH
Ë†Ë†Ë†Â¯ Ë† Ë† Â¯ Â¯ =Rn(hn)âˆ’Rn(h)+R(hn)âˆ’RË†n(h)+RË†n n(h)âˆ’R(h)/bracehtipupleft
â‰¤/bracehtipdownright/bracehtipdownleft
0/bracehtipupright
â‰¤/vextendsingle/vextendsingleË†Ë† Ë† Ë†Â¯ Â¯ Rn(hn)âˆ’R(hn)/vextendsingle/vextendsingle+/vextendsingle/vextendsingleRn(h)âˆ’R(h)/vextendsingle/vextendsingle./ne}ationslash /ne}ationslash
5
Â¯ Ë†Â¯ Â¯ Sincehis deterministic, we can use a concentration inequality to control/vextendsingle/vextendsingleRn(h)âˆ’R(h)/vextendsingle/vextendsingle.
However,
n1Ë†Ë† Ë† Rn(hn) =/summationdisplay
1I(hn(Xi) =Yi)ni=1
isnot Ë† the average of independent random variables since hndepends in a complicated
manner on all of the pairs ( Xi,Yi),i= 1,...,n. To overcome this limitation, we often use
Ë† a blunt, but surprisingly accurate tool: we â€œsup outâ€ hn,
/vextendsingle/vextendsingleË†Ë† Ë† Ë†Ë† Ë† Rn(hn)âˆ’R(hn)/vextendsingle/vextendsingleâ‰¤sup/vextendsingle
hâˆˆ/vextendsingleRn(hn)âˆ’R(hn)/vextendsingle
H/vextendsingle.
Controlling this supremum falls in the scope of suprema of empirical processes that we will
study in quite a bit of detail. Clearly the supremum is smaller as His smaller but Hshould
be kept large enough to have good approximation properties. This is the tradeoï¬€ between
approximation and estimation. It is also know in statistics as the bias-variance tradeoï¬€./ne}ationslash
6
18.657: Mathematics of Machine Learning
Lecturer: Philippe Rigollet Lecture 2
Scribe: Jonathan Weed Sep. 14, 2015
Part I
Statistical Learning Theory
1. BINARY CLASSIFICATION
In the last lecture, we looked broadly at the problems that machine learning seeks to solve
and the techniques we will cover in this course. Today, we will focus on one such problem,
binary classication , and review some important notions that will be foundational for the
rest of the course.
Our present focus on the problem of binary classication is justied because both binary
classication encompasses much of what we want to accomplish in practice and because the
response variables in the binary classication problem are bounded. (We will see a very
important application of this fact below.) It also happens that there are some nasty surprises
in non-binary classication, which we avoid by focusing on the binary case here.
1.1 Bayes Classier
Recall the setup of binary classication: we observe a sequence ( X1;Y1);:::; (Xn;Yn) ofn
independent draws from a joint distribution PX;Y. The variable Y(called the label) takes
values inf0;1g, and the variable Xtakes values in some space Xrepresenting \features" of
the problem. We can of course speak of the marginal distribution PXofXalone; moreover,
sinceYis supported onf0;1g, the conditional random variable YjXis distributed according
to a Bernoulli distribution. We write YjXBernoulli( (X)), where
(X) = I P(Y = 1jX ) = I E[YjX]:
(The function is called the regression function .)
We begin by dening an optimal classier called the Bayes classier. Intuitively, the
Bayes classier is the classier that \knows" |it is the classier we would use if we had
perfect access to the distribution YjX.
Denition: The Bayes classier ofXgivenY, denotedh, is the function dened by the
rule
(h1 if x)>1=2 (x) =0 if(x)1=2.
In other words, h(X) = 1 whenever I P(Y = 1jX )>I P(Y = 0jX ).
Our measure of performance for any classier h(that is, any function mapping Xto
f0;1g) will be the classication error :R(h) = I P(Y=h(X)). The Bayes risk is the value
R=R(h) of the classication error associated with the Bayes classier. The following
theorem establishes that the Bayes classier is optimal with respect to this metric.6
7
Theorem: For any classier h, the following identity holds:
R(h) R(h) =Z
j2(x) 1jPx(dx) = I E X[j2(X) 1j1(h(X ) =h(X))] (1.1)
h=h
Whereh=his the (measurable) set fx2Xjh(x) =h(x)g.
In particular, since the integrand is nonnegative, the classication error Rof the
Bayes classier is the minimizer of R(h) over all classiers h.
Moreover,
1R(h) = I E[min( (X);1 (X))]: (1.2)2
Proof. We begin by proving Equation (1.2). The denition of R(h) implies
R(h) = I P(Y=h(X)) = I P(Y = 1;h(X ) = 0) + I P( Y= 0;h(X ) = 1);
where the second equality follows since the two events are disjoint. By conditioning on X
and using the tower law, this last quantity is equal to
I E[I E[1(Y = 1;h(X ) = 0)jX ]] + I E[I E[1(Y = 0;h(X ) = 1)jX]]
Now,h(X) is measurable with respect to X, so we can factor it out to yield
I E[1(h(X ) = 0)(X) +1(h(X ) = 1)(1 (X))]]; (1.3)
where we have replaced I E[ YjX] by(X).
In particular, if h=h, then Equation 1.3 becomes
I E[1( (X)1=2) (X) +1((x)>1=2)(1 (X))]:
But(X)1=2 implies (X)1 (X) and conversely, so we nally obtain
R(h) = I E[1( (X)1=2) (X) +1((x)>1=2)(1 (X))]
= I E[(1( (X)1=2) + 1((x)>1=2)) min((X);1 (X))]
= I E[min( (X);1 (X))];
as claimed. Since min( (X);1 (X))1=2, its expectation is also certainly at most 1 =2
as well.
Now, given an arbitrary h, applying Equation 1.3 to both handhyields
R(h) R(h) = I E[ 1(h(X ) = 0) (X) +1(h(X ) = 1)(1 (X))]
 1(h(X) = 0) (X) +1(h(X) = 1)(1 (X))]];
which is equal to
I E[(1(h(X ) = 0) 1(h(X) = 0)) (X) + (1(h(X ) = 1) 1(h(X) = 1))(1 (X))]:
Sinceh(X) takes only the values 0 and 1, the second term can be rewritten as  (1(h(X ) =
0) 1(h(X) = 0)). Factoring yields
I E[(2 (X) 1)(1(h(X ) = 0) 1(h(X) = 0))]:66
6
6
8
The term 1(h(X ) = 0) 1(h(X) = 0) is equal to  1, 0, or 1 depending on whether h
andhagree. When h(X) =h(X), it is zero. When h(X) =h(X), it equals 1 whenever
h(X) = 0 and 1 otherwise. Applying the denition of the Bayes classier, we obtain
I E[(2 (X) 1)1(h(X ) =h(X)) sign( 1=2)] = I E[j2(X) 1j1(h(X ) =h(X))];
as desired.
We make several remarks. First, the quantity R(h) R(h) in the statement of the
theorem above is called the excess risk ofhand denotedE(h). (\Excess," that is, above
the Bayes classier.) The theorem implies that E(h)0.
Second, the risk of the Bayes classier Requals 1=2 if and only if (X) = 1=2 almost
surely. This maximal risk for the Bayes classier occurs precisely when Y\contains no
information" about the feature variable X. Equation (1.1) makes clear that the excess risk
weighs the discrepancy between handhaccording to how far is from 1=2. When is
close to 1=2, no classier can perform well and the excess risk is low. When is far from
1=2, the Bayes classier performs well and we penalize classiers that fail to do so more
heavily.
As noted last time, linear discriminant analysis attacks binary classication by putting
some model on the data. One way to achieve this is to impose some distributional assump-
tions on the conditional distributions XjY= 0 andXjY= 1.
We can reformulate the Bayes classier in these terms by applying Bayes' rule:
I P(X =xY= 1)I P(Y= 1)(x) = I P(Y = 1jjX=x) = :I P(X =xjY= 1)I P(Y= 1) + I P(X=xjY= 0)I P(Y= 0)
(In general, when PXis a continuous distribution, we should consider innitesimal proba-
bilities I P(X2dx).)
Assume that XjY= 0 andXjY= 1 have densities p0andp1, and I P(Y = 1) =is
some constant re
ecting the underlying tendency of the label Y. (Typically, we imagine
thatis close to 1=2, but that need not be the case: in many applications, such as anomaly
detection,Y= 1 is a rare event.) Then h(X) = 1 whenever (X)1=2, or, equivalently,
whenever
p1(x) 1 :p0(x)
When= 1=2, this rule amounts to reporting 1 or 0 by comparing the densities p1
andp0. For instance, in Figure 1, if = 1=2 then the Bayes classier reports 1 whenever
p1p0, i.e., to the right of the dotted line, and 0 otherwise.
On the other hand, when is far from 1=2, the Bayes classier is weighed towards the
underlying bias of the label variable Y.
1.2 Empirical Risk Minimization
The above considerations are all probabilistic , in the sense that they discuss properties of
some underlying probability distribution. The statistician does nothave access to the true
probability distribution PX;Y; she only has access to i.i.d. samples (X 1;Y1);:::; (Xn;Yn).
We consider now this statistical perspective. Note that the underlying distribution PX;Y
still appears explicitly in what follows, since that is how we measure our performance: we
judge the classiers we produced on future i.i.d. draws from PX;Y.6
6 6
9
Figure 1: The Bayes classier when = 1=2.
Given dataDn=f ^ (X1;Y1);:::; (Xn;Yn)g, we build a classier hn(X), which is random
in two senses: it is a function of a random variable Xand also depends implicitly on the
^ random dataDn. As above, we judge a classier according to the quantity E(hn). This is
a random variable: though we have integrated out X, the excess risk still depends on the
dataDn. We therefore will consider bounds both on its expected value and bounds that
^ hold in high probability. In any case, the bound E(hn)0 always holds. (This inequality
does not merely hold \almost surely," since we proved that R(h)R(h) uniformly over
all choices of classier h.)
Last time, we proposed two dierent philosophical approaches to this problem. In
particular, generative approaches make distributional assumptions about the data, attempt
to learn parameters of these distributions, and then plug the resulting values into the model.
The discriminative approach|the one taken in machine learning|will be described in great
detail over the course of this semester. However, there is some middle ground, which is worth
mentioning brie
y. This middle ground avoids making explicit distributional assumptions
aboutXwhile maintaining some of the 
avor of the generative model.
The central insight of this middle approach is the following: since by denition h(x) =
^ 1((X)>1=2), we estimate by some^nand thereby produce the estimator hn=
1(^n(X)>1=2). The result is called a plug-in estimator.
Of course, achieving good performance with a plug-in estimator requires some assump-
tions. (No-free-lunch theorems imply that we can't avoid making an assumption some-
where!) One possible assumption is that (X) is smooth; in that case, there are many
nonparamteric regression techniques available (Nadaraya-Watson kernel regression, wavelet
bases, etc.).
We could also assume that (X) is a function of a particular form. Since (X) is only
supported on [0; 1], standard linear models are generally inapplicable; rather, by applying
the logit transform we obtain logistic regression , which assumes that satises an identity
of the form
log(X)
1 (X)
=TX:
Plug-in estimators are called \semi-paramteric" since they avoid making any assumptions
about the distribution of X. These estimators are widely used because they perform fairly
well in practice and are very easy to compute. Nevertheless, they will not be our focus here.
In what follows, we focus here on the discriminative framework and empirical risk min-
imization. Our benchmark continues to be the risk function R(h) = I E1(Y =h(X)), which6
10
is clearly not computable based on the data alone; however, we can attempt to use a na ve
statistical \hammer" and replace the expectation with an average.
Denition: The empirical risk of a classier his given by
n1^Rn(h) =X
1(Yi=h(Xi)):ni=1
Minimizing the empirical risk over the family of all classiers is useless, since we can
always minimize the empirical risk by mimicking the data and classifying arbitrarily other-
wise. We therefore limit our attention to classiers in a certain family H.
^ Denition: The Empirical Risk Minimizer (ERM) overHis any element1hermof the set
^ argminhRn(h).2H
In order for our results to be meaningful, the class Hmust be much smaller than the
^ space of all classiers. On the other hand, we also hope that the risk of hermwill be close
to the Bayes risk, but that is unlikely if His too small. The next section will give us tools
for quantifying this tradeo.
1.3 Oracle Inequalities
An oracle is a mythical classier, one that is impossible to construct from data alone but
 whose performance we nevertheless hope to mimic. Specically, given Hwe denehto be
an element of argminhR(h)|a classier in2H Hthat minimizes the true risk. Of course,
 we cannot determine h, but we can hope to prove a bound of the form
^R(h) R(h) + something small: (1.4)
 Sincehis the best minimizer in Hgiven perfect knowledge of the distribution, a bound of
^ the form given in Equation 1.4 would imply that hhas performance that is almost best-in-
class. We can also apply such an inequality in the so-called improper learning framework,
^ where we allow hto lie in a slightly larger class H0
^H; in that case, we still get nontrivial
 guarantees on the performance of hif we know how to control R(h)
There is a natural tradeo between the two terms on the right-hand side of Equation 1.4.
WhenH  is small, we expect the performance of the oracle hto suer, but we may hope
 to approximate hquite closely. (Indeed, at the limit where His a single function, the
\something small" in Equation 1.4 is equal to zero.) On the other hand, as Hgrows the
oracle will become more powerful but approximating it becomes more statistically dicult.
(In other words, we need a larger sample size to achieve the same measure of performance.)
^ SinceR(h) is a random variable, we ultimately want to prove a bound in expectation
or tail bound of the form
^ I P(R(h) R(h) + n;(H))1 ;
where n;(H) is some explicit term depending on our sample size and our desired level of
condence.
1In fact, even an approximate solution will do: our bounds will still hold whenever we produce a classier
^ ^^ hsatisfying Rn(h)infhR2H n(h) + ".6
11
In the end, we should recall that
E^ ^  ^    (h) =R(h)R(h) = (R(h)R(h)) + (R (h) R(h)):
The second term in the above equation is the approximation error, which is unavoidable
once we x the class H. Oracle inequalities give a means of bounding the rst term, the
stochastic error.
1.4 Hoeding's Theorem
Our primary building block is the following important result, which allows us to understand
how closely the average of random variables matches their expectation.
Theorem (Hoeding's Theorem): LetX1;:::;Xnbenindependent random vari-
ables such that Xi2[0;1] almost surely.
Then for any t>0,
I P n1X
XiI EXini=1 
>!
2t2e 2nt:
In other words, deviations from
the mean deca
y exponentially fast in nandt.
Proof. Dene centered random variables Zi=Xi I EXi. It suces to show that
1X
 2nt2I PZi>t e ;n
since the lower tail bound follows analogously. (Exercise!)
We apply Cherno bounds. Since the exponential function is an order-preserving bijec-
tion, we have for any s>0
I P1X
Zstni>t
= I P
exp
sX
Zstn s Z ii ]n
>e
e I E[eP
(Markov)
=e stnI E[esZi]; (1.5)
where in the last equality we have used the independence of theY
Zi.
We therefore need to control the term I E[ esZi], known as the moment-generating func-
tion ofZi. If theZiwere normally distributed, we could compute the moment-generating
function analytically. The following lemma establishes that we can do something similar
when theZiare bounded.
Lemma (Hoeding's Lemma): IfZ2[a;b] almost surely and I EZ = 0, then
2 2
s(b a)I EesZe 8:
Proof of Lemma. Consider the log-moment generating function (s) = log I E[esZ], and note
that it suces to show that (s)s2(b a)2=8. We will investigate by computing the
12
rst several terms of its Taylor expansion. Standard regularity conditions imply that we
can interchange the order of dierentiation and integration to obtain
I E[ZesZ] 0(s) = ;I E[esZ]
2I E[Z2esZ]I E[esZ] I E[ZesZ]2esZesZ
 00(s) = = I E
Z2
 
I EZI E[esZ]2 I E[esZ]
I E[esZ]
:
SinceesZ
sZintegrates to 1, we can interpret 00(s) as the variance of Zunder the probabilityI E[e]
measuredF=esZ
sZdI E. We obtainI E[e]
 00(s) = var F(Z) = var Fa+bZ 2
;
since the variance is unaected under shifts. But jZ a+b
2jb aalmost surely since2
Z2[a;b] almost surely, so
varF+bZ 2
F"2a a+bZ 2#
(b a)2
:4
Finally, the fundamental theorem of calculus yields
s us2(b a)2
 (s) =Z
(u du
0Z
 00) :
08
This concludes the proof of the Lemma.
Applying Hoeding's Lemma to Equation (1.5), we obtain
I P1X2Z >t
e stnY
es2=8=ens =8stni ;n
for anys>0. Plugging in s= 4t> 0 yields
I P1X
Zi>t
e 2nt2;n
as desired.
Hoeding's Theorem implies that, for any classier h, the bound
log(2= )j^Rn(h) R(h)jr
2n
holds with probability 1  . We can immediately apply this formula to yield a maximal
inequality: ifHis a nite family, i.e., H=fh1;:::;hMg, then with probability 1  =M
the bound
logj^Rn(hj) R(hj)jr
(2M=)
2n
13
^ holds. The event that max jjRn(hj) R(hj)j ^ >tis the union of the events jRn(hj) R(hj)j>
tforj= 1;:::;M , so the union bound immediately implies that
log(2M= )maxj^Rn(hj) R(hj)
jjr
2n
with probability 1  . In other words, for such a family, we can be assured that the empirical
risk and the true risk are close. Moreover, the logarithmic dependence on Mimplies that
we can increase the size of the family Hexponentially quickly with nand maintain the
same guarantees on our estimate.
14
18.657: Mathematics of Machine Learning
Lecturer: Philippe Rigollet Lecture 3
Scribe:James Hirst Sep. 16, 2015
1.5 Learning with a ï¬nite dictionary
Recall from the end of last lecture our setup: We are working with a ï¬nite dictionary
H={h1,...,h M}of estimators, andwewouldliketounderstandthescaling ofthis problem
with respect to Mand the sample size n. GivenH, one idea is to simply try to minimize
Ë† the empirical risk based on the samples, and so we deï¬nethe empirical risk minimizer, herm,
by
Ë† Ë† hermâˆˆargminRn(h).
hâˆˆH
Ë† Ë† In what follows, we will simply write hinstead of hermwhen possible. Also recall the
Â¯ deï¬nition of the oracle, h, which (somehow) minimizes the true risk and is deï¬ned by
Â¯hâˆˆargminR(h).
hâˆˆH
Ë† Â¯ The following theorem shows that, although hcannot hope to do better than hin
general, the diï¬€erence should not be too large as long as the sample size is not too small
compared to M.
Ë† Theorem: The estimator hsatisï¬es
Ë† Â¯R(h)â‰¤R(h)+/radicalbigg
2log(2M/Î´)
n
with probability at least 1 âˆ’Î´. In expectation, it holds that
/radicalbigg
2log(2M)Ë† Â¯IE[R(h)]â‰¤R(h)+ .n
Proof. Ë† Ë†Ë†Ë†Â¯ From the deï¬nition of h, we have Rn(h)â‰¤Rn(h), which gives
Ë† Â¯ Ë†Â¯ Â¯ Ë† Ë†Ë† R(h)â‰¤R(h)+[Rn(h)âˆ’R(h)]+[R(h)âˆ’Rn(h)].
The only term here that we need to control is the second one, but since we donâ€™t have
Â¯ any real information about h, we will bound it by a maximum over Hand then apply
Hoeï¬€ding:
log(2M/Î´)Ë†Â¯ Â¯ Ë† Ë†Ë† Ë† [Rn(h)âˆ’R(h)]+[R(h)âˆ’Rn(h)]â‰¤2max|Rn(hj)âˆ’R(hj)| â‰¤2
j/radicalbigg
2n
with probability at least 1 âˆ’Î´, which completes the ï¬rst part of the proof.
15
To obtain the bound in expectation, we start with a standard trick from probability
which bounds a max by its sum in a slightly more clever way. Here, let {Zj}jbe centered
random variables, then
/bracketleftbigg /bracketrightbigg1/parenleftbigg /bracketleftbigg1IE max|Zj|= logexp sIE max|Zj|/bracketrightbigg/parenrightbigg
â‰¤logIE/bracketleftbigg
exp/parenleftbigg
smax|Zj|
j s j s j/parenrightbigg/bracketrightbigg
,
where the last inequality comes from applying Jensenâ€™s inequality to the convex function
exp(Â·). Now we bound the max by a sum to get
2M1 )â‰¤log/summationdisplay 1 s2log(2M sIE[exp(sZj)]â‰¤log/parenleftbigg
2Mexp/parenleftbigg /parenrightbigg/parenrightbigg
= + ,s s 8n s 8nj=1
Ë† where we used Zj=Rn(hj)âˆ’R(hj) in our case and then applied Hoeï¬€dingâ€™s Lemma. Bal-
ancing terms by minimizing over s, this gives s= 2/radicalbig
2nlog(2M) and plugging in produces
/bracketleftbigglog(2M)Ë† IE max|Rn(hj)âˆ’R(hj)| â‰¤
j/bracketrightbigg/radicalbigg
,2n
which ï¬nishes the proof.
2. CONCENTRATION INEQUALITIES
Concentration inequalities are results that allow us to bound the deviations of a function of
randomvariablesfromitsaverage. Theï¬rstofthesewewillconsiderisadirect improvement
to Hoeï¬€dingâ€™s Inequality that allows some dependence between the random variables.
2.1 Azuma-Hoeï¬€ding Inequality
Given a ï¬ltration {Fi}iof our underlying space X, recall that {âˆ†i}iare called martingale
diï¬€erences if, for every i, it holds that âˆ† iâˆˆ Fiand IE[âˆ† i|Fi] = 0. The following theorem
gives a very useful concentration bound for averages of bounded martingale diï¬€erences.
Theorem (Azuma-Hoeï¬€ding): Suppose that {âˆ†i}iare margingale diï¬€erences with
respect to the ï¬ltration {Fi}i, and let Ai,Biâˆˆ Fiâˆ’1satisfyAiâ‰¤âˆ†iâ‰¤Bialmost surely
for every i. Then
IP/bracketleftBigg
1/summationdisplay 2nâˆ†i> t/bracketrightBigg
2t2
â‰¤expni/parenleftbigg
âˆ’/summationtextn
i=1/bardblBiâˆ’Ai/bardbl2âˆ/parenrightbigg
.
In comparison to Hoeï¬€dingâ€™s inequality, Azuma-Hoeï¬€ding aï¬€ords not only the use of
non-uniform boundedness, but additionally requires no independence of the random vari-
ables.
Proof.We start with a typical Chernoï¬€ bound.
IP/bracketleftBigg /bracketrightBigg/summationdisplay
âˆ†i> tâ‰¤IE/bracketleftBig
es/summationtextâˆ†i/bracketrightBig
eâˆ’st= IE
i/bracketleftBig
IE/bracketleftBig
es/summationtextâˆ†i|Fnâˆ’1/bracketrightBig/bracketrightBig
eâˆ’st
16
nâˆ’1 nâˆ’1 2 2= IE/bracketleftBig
es/summationtextâˆ†iIE[esâˆ†n|Fn1]eâˆ’stâˆ’ â‰¤IE[es/summationtextâˆ†iÂ·es(Bnâˆ’An)/8]eâˆ’st,
where we have used the fact that the âˆ†/bracketrightBig
i,i < n, are allFnmeasureable, and then applied
Hoeï¬€dingâ€™s lemma on the inner expectation. Iteratively isolating each âˆ† ilike this and
applying Hoeï¬€dingâ€™s lemma, we get
IP/bracketleftBiggn/summationdisplay s2
âˆ†> t/bracketrightBigg
â‰¤exp/parenleftBigg/summationdisplay
/bardblBâˆ’A/bardbl2/parenrightBigg
eâˆ’sti i i8âˆ.
i i=1
Optimizing over sas usual then gives the result.
2.2 Bounded Diï¬€erences Inequality
Although Azuma-Hoeï¬€ding is a powerful result, its full generality is often wasted and can
be cumbersome to apply to a given problem. Fortunately, there is a natural choice of the
{Fi}iand{âˆ†i}i, giving a similarly strong result which can be much easier to apply. Before
we get to this, we need one deï¬nition.
Deï¬nition (Bounded Diï¬€erences Condition): Letg:X â†’IR and constants cibe
given. Then gis said to satisfy the bounded diï¬€erences condition (with constants ci) if
sup|g(x ,...,x )âˆ’g(x ,...,xâ€²1 n 1 i,...,x n)| â‰¤ci
xâ€²1,...,xn,xi
for every i.
Intuitively, gsatisï¬es the bounded diï¬€erences condition if changing only one coordinate
ofgat a time cannot make the value of gdeviate too far. It should not be too surprising
that these types of functions thus concentrate somewhat strongly around their average, and
this intuition is made precise by the following theorem.
Theorem (Bounded Diï¬€erences Inequality): Ifg:X â†’IR satisï¬es the bounded
diï¬€erences condition, then
2t2
IP[|g(X1,...,X n)âˆ’IE[g(X1,...,X n)|> t]â‰¤2exp/parenleftbigg
âˆ’/summationtext
ic2
i/parenrightbigg
.
Proof.Let{Fi}ibe given by Fi=Ïƒ(X1,...,X i), and deï¬ne the martingale diï¬€erences
{âˆ†i}iby
âˆ†i= IE[g(X1,...,X n)|Fi]âˆ’IE[g(X1,...,X n)|Fiâˆ’1].
Then
IP/bracketleftBigg
|/summationdisplay
âˆ†i|> t/bracketrightBigg
= IP/vextendsingle
g(X1,...,X n)âˆ’IE[g(X1,...,X n)
i/vextendsingle
> t ,
exactly the quantity we want to bou/bracketleftbig/vextendsingle
nd. Now, note that/vextendsingle/bracketrightbig
âˆ†iâ‰¤IE/bracketleftbigg
supg(X1,...,x i,...,X n)|Fiâˆ’IE[g(X1,...,X n)|Fiâˆ’1]
xi/bracketrightbigg
17
= IE/bracketleftbigg
supg(X1,...,x i,...,X n)âˆ’g(X1,...,X n)|Fiâˆ’1
xi/bracketrightbigg
=:Bi.
Similarly,
âˆ†iâ‰¥IE/bracketleftbigg
infg(X1,...,x i,...,X n)âˆ’g(X1,...,X n)|Fiâˆ’1=:Ai.
xi/bracketrightbigg
At this point, our assumption on gimplies that /bardblBiâˆ’Ai/bardblâˆâ‰¤cifor every i, and since
Aiâ‰¤âˆ†iâ‰¤BiwithAi,Biâˆˆ Fiâˆ’1, an application of Azuma-Hoeï¬€ding gives the result.
2.3 Bernsteinâ€™s Inequality
Hoeï¬€dingâ€™s inequality is certainly a powerful concentration inequality for how little it as-
sumes about the random variables. However, one of the major limitations of Hoeï¬€ding is
just this: Since it only assumes boundedness of the random variables, it is completely obliv-
ious to their actual variances. When the random variables in question have some known
variance, an ideal concentration inequality should capture the idea that variance controls
concentration to some degree. Bernsteinâ€™s inequality does exactly this.
Theorem (Bernsteinâ€™s Inequality): LetX1,...,X nbe independent, centered ran-
dom variables with |X| â‰¤cfor every i, and write Ïƒ2=nâˆ’1i iVar(Xi) for the average
variance. Then/summationtext
IP/bracketleftBigg
1/summationdisplay nt2
Xi> t/bracketrightBigg
â‰¤exp/parenleftBigg
âˆ’n 2Ïƒ2+2tci 3/parenrightBigg
.
Here, one should think of tas being ï¬xed and relatively small compared to n, so that
strength of the inequality indeed depends mostly on nand 1/Ïƒ2.
Proof.The idea of the proof is to do a Chernoï¬€ bound as usual, but to ï¬rst use our
assumptions on the variance to obtain a slightly better bound on the moment generating
functions. To this end, we expand
âˆ(sk âˆX) skckâˆ’2iIE[esXi] = 1+IE[ sXi]+IE/bracketleftBigg /bracketrightBigg/summationdisplay
â‰¤1+Var(Xi)/summationdisplay
,k! k!k=2 k=2
where we have used IE[ Xk
i]â‰¤IE[X2
i|Xi|kâˆ’2]â‰¤Var(Xkâˆ’2i)c. Rewriting the sum as an
exponential, we get
esc
sXi2 âˆ’scâˆ’1IE[e]â‰¤sVar(Xi)g(s), g(s) := .c2s2
The Chernoï¬€ bound now gives
IP/bracketleftBigg
1/summationdisplay
Xi> t/bracketrightBigg
â‰¤exp/parenleftBigg
inf[s2(/summationdisplay
Var(Xi))g(s)âˆ’nst]/parenrightBigg
= exp/parenleftbigg
nÂ·inf[s2Ïƒ2g(s)âˆ’st],n s>0 s>0i i/parenrightbigg
and optimizing this over s(a fun calculus exercise) gives exactly the desired result.
18
3. NOISE CONDITIONS AND FAST RATES
Ë† To measure the eï¬€ectiveness of the estimator h, we would like to obtain an upper bound
Ë† Ë† on the excess risk E(h) =R(h)âˆ’R(hâˆ—). It should be clear, however, that this must depend
signiï¬cantly on the amount of noise that we allow. In particular, if Î·(X) is identically equal
Ë† to 1/2, then we should not expect to be able to say anything meaningful about E(h) in
general. Understanding this trade-oï¬€ between noise and rates will be the main subject of
this chapter.
3.1 The Noiseless Case
A natural (albeit somewhat naÂ¨ Ä±ve) case to examine is the completely noiseless case. Here,
we will have Î·(X)âˆˆ {0,1}everywhere, Var( Y|X) = 0, and
E(h) =R(h)âˆ’R(hâˆ—) = IE[|2Î·(X)âˆ’1|1I(h(X) =hâˆ—(X))] = IP[h(X) =hâˆ—(X)].
Let us now denote
Â¯ Ë† Zi= 1I(h(Xi) =Yi)âˆ’1I(h(Xi) =Yi),
Â¯ and write Zi=Ziâˆ’IE[Zi]. Then notice that we have
Ë† Â¯ |Zi|= 1I(h(Xi) =h(Xi)),
and
Var(Zi)â‰¤IE[Z2Ë† Â¯i] = IP[h(Xi) =h(Xi)].
Ë† For any classiï¬er hjâˆˆ H, we can similarly deï¬ne Zi(hj) (by replacing hwithhjthrough-
out). Then, to set up an application of Bernsteinâ€™s inequality, we can compute
n1/summationdisplayÂ¯ Var(Zi(hj))â‰¤IP[hj(Xi) =h(Xi)] =:Ïƒ2
nj.
i=1
At this point, we will make a (fairly strong) assumption about our dictionary H, which
Â¯ is thathâˆ—âˆˆ H, which further implies that h=hâˆ—. Since the random variables Zicompare
Â¯ Ë† toh, this will allow us to use them to bound E(h), which rather compares to hâˆ—. Now,
Â¯ applying Bernstein (with c= 2) to the {Zi(hj)}ifor every jgives
/bracketleftBiggn1/bracketrightBigg/summationdisplay nt2Î´Â¯ IP Zi(hj)> tâ‰¤exp/parenleftBigg
âˆ’ =2Ïƒ2
i=1 j+4t3/parenrightBigg
:,n M
and a simple computation here shows that it is enough to take
ï£«/radicalBigg
2Ïƒ2
jlog(M/Î´)4tâ‰¥maxï£­ ,log(M/Î´)n 3nï£¶
ï£¸=:t0(j)
Â¯ for this to hold. From here, we may use the assumption h=hâˆ—to conclude that
Ë† Ë† IP/bracketleftBig
E(h)> t0(Ë†j)/bracketrightBig
â‰¤Î´, hË†=h.j/ne}ationslash /ne}ationslash
/ne}ationslash /ne}ationslash
/ne}ationslash
/ne}ationslash
/ne}ationslash
19
Ë† However, we also know that Ïƒ2
Ë†â‰¤ E(h), which implies thatj
ï£«/radicalBigg
Ë†2E(h)log(M/Î´) 4Ë†E(h)â‰¤maxï£­ ,log(M/Î´)n 3nï£¶
ï£¸
Ë† with probability 1 âˆ’Î´, and solving for E(h) gives the improved rate
log(M/Î´)Ë†E(h)â‰¤2 .n
20
18.657: Mathematics of Machine Learning
Lecturer: Philippe Rigollet Lecture 4
Scribe:Cheng Mao Sep. 21, 2015
In this lecture, we continue to discuss the eï¬€ect of noise on the rate of the excess risk
Ë† Ë† Ë† E(h) =R(h)âˆ’R(hâˆ—) wherehis the empirical risk minimizer. In the binary classiï¬cation
model, noise roughly means how close the regression function Î·is from1
2. In particular, if
Î·=1then we observe only noise, and if Î·âˆˆ {0,1}we are in the noiseless case which has2
been studied last time. Especially, we achieved the fast ratelogMin the noiseless case bynÂ¯ assuming hâˆ—âˆˆ Hwhich implies that h=hâˆ—. This assumption was essential for the proof
and we will see why it is necessary again in the following section.
3.2 Noise conditions
The noiseless assumption is rather unrealistic, so it is natural to ask what the rate of excess
risk is when the noise is present but can be controlled. Instead of the condition Î·âˆˆ {0,1},
we can control the noise by assuming that Î·is uniformly bounded away from1
2, which is
the motivation of the following deï¬nition.
Deï¬nition (Massartâ€™s noise condition): The noise in binary classiï¬cation is said
to satisfy Massartâ€™s condition with constant Î³âˆˆ(0,1
2] if|Î·(X)âˆ’1| â‰¥Î³almost surely.2
Once uniform boundedness is assumed, the fast rate simply follows from last proof with
appropriate modiï¬cation of constants.
Ë† Ë† Ë† Theorem: LetcE(h) denote the excess risk of the empirical risk minimizer h=herm.
If Massartâ€™s noise condition is satisï¬ed with constant Î³, then
log(M/Î´)Ë†E(h)â‰¤Î³n
with probability at least 1 âˆ’Î´. (In particular Î³=1gives exactly the noiseless case.)2
Proof. Â¯ Â¯ Deï¬neZi(h) = 1I(h(Xi) =Yi)âˆ’1I(h(Xi) =Yi). By the assumption h=hâˆ—and the
Ë† Ë† deï¬nition of h=herm,
Ë† Ë† Â¯E(h) =R(h)âˆ’R(h)
Ë†Ë†Ë†Â¯Ë†Â¯Ë†Ë† Â¯ Ë† =Rn(h)âˆ’Rn(h)+Rn(h)âˆ’Rn(h)âˆ’R(h)âˆ’R(h) (3.1)
n1Ë† â‰¤ )/parenleftbig /parenrightbig
/summationdisplay/parenleftbigË† Zi(h)âˆ’IE[Zi(h]/parenrightbig
. (3.2)ni=1
Hence it suï¬ƒces to bound the deviation of/summationtext
iZifrom its expectation. To this end, we
hope to apply Bernsteinâ€™s inequality. Since
Var[Zi(h)]â‰¤IE[Z2 Â¯i(h) ] = IP[h(Xi) =h(Xi)],/ne}ationslash /ne}ationslash
/ne}ationslash
21
we have that for any 1 â‰¤jâ‰¤M,
n1/summationdisplayÂ¯ Var[Zi(hj)]â‰¤IP[hj(X) =h(X)] =:Ïƒ2
nj.
i=1
Bernsteinâ€™s inequality implies that
n/bracketleftbig1/summationdisplay /bracketrightbig /parenleftbig nt2
IP ( Zi(hj)âˆ’IE[Zi(hj)])> tâ‰¤expâˆ’n 2Ïƒ2
i=1 j+2
3t/parenrightbigÎ´=:.M
Applying a union bound over 1 â‰¤jâ‰¤Mand taking
2Ïƒ2
jlog(M/Î´)2log(M/Î´)t=t0(j) := max/radicalBigg
/parenleftbig
,n 3n/parenrightbig
,
we get that
n1/summationdisplay
(Zi(hj)âˆ’IE[Zi(hj)])â‰¤t0(j) (3.3)ni=1
for all 1â‰¤jâ‰¤Mwith probability at least 1 âˆ’Î´.
Ë† Suppose h=hË†. It follows from (3.2) and (3.3) that with probability at least 1 âˆ’Î´,j
Ë†E(h)â‰¤tË†0(j).
(Note that so far the proof is exactly the same as the noiseless case.) Since |Î·(X)âˆ’1
2| â‰¥Î³
Â¯ a.s. and h=hâˆ—,
Ë† Ë† Â¯ E(h) = IE[|2Î·(X)âˆ’1|1I(h(X) =hâˆ—(X))]â‰¥2Î³IP[hË†(X) =h(X)] = 2Î³Ïƒ2
Ë†.j j
Therefore,/radicalBigg
Ë†E(h)log(M/Î´) 2log(M/Î´)Ë†E(h)â‰¤max , , (3.4)Î³n 3n
so we conclude that with probabilit/parenleftbig
y at least 1 âˆ’Î´,/parenrightbig
log(M/Î´)Ë†E(h)â‰¤ .Î³n
Â¯ The assumption that h=hâˆ—was used twice in the proof. First it enables us to ignore
the approximation error and only study the stochastic error. More importantly, it makes
the excess risk appear on the right-hand side of (3.4) so that we can rearrange the excess
risk to get the fast rate.
Massartâ€™s noise condition is still somewhat strong because it assumes uniform bounded-
ness ofÎ·from1
2. Instead, we can allow Î·to be close to1
2but only with small probability,
and this is the content of next deï¬nition./ne}ationslash
/ne}ationslash /ne}ationslash
22
Deï¬nition (Tsybakovâ€™s noise condition or Mammen-Tsybakov noise condi-
tion):The noise in binary classiï¬cation is said to satisfy Tsybakovâ€™s condition if there
existsÎ±âˆˆ(0,1),C10>0 andt0âˆˆ(0,2] such that
1 Î±IP[|Î·(X)âˆ’ | â‰¤t]â‰¤C10tâˆ’Î±
2
for alltâˆˆ[0,t0].
Î±In particular, as Î±â†’1,t1âˆ’Î±â†’0
Î±, so this recovers Massartâ€™s condition with Î³=t0and
we have the fast rate. As Î±â†’0,t1âˆ’Î±â†’1, so the condition is void and we have the slow
rate. In between, it is natural to expect fast rate (meaning faster than slow rate) whose
order depends on Î±. We will see that this is indeed the case.
Lemma: Under Tsybakovâ€™s noise condition with constants Î±,C0andt0, we have
IP[h(X) =hâˆ—(X)]â‰¤CE(h)Î±
for any classiï¬er hwhereC=C(Î±,C0,t0) is a constant.
Proof.We have
E(h) = IE[|2Î·(X)âˆ’1|1I(h(X) =hâˆ—(X))]
1â‰¥IE[|2Î·(X)âˆ’1|1I(|Î·(X)âˆ’ |> t)1I(h(X) =hâˆ—(X))]2
1â‰¥2tIP[|Î·(X)âˆ’ |> t,h(X) =hâˆ—(X)]2
1â‰¥2tIP[h(X) =hâˆ—(X)]âˆ’2tIP[|Î·(X)âˆ’ | â‰¤t]2
1â‰¥2tIP[h(X) =hâˆ—(X)]âˆ’2C0t1âˆ’Î±
1where Tsybakovâ€™s condition was used in the last step. Take t=cIP[h(X) =hâˆ—(X)]âˆ’Î±
Î±for
some positive c=c(Î±,C0,t0) to be chosen later. We assume that câ‰¤t0to guarantee that
tâˆˆ[0,t0]. SinceÎ±âˆˆ(0,1),
E(h)â‰¥2cIP[h(X) =hâˆ—(X)]1/Î±1âˆ’2C c1âˆ’Î±IP[h(X) =hâˆ—10 (X)]/Î±
â‰¥cIP[h(X) =hâˆ—(X)]1/Î±
by selecting csuï¬ƒciently small depending on Î±andC0. Therefore
1IP[h(X) =hâˆ—(X)]â‰¤ E(h)Î±
cÎ±
and choosing C=C(Î±,C0,t0) :=câˆ’Î±completes the proof.
Having established the key lemma, we are ready to prove the promised fast rate under
Tsybakovâ€™s noise condition./ne}ationslash
/ne}ationslash
/ne}ationslash
/ne}ationslash
/ne}ationslash
/ne}ationslash
/ne}ationslash
/ne}ationslash /ne}ationslash
/ne}ationslash
/ne}ationslash
23
Theorem: If Tsybakovâ€™s noise condition is satisï¬ed with constant Î±,C0andt0, then
there exists a constant C=C(Î±,C0,t0) such that
l )Ë†Eh)â‰¤C/parenleftbigog(M/Î´(1
n/parenrightbig
2âˆ’Î±
with probability at least 1 âˆ’Î´.
This rate of excess risk parametrized by Î±is indeed an interpolation of the slow ( Î±â†’0)
Ë† and the fast rate ( Î±â†’1). Futhermore, note that the empirical risk minimizer hdoes not
depend on the parameter Î±at all! It automatically adjusts to the noise level, which is a
very nice feature of the empirical risk minimizer.
Proof.The majority of last proof remains valid and we will explain the diï¬€erence. After
establishing that
Ë†E(h)â‰¤t0(Ë†j),
we note that the lemma gives
Ë†Ïƒ2 Â¯ Ë†Ë†= IP[h(X)/ne}ationslash=h(X)]â‰¤CE(h)Î±.j
It follows
that /radicalBigg
Ë† /parenleftbig2CE(h)Î±log(M/Î´) 2log(M/Î´)Ë†E(h)â‰¤max ,n 3n/parenrightbig
/parenleftBig2ClogM2Ë†1
E(h)â‰¤max/parenleftbigÎ´/parenrightbig
2âˆ’Î±log(M/Î´),/parenrightBig
.n 3nand thus
4. VAPNIK-CHERVONENKIS (VC) THEORY
The upper bounds proved so far are meaningful only for a ï¬nite dictionary H, because if
M=|H|is inï¬nite all of the bounds we have will simply be inï¬nity. To extend previous
results to the inï¬nite case, we essentially need the condition that only a ï¬nite number of
elements in an inï¬nite dictionary Hreally matter. This is the objective of the Vapnik-
Chervonenkis (VC) theory which was developed in 1971.
4.1 Empirical measure
Recall from previous proofs (see (3.1) for example) that the key quantity we need to control
is
Ë†2sup/parenleftbig
Rn(h)âˆ’R(h).
hâˆˆH
Instead of the union bound which would not work in th/parenrightbig
e inï¬nite case, we seek some bound
that potentially depends on nand the complexity of the set H. One approach is to consider
some metric structure on Hand hope that if two elements in Hare close, then the quantity
evaluated at these two elements are also close. On the other hand, the VC theory is more
combinatorial and does not involve any metric space structure as we will see.
24
By deï¬nition
n1Ë†Rn(h)âˆ’R(h) =/summationdisplay/parenleftbig
1I(h(Xi) =Yi)âˆ’IE[1I(h(Xi) =Yi)]ni=1/parenrightbig
.
LetZ= (X,Y) andZi= (Xi,Yi), and let Adenote the class of measurable sets in the
sample space X Ã—{0,1}. For a classiï¬er h, deï¬neAhâˆˆ Aby
{ZiâˆˆAh}={h(Xi) =Yi}.
Moreover, deï¬ne measures ÂµnandÂµonAby
n1Âµn(A) =/summationdisplay
1I(ZiâˆˆA) and Âµ(A) = IP[ZiâˆˆA]ni=1
forAâˆˆ A. With this notation, the slow rate we proved is just
log(2|A|/Î´)Ë†supRn(h)âˆ’R(h) = sup|Âµn(A)âˆ’Âµ(A)| â‰¤
hâˆˆH AâˆˆA/radicalbigg
.2n
Since this is not accessible in the inï¬nite case, we hope to use one of the concentration
inequalities to give an upperbound. Note that Âµn(A) is a sum of random variables that may
not be independent, so the only tool we can use now is the bounded diï¬€erence inequality.
If we change the value of only one ziin the function
z1,...,zn/mapstoâ†’sup|Âµn(A)âˆ’Âµ(A)|,
AâˆˆA
the value of the function will diï¬€er by at most 1 /n. Hence it satisï¬es the boundeddiï¬€erence
assumption with ci= 1/nfor all 1â‰¤iâ‰¤n. Applying the bounded diï¬€erence inequality, we
get that
/vextendsinglelog(2/Î´) /vextendsinglesup|Âµn(A)âˆ’Âµ(A)|âˆ’IE[sup|Âµn(A)âˆ’Âµ(A)|]â‰¤
AâˆˆA AâˆˆA/radicalbigg
2n
with probability/vextendsingle
at least 1 âˆ’Î´. Note that this already preclu/vextendsingle/vextendsingle
/vextendsingle
des any fast rate (faster than
nâˆ’1/2). Toachieve fast rate, weneedTalagrand inequality andlocalization techniques which
are beyond the scope of this section.
It follows that with probability at least 1 âˆ’Î´,
log(2/Î´)sup|Âµn(A)âˆ’Âµ(A)| â‰¤IE[sup|Âµn(A)âˆ’Âµ(A)|]+
A AâˆˆA/radicalbigg
.
Aâˆˆ 2n
We will now focus on bounding the ï¬rst term on the right-hand side. To this end, we need
a technique called symmetrization, which is the subject of the next section.
4.2 Symmetrization and Rademacher complexity
Symmetrization is a frequently used technique in machine learning. Let D={Z1,...,Z n}
be the sample set. To employ symmetrization, we take another independent copy of the
sample set Dâ€²={Zâ€²
1,...,Zâ€²
n}. This sample only exists for the proof, so it is sometimes
referred to as a ghost sample. Then we have
n n1 1Âµ(A) = IP[ZâˆˆA] = IE[/summationdisplay
1I(Zâ€²
iâˆˆA)] = IE[ 1I( Zâ€²
iâˆˆA)|D] = IE[Âµâ€²
n nn(A)|D]
i=1/summationdisplay
i=1/ne}ationslash /ne}ationslash
/ne}ationslash
25
nwhereÂµâ€²
n:=1/summationtext
i=11I(Zâ€²
iâˆˆA). Thus by Jensenâ€™s inequality,n
IE[sup|Âµn(A)âˆ’Âµ(A)|] = IE/bracketleftbig
sup/vextendsingle/vextendsingle
Âµn(A)âˆ’IE[Âµâ€²
n(A)|D]
AâˆˆA AâˆˆA
â‰¤IE sup IE[ |Âµn(A)âˆ’Âµâ€²
n(A)||D/vextendsingle/vextendsingle
]/bracketrightbig
â‰¤/bracketleftbig
AâˆˆA
IE/bracketleftbig
sup|Âµâ€²n(A)âˆ’Âµn(A)|/bracketrightbig
AâˆˆA
n1/bracketrightbig
= IE/bracketleftbig
sup/vextendsingle/summationdisplay/parenleftbig
1I(Zâ€²iâˆˆA)âˆ’1I(Z
AâˆˆAniâˆˆA)
i=1/parenrightbig/vextendsingle/bracketrightbig
.
SinceDâ€²has the same distribution of D, by sy/vextendsingle
mmetry 1I( ZiâˆˆA)âˆ’1I(Zâ€²/vextendsingle
iâˆˆA) has the same
distribution as Ïƒi/parenleftbig
1I(ZiâˆˆA)âˆ’1I(Zâ€²
iâˆˆA)/parenrightbig
whereÏƒ1,...,Ïƒ nare i.i.d. Rad(1
2), i.e.
1IP[Ïƒi= 1] = IP[ Ïƒi=âˆ’1] =,2
andÏƒiâ€™s are taken to be independent of both samples. Therefore,
n
IE[sup|Âµn(A)âˆ’Âµ(A)|]â‰¤IE
AA/bracketleftbig
supâ€²
âˆˆ AâˆˆA/vextendsingle/vextendsingle1/summationdisplay
Ïƒi/parenleftbig
1I(ZiâˆˆA)âˆ’1I(ZniâˆˆA)
i=1
n/parenrightbig/vextendsingle/bracketrightbig
â‰¤2IE/bracketleftbig1/vextendsingle
sup/vextendsingle/summationdisplay
Ïƒi1I(ZiâˆˆA).
AâˆˆAni=1/vextendsingle/bracketrightbig
(4.5)
Usingsymmetrization we have boundedIE[sup/vextendsingle
AâˆˆA|Âµn(A)âˆ’Âµ(A)|/vextendsingle
] by amuch nicer quantity.
Yet we still need an upper bound of the last quantity that depends only on the structure
ofAbut not on the random sample {Zi}. This is achieved by taking the supremum over
allziâˆˆ X Ã—{0,1}=:Y.
Deï¬nition: The Rademacher complexity of a family of sets Ain a space Yis deï¬ned
to be the quantity
n
Rn(A) = sup sup/vextendsingle1IE/bracketleftbig /summationdisplay
Ïƒi1I(ziâˆˆA)
z1,...,znâˆˆYAâˆˆAni=1/vextendsingle/bracketrightbig
.
The Rademacher complexity of a set BâŠ‚I/vextendsingle
Rnis deï¬ned to b/vextendsingle
e
n1Rn(B) = IE/bracketleftbig
sup
bâˆˆB/vextendsingle/vextendsingle
n/summationdisplay
Ïƒibi
i=1/vextendsingle/vextendsingle/bracketrightbig
.
We conclude from (4.5) and the deï¬nition that
IE[sup|Âµn(A)âˆ’Âµ(A)|]â‰¤2Rn(A).
AâˆˆA
nIn the deï¬nition of Rademacher complexity of a set, the quantity1sni=1Ïƒibimeasure
how well a vector bâˆˆBcorrelates with a random sign pattern {Ïƒi}. The more complex
Bis, the better some vector in Bcan replicate a sign pattern. In/vextendsingle/vextendsingle
pa/summationtext
rticular, i/vextendsingle/vextendsingle
fBis the
full hypercube [ âˆ’1,1]n, thenRn(B) = 1. However, if BâŠ‚[âˆ’1,1]ncontains only k-sparse
26
vectors, then Rn(B) =k/n. Hence Rn(B) is indeed a measurement of the complexity of
the setB.
The set of vectors to our interest in the deï¬nition of Rademacher complexity of Ais
T(z) :={(1I(z1âˆˆA),...,1I(znâˆˆA))T,Aâˆˆ A}.
Thus the key quantity here is the cardinality of T(z), i.e., the number of sign patterns these
vectors can replicate as Aranges over A. Although the cardinality of Amay be inï¬nite,
the cardinality of T(z) is bounded by 2n.
27
18.657: Mathematics of Machine Learning
Lecturer: Philippe Rigollet Lecture
Scribe:Vira Semenova andPhilippe Rigollet Sep. 23, 2015
In this lecture, we complete the analysis of the performance of the empirical risk mini-
mizer under a constraint on the VC dimension of the family of classiï¬ers. To that end, we
will see how to control Rademacher complexities using shatter coeï¬ƒcients. Moreover, we
will see how the problem of controlling uniform deviations of the empirical measure Âµnfrom
the true measure Âµas done by Vapnik and Chervonenkis relates to our original classiï¬cation
problem.
4.1 Shattering
Recall from the previous lecture that we are interested in sets of the form
T(z) :=/braceleftbig
(1I(z1âˆˆA),...,1I(znâˆˆA)),Aâˆˆ A, z= (z1,...,zn). (4.1)
In particular, the cardinality of T(z), i.e., the numbe/bracerightbig
r of binary patterns these vectors
can replicate as Aranges over A, will be of critical importance, as it will arise when
controlling the Rademacher complexity. Although the cardinality of Amay be inï¬nite, the
cardinality of T(z) is always at most 2n. When it is of the size 2n, we say that Ashatters
the setz1,...,zn. Formally, we have the following deï¬nition.
Deï¬nition: A collection of sets Ashatters the set of points {z1,z2,...,zn}
card{(1I(z1âˆˆA),...,1I(znâˆˆA)),Aâˆˆ A}= 2n.
The sets of points {z1,z2,...,zn}that we are interested are realizations of the pairs Z1=
(X1,Y1),...,Z n= (Xn,Yn) and may, in principle take any value over the sample space.
Therefore, we deï¬ne the shatter coeï¬ƒcient to be the largest cardinality that we may obtain.
Deï¬nition: Theshatter coeï¬ƒcients of a class of sets Ais the sequence of numbers
{SA(n)}nâ‰¥1, where for any nâ‰¥1,
SA(n) = sup card (1I( z1A),...,1I(znA)),A
z1,...,zn{ âˆˆ âˆˆ âˆˆ A}
and the suprema are taken over the whole sample space.
Bydeï¬nition,the nthshattercoeï¬ƒcient SA(n)isequalto2nifthereexistsaset {z1,z2,...,zn}
thatAshatters. The largest of such sets is precisely the Vapnik-Chervonenkis or VC di-
mension.
Deï¬nition: The Vapnik-Chervonenkis dimension, or VC-dimension of
dVCAis the largest
integerdsuch that SA(d) = 2 . We write ( A) =d.5
28
IfSA(n) = 2nfor all positive integers n, thenVC(A) :=âˆ
In words, Ashatters someset of points of cardinality dbut shatters noset of points of
cardinality d+1. In particular, Aalso shatters no set of points of cardinality dâ€²> dso that
the VC dimension is well deï¬ned.
Inthesequel, wewillseethattheVCdimensionwillplaytherolesimilartoofcardinality,
but on an exponential scale. For interesting classes Asuch that card( A) =âˆ, we also may
haveVC(A)<âˆ. For example, assume that Ais the class of half-lines ,A={(âˆ’âˆ,a],aâˆˆ
IR}âˆª {[a,âˆ),aâˆˆIR}, which is clearly inï¬nite. Then, we can clearly shatter a set of size
2 but we for three points z1,z2,z3,âˆˆIR, if for example z1< z2< z3, we cannot create the
pattern (0 ,1,0) (see Figure 4.1). Indeed, half lines can can only create patterns with zeros
followed by ones or with ones followed by zeros but not an alternating pattern like (0 ,1,0).
00
10
01
11000
100
110
111
001
011
101
Figure 1: If A={halï¬‚ines}, then any set of size n= 2 is shattered because we can
create all 2n= 4 0/1 patterns (left); if n= 3 the pattern (0 ,1,0) cannot be reconstructed:
SA(3) = 7<23(right). Therefore, VC(A) = 2.
4.2 The VC inequality
We have now introducedall the ingredients necessary tostate themain result of this section:
the VC inequality.
Theorem (VC inequality): For any family of sets Awith VC dimension VC(A) =d,
it holds /radicalbigg
2dlog(2en/d)IE sup|Âµn(A)âˆ’Âµ(A)| â‰¤2
AâˆˆA n
Notethatthisresultholdsevenif Aisinï¬niteaslongasitsVCdimensionisï¬nite. Moreover,
observe that log( |A|) has been replaced by a term of order dlog 2en/d.
To prove the VC inequality, we proceed in three steps:/parenleftbig /parenrightbig
29
1. Symmetrization, to bound the quantity of interest by the Rademacher complexity:
IE[sup|Âµn(A)âˆ’Âµ(A)|]â‰¤2Rn( )
AâˆˆAA.
We have already done this step in the previous lecture.
2. Control of the Rademacher complexity using shatter coeï¬ƒcients. We are going to
show that
gR(A)â‰¤/radicalï£¬igg
2lo
n/parenleftbig
2SA(n)
n/parenrightbig
3. We are going to need the Sauer-Shelah lemma to bound the shatter coeï¬ƒcients by
the VC dimension. It will yield
S(n)â‰¤/parenleftï£¬igen/parenrightï£¬igd
, d=VC A (dA).
Put together, these three steps yield the VC inequality.
Step 2: Control of the Rademacher complexity
We prove the following Lemma.
Lemma: For anyBâŠ‚IRn, such that |B|<âˆ:, it holds
n/bracketleftbig/vextendsingle1 2Ïƒ/vextendsingle )B/bracketrightbig (Rn( ) = IE max /vextendsingle/summationdisplay log 2B
ibi/vextendsingleâ‰¤max| |
bâˆˆBn bâˆˆBi=1|b|2/radicalbig
n
where|Â·|2denotes the Euclidean norm.
Proof.Note that
1Rn(B) = IEn/bracketleftbig
maxZb,
bâˆˆB|
whereZb=/summationtextn
i=1Ïƒibi. In particular, since/vextendsingle/vextendsingle/bracketrightbig
âˆ’|bi| â‰¤Ïƒi|bi| â‰¤ |bi|, a.s., Hoeï¬€dingâ€™s lemma
implies that the moment generating function of Zbis controlled by
n n
IE/bracketleftbig
exp(sZb)/bracketrightbig
=/productdisplay
IE
i=1/bracketleftbig
exp(sÏƒibi)/bracketrightbig
â‰¤/productdisplay
exp(s2b2
i/2) = exp( s2b2
2/2) (4.2)
i=1| |
Next, to control IE max bâˆˆBZb|, we use the same technique as in Lecture 3, section 1.5.
Â¯ To that end, deï¬ne/bracketleftbig
B=Bâˆª/vextendsingle/vextendsingle
{âˆ’B/bracketrightbig
}and observe that for any s >0,
IE/bracketleftbigg1max|Zb|/bracketrightbigg
= IE/bracketleftbigg
maxZb/bracketrightbigg
= logexp/parenleftbigg
sIE/bracketleftbigg
maxZb/bracketrightbigg/parenrightbigg1â‰¤logIE exp smaxZb,
bâˆˆB bâˆˆBÂ¯ s bÂ¯âˆˆB s/bracketleftbigg /parenleftbigg
bÂ¯âˆˆB/parenrightbigg/bracketrightbigg
where the last inequality follows from Jensenâ€™s inequality. Now we bound the max by a
sum to get/bracketleftbigg /bracketrightbigg1/summationdisplay log|Â¯B|s b2
IE max|Zb| â‰¤log IE[exp( sZb)]â‰¤ +| |2,
bâˆˆB s s2n
bâˆˆBÂ¯
where in the last inequality, we used (4.2). Optimizing over s >0 yields the desired
result.
30
We apply this result to our problem by observing that
Rn(A) = sup (
,Rn(T z))
z1,... zn
whereT(z) is deï¬ned in (4.1). In particular, since T(z)âŠ‚ {0,1}, we have |bâˆš|2â‰¤n
for allbâˆˆT(z). Moreover, by deï¬nition of the shatter coeï¬ƒcients, if B=T(z), then
|Â¯B| â‰¤2|T(z)| â‰¤2SA(n). Together with the above lemma, it yields the desired inequality:
/radicalbigg
2log(2SA(n))Rn(A)â‰¤ .n
Step 3: Sauer-Shelah Lemma
We need to use a lemma from combinatorics to relate the shatter coeï¬ƒcients to the VC
dimension. A priori, it is not clear from its deï¬nition that the VC dimension may be at
all useful to get better bounds. Recall that steps 1 and 2 put together yield the following
bound
2log(2S(n))IE[sup n(A)
Aâˆ’Âµ(A) ]
Aâˆˆ|Âµ | â‰¤A2/radicalbigg
(4.3)n
In particular, if SA(n) is exponential in n, the bound (4.3) is not informative, i.e., it does
not imply that the uniform deviations go to zero as the sample size ngoes to inï¬nity. The
VC inequality suggest that this is not the case as soon as VC(A)<âˆbut it is not clear a
priori. Indeed, it may be the case that
VCSA(n) = 2nfornâ‰¤dandSA(n) = 2nâˆ’1 forn > d,
which would imply that ( A) =d <âˆbut that the right-hand side in (4.3) is larger than
2 for alln. It turns our that this can never be the case: if the VC dimension is ï¬nite, then
the shatter coeï¬ƒcients are at most polynomial inn. This result is captured by the Sauer-
Shelah lemma, whose proof is omitted. The reading section of the course contains pointers
to various proofs, speciï¬cally the one based on shiftingwhich is an important technique in
enumerative combinatorics.
Lemma (Sauer-Shelah): IfVC(A) =d, thenâˆ€nâ‰¥1,
d
SA(n)â‰¤/summationdisplay/parenleftbiggn en d
.k/parenrightbigg
â‰¤dk=0/parenleftï£¬ig /parenrightï£¬ig
Together with (4.3), it clearly yields the VC inequality. By applying the bounded diï¬€erence
inequality, we also obtain the following VC inequality that holds with high probability. This
is often the preferred from for this inequality in the literature.
Corollary (VC inequality): For any family of sets Asuch that VC(A) =dand any
Î´âˆˆ(0,1), it holds with probability at least 1 âˆ’Î´,
/radicalbigg
2dlog(2en/d)/radicalbigg
log(2/Î´)supÂµnA)âˆ’Âµ(A)| â‰¤2 +
AâˆˆA|( .n 2n
31
Note that the logarithmic term log(2 en/d) is actually superï¬‚uous and can be replaced
by a numerical constant using a more careful bounding technique. This is beyond the scope
of this class and the interested reader should take a look at the recommending readings.
4.3 Application to ERM
The VC inequality provides an upper bound for supAâˆˆA|Âµn(A)âˆ’Âµ(A)|in terms of the VC
dimension of the class of sets A. This result translates directly to our quantity of interest:
2VC( )log2en)Ë†sup|Rnh)âˆ’VC(A)log(2/Î´(R(h) 2n/parenrightbig
+
hâˆˆHâ‰¤/radicalï£¬igg
A
|/parenleftbig /radicalbigg
(4.4)2n
whereA={Ah:hâˆˆ H}andAh={(x,y)âˆˆ X Ã—{0,1}:h(x) =y}. Unfortunately, the
VC dimension of this class of subsets of X Ã—{0,1}is not very natural. Since, a classiï¬er h
is a{0,1}valued function, it is more natural to consider the VC dimension of the family
A=/braceleftbig
{h= 1}:hâˆˆ H/bracerightbig
.
Deï¬nition: LetHbe a collection of classiï¬ers and deï¬ne
AÂ¯={h= 1}:hâˆˆ H
We deï¬ne the VC d/braceleftbig
imension VC( ) o/bracerightbig
=/braceleftbig
A:âˆƒhâˆˆ H,h(Â·) = 1I(Â· âˆˆA).
H Â¯ fHto be the VC dimension of/bracerightbig
A.
Â¯ Â¯ It is not clear how VC(A) relates to the quantity VC(A), where A={Ah:hâˆˆ H}and
Ah={(x,y)âˆˆ X Ã—{0,1}:h(x) =y}that appears in the VC inequality. Fortunately, these
two are actually equal as indicated in the following lemma.
Lemma: Deï¬ne the two families for sets: = AXÃ—h:h 2{0,1}where
{ Â¯A { âˆˆ H} âˆˆ
Ah= (x,y)âˆˆ X Ã—{0,1}:h(x) =y}andA=/braceleftbig
{h= 1 :h 2X.
S S â‰¥ VCAÂ¯} âˆˆ H âˆˆ
Then,AÂ¯(n) =AÂ¯(n) for alln1. It implies ( ) = VC(A/bracerightbig
).
Proof.Fixx= (x1,...,xn)âˆˆ Xnandy= (y1,y2,...,yn)âˆˆ {0,1}nand deï¬ne
T(x,y) ={(1I(h(x1) =y1),...,1I(h(xn) =yn)),hâˆˆ H}
and
Â¯T(x) ={(1I(h(x1) = 1),...,1I(h(xn) = 1)),hâˆˆ H}
To that end, ï¬x vâˆˆ {0,1}and recall the XOR (exclusive OR) boolean function from {0,1}
to{0,1}deï¬ned by uâŠ•v= 1I(u=v). It is clearly1a bijection since ( uâŠ•v)âŠ•v=u.
1One way to see that is to introduce the â€œspinnedâ€ variables uËœ = 2uâˆ’1 andvËœ = 2vâˆ’1 that live in
/tildewider {âˆ’1,1}. ThenuâŠ•v=uËœÂ·vËœ, and the claim follows by observing that ( uËœÂ·vËœ)Â·vËœ =uËœ. Another way is to simply
write a truth table./ne}ationslash
/ne}ationslash
/ne}ationslash
/ne}ationslash /ne}ationslash
/ne}ationslash
32
When applying XOR componentwise, we have
ï£«
1I(h(x1) =y1)ï£«
1I(h(x1= 1) y
ï£¬ ..ï£¶
) 1ï£· ..ï£¶
ï£¬ .
1I(h(xi)ï£·ï£·ï£· .ï£«ï£¶
ï£¬ï£¬
=yi) = 1I(h(xi) = 1)
ï£¬ï£¬..ï£¬ï£¬
.âŠ•
.ï£·
ï£­ .ï£¬
.ï£·ï£·
1I(h(xn) =yn)ï£·ï£¬
ï£· ï£¬ï£¬
ï£¸ ï£­
1I(h(xn) = 1)ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬...
ï£·yi
ï£¸ï£· ..ï£­.
ynï£·ï£·ï£·ï£·ï£·ï£· ï£·
Â¯ï£¸
Since XOR is a bijection, we must have card[ T(x,y)] = card[ T(x)]. The lemma follows
by taking the supremum on each side of the equality.
It yields the following corollary to the VC inequality.
Corollary: LetHbe a family of classiï¬ers with VC dimension d. Then the empirical
Ë† risk classiï¬er hermoverHsatisï¬es
erm/radicalbigg
2dlog(2en/d)Ë†R(h)â‰¤minR(h)+4 +
hâˆˆH n/radicalbigg
log(2/Î´)
2n
with probability 1 âˆ’Î´.
Proof.Recall from Lecture 3 that
Ë†R(herm)âˆ’min ) â‰¤ Ë† R(h2sup
hâˆˆH hâˆˆH
The proof follows directly by applyi/vextendsingle/vextendsingleRn(h)âˆ’R(h)/vextendsingle
ng (4.4) and the above lemma./vextendsingle/ne}ationslash
/ne}ationslash
/ne}ationslash
33
18.657: Mathematics of Machine Learning
Lecturer: Philippe Rigollet Lecture 6
Scribe:Ali Makhdoumi Sep. 28, 2015
5. LEARNING WITH A GENERAL LOSS FUNCTION
In the previous lectures we have focused on binary losses for the classiï¬cation problem and
developed VCtheory forit. Inparticular, theriskforaclassiï¬cation function h:X â†’ {0,1}
and binary loss function the risk was
R(h) = IP(h(X) =Y) = IE[1I( h(X) =Y)].
In this lecture we will consider a general loss function and a general regression model where
Yis not necessarily a binary variable. For the binary classiï¬cation problem, we then used
the followings:
â€¢Hoeï¬€dingâ€™s inequality: it requires boundedness of the loss functions.
â€¢Bounded diï¬€erence inequality: again it requires boundedness of the loss functions.
â€¢VC theory: it requires binary nature of the loss function.
Limitations of the VC theory:
â€¢Hard to ï¬nd the optimal classiï¬cation: the empirical risk minimization optimization,
i.e.,
n1min
hn/summationdisplay
1I(h(Xi) =Yi)
i=1
is a diï¬ƒcult optimization. Even though it is a hard optimization, there are some
algorithms that try to optimize this function such as Perceptron and Adaboost.
â€¢This is not suited for regression. We indeed know that classiï¬cation problem is a
subset of Regression problem as in regression the goal is to ï¬nd IE[ Y|X] for a general
Y(not necessarily binary).
In this section, we assume that Yâˆˆ[âˆ’1,1] (this is not a limiting assumption as all the
results can bederived for any bounded Y) and we have a regression problem where ( X,Y)âˆˆ
X Ã—[âˆ’1,1]. Most of the results that we preset here are the analogous to the results we had
in binary classiï¬cation. This would be a good place to review those materials and we will
refer to the techniques we have used in classiï¬cation when needed.
5.1 Empirical Risk Minimization
5.1.1 Notations
Loss function: In binary classiï¬cation the loss function was 1I( h(X) =Y). Here, we
replace this loss function by â„“(Y,f(X)) which we assume is symmetric, where fâˆˆ F,
f:X â†’[âˆ’1,1] is the regression functions. Examples of loss function include/\e}atio\slash /\e}atio\slash
/\e}atio\slash
/\e}atio\slash
34
â€¢â„“(a,b) = 1I( a=b) ( this is the classiï¬cation loss function).
â€¢â„“(a,b) =|aâˆ’b|.
â€¢â„“(a,b) = (aâˆ’b)2.
â€¢â„“(a,b) =|aâˆ’b|p,pâ‰¥1.
We further assume that 0 â‰¤â„“(a,b)â‰¤1.
Risk: risk is the expectation of the loss function, i.e.,
R(f) = IEX,Y[â„“(Y,f(X))],
where the joint distribution is typically unknown and it must be learned from data.
Data: we observe a sequence ( X1,Y1),...,(Xn,Yn) ofnindependent draws from a joint
distribution PX,Y, where ( X,Y)âˆˆ X Ã—[âˆ’1,1]. We denote the data points by Dn=
{(X1,Y1),...,(Xn,Yn)}.
Empirical Risk : the empirical risk is deï¬ned as
n1Ë†Rn(f) =n/summationdisplay
â„“(Yi,f(Xi)),
i=1
Ë† Ë† and the empirical risk minimizer denoted by ferm(orf) is deï¬ned as the minimizer of
empirical risk, i.e.,
Ë†argminRn(f).
fâˆˆF
Ë† In order to control the risk of fwe shall compare its performance with the following oracle:
Â¯fâˆˆargminR(f).
fâˆˆF
Note that this is an oracle as in order to ï¬nd it one need to have access to PXYand then
Ë† optimize R(f) (we only observe the data Dn). Since fis the minimizer of the empirical
Ë†Ë†Ë†Â¯ risk minimizer, we have that Rn(f)â‰¤Rn(f), which leads to
Ë†R(f)â‰¤Ë†R(f)âˆ’Ë†Ë†Ë†Ë†Ë†Â¯Ë†Â¯ Â¯ Â¯ Rn(f)+Rn(f)âˆ’Rn(f)+Rn(f)âˆ’R(f)+R(f)
â‰¤Â¯ Ë† Ë†Ë†Ë†Â¯ Â¯ Â¯ Ë† R(f)+R(f)âˆ’Rn(f)+Rn(f)âˆ’R(f)â‰¤R(f)+2sup
fâˆˆF|Rn(f)âˆ’R(f)|.
Therefore, the quantity of interest that we need to bound is
sup|Ë†Rn(f)âˆ’R(f)
fâˆˆF|.
Moreover, from theboundeddiï¬€erence inequality, we know that sincethe loss function â„“(Â·,Â·)
Ë† is bounded by 1, supfâˆˆF|Rn(f)âˆ’R(f)|has the bounded diï¬€erence property with ci=1
n
fori= 1,...,n, and the bounded diï¬€erence inequality establishes
P/bracketleftï£¬igg
2t2
sup|Ë† Ë† Rn(f)âˆ’R(f)|âˆ’IE/bracketleftï£¬igg
sup|Rn(f)âˆ’R(f)
fâˆˆF|/bracketrightï£¬igg
â‰¥t
fâˆˆF/bracketrightï£¬igg
âˆ’â‰¤exp/parenleftbigg
x2
i i/parenrightbigg
= e pcâˆ’2nt2,
which in turn yields/parenleftbig /parenrightbig/summationtext
log(1/delta)|Ë†supRn(f)âˆ’R(f)| â‰¤I|Ë†E/bracketleftï£¬igg
supRn(f)âˆ’R(f) Î´
fâˆˆF f|/bracketrightï£¬igg
+
âˆˆ/radicalbigg
,w.p. 1
F 2nâˆ’.
Ë† As a result we only need to bound the expectation IE[supfâˆˆF|Rn(f)âˆ’R(f)|]./\e}atio\slash
35
5.1.2 Symmetrization and Rademacher Complexity
Similar to the binary loss case we ï¬rst use symmetrization technique and then intro-
duce Rademacher random variables. Let Dn={(X1,Y1),...(Xn,Yn)}be the sample set
and deï¬ne an independent sample (ghost sample) with the same distribution denoted by
Dâ€²
n={(Xâ€²
1,Yâ€²
1),...(Xâ€²
n,Yâ€²
n)}( for each i, (Xâ€²
i,Yâ€²
i) is independent from Dnwith the same
distribution as of ( Xi,Yi)). Also, let Ïƒiâˆˆ {âˆ’1,+1}be i.i.d. Rad(1) random variables2
independent of DnandDâ€²
n. We have
IE/bracketleftï£¬iggn1sup|â„“ i
fâˆˆFn/summationdisplay
(Yi,f(X))
i=1âˆ’IE[â„“(Yi,f(Xi))]|/bracketrightï£¬igg
n n1 1= IE/bracketleftï£¬igg
sup â„“(Yi,f(X â„“(Yâ€²i)) IEi,f(Xâ€²
i))Dn
fâˆˆF|n/summationdisplay
i=1âˆ’/bracketleftï£¬igg
n/summationdisplay
i=1|/bracketrightï£¬igg
|/bracketrightï£¬igg
n n1 1= IE/bracketleftï£¬igg
sup|IE/bracketleftï£¬igg/summationdisplay
â„“(Yi,f(Xâ€²i)) â„“(Yi,f(Xâ€²
i))Dn
fâˆˆFni=1âˆ’n/summationdisplay
i=1|/bracketrightï£¬igg
|/bracketrightï£¬igg
n(a)
â‰¤IE/bracketleftï£¬iggn1 1supIE/bracketleftï£¬igg
|/summationdisplay
â„“(Yi,f(Xâ€²i))âˆ’/summationdisplay
â„“(Y ,f(Xâ€²
i i))| |Dn
fâˆˆFn ni=1 i=1/bracketrightï£¬igg/bracketrightï£¬igg
â‰¤IE/bracketleftï£¬iggn n1sup|/summationdisplay 1â„“(Yi,f(Xi)) â„“(Yâ€²
i,f(Xâ€²
fâˆˆFn ni))
i=1âˆ’/summationdisplay
i=1|/bracketrightï£¬igg
(b) 1= IE/bracketleftï£¬iggn
sup|/summationdisplay
Ïƒi/parenleftbig
â„“(Yi,f(Xi))âˆ’â„“(Yâ€²X
fFni,f(â€²
i))
âˆˆi=1/parenrightbig
|/bracketrightï£¬igg
n(c) 1â‰¤2IE/bracketleftï£¬igg
sup
fâˆˆF|n/summationdisplay
Ïƒiâ„“(Yi,f(Xi))
i=1|/bracketrightï£¬igg
n
â‰¤2supIE/bracketleftï£¬igg
1sup|/summationdisplay
Ïƒiâ„“(yi,f(xi))
DnfâˆˆFni=1|/bracketrightï£¬igg
.
where (a) follows from Jensenâ€™s inequality with convex function f(x) =x, (b) follows from
the fact that ( X ,Y) and (Xâ€² â€²| |
i i i,Yi) has the same distributions, and (c) follows from triangle
inequality.
Rademacher complexity: of a class Fof functions for a given loss function â„“(Â·,Â·) and
samplesDnis deï¬ned as
n1Rn(â„“â—¦F) = supIE/bracketleftï£¬igg
sup|/summationdisplay
Ïƒiâ„“(yi,f(xi)).
DnfâˆˆFni=1|/bracketrightï£¬igg
Therefore, we have
IE/bracketleftï£¬iggn1sup|/summationdisplay
â„“(Yi,f(Xi))
fâˆˆFni=1âˆ’IE[â„“(Yi,f(Xi))]|/bracketrightï£¬igg
â‰¤2Rn(â„“â—¦F)
and we only require to bound the Rademacher complexity.
5.1.3 Finite Class of functions
Suppose that the class of functions Fis ï¬nite. We have the following bound.
36
Theorem: Assume that Fis ï¬nite and that â„“takes values in [0 ,1]. We have
/radicalbigg
2log(2Rn(â„“â—¦F)|F|)â‰¤ .n
Proof.From the previous lecture, for BâŠ†nR, we have that
n1 2log(2B)Rn(B) = IE/bracketleftï£¬igg
max
bâˆˆB|n/summationdisplay
Ïƒibi
i=1|/bracketrightï£¬igg
| |â‰¤max
bâˆˆB|b|2/radicalbig
.n
Here, we haveï£±ï£«
â„“(y(xï£´ï£² 1,f1))
ï£¬.B= .,.ï£¶
â„“(yn,f(xn)ï£·fâˆˆ Fï£¼
ï£½
.
)ï£´
Sinceâ„“takes values in [0 ,1], thisï£´ï£³
imï£­
pliesBï£¸
âŠ† {b:|b|2âˆšâ‰¤ï£´ï£¾
n}. Plugging this bound in the
previous inequality completes the proof.
5.2 The General Case
Recall that for the classiï¬cation problem, we had F âŠ‚ {0,1}X. We have seen that the
cardinality of the set {(f(x1),...,f(xn)),f
Ë†ermâˆˆ F}plays an important role in bounding the
risk off(this is not exactly what we used but the XOR argument of the previous lecture
allows us to show that the cardinality of this set is the same as the cardinality of the set
that interests us). In this lecture, this set might be uncountable. Therefore, we need to
introduce a metric on this set so that we can treat the close points in the same manner. To
this end we will deï¬ne covering numbers (which basically plays the role of VC dimension
in the classiï¬cation).
5.2.1 Covering Numbers
Deï¬nition: Given a set of functions Fand a pseudo metric donF((F,d) is a metric
space) and Îµ >0. AnÎµ-netof (F,d) is a set Vsuch that for any fâˆˆ F, there exists
gâˆˆVsuch that d(f,g)â‰¤Îµ. Moreover, the covering numbers of (F,d) are deï¬ned by
N(F,d,Îµ) = inf{|V|:Vis anÎµ-net}.
For instance, for the Fshown in the Figure 5.2.1 the set of points {1,2,3,4,5,6}is a
covering. However, the covering number is 5 as point 6 can be removed from Vand the
resulting points are still a covering.
Deï¬nition: Givenx= (x1,...,x n), theconditional Rademacher average of a class of
37
functions Fis deï¬ned as
RË†x
n= IE/bracketleftï£¬iggn1sup Ïƒ
fâˆˆF/vextendsingle/vextendsingle
n/summationdisplay
if(xi)
i=1/bracketrightï£¬igg
/vextendsingle/vextendsingle.
Note that in what follows we consider a general class of functions F. However, for
applying the results in order to bound empirical risk minimization, we take xito be (xi,yi)
andFto beâ„“â—¦F. We deï¬ne the empirical l1distance as
n
dx 1
1(f,g) =n/summationdisplay
i
=1|f(x)
iâˆ’g(xi)|.
Theorem: If 0â‰¤fâ‰¤1 for all fâˆˆ F, then for any x= (x1,...,x n), we have
Ë†Rx
n(F)â‰¤inf
Îµâ‰¥0/radicalbigg
x /braceleftbig2log(2N(F,dÎµ+1,Îµ))
n/bracerightbig
.
Proof.Fixx= (x1,...,x n) andÎµ >0. LetVbe a minimal Îµ-net of ( F,dx
1). Thus,
by deï¬nition we have that |V|=N(F,dx
1,Îµ). For any fâˆˆ F, deï¬nefâ—¦âˆˆVsuch that6
5 432 1 F
Ç«
38
dx
1(f,fâ—¦)â‰¤Îµ. We have that
n1Rx
n(F) = IE/bracketleftï£¬igg
sup Ïƒif(xi)
fâˆˆF|n/summationdisplay
i=1|/bracketrightï£¬igg
â‰¤IE/bracketleftï£¬iggn n1 1sup|/summationdisplay
Ïƒi(f(xi)fâ—¦(xi)) +IE sup Ïƒifâ—¦(xi)
fâˆˆFn fâˆˆFni=1âˆ’ |/bracketrightï£¬igg /bracketleftï£¬igg
|/summationdisplay
i=1|/bracketrightï£¬igg
â‰¤Îµ+IE/bracketleftï£¬iggn1max Ïƒif(xi)
fâˆˆV|n/summationdisplay
i=1|/bracketrightï£¬igg
/radicalbigg
2log(2â‰¤Îµ+|V|)
n/radicalbigg
2log(2N(=Îµ+F,dx
1,Îµ)).n
Since the previous bound holds for any Îµ, we can take the inï¬mum over all Îµâ‰¥0 to obtain
x/radicalbigg
/braceleftbig2log(2N(F,dx
Rn(F)â‰¤infÎµ+1,Îµ))
Îµâ‰¥0 n/bracerightbig
.
The previous bound clearly establishes a trade-oï¬€ because as Îµdecreases N(F,dx
1,Îµ) in-
creases.
5.2.2 Computing Covering Numbers
As a warm-up, we will compute the covering number of the â„“2ball of radius 1 indRdenoted
byB2. We will show that the covering is at most (3
Îµ)d. There are several techniques to
prove this result: one is based on a probabilistic method argument and one is based on
greedily ï¬nding an Îµ-net. We will describe the later approach here. We select points in V
one after another so that at step k, we have ukâˆˆB2\âˆªk
j=1B(uj,Îµ). We will continue this
procedure until we run out of points. Let it be step N. This means that V={u1,...,u N}
is anÎµ-net. We claim that the balls B(ui,Îµ) andB(uj,Îµ) for any i,j12 2âˆˆ {,...,N}are
disjoint. The reason is that if vâˆˆB(ui,Îµ)âˆ©B(uj,Îµ), then we would have2 2
Îµ Îµ/bardbluiâˆ’uj/bardbl2â‰¤ /bardbluiâˆ’v/bardbl2+/bardblvâˆ’uj/bardbl2â‰¤+ =Îµ,2 2
which contradicts the way we have chosen the points. On the other hand, we have that
âˆªN
j=1B(uj,Îµ)âŠ†(1+Îµ)B2. Comparing the volume of these two sets leads to2 2
Îµ Îµ|V|( )dvol(B2)â‰¤(1+ )dvol(B2),2 2
wherevol(B2) denotes the volume of the unit Euclidean ball in ddimensions. It yields,
|V| â‰¤/parenleftbig
1+Îµd
22d3d
= +1 . /parenleftbigÎµ Îµ
2/parenrightbig/parenrightbigg
/parenrightbigd/parenleftbigg
â‰¤/parenleftbigg
Îµ/parenrightbigg
39
For anypâ‰¥1, deï¬ne
1
dx
p(f,g) =/parenleftï£¬iggn1p /summationdisplay
|f(xi)g(x)pi,ni=1âˆ’ |/parenrightï£¬igg
and forp=âˆ, deï¬ne
dx
âˆ(f,g) = max |f(xi)âˆ’g(xi)
i|.
Ë† Using the previous theorem, in order to bound Rx
nwe need to bound the covering number
withdx
1norm. We claim that it is suï¬ƒcient to bound the covering number for the inï¬nity-
norm. In order to show this, we will compare the covering number of the norms dx
p(f,g) =
1 /parenleftbig1
n/summationtextn
i=1|f(xpi)âˆ’g(xi)|/parenrightbig
pforpâ‰¥1 and conclude that a bound on N(F,dx
âˆ,Îµ) implies a
bound on N(F,dx
p,Îµ) for any pâ‰¥1.
Proposition: For any 1 â‰¤pâ‰¤qandÎµ >0, we have that
N(F,dx
p,Îµ)â‰¤N(F,dx
q,Îµ).
Proof.First note that if q=âˆ, then the inequality evidently holds. Because, we have
n1(/summationdisplay 1
|zi|p)pâ‰¤maxn ii=1|zi|,
which leads to B(f,dx
âˆ,Îµ)âŠ†B(f,dx
p,Îµ) andN(f,dâˆ,Îµ)â‰¥N(f,dp,Îµ). Now suppose that
1â‰¤pâ‰¤q <âˆ. Using HÂ¨ olderâ€™s inequality with r=q
pâ‰¥1 we obtain
/parenleftï£¬igg /parenrightï£¬igg 1/parenleftï£¬igg /parenrightï£¬igg(11)1/parenleftï£¬igg /parenrightï£¬igg 1/parenleftï£¬igg /parenrightï£¬igg 1âˆ’ n n n1p r p pr n1q
1n/summationdisplay 1
|z|piâ‰¤âˆ’np/summationdisplay
i1/summationdisplay
i=1|zi|pr=ni=1 =/summationdisplay
.
i|zi|q
=1
This inequality yeilds
B(f,dx
q,Îµ) ={g:dx
q(f,g)â‰¤Îµ} âŠ†B(f,dx
p,Îµ),
which leads to N(f,dq,Îµ)â‰¥N(f,dp,Îµ).
Using this propositions we only need to bound N(F,dx
âˆ,Îµ).
Let the function class be F={f(x) =/a\}bracketle{tf,x/a\}bracketri}ht,fâˆˆBd,xâˆˆBd}, where1 1
p q + = 1. Thisp q
leads to|f| â‰¤1.
Claim: N(F,dx
âˆ,Îµ)â‰¤(2)d.Îµ
This leads to
x/radicalbigg
2dlog(4/Îµ)Ë†Rn(F)â‰¤inf
0{Îµ+ .
Îµ> n}
TakingÎµ=O(/radicalï£¬ig
dlogn), we obtainn
Ë†Rx d
n(F)â‰¤O(/radicalbigg
logn).n
40
18.657: Mathematics of Machine Learning
Lecturer: Philippe Rigollet Lecture 7
Scribe: Zach Izzo Sep. 30, 2015
In this lecture, we continue our discussion of covering numbers and compute upper
^ bounds for specic conditional Rademacher averages Rx
n(F). We then discuss chaining and
conclude by applying it to learning.
Recall the following denitions. We dene the risk function
R(f) = I E[`(X;f (X))]; (X;Y )2X [ 1;1];
for some loss function `(;). The conditiona Rademacher average that we need to control
is
n1R(`lF) = sup I E sup i`(yi;f(xi)):
(x1;y1);:::;(x n;yn)"
fn2FX
i=1#
Furthermore, we dened the conditional Rademacher average for a poin
tx= (x1;:::;x n)
to be
R^x
n(F) = I E"
sup
f2F n1if(xi)ni=1#
:
Lastly, we dene the "-covering number N(X
F;d;
") to be the m
inimum number of balls (with
respect to the metric d) of radius"needed to
cover
F. We proved the following theorem:
Theorem: Assumejfj1 for allf2F. Then
2 log(2N(;dx;"))R^x
n(F) +r
Finf1
">0(
"n)
;
wheredx
1is given by
n
dx 1
1(f;g) =nX
i=1jf(xi) g(xi)j:
We make use of this theorem in the following example. Dene Bd
p=fx2I Rd:jxjp1g.
Then takef(x) =ha;xi, setF=fha;i:a2Bdg, andX=Bd
1. By H older's inequality,1
we have
jf(x)jjajx1j j11;
so the theorem above holds. We need to compute the covering number N(F;dx
1;"). Note
that for all a2Bd, there exists v= (v1;:::;v n) such that vi=g(xi) and1
n1
nX
a;xivi"
i=1jh i  j
for some function g. For this case, we will take g(x) =hb;xi, sovi=hb;x ii. Now, note the
following. Given this denition of g, we have
n n
dx 1 1
1(f;g) = a;x 1b;xi=a b;x ia bnX
ni=1jh i h ijX
i=1jh   ijj   j 1
41
by H older's inequality and the fact that jxj1= 1. So ifja bj1", we can take vi=hb;x ii.
We just need to nd a set of fb1;:::;b MgI Rdsuch that, for any athere exists bjsuch
thatja bjj<1. We can do this by dividing Bdinto cubes with side length "and 1 1
taking theb's to be the set of vertices of these cubes. Then any a2Bdj must land in one1
of these cubes, so ja bjj "as desired. There are c="dof suchb 1 j's for some constant
c>0. Thus
N(Bd;dx
1 1;")c="d:
We now plug this value into the theorem to obtain
R^x 2 log(c="d)
n(F)inf :
"(
"+
0r
n)
Optimizing over all choices of "gives
r
dlog(n)"=cn) R ^x
n(F)cr
dlog(n):n
Note that in this nal inequality, the conditional empirical risk no longer depends on
x, since we \sup'd" xout of the bound during our computations. In general, one should
ignorexunless it has properties which will guarantee a bound which is better than the sup.
Another important thing to note is that we are only considering one granularity of Fin our
nal result, namely the one associated to ". It is for this reason that we pick up an extra
log factor in our risk bound. In order to remove this term, we will need to use a technique
called chaining.
5.4 Chaining
We have the following theorem.
Theorem: Assume thatjfj1 for allf2F. Then
12Z1
R^x
ninf 4" +
">0p log(N (;dx))dt :n2;t
"q
F
(Note that the integrand decays with t.)
Proof. Fixx= (x1;:::;x n), and for all j= 1;:::;N , letVjbe a minimal 2 j-net ofF
under thedx
2metric. (The number Nwill be determined later.) For a xed f2F, this
process will give us a \chain" of points fiwhich converges to f:dx
2(fi;f)2 j.
DeneF=f(f(x1);:::;f (xn))>; f2Fg [ 1;1]n. Note that
R^x 1
n(F) = I E supn f2Fh;fi
where= (1;:::; n). Observe that for all N, we can rewriteh;fias a telescoping sum:
h;fi=h;f fNi+h;fN fN
 1i+:::+h;f1 f0i
42
wheref0:= 0. Thus
N
R^x 1 1
n(F)I E supjh;f fNij+ I fn f2FX
E sup;njfj
f F 1:
j=1jh   ij
2
We can control the two terms in this inequality separately. Note rst that by the Cauchy-
Schwarz inequality,
1 dx
2(f;fN)I E supjh;f fNijjj2n fp:n 2F
Sincejj2=pnanddx
2(f;fN)2 N, we have
1I E supjh;f fN2n f2Fij N:
Now we turn our attention to the second term in the inequality, that is
N
S=X1I E supjh;fj fj
n fj=12F 1ij:
Note that since fj2Vjandfj
 12Vj V  1, there are at most jjjjVj 1jpossible dierences
fj fj. 1SincejV2j1jjV  jj=2,jVjjjVj1jjVjj=2 and we nd ourselves in the nite 
dictionary case. We employ a risk bound from earlier in the course to obtain the inequality
p
2 log(2Rn(B)max
b Bjbj2jBj):
2 n
In the present case, B=ffj fj
 1;f2Fgso thatjBjjV jj2=2. It yields
2j22 log(jVj) logR2 Vj
n(B)jq
jr = 2rnp
;n
wherer= supf2Fjfj fj
 1j2. Next, observe that
jfj fj
1j2=pn d x
2(fj;fj
 1)p pn(dx
2(fj;f) +dx
2(f;fj)) 3 2 jn: 1 
by the triangle inequality and the fact that dx(f;f)2 j
2j . Substituting this back into our
bound forRn(B), we have
log(B)jVj6 2 jnr
jj ;2j ( ))  = 6n r
log(NFdx
2;2 j
Rn
sinceVjjwas chosen to be a minimal 2 -net.
The proof is almost complete. Note that 2 j= 2(2 j 2 j 1) so that
N6pXN122 jq
log(N (F;dx
2;2 j)) =pX
(2 j 2 j 1)q
log(N (F;dx
2;2 j)):n nj=1 j=1
Next, by comparing sums and integrals (Figure 1), we see that
XN
(2 j
j=1 2 j 1)q 1
log(N (F;dx
2;2 j))Z=2
log(N (;dx
2;t))dt:
2 (N+1)q
F
43
Figure 1: A comparison of the sum and integral in question.
So we choose Nsuch that 2 (N +2)"2 (N +1), and by combining our bounds we obtain
121=2 1
R^x)2 N
n(F +pnZq
log(N (F;dx
2;t))dt
2 ( +1)4"+
NZp
log(N;
"F;t)dt
since the integrand is non-negative. (Note: this integral is known as the \Dudley Entropy
Integral.")
Returning to our earlier example, since N(F;dx
2;")c="d, we have
1
R^x
n(F)inf124"+pZq
log((c0=t)d)dt
">0 n"
:
SinceR1p
log(c=t)dt =c is nite, we then have0
R^x
n(F)12cp
d=n:
Using chaining, we've been able to remove the log factor!
5.5 Back to Learning
We want to bound
n1Rn(`F) = sup I E sup i`(yi;f(xi)):
(x1;y1);:::;(x n;yn)"
f2F
nX
i
x
=1#

R^ We considern(Fn) = I E i

supf1P
i=1f(x)2F ifor someLn
-Lipschitz function
, that isj(a) (b)jLja bjfor alla;b2[ 1;1]. We
have the following lemma.
44
Theorem: (Contraction Inequality) Let  be L-Lipschitz and such that (0) = 0,
then
R^x
n(F) R ^ 2Lx
n(F):
The proof is omitted and the interested reader should take a look at [LT91, Kol11] for
example.
As a nal remark, note that requiring the loss function to be Lipschitz prohibits the use
ofR-valued loss functions, for example `(Y;) = (Y )2:
45
18.657: Mathematics of Machine Learning
Lecturer: Philippe Rigollet Lecture 8
Scribe:Quan Li Oct. 5, 2015
Part II
Convexity
1. CONVEX RELAXATION OF THE EMPIRICAL RISK MINIMIZATION
Ë† In the previous lectures, we have proved upper bounds on the excess risk R(herm)âˆ’R(hâˆ—)
of the Empirical Risk Minimizer
Ë†herm 1= argmin
hâˆˆHn
1I(Yi=h(Xi)). (1.1)n/summationdisplay
i=1/ne}ationslash
However due to the nonconvexity of the objective function, the optimization problem
(1.1) in general can not be solved eï¬ƒciently. For some choices of Hand the classiï¬cation
error function (e.g. 1I( Â·)), the optimization problem can be NP-hard. However, the problem
we deal with has some special features:
1. Since the upper bound we obtained on the excess risk is O(/radicalBig
dlogn), we only need ton
approximate the optimization problem with error up to O(/radicalBig
dlogn
n).
2. The optimization problem corresponds to the average case problem where the data
i.i.d(Xi,Yi)âˆ¼PX,Y.
3.Hcan be chosen to be some â€™naturalâ€™ classiï¬ers, e.g. H={half spaces }.
These special features might help us bypass the computational issue. Computational
issueinmachinelearninghavebeenstudiedforquitesometime(see, e.g. [Kea90]), especially
in the context of PAC learning. However, many of these problems are somewhat abstract
and do not shed much light on the practical performance of machine learning algorithms.
Toavoid thecomputational problem, thebasicideais to minimizea convex upperbound
of the classiï¬cation error function 1I( Â·) in (1.1). For the purpose of computation, we shall
also require that the function class Hbe a convex set. Hence the resulting minimization
becomes a convex optimization problem which can be solved eï¬ƒciently.
1.1 Convexity
Deï¬nition: A setCis convex if for all x,yâˆˆCandÎ»âˆˆ[0,1],Î»x+(1âˆ’Î»)yâˆˆC.
46
Deï¬nition: A function f:Dâ†’IR on a convex domain Dis convex if it satisï¬es
f(Î»x+(1âˆ’Î»)y)â‰¤Î»f(x)+(1âˆ’Î»)f(y),âˆ€x,yâˆˆD,andÎ»âˆˆ[0,1].
1.2 Convex relaxation
The convex relaxation takes three steps.
Step 1: Spinning.
Using a mapping Y/maâˆšstoâ†’2Yâˆ’1, the i.i.d. data ( X1,Y1),(X2,Y2),...,(Xn,Yn) is transformed
to lie inX Ã—{âˆ’1,1}. These new labels are called spinnedlabels. Correspondingly, the task
becomes to ï¬nd a classiï¬er h:X /maâˆšstoâ†’ {âˆ’1,1}. By the relation
h(X)/ne}ationslash=Yâ‡” âˆ’h(X)Y >0,
we can rewr i
te the objective function in (1.1) by
n n1/summationdisplay 11I(h(Xi) =Yi) =/summationdisplay
Ï•1 I(hn ni=1 i=1âˆ’(Xi)Yi) (1.2)
whereÏ•1 I(z) = 1I(z >0).
Step 2: Soft classiï¬ers.
The setHof classiï¬ers in (1.1) contains only functions taking values in {âˆ’1,1}. As a result,
it is non convex if it contains at least two distinct classiï¬ers. Soft classiï¬ers provide a way
to remedy this nuisance.
Deï¬nition: Asoft classiï¬er is any measurable function f:X â†’[âˆ’1,1]. The hard
classiï¬er (or simply â€œclassiï¬erâ€) associated to a soft classiï¬er fis given by h= sign(f).
LetF âŠ‚IRXbe aconvexset soft classiï¬ers. Several popular choices for Fare:
â€¢Linear functions:
F:={/an}bracketle{ta,x/an}bracketri}ht:aâˆˆ A}.
for some convex set A âˆˆIRd. The associated hard classiï¬er h= sign(f) splits IRdinto
two half spaces.
â€¢Majority votes: given weak classiï¬ers h1,...,h M,
M M
F:=/braceleftBig/summationdisplay
Î»jhj(x) :Î»j
j=â‰¥0,/summationdisplay
Î»j= 1/bracerightBig
.
1 j=1
â€¢LetÏ•j,j= 1,2,...a family of functions, e.g., Fourier basis or Wavelet basis. Deï¬ne
âˆ
F:={/summationdisplay
Î¸jÏ•j(x) : (Î¸1,Î¸2,...)
j=1âˆˆÎ˜},
where Î˜ is some convex set./ne}ationslash
47
Step 3: Convex surrogate.
Given a convex set Fof soft classiï¬ers, using the rewriting in (1.2), we need to solve that
minimizes the empirical classiï¬cation error
1min
fâˆˆFn
Ï•1 I(f(Xi)Yi),n/summationdisplay
i=1âˆ’
However, while we are now working with a convex constraint, our objective is still not
convex: we need a surrogate for the classiï¬cation error.
Deï¬nition: A function Ï•: IR/maâˆšstoâ†’IR+is called a convex surrogate if it is a convex
non-decreasing function such that Ï•(0) = 1 and Ï•(z)â‰¥Ï•1 I(z) for allzâˆˆIR.
The following is a list of convex surrogates of loss functions.
â€¢Hinge loss: Ï•(z) = max(1+ z,0).
â€¢Exponential loss: Ï•(z) = exp(z).
â€¢Logistic loss: Ï•(z) = log2(1+exp( z)).
To bypass the nonconvexity of Ï•1 I(Â·), we may use a convex surrogate Ï•(Â·) in place of
Ë† Ï•1 I(Â·) and consider the minimizing the empirical Ï•-riskRn,Ï•deï¬ned by
1Ë†Rn,Ï•(f) =nn/summationdisplay
i=1Ï•(âˆ’Yif(Xi))
It is the empi
rical counterpart of the Ï•-riskRÏ•deï¬ned by
RÏ•(f) = IE[Ï•(âˆ’Yf(X))].
1.3Ï•-risk minimization
In this section, we will derive the relation between the Ï•-riskRÏ•(f) of a soft classiï¬er fand
the classiï¬cation error R(h) = IP(h(X) =Y) of its associated hard classiï¬er h= sign(f)
Let
fâˆ—
Ï•= argmin E[Ï•(Y
fâˆˆIRXâˆ’f(X))]
where the inï¬mum is taken over all measurable functions f:X â†’IR.
To verify that minimizing the Ï•serves our purpose, we will ï¬rst show that if the convex
surrogate Ï•(Â·) is diï¬€erentiable, then sign( fâˆ—
Ï•(X))â‰¥0 is equivalent to Î·(X)â‰¥1/2 where
Î·(X) = IP(Y= 1|X). Conditional on {X=x}, we have
IE[Ï•(âˆ’Yf(X))|X=x] =Î·(x)Ï•(âˆ’f(x))+(1âˆ’Î·(x))Ï•(f(x)).
Let
HÎ·(Î±) =Î·(x)Ï•(âˆ’Î±)+(1âˆ’Î·(x))Ï•(Î±) (1.3)/ne}ationslash
48
so that
fâˆ—
Ï•(x) = argmin Hâˆ—Î·(Î±),andRÏ•= minRÏ•(f) = minHÎ·)
Î± fâˆˆIRX(x)(Î± .
Î±âˆˆIR âˆˆIR
SinceÏ•(Â·) is diï¬€erentiable, setting thederivative of Hâˆ—Î·(Î±) to zero gives fÏ•(x) =Î±Â¯, where
Hâ€²
Î·(Î±Â¯) =âˆ’Î·(x)Ï•â€²(âˆ’Î±Â¯)+(1âˆ’Î·(x))Ï•â€²(Î±Â¯) = 0,
which gives
Î·(x)Ï•â€²(Î±Â¯)=1âˆ’Î·(x)Ï•â€²(âˆ’Î±Â¯)
SinceÏ•(Â·)isaconvex function, itsderivative Ï•â€²(Â·) isnon-decreasing. Thenfromtheequation
above, we have the following equivalence relation
1Î·(x)â‰¥ â‡”Î±Â¯â‰¥0â‡”sign(fâˆ—
2Ï•(x))â‰¥0. (1.4)
Since the equivalence relation holds for all xâˆˆ X,
1Î·(X)â‰¥ â‡”sign(fâˆ—
Ï•(X))2â‰¥0.
The following lemma shows that if the excessÏ•-riskR(f)âˆ’Râˆ—Ï• Ï•of a soft classiï¬er fis
small, then the excess-risk of its associated hard classiï¬er sign( f) is also small.
Lemma (Zhangâ€™s Lemma [Zha04]): LetÏ•: IR/maâˆšstoâ†’IR+be a convex non-decreasing
function such that Ï•(0) = 1. Deï¬ne for any Î·âˆˆ[0,1],
Ï„(Î·) := inf HÎ·(Î±).
Î±âˆˆIR
If there exists c >0 andÎ³âˆˆ[0,1] such that
1|Î·âˆ’c2| â‰¤(1âˆ’Ï„(Î·))Î³,âˆ€Î·âˆˆ[0,1], (1.5)
then
R(sign(f))âˆ’Râˆ—â‰¤2c(RÏ•(f)âˆ’Râˆ—
Ï•)Î³
Proof.Note ï¬rst that Ï„(Î·)â‰¤HÎ·(0) =Ï•(0) = 1 so that condition (2.5) is well deï¬ned.
Next, let hâˆ—= argminhâˆˆ{âˆ’1,1}XIP[h(X) =Y] = sign(Î·âˆ’1/2) denote the Bayes classiï¬er,
whereÎ·= IP[Y= 1|X=x], . Then it is easy to verify that
R(sign(f))âˆ’Râˆ—= IE[|2Î·(X)âˆ’1|1I(sign(f(X)) =hâˆ—(X))]
= IE[|2Î·(X)âˆ’1|1I(f(X)(Î·(X)âˆ’1/2)<0)]
â‰¤2cIE[((1âˆ’Ï„(Î·(X)))1I(f(X)(Î·(X)âˆ’1/2)<0))Î³]
â‰¤2c(IE[(1âˆ’Ï„(Î·(X)))1I(f(X)(Î·(X)âˆ’1/2)<0)])Î³,
where the last inequality above follows from Jensenâ€™s inequality./ne}ationslash
/ne}ationslash
49
We are going to show that for any xâˆˆ X, it holds
(1âˆ’Ï„(Î·))1I(f(x)(Î·(x)âˆ’1/2)<0)]â‰¤IE[Ï•(âˆ’Yf(x))|X=x]âˆ’Râˆ—
Ï•.(1.6)
This will clearly imply the result by integrating with respect to x.
Recall ï¬rst that
IE[Ï•(âˆ’Yf(x))|X=x] =HÎ·(x)(f(x)) and Râˆ—
Ï•= minHÎ·(x)(Î±) =Ï„(Î·(x)).
Î±âˆˆIR
so that (2.6) is equivalent to
(1âˆ’Ï„(Î·))1I(f(x)(Î·(x)âˆ’1/2)<0)]â‰¤HÎ·(x)(Î±)âˆ’Ï„(Î·(x))
Since the right-hand side above is nonnegative, the case where f(x)(Î·(x)âˆ’1/2)â‰¥0 follows
trivially. If f(x)(Î·(x)âˆ’1/2)<0, (2.6) follows if we prove that HÎ·(x)(Î±)â‰¥1. The convexity
ofÏ•(Â·) gives
HÎ·(x)(Î±) =Î·(x)Ï•(âˆ’f(x))+(1âˆ’Î·(x))Ï•(f(x))
â‰¥Ï•(âˆ’Î·(x)f(x)+(1âˆ’Î·(x))f(x))
=Ï•((1âˆ’2Î·(x))f(x))
â‰¥Ï•(0) = 1,
where the last inequality follows from the fact that Ï•is non decreasing and f(x)(Î·(x)âˆ’
1/2)<0. This completes the proof of (2.6) and thus of the Lemma.
IT is not hard to check the following values for the quantities Ï„(Î·),candÎ³for the three
losses introduced above:
â€¢Hinge loss: Ï„(Î·) = 1âˆ’|1âˆ’2Î·|withc= 1/2 andÎ³= 1.
â€¢Exponential loss: Ï„(Î·) = 2/radicalbig
Î·(1âˆ’Î·) withc= 1/âˆš
2 andÎ³= 1/2.
â€¢Logistic loss: Ï„(Î·) =âˆ’Î·logÎ·âˆ’(1âˆ’Î·)log(1âˆ’Î·) withc= 1/âˆš
2 andÎ³= 1/2.
50
18.657: Mathematics of Machine Learning
Lecturer: Philippe Rigollet Lecture 9
Scribe:Xuhong Zhang Oct. 7, 2015
Recall that last lecture we talked about convex relaxation of the original problem
n1Ë†h= argmin 1I( h(Xi) =Yi)
hâˆˆHn/summationdisplay
i=1
by considering soft classiï¬ers (i.e. whose output is in [ âˆ’1,1] rather than in {0,1}) and
convex surrogates of the loss function (e.g. hinge loss, exponential loss, logistic loss):
n1Ë† Ë† f= argminRÏ•,n(f) = argmin Ï•(Yif(Xi))
fâˆˆF fâˆˆFn/summationdisplay
i=1âˆ’
Ë† Ë†Andh= sign(f) will be used as the â€˜hardâ€™ classiï¬er.
Ë† Â¯ Â¯ We want to bound the quantity RÏ•(f)âˆ’RÏ•(f), wheref= argminfâˆˆFRÏ•(f).
Ë† Ë† (1)f= argminfâˆˆFRÏ•,n(f), thus
Ë† Â¯ Ë†Â¯Ë†Â¯Ë†Ë†Ë†Ë† Ë† Â¯ RÏ•(f) =RÏ•(f)+RÏ•,n(f)âˆ’RÏ•,n(f)+RÏ•,n(f)âˆ’RÏ•,n(f)+RÏ•(f)âˆ’RÏ•(f)
â‰¤Â¯Ë†Â¯Ë†Ë† Ë† Â¯ RÏ•(f)+RÏ•,n(f)âˆ’RÏ•,n(f)+RÏ•(f)âˆ’RÏ•(f)
â‰¤Â¯ Ë† RÏ•(f)+2sup |RÏ•,n(f)âˆ’RÏ•(f)
fâˆˆF|
Ë† (2) Let us ï¬rst focus on E[supfâˆˆF|RÏ•,n(f)âˆ’RÏ•(f)|]. Using the symmetrization trick as
before, weknowit isupper-boundedby2 Rn(Ï•â—¦F), wheretheRademacher complexity
n1Rn(Ï•â—¦F) = sup E[sup|/summationdisplay
ÏƒiÏ•(âˆ’Yif(Xi)) ]
X1,...,Xn,Y1,...,YnfâˆˆFni=1|
One thing to notice is that Ï•(0) = 1 for the loss functions we consider (hinge loss,
exponential loss and logistic loss), but in order to apply contraction inequality later,
we requireÏ•(0) = 0. Let us deï¬ne Ïˆ(Â·) =Ï•(Â·)âˆ’1. ClearlyÏˆ(0) = 0, and
n1E[sup|/summationdisplay
(Ï•(âˆ’Yif(Xi))âˆ’E[Ï•(âˆ’Yif(Xi))]) ]
fâˆˆFni=1|
n1=E[sup|/summationdisplay
(Ïˆ(âˆ’Yif(X)âˆ’Ei) [Ïˆ( i
1âˆ’Yif(X))])
fâˆˆFni=|]
â‰¤2Rn(Ïˆâ—¦F)
(3) The Rademacher complexity of Ïˆâ—¦ Fis still diï¬ƒcult to deal with. Let us assume
thatÏ•(Â·) isL-Lipschitz, (as a result, Ïˆ(Â·) is alsoL-Lipschitz), apply the contraction
inequality, we have
Rn(Ïˆâ—¦F)â‰¤2LRn(F)/ne}ationslash
51
(4) LetZi= (Xi,Yi),i= 1,2,...,nand
n1g(Z1,Z2Ë† ,...,Zn) = sup|RÏ•,n(f)âˆ’RÏ•(f) =
f|sup
âˆˆF fâˆˆF|n/summationdisplay
(Ï•(
i=1âˆ’Yif(Xi))âˆ’E[Ï•(âˆ’Yif(Xi))])|
SinceÏ•(Â·) ismonotonically increasing, itisnotdiï¬ƒculttoverifythat âˆ€Z1,Z2,...,Zn,Zâ€²
i
1 2L|g(Z1,...,Zi,...,Zn)âˆ’g(Z1,...,Zâ€²
i,...,Zn)| â‰¤(Ï•(1)âˆ’Ï•(nâˆ’1))â‰¤n
The last inequality holds since gisL-Lipschitz. Apply Bounded Diï¬€erence Inequality,
2t2
P(|s|Ë† Ë† upRÏ•,n(f)âˆ’RÏ•(f)|âˆ’E[sup|RÏ•,n(f)âˆ’RÏ•(f)|]>
F âˆˆF|t)â‰¤2exp(
âˆˆâˆ’ )
f/summationtextnf i=(2L)21n
Set the RHS of above equation to Î´, we get:
log(2/Î´)Ë† Ë† supR E Ï•,n(f)RÏ•(f) [sup RÏ•,n(f)
fâˆˆF| âˆ’ | â‰¤
fâˆˆF| âˆ’RÏ•(f)|]+2L/radicalbigg
2n
with probability 1 âˆ’Î´.
(5) Combining (1) - (4), we have
Ë† Â¯RÏ•(f)â‰¤RÏ•(f)+8LRn(F)+2L/radicalbigg
log(2/Î´)
2n
with probability 1 âˆ’Î´.
1.4 Boosting
Inthissection, wewillspecializetheaboveanalysistoaparticularlearningmodel: Boosting.
The basic idea of Boosting is to convert a set of weak learners (i.e. classiï¬ers that do better
than random, but have high error probability) into a strong one by using the weighted
average of weak learnersâ€™ opinions. More precisely, we consider the following function class
M
F={/summationdisplay
Î¸jhj(Â·) :|Î¸|1â‰¤1,hj:X /maâˆšstoâ†’[âˆ’1,1],jâˆˆ {1,2,...,Ma
j=1}re classiï¬ers }
and we want to upper bound Rn(F) for this choice of F.
n M n1 1Rn(F) = sup E[sup ÏƒiYif(Xi) ] = sup E[ supÎ¸jYiÏƒihj(Xi) ]
Z1,...,ZnfâˆˆF|n/summationdisplay
nZ |Î¸|1â‰¤11|
1,...,Zi=n|/summationdisplay
j=1/summationdisplay
i=1|
Letg(Î¸) =|/summationtextM
j=1Î¸j/summationtextn
i=1YiÏƒihj(Xi)|. It is easy to see that g(Î¸) is a convex function, thus
sup|Î¸|1â‰¤1g(Î¸) is achieved at a vertex of the unit â„“1ball{Î¸:/bardblÎ¸/bardbl1â‰¤1}. Deï¬ne the ï¬nite set
ï£«Y1h1(X1)ï£¶ ï£«Y1h2(X1)ï£¶ ï£«Y1hM(X1)/braceleftï£¬iggï£¬Y2h1(X2)ï£· ï£¬Y2h2(X2)ï£· ï£¬ Y2hM(X2)
BX,Y/definesï£¬., ,...,.ï£¶
Â±ï£­ï£¬
.Â± Â±/bracerightï£¬igg
Ynh1(Xn)ï£·ï£·
ï£¸ï£· ï£¬ï£¬
ï£­. ..
Ynh2(Xn)ï£·
ï£¸ï£·ï£­ï£¬ï£¬
.. .
YnhM(Xn)ï£·ï£·ï£¸
52
Then
Rn(F) = supRn(BX,Y).
X,Y
Notice max bâˆˆBX,Ybâˆš| |2â‰¤nand|BX,Y|= 2M. Therefore, using a lemma from Lecture 5,
we get
2log(2B 2 4RX,Y)log(M)
n(BX,Y)â‰¤max
bâˆˆBX,Y|b|2/radicalbig
| |
nâ‰¤/radicalbigg
n
Thus for Boosting,/bracketleftbig /bracketrightbig
2log(4M) log(2 /Î´)Ë† Â¯RÏ•(f)â‰¤RÏ•(f)+8L/radicalbigg
+2L/radicalbigg
with probability 1 - Î´n 2n
To get some ideas of what values Lusually takes, consider the following examples:
(1) for hinge loss, i.e. Ï•(x) = (1+x)+,L= 1.
(2) for exponential loss, i.e. Ï•(x) =ex,L=e.
(3) for logistic loss, i.e. Ï•(x) = log2(1+ex),L=e
1+elog2(e)â‰ˆ2.43
Ë† Â¯ Now we have bounded RÏ•(f)âˆ’RÏ•(f), but this is not yet the excess risk. Excess risk is
Ë† deï¬ned asR(f)âˆ’R(fâˆ—), wherefâˆ—= argminfRÏ•(f). The following theorem provides a
bound for excess risk for Boosting.
Theorem: LetF={/summationtextM
j=1Î¸jhj:/bardblÎ¸/bardbl1â‰¤1,hjsare weak classiï¬ers }andÏ•is anL-
Ë† Ë† Ë† Lipschitz convex surrogate. Deï¬ne f= argminfâˆˆFRÏ•,n(f) andh= sign(f). Then
Î³ Î³
âˆ—/parenleftbigâˆ—/parenrightbigÎ³/parenleftï£¬igg
2log(4M) log(2/Î´)Ë†R(h)âˆ’Râ‰¤2cinfRÏ•(f)âˆ’RÏ•(f) +2c8L +
fâˆˆF/radicalbigg
n/parenrightï£¬igg
2c/parenleftï£¬igg
2L/radicalbigg
2n/parenrightï£¬igg
with probability 1 âˆ’Î´
Proof.
Ë†R(h)âˆ’Râˆ—â‰¤2c/parenleftbigÎ³RÏ•(f)âˆ’RÏ•(fâˆ—)/parenrightbig
/parenleftï£¬iggÎ³
âˆ— 2log(4M) log(2 /Î´)â‰¤2cinfRÏ•(f)RÏ•(f)+8L +2L
fâˆˆFâˆ’/radicalbigg
n/radicalbigg
2n/parenrightï£¬igg
Î³
âˆ—Î³ 2log(4M) log(2/Î´)â‰¤2cinfRÏ•(f)âˆ’RÏ•(f) +2c
fâˆˆF/parenleftï£¬igg
8L/radicalbigg
n/parenrightï£¬igg
+2c/parenleftï£¬igg
2L/radicalbigg
2n/parenrightï£¬iggÎ³/parenleftbig /parenrightbig
Here the ï¬rst inequality uses Zhangâ€™s lemma and the last one uses the fact that for aiâ‰¥0
andÎ³âˆˆ[0,1], (a1+aÎ³2+a3)â‰¤aÎ³
1+aÎ³
2+aÎ³
3.
1.5 Support Vector Machines
In this section, we will apply our analysis to another important learning model: Support
Vector Machines (SVMs). We will see that hinge loss Ï•(x) = (1 +x)+is used and the
associated function class is F={f:/bardblf/bardblWâ‰¤Î»}whereWis a Hilbert space. Before
analyzing SVMs, let us ï¬rst introduce Reproducing Kernel Hilbert Spaces (RKHS).
53
1.5.1 Reproducing Kernel Hilbert Spaces (RKHS)
Deï¬nition: A function K:X Ã—X /maâˆšstoâ†’ IR is called a positive symmetric deï¬nite kernel
(PSD kernel) if
(1)âˆ€x,xâ€²âˆˆ X,K(x,xâ€²) =K(xâ€²,x)
(2)âˆ€nâˆˆZ+,âˆ€x1,x2,...,xn, then
thÃ—nmatrix with K(xi,xj) as its element in ithrow
andjcolumn is positive semi-deï¬nite. In other words, for any a1,a2,...,anâˆˆIR,
/summationdisplay
aiajK(xi,xj) 0
i,jâ‰¥
Let us look at a few examples of PSD kernels.
Example 1 LetX= IR,K(x,xâ€²) =/an}bracketle{tx,xâ€²/an}bracketri}htIRdis a PSD kernel, since âˆ€a1,a2,...,anâˆˆIR
/summationdisplay
aiaj/an}bracketle{txi,xj/an}bracketri}htIRd=/summationdisplay
/an}bracketle{taixi,ajxj/an}bracketri}htIRd=/an}bracketle{t/summationdisplay
aixi,/summationdisplay
ajxj/an}bracketri}htIRd=/bardbl/summationdisplay
a2ixi/bardblIRd0
i,j i,j i j iâ‰¥
Example 2 The Gaussian kernel K(x,xâ€²) = exp(âˆ’1 2
2/bardblâ€²
2xâˆ’x/bardblIRd) is also a PSD kernel.Ïƒ
Note that here and in the sequel, /bardbl Â· /bardblWand/an}bracketle{tÂ·,Â·/an}bracketri}htWdenote the norm and inner product
of Hilbert space W.
Deï¬nition: LetWbe a Hilbert space of functions X /maâˆšstoâ†’IR. A symmetric kernel K(Â·,Â·)
is called reproducing kernel ofWif
(1)âˆ€xâˆˆ X, the function K(x,Â·)âˆˆW.
(2)âˆ€xâˆˆ X,fâˆˆW,/an}bracketle{tf(Â·),K(x,Â·)/an}bracketri}htW=f(x).
If such aK(x,Â·) exists,Wis called a reproducing kernel Hilbert space (RKHS).
Claim: IfK(Â·,Â·) is a reproducing kernel for some Hilbert space W, thenK(Â·,Â·) is a
PSD kernel.
Proof.âˆ€a1,a2,...,anâˆˆIR, we have
/summationdisplay
aiajK(xi,xj) =/summationdisplay
aiaj/an}bracketle{tK(xi,Â·),K(xj,Â·)/an}bracketri}ht(sinceK(,) is reproducing)
i,j i,jÂ· Â·
=/an}bracketle{t/summationdisplay
aiK(xi,), ajK(xj,)W
iÂ·/summationdisplay
jÂ· /an}bracketri}ht
=/bardbl/summationdisplay
aiK(xi,
iÂ·)/bardbl2
Wâ‰¥0
54
In fact, the above claim holds both directions, i.e. if a kernel K(Â·,Â·) is PSD, it is also a
reproducing kernel.
A natural question to ask is, given a PSD kernel K(Â·,Â·), how can we build the corresponding
Hilbert space (for which K(Â·,Â·) is a reproducing kernel)? Let us look at a few examples.
Example 3 LetÏ•1,Ï•2,...,Ï•Mbe a set of orthonormal functions in L2([0,1]), i.e. for any
j,kâˆˆ {1,2,...,M}/integraldisplay
Ï•j(x)Ï•k(x)dx=
x/an}bracketle{tÏ•j,Ï•k/an}bracketri}ht=Î´jk
LetK(x,xâ€²) =/summationtextM
j=1Ï•j(x)Ï•j(xâ€²). We claim that the Hilbert space
M
W={/summationdisplay
ajÏ•j( :
=1Â·)a1,a2,...,aM
jâˆˆIR}
equipped with inner product /an}bracketle{tÂ·,Â·/an}bracketri}htL2is a RKHS with reproducing kernel K(Â·,Â·).
MProof. (1)K(x,Â·) =j=1Ï•j(x)Ï•j(Â·)âˆˆW. (Chooseaj=Ï•j(x)).
(2) Iff(Â·) =/summationtextM
j=1aj/summationtext
Ï•j(Â·),
M M M
/an}bracketle{tf(Â·),K(x,Â·)/an}bracketri}htL2=/an}bracketle{t/summationdisplay
ajÏ•j(Â·),/summationdisplay
Ï•k(x)Ï•k(Â·)/an}bracketri}htL2=/summationdisplay
ajÏ•j(x) =f(x)
j=1 k=1 j=1
(3)K(x,xâ€²) is a PSD kernel: âˆ€a1,a2,...,anâˆˆIR,
/summationdisplay
aiajK(x2i,xj) =/summationdisplay
aiajÏ•k(xi)Ï•k(xj) =/summationdisplay
(/summationdisplay
aiÏ•k(xi))
i,j i,j,k kiâ‰¥0
Example 4 IfX= IRd, andK(x,xâ€²) =/an}bracketle{tx,xâ€²/an}bracketri}htIRd, the corresponding Hilbert space is
W={/an}bracketle{tw,Â·/an}bracketri}ht:wâˆˆIRd}(i.e. all linear functions) equipped with the following inner product:
iff=/an}bracketle{tw,Â·/an}bracketri}ht,g=/an}bracketle{tv,Â·/an}bracketri}ht,/an}bracketle{tf,g/an}bracketri}ht/defines/an}bracketle{tw,v/an}bracketri}htIRd.
Proof. (1)âˆ€xâˆˆIRd,K(x,Â·) =/an}bracketle{tx,Â·/an}bracketri}htIRdâˆˆW.
(2)âˆ€f=/an}bracketle{tw,Â·/an}bracketri}htIRdâˆˆW,âˆ€xâˆˆIRd,/an}bracketle{tf,K(x,Â·)/an}bracketri}ht=/an}bracketle{tw,x/an}bracketri}htIRd=f(x)
(3)K(x,xâ€²) is a PSD kernel: âˆ€a1,a2,...,anâˆˆIR,
/summationdisplay
aiajK(xi,xj) =/summationdisplay
aiaj,
,j i,j/an}bracketle{txixj
i/an}bracketri}ht=/an}bracketle{t/summationdisplay
aixi,
i/summationdisplay
ajxj
j/an}bracketri}htIRd=/bardbl/summationdisplay
aix2iIRd0
i/bardbl â‰¥
55
18.657: Mathematics of Machine Learning
Lecturer: Philippe Rigollet Lecture 10
Scribe:Aden Forrow Oct. 13, 2015
Recall the following deï¬nitions from last time:
Deï¬nition: A function K:X Ã—X /maâˆšstoâ†’ Ris called a positive symmetric deï¬nite kernel
(PSD kernel) if
1.âˆ€x,xâ€²âˆˆ X,K(x,xâ€²) =K(xâ€²,x)
2.âˆ€nâˆˆZ+,âˆ€x1,x2,...,x n, thenÃ—nmatrix with entries K(xi,xj) is positive deï¬-
nite. Equivalently, âˆ€a R 1,a2,...,a nâˆˆ,
n/summationdisplay
aiajK(xi,xj)
i,j=1â‰¥0
Deï¬nition: LetWbe a Hilbert space of functions X /maâˆšstoâ†’R. A symmetric kernel K(Â·,Â·)
is called a reproducing kernel ofWif
1.âˆ€xâˆˆ X, the function K(x,Â·)âˆˆW.
2.âˆ€xâˆˆ X,âˆ€fâˆˆW,/an}bracketle{tf(Â·),K(x,Â·)/an}bracketri}htW=f(x).
If such a K(x,Â·) exists, Wis called a reproducing kernel Hilbert space (RKHS).
As before, /an}bracketle{tÂ·,Â·/an}bracketri}htWand/bardblÂ·/bardblWrespectively denote the inner product and norm of W. The
subscript Wwill occasionally be omitted. We can think of the elements of Was inï¬nite
linear combinations of functions of the form K(x,Â·). Also note that
/an}bracketle{tK(x,Â·),K(y,Â·)/an}bracketri}htW=K(x,y)
Since so many of our tools rely on functions being bounded, weâ€™d like to be able to
bound the functions in W. We can do this uniformly over xâˆˆ Xif the diagonal K(x,x) is
bounded.
Proposition: LetWbe a RKHS with PSD Ksuch that supxâˆˆXK(x,x) =kmaxis
ï¬nite. Then âˆ€fâˆˆW,
sup|f(x)| â‰¤ /bardblf/bardblW/radicalbig
kmax
xâˆˆX
.
Proof.We rewrite f(x) as an inner product and apply Cauchy-Schwartz.
f(x) =/an}bracketle{tf,K(x,Â·)/an}bracketri}htWâ‰¤ /bardblf/bardblW/bardblK(x,Â·)/bardblW
Now/bardblK(x,Â·)/bardbl2
W=/an}bracketle{tK(x,Â·),K(x,Â·)/an}bracketri}htW=K(x,x)â‰¤kmax. The result follows immediately.
56
1.5.2 Risk Bounds for SVM
We now analyze support vector machines (SVM) the same way we analyzed boosting. The
general idea is to choose a linear classiï¬er that maximizes the margin (distance to classiï¬ers)
while minimizing empirical risk. Classes that are not linearly separable can be embedded
in a higher dimensional space so that they are linearly separable. We wonâ€™t go into that,
however; weâ€™ll just consider the abstract optimization over a RKHS W.
Explicitly, we minimize the empirical Ï•-risk over a ball in Wwith radius Î»:
Ë† Ë† f= min Rn,Ï•(f)
fâˆˆW,/bardblf/bardblWâ‰¤Î»
Ë† Ë† Ë† The soft classiï¬er fis then turned into a hard classiï¬er h= sign(f). Typically in SVM Ï•
is the hinge loss, though all our convex surrogates behave similarly. To choose W(the only
other free parameter), we choose a PSD K(x1,x2) that measures the similarity between two
pointsx1andx2.
As written, this is an intractable minimum over an inï¬nite dimensional ball {f,/bardblf/bardblWâ‰¤
Î»}. The minimizers, however, will all be contained in a ï¬nite dimensional subset.
Theorem: Representer Theorem. LetWbe a RKHS with PSD Kand letG:
nR/maâˆšstoâ†’Rbe any function. Then
minG(f(x1), ...,f(xn)) = min G(f(x1),...,f(xn))
fâˆˆW,/bardblf/bardblâ‰¤Î» âˆˆÂ¯f Wn,/bardblf/bardblâ‰¤Î»
= min G(gÎ±(x1),...,gÎ±(xn)),
Î±âˆˆRn,Î±âŠ¤IKÎ±â‰¤Î»2
wheren
Â¯Wn={fâˆˆW|f(Â·) =gÎ±(Â·) =/summationdisplay
Î±iK(xi,
i=1Â·)}
and IK ij=K(xi,xj).
Proof. Â¯SinceWnis a linear subspace of W, we can decompose any f Wuniquely as
Â¯âŠ¥Â¯âˆˆÂ¯ âŠ¥âˆˆ
f=f+fwithf W nandfâˆˆÂ¯WâŠ¥
n. The Pythagorean theorem then gives
/bardblf/bardbl2
W=/bardblÂ¯f/bardbl2
W+/bardblfâŠ¥/bardbl2
W
Â¯ Moreover, since K(xi,Â·)âˆˆWn,
fâŠ¥(xi) =/an}bracketle{tfâŠ¥,K(xi,Â·)/an}bracketri}htW= 0
Â¯ Sof(xi) =f(xi) and
G(f(x1Â¯ Â¯ ),...,f(xn)) =G(f(x1),...,f(xn)).
Because fâŠ¥does not contribute to G, we can remove it from the constraint:
Â¯ Â¯ min G(f(x1), ...,f(xn)) = min G(f(x1),...,f(xn)).Â¯fâˆˆW,/bardblf/bardbl2+/bardblfâŠ¥/bardbl2â‰¤Î»2 fâˆˆ /bardblÂ¯W, f/bardbl2â‰¤Î»2
57
Â¯ Restricting to fâˆˆWnnow does not change the minimum, which gives us the ï¬rst equality.
For the second, we need to show that /bardblgÎ±/bardblWâ‰¤Î»is equivalent to Î±âŠ¤IKÎ±â‰¤Î»2.
/bardblgÎ±/bardbl2=/an}bracketle{tgÎ±,gÎ±
n/an}bracketri}ht
n
=/an}bracketle{t/summationdisplay
Î±iK(xi,
i=1Â·),
n/summationdisplay
Î±jK(xj,
=1Â·)
j/an}bracketri}ht
=/summationdisplay
Î±iÎ±j/an}bracketle{tK(xi,(
,j=1Â·),K xj,
iÂ·)/an}bracketri}ht
n
=/summationdisplay
Î±iÎ±jK(xi,xj)
i,j=1
=Î±âŠ¤IKÎ±
Weâ€™ve reduced the inï¬nite dimensional problem to a minimization over Î±âˆˆnR. This
works because weâ€™re only interested in Gevaluated at a ï¬nite set of points. The matrix
IK here is a Gram matrix, though we will not not use that. IK should be a measure of the
similarity of the points xi. For example, we could have W={/an}bracketle{tx,Â·/an}bracketri}htRd,xâˆˆdR}withK(x,y
the usual inner product K(x,y) =/an}bracketle{tx,y/an}bracketri}htRd.
Ë† Ë† Weâ€™ve shown that fonly depends on Kthrough IK, but does Rn,Ï•depend on K(x,y)
forx,yâˆˆ/{xi}? It turns out not to:
n n n1Ë†Rn,Ï•=/summationdisplay 1Ï•(âˆ’YigÎ±(xi)) =/summationdisplay
Ï•(âˆ’Yi/summationdisplay
Î±jK(xj,xi)).n ni=1 i=1 j=1
The last expression only involves IK. This makes it easy to encode all the knowledge about
our problem that we need. The hard classiï¬er is
n
Ë† Ë† h(x) = sign( f(x)) = sign( gÎ±Ë†(x)) = sign(/summationdisplay
Î±Ë†jK(xj,x))
j=1
If we are given a new point xn+1, we need to compute a new column for IK. Note that
xn+1must be in some way comparable or similar to the previous {xi}for the whole idea of
extrapolating from data to make sense.
The expensive part of SVMs is calculating the nÃ—nmatrix IK. In some applications,
IK may be sparse; this is faster, but still not as fast as deep learning. The minimization
over the ellipsoid Î±âŠ¤IKÎ±requires quadratic programming, which is also relatively slow. In
practice, itâ€™s easier to solve the Lagrangian form of the problem
n1Î±Ë† = argmin/summationdisplay
Ï•(âˆ’YigÎ±(xâ€² âŠ¤i))+Î» Î±IKÎ±
Î±âˆˆRnni=1
This formulation is equivalent to the constrained one. Note that Î»andÎ»â€²are diï¬€erent.
SVMs have few tuning parameters and so have less ï¬‚exibility than other methods.
We now turn to analyzing the performance of SVM.
58
Theorem: Excess Risk for SVM. LetÏ•be anL-Lipschitz convex surrogate and
Ë† Ë† Wa RKHS with PSD Ksuch that max x|K(x,x)|=kmax<âˆ. Lethn,Ï•= signfn,Ï•,
Ë†wherefn,Ï•is the empirical Ï•-risk minimizer over F={f
Ë†Ë†Ë†âˆˆW./bardblf/bardblWâ‰¤Î»}(that is,
Rn,Ï•(fn,Ï•)â‰¤Rn,Ï•(f)âˆ€fâˆˆ F). Suppose Î»âˆškmaxâ‰¤1. Then
Ë†R(hn,Ï•)âˆ’Râˆ—â‰¤2c/parenleftbigg Î³ Î³Î³kmax 2log(2/Î´)inf (RÏ•(f)âˆ’Râˆ—
Ï•)/parenrightbigg
+2c/parenleftBigg
8LÎ» +
fâˆˆ/radicalbigg
2L
F n/parenrightBigg
2c/parenleftBigg/radicalbigg
n/parenrightBigg
with probability 1 âˆ’Î´. The constants candÎ³are those from Zhangâ€™s lemma. For the
hinge loss, c=1
2andÎ³= 1.
Proof.The ï¬rst term comes from optimizing over a restricted set Finstead of all classiï¬ers.
The third term comes from applying the bounded diï¬€erence inequality. These arise in
exactly the same way as they do for boosting, so we will omit the proof for those parts. For
the middle term, we need to show that Rn,Ï•(F)â‰¤Î»kmax
n.
First,|f(x)| â‰¤ /bardblf/bardblWâˆškmaxâ‰¤Î»âˆškmaxâ‰¤1 for all/radicalBig
fâˆˆ F, so we can use the contraction
inequality to replace Rn,Ï•(F) withRn(F). Next weâ€™ll expand f(xi) inside the Rademacher
complexity and bound inner products using Cauchy-Schwartz.
n1Rn(F) = sup Esup Ïƒif(xi)
x1,...,xn/bracketleftBigg
fâˆˆF/vextendsingle/vextendsingle
/vextendsingle
ni=1/vextendsingle/vextendsingle/bracketrightBigg/summationdisplay
/vextendsingle/vextendsingle
/vextendsingle
n1= sup E/bracketleftBigg
sup/vextendsingle/vextendsingle
/vextendsingle/summationdisplay
Ïƒ/vextendsingle
iK(x/vextendsingle
/an}bracketle{ti,Â·),fnx1,...,xnfâˆˆFi=1/an}bracketri}ht/vextendsingle/vextendsingle/bracketrightBigg
n1/vextendsingle
= sup E/vextendsingle/vextendsingle
/bracketleftBigg
sup/vextendsingle/vextendsingle/vextendsingle/an}bracketle{t/summationdisplay
ÏƒiK(xi,Â·),f/vextendsingle
n/vextendsingle
x1,...,xnfâˆˆFi=1/an}bracketri}ht/vextendsingle/vextendsingle/bracketrightBigg
/vextendsingle
Î»/vextendsingle/vextendsingle /vextendsingle
â‰¤sup/radicaltp
/radicalvertex/radicalvertex
/radicalbtE/bracketleftBiggn
/bardbl/summationdisplay
ÏƒiK(x2i,/vextendsingle
nx1,...,xni=1Â·)/bardblW/bracketrightBigg
Now,
n n n
2E/bracketleftBigg
/bardbl/summationdisplay
ÏƒiK(xi,Â·)/bardblW/bracketrightBigg
=Eï£®
ï£°/an}bracketle{t/summationdisplay
ÏƒiK(xi,Â·),/summationdisplay
ÏƒjK(xj,
i=1 i=1 j=1Â·)/an}bracketri}htWï£¹
nï£»
=/summationdisplay
/an}bracketle{tK(x E i,Â·),K(xj,Â·)/an}bracketri}ht[ÏƒiÏƒj]
i,j=1
n
=
i/summationdisplay
K(xi,xj)Î´ij
,j=1
â‰¤nkmax
SoRn(F)â‰¤Î»kmax
nandwearedonewiththenewpartsoftheproof. Theremainderfollows
as with boosti/radicalBig
ng, using symmetrization, contraction, the bounded diï¬€erence inequality, and
Zhangâ€™s lemma.
59
18.657: Mathematics of Machine Learning
Lecturer: Philippe Rigollet Lecture 11
Scribe: Kevin Li Oct. 14, 2015
2. CONVEX OPTIMIZATION FOR MACHINE LEARNING
In this lecture, we will cover the basics of convex optimization as it applies to machine
learning. There is much more to this topic than will be covered in this class so you may beinterested in the following books.
Convex Optimization by Boyd and Vandenberghe
Lecture notes on Convex Optimization by Nesterov
Convex Optimization: Algorithms and Complexity by Bubeck
Online Convex Optimization by Hazan
The last two are drafts and can be obtained online.
2.1 Convex Problems
A convex problem is an optimization problem of the form min f(x) wherefand are
x2CC
convex. First, we will debunk the idea that convex problems are easy by showing that
virtually all optimization problems can be written as a convex problem. We can rewrite anoptimization problem as follows.
minf(x), mint, mint
X2X tf(x);x2X (x;t)2epi(f )
where the epigraph of a function is dened by
epi(f)=f(x;t )2X IR :tf(x)g
, ,
f 2   g
Figure 1: An example of an epigraph.
Source: https://en.wikipedia.org/wiki/Epigraph_(mathematics)
60
Now we observe that for linear functions,
minc>x= min c>x
x2D x2conv (D)
where the convex hull is dened
N N
conv(D) =fy:9N2Z+;x1;:::;xN2D;i0;X
i= 1;y =
i=1X
ixi
i=1g
To prove this, we know that the left side is a least as big as the right side since Dconv(D).
For the other direction, we have
N
minc>x= min min min c>ixix2conv (D) N x 1;:::;x N2D1;:::; N
NX
i=1
= min min min ic>ximinc>x
N x 1;:::;x N2D1;:::; NX
=1
x2Di
N
min min min iminc>x
N x 1;:::;x N2D1;:::; NX
xi=12D
= minc>x
x2D
Therefore we have
minf(x) min
x2X, t
(x;t)2conv (epi(f ))
which is a convex problem.
Why do we want convexity? As we will show, convexity allows us to infer global infor-
mation from local information. First, we must dene the notion of subgradient .
Denition (Subgradient): LetC I Rd,f:C! I R. A vector g2I Rdis called a
subgradient offatx2Cif
f(x) f(y)g>(x y)8y2C:
The set of such vectors gis denoted by @f(x).
Subgradients essentially correspond to gradients but unlike gradients, they always ex-
ist for convex functions, even when they are not dierentiable as illustrated by the next
theorem.
Theorem: Iff:C ! I R is convex, then for all x,@f(x) =;. In addition, if fis
dierentiable at x, then@f(x) =frf(x)g.
Proof. Omitted. Requires separating hyperplanes for convex sets.6f 9 2 2  g



,
! 2
2
     2
! 6=;
fr g8
6
61
Theorem: Letf;Cbe convex. If xis a local minimum of fonC, then it is also global
minimum. Furthermore this happens if and only if 0 2@f(x).
Proof. 02@f(x) if and only if f(x)âˆ’f(y)0 for ally2C. This is clearly equivalent to
xbeing a global minimizer.
Next assumexis a local minimum. Then for all y2Cthere exists "small enough such
thatf(x)f(1âˆ’")x+"y
(1âˆ’")f(x)+"f(y)=)f(x)f(y) for ally2C.
Not only do we know that local minimums are global minimums, looking at the subgra-
dient also tells us where the minimum can be. If g>(xâˆ’y)<0 thenf(x)<f(y). This
meansf(y) cannot possibly be a minimum so we can narrow our search to ys such that
g>(xâˆ’y). In one dimension, this corresponds to the half line fy2IR :yxgifg>0 and
the half linefy2IR :yxgifg<0 . This concept leads to the idea of gradient descent.
2.2 Gradient Descent
yxandfdierentiable the rst order Taylor expansion of fatxyieldsf(y)f(x)+
g>(yâˆ’x). This means that
minf(x+"^)minf(x)+g>("^)
j^j2=1
gwhich is minimized at ^=âˆ’ . Therefore to minimizes the linear approximation of fatjgj2x, one should move in direction opposite to the gradient.
Gradient descent is an algorithm that produces a sequence of points fxjgj1such that
(hopefully) f(xj+1)<f(xj).
2
2  2
2
  )  2
f 2  g
f 2  g
 

f g
Figure 2: Example where the subgradient of x1is a singleton and and the subgradient of
x2contains multiple elements.
Source: https://optimization.mccormick.northwestern.edu/index.php/
Subgradient_optimization
62
Algorithm 1 Gradient Descent algorithm
Input:x12C, positive sequence fsgs1
fors= 1 tok 1do
xs+1=xs sgs; gs2@f(xs)
end for
k1return Eitherx =kX
xsorx
s=12argminf(x)
x2fx 1;:::;x kg
Theorem: Letfbe a convex L-Lipschitz function on I Rdsuch thatx2argminI Rdf(x)
exists. Assume that jx1 xj2R. Then ifRs==Lpfor allsk1, then
k1 LRf(kX
xs)
s=1 f(x)p
k
andLRminf(xs)
1s k f(x)p
 k
Proof. Using the fact that gs=1(x2s+1 + xs) and the equality 2a>b=kak kbk2 ka bk2,
1f(xs) f(x)gs>(xs x) = (xs xs+1)>(xs x)
1=x2sxs+1 +x x2x2s s+1x
2h
k   k k   k  k   k
 1i
=2kg2sk+ (2
2s 2
s+1)
where we have dened s=kxs xk. Using the Lipschitz condition
 1f(xs) f(x)L2+ (2
2 2s 2
s+1)
Taking the average from 1, to kwe get
k1X   1 R2
f(xs)f(x)L2 1   + (2
k1 2
ks)L2
+1 +2
2 2 2k1L2+2 2 2ks=1
Taking=R
Lpto minimize the expression, we obtaink
k1
kX LRf(xs)
s=1 f(x)p
k
k
Noticing that the left-hand side of the inequality is larger than both f(Pxs) f(x) by
s=1
Jensen's inequality and min f(xs)
1sk f(x) respectively, completes the proof.2 f g
 
  2
2
2
j   j  
  
  p
  k k kk k k
        
k   k k   k  k   k
k k  
k   k
    
      
  p
 
 p
   
63
One 
aw with this theorem is that the step size depends on k. We would rather have
step sizessthat does not depend on kso the inequalities hold for all k. With the new step
sizes,
XkXk k k2
s21[X R2
sf(x) f x)]L 2 L
s ( + (2
s s+1)2+2 2s2 2s=1 s=1 s=1X
s=1
After dividing byPk
Ps=1s, wePwould like the right-hand side to approach 0. For this to
2happen we need Ps!0 ands!1. One candidate for the step size is s=G
spsinces
k
thenP k
2
sc1G2log(k ) andsc2Gp
k. So we get
s=1 sP
=1
Xkc1sk 1X GLlogk R2
s[f(xs)f(x)]
2c2p +
k 2c2Gp
ks=1 s=1  
ChoosingGappropriately, the right-hand side approaches 0 at the rate of LRlogk. Noticepk
that we get an extra factor of log k. However, if we look at the sum from k=q
2 tokinstead
k
of 1 tok,P k
2
sc0
1G2andPsc0
2Gp
k. Now we have
=k s=1 s2
k k 1 cLRminf(xs)f(x) minf(xs)f(x)ss[f(xs)f(x)]
1sk  
ks k2  X
kX
k  p
 ks= s=2 2
which is the same rate as in the theorem and the step sizes are independent of k.
Important Remark: Note this rate only holds if we can ensure that jxk=2 xj2R
since we have replaced x1byxk=2in the telescoping sum. In general, this is not true for
gradient descent, but it will be true for projected gradient descent in the next lecture.
One nal remark is that the dimension ddoes not appear anywhere in the proof. How-
ever, the dimension does have an eect because for larger dimensions, the conditions fis
L-Lipschitz andjx1 xj2Rare stronger conditions in higher dimensions.     
!!1
 p
  pp
p
 p
        p
j   j 
j j   
64
18.657: Mathematics of Machine Learning
Lecturer: Philippe Rigollet Lecture 12
Scribe:Michael Traub Oct. 19, 2015
2.3 Projected Gradient Descent
In the original gradient descent formulation, we hope to optimize min xf(x) where nC Ca d âˆˆ
fare convex, but we did not constrain the intermediate xk. Projected gradient descent will
incorporate this condition.
2.3.1 Projection onto Closed Convex Set
First we must establish that it is possible to always be able to keep xkin the convex set C.
One approach is to take the closest point Ï€(xk)âˆˆ C.
Deï¬nition: LetCbe a closed convex subset of IRd. Thenâˆ€xâˆˆIRd, letÏ€(x)âˆˆ Cbe
the minimizer of
/baâˆ‡dblxâˆ’Ï€(x)/baâˆ‡dbl= minx z
zâˆˆC/baâˆ‡dbl âˆ’ /baâˆ‡dbl
where/baâˆ‡dblÂ·/baâˆ‡dbldenotes the Euclidean norm. Then Ï€(x) is unique and,
/an}bâˆ‡acketle{tÏ€(x)âˆ’x,Ï€(x)âˆ’z/an}bâˆ‡acketâˆ‡i}ht â‰¤0âˆ€zâˆˆ C (2.1)
Proof.From the deï¬nition of Ï€:=Ï€(x), we have /baâˆ‡dblxâˆ’Ï€/baâˆ‡dbl2â‰¤ /baâˆ‡dblxâˆ’v/baâˆ‡dbl2for anyvâˆˆ C. Fix
wâˆˆ Cand deï¬ne v= (1âˆ’t)Ï€+twfortâˆˆ(0,1]. Observe that since Cis convex we have
vâˆˆ Cso that
/baâˆ‡dbl âˆ’ /baâˆ‡dbl2â‰¤ /baâˆ‡dbl âˆ’ /baâˆ‡dbl2 2x Ï€ x v =/baâˆ‡dblxâˆ’Ï€âˆ’t(wâˆ’Ï€)/baâˆ‡dbl
Expanding the right-hand side yields
/baâˆ‡dbl2 2 2xâˆ’Ï€/baâˆ‡dbl â‰¤ /baâˆ‡dblxâˆ’Ï€/baâˆ‡dbl âˆ’2t/an}bâˆ‡acketle{txâˆ’Ï€,wâˆ’Ï€/an}bâˆ‡acketâˆ‡i}ht+t2/baâˆ‡dblwâˆ’Ï€/baâˆ‡dbl
This is equivalent to
/an}bâˆ‡acketle{txâˆ’Ï€,wâˆ’ /an}bâˆ‡acketâˆ‡i}ht â‰¤2Ï€ t/baâˆ‡dblwâˆ’Ï€/baâˆ‡dbl
Since this is valid for all tâˆˆ(0,1), letting tâ†’0 yields (2.1).
Proof of Uniqueness. AssumeÏ€1,Ï€2âˆˆ Csatisfy
/an}bâˆ‡acketle{tÏ€1âˆ’x,Ï€1âˆ’z/an}bâˆ‡acketâˆ‡i}ht â‰¤0âˆ€zâˆˆC
/an}bâˆ‡acketle{tÏ€2âˆ’x,Ï€2âˆ’z/an}bâˆ‡acketâˆ‡i}ht â‰¤0âˆ€zâˆˆC
Takingz=Ï€2in the ï¬rst inequality and z=Ï€1in the second, we get
/an}bâˆ‡acketle{tÏ€1âˆ’x,Ï€1âˆ’Ï€2/an}bâˆ‡acketâˆ‡i}ht â‰¤0
/an}bâˆ‡acketle{txâˆ’Ï€2,Ï€1âˆ’Ï€2/an}bâˆ‡acketâˆ‡i}ht â‰¤0
Adding these two inequalities yields /baâˆ‡dblÏ€1âˆ’Ï€2/baâˆ‡dbl2â‰¤0 so that Ï€1=Ï€2.
65
2.3.2 Projected Gradient Descent
Algorithm 1 Projected Gradient Descent algorithm
Input:x1âˆˆ C, positive sequence {Î·s}sâ‰¥1
fors= 1 tokâˆ’1do
ys+1=xsâˆ’Î·sgs, gsâˆˆâˆ‚f(xs)
xs+1=Ï€(ys+1)
end for
k1return EitherxÂ¯ =/summationdisplay
xsorxâ—¦âˆˆargmin f(x)kxs={x1,...,x1âˆˆ k}
Theorem: LetCbe a closed, nonempty convex subset of IRdsuch that diam( C)â‰¤R.
Letfbe a convex L-Lipschitz function on
RCsuch that xâˆ—âˆˆargminxf(x) exists.âˆˆC
Then ifÎ·sâ‰¡Î·=Lâˆšthenk
LR LRf(xÂ¯)âˆ’f(xâˆ—)â‰¤âˆšandf(xÂ¯â—¦)âˆ’f(xâˆ—)
kâ‰¤âˆš
k
Moreover, if Î·s=Râˆš, thenâˆƒc >0 such thatL s
LR LRf(xÂ¯)âˆ’f(xâˆ—)â‰¤câˆšandf(xÂ¯â—¦)f(xâˆ—)
kâˆ’ â‰¤ câˆš
k
Proof.Again we will use the identity that 2 aâŠ¤b=/baâˆ‡dbla/baâˆ‡dbl2+/baâˆ‡dblb/baâˆ‡dbl2âˆ’/baâˆ‡dbl2aâˆ’b/baâˆ‡dbl.
By convexity, we have
f(xs)âˆ’f(xâˆ—)â‰¤gsâŠ¤(xsâˆ’xâˆ—)
1= (xsâˆ’ys+1)âŠ¤(xsÎ·âˆ’xâˆ—)
1=2Î·/bracketleftBig
/baâˆ‡dbl2xsâˆ’ys+1/baâˆ‡dbl+/baâˆ‡dblxsâˆ’xâˆ—/baâˆ‡dbl2âˆ’/baâˆ‡dblys+1âˆ’xâˆ—/baâˆ‡dbl2/bracketrightBig
Next,
/baâˆ‡dblys+1âˆ’xâˆ—/baâˆ‡dbl2=/baâˆ‡dbl2ys+1âˆ’xs+1/baâˆ‡dbl+/baâˆ‡dblxs+1âˆ’xâˆ—/baâˆ‡dbl2+2/an}bâˆ‡acketle{tys+1
2âˆ’xs+1,xs+1âˆ’xâˆ—/an}bâˆ‡acketâˆ‡i}ht
=/baâˆ‡dblys+1âˆ’xs+1/baâˆ‡dbl+/baâˆ‡dbl2xs+1âˆ’xâˆ—/baâˆ‡dbl+2/an}bâˆ‡acketle{tys+1âˆ’Ï€(ys+1),Ï€(ys+1)âˆ’xâˆ—/an}bâˆ‡acketâˆ‡i}ht
â‰¥ /baâˆ‡dblxs+1âˆ’xâˆ—/baâˆ‡dbl2
where we used that /an}bâˆ‡acketle{txâˆ’Ï€(x),Ï€(x)âˆ’z/an}bâˆ‡acketâˆ‡i}ht â‰¥0âˆ€zâˆˆ C, andxâˆ—
2 2âˆˆ C. Also notice that
/baâˆ‡dblxsâˆ’2y2 2s+1/baâˆ‡dbl=Î·/baâˆ‡dblgs/baâˆ‡dbl â‰¤Î· LsincefisL-Lipschitz with respect to /baâˆ‡dblÂ·/baâˆ‡dbl. Using this
we ï¬nd
k k1
k/summationdisplay 1 1( )âˆ’(âˆ—)â‰¤/summationdisplay2 2+âˆ—2 âˆ—2f xsf x Î· L x sx x s+1xk2Î·s=1 s=1/bracketleftBig
/baâˆ‡dbl âˆ’ /baâˆ‡dbl âˆ’/baâˆ‡dbl âˆ’ /baâˆ‡dbl/bracketrightBig
Î·L212Î·L2R2
â‰¤+x2k/baâˆ‡dbl1âˆ’xâˆ—
2Î·/baâˆ‡dbl â‰¤ +2 2Î·k
66
2 2Minimizing over Î·we getL=R
2=â‡’Î·=Râˆš, completing the proof22Î· k L k
RLf(xÂ¯)âˆ’f(xâˆ—)â‰¤âˆš
k
2
Moreover, the proof of the bound for f(/summationtextk
kxs)âˆ’f(xâˆ—) is identical because xkxs=/vextenddouble/vextenddouble
2âˆ’âˆ—
2/vextenddouble/vextenddoubleâ‰¤
R2as well./vextenddouble /vextenddouble
2.3.3 Examples
Support Vector Machines
The SVM minimization as we have shown before is
n1minn/summationdisplay
max(0,1YifÎ±(Xi))
Î±R
âŠ¤âˆˆIn
â‰¤2i=1âˆ’
Î±IKÎ± C
wherefÎ±(Xi) =Î±âŠ¤IKei=/summationtextn
=1Î±jK(X ,j jXi). Forconvenience, call gi(Î±) = max(0 ,1âˆ’YifÎ±(Xi)).
In this case executing the projection onto the ellipsoid {Î±:Î±âŠ¤IKÎ±â‰¤C2}is not too hard,
but we do not know about C,R, orL. We must determine these we can know that our
bound is not exponential with respect to n. First we ï¬nd Land start with the gradient of
gi(Î±):
âˆ‡gi(Î±) = 1I(1âˆ’YifÎ±(Xi)â‰¥0)YiIKei
Ë† With this we bound the gradient of the Ï•-riskRn,Ï•(fÎ±) =1
n
n n/summationtextn
=1gi(Î±i).
/vextenddouble/vextenddoubleâˆ‚ 1 1/vextenddoubleRË†n,Ï•(fÎ±)/vextenddouble/vextenddouble/summationdisplay/vextenddouble=/vextenddouble/vextenddouble/vextenddoubleâˆ‡gi(Î±)âˆ‚Î±/vextenddouble/vextenddoublen/vextenddouble
/vextenddoublei=1/vextenddouble/vextenddoubleâ‰¤/summationdisplay
IKe/vextenddouble /vextenddouble in2
i=1/baâˆ‡dbl /baâˆ‡dbl
by the triangle inequality and the fact that that 1I(1/vextenddouble
âˆ’YifÎ±(Xi)â‰¥0)Yiâ‰¤1. We can now
use the properties of our kernel K. Notice that /baâˆ‡dblIKei
1
2/baâˆ‡dblis theâ„“2norm of the ithcolumn so
/baâˆ‡dblIKei/baâˆ‡dbln
2=/parenleftBig/summationtext
j=1K(Xj,Xi)2/parenrightBig
. We also know that
K(Xj,Xi)2=/an}bâˆ‡acketle{tK(X2j,Â·),K(Xi,Â·)/an}bâˆ‡acketâˆ‡i}ht â‰¤ /baâˆ‡dblK(Xj,Â·)/baâˆ‡dblKH/baâˆ‡dbl(Xi,Â·)/baâˆ‡dblHâ‰¤kmax
Combining all of these we get
1/vextenddouble n n2/vextenddoubleâˆ‚ 1/vextenddoubleRË†n,Ï•(fÎ±)/vextenddoubleï£«
â‰¤maxï£¶
/summationdisplay /summationdisplay/vextenddouble/vextenddoubleï£­k2ï£¸=kmaxâˆšn=L/vextenddoubleâˆ‚Î±/vextenddoubleni=1j=1
To ï¬ndRwe try to evaluate diam {Î±âŠ¤IKÎ±â‰¤C2}= 2 maxâˆš
Î±
Î±âŠ¤âŠ¤Î±. We can use the
IKÎ±â‰¤C2
condition to put bounds on the diameter
C2 2Câ‰¥Î±âŠ¤IKÎ±â‰¥Î»min(IK)Î±âŠ¤Î±=â‡’diam{Î±âŠ¤IKÎ±â‰¤C2} â‰¤/radicalbig
Î»min(IK)
We need to understand how small Î»mincan get. While it is true that these exist random
samples selected by an adversary that make Î»min= 0, we will consider a random sample of
67
i.i.dX1,...,X nâˆ¼ N(0,Id). This we can write these d-dimensional samples as a dÃ—nmatrix
X. We can rewrite the matrix IK with entries IK ij=K(Xi,Xj) =/an}bâˆ‡acketle{tXi,Xj/an}bâˆ‡acketâˆ‡i}htIRdas a Wishart
matrix IK = XâŠ¤X(in particular,1XdâŠ¤Xis Wishart). Using results from random matrix
theory, if we take n,dâ†’ âˆbut holdnas a constant Î³, thenÎ»(IK 2min) (dâ†’1âˆšâˆ’Î³) . Takingd
an approximation since we cannot take n,dto inï¬nity, we get
Î»min(IK)â‰ƒd/parenleftbiggn d1âˆ’2/radicalbigg
d/parenrightbigg
â‰¥2
using the fact that dâ‰«n. This means that Î»minbecoming too small is not a problem when
we model our samples as coming from multivariate Gaussians.
Now we turn our focus to the number of iterations k. Looking at our bound on the
excess risk
nRË†n,Ï•(f Ë†Î±â—¦
R)â‰¤minRn,Ï•(fÎ±)+C/radicalbigg
kmax
Î±âŠ¤IKÎ±â‰¤C2 kÎ»min(IK)
we notice that our all of the constants in our stochastic term can be computed given the
number of points and the kernel. Since statistical error is often âˆš1, to be generous we wantn
to have precision up to1
nto allow for fast rates in special cases. This gives us
n3k2C2
kâ‰¥max
Î»min(IK)
which is not bad since nis often not very big.
In [Bub15], the rates for many a wide rage of problems with various assumptions are
available. For example, if we assume strong convexity and Lipschitz we can get an exponen-
tial rate so kâˆ¼logn. If gradient is Lipschitz, then we get get1
kinstead of âˆš1in the bound.k
However, often times we are not optimizing over functions with these nice properties.
Boosting
We already know that Ï•isL-Lipschitz for boosting because we required it before.
Remember that our optimization problem is
n1min/summationdisplay
Ï•(âˆ’YifÎ±(Xi))
Î±RNn
|Î±âˆˆI
|1â‰¤i=11
wherefÎ±=N
j=1Î±jfjandfjis thejthweak classiï¬er. Remember before we had some rate
like/radicalBig
logNc/summationtext
nand we would hope to get some other rate that grows with log NsinceNcan
be very large. Taking the gradient of the Ï•-loss in this case we ï¬nd
N1âˆ‡RË†n,Ï•(fÎ±) =/summationdisplay
Ï•â€²(âˆ’YifÎ±(Xi))(âˆ’Yi)F(Xi)ni=1
whereF(x) is the column vector [ f1(x),...,fN(x)]âŠ¤. Since|Yi| â‰¤1 andÏ•â€²â‰¤L, we can
bound the â„“2norm of the gradient as
/vextenddouble nL /vextenddoubleâˆ‡RË†/vextenddoublen,Ï•(fÎ±)/vextenddouble/vextenddouble/vextenddouble
2â‰¤n/vextenddouble /vextenddouble/vextenddouble/vextenddouble/summationdisplay
F(X/vextenddouble i)/vextenddoublei=1/vextenddouble
n/vextenddouble
L/vextenddouble/vextenddouble
â‰¤n/summationdisplay
)
i=/baâˆ‡dblF(Xi
1/baâˆ‡dbl â‰¤Lâˆš
N
68
using triangle inequality and the fact that F(Xi) is aN-dimensional vector with each
component bounded in absolute value by 1.
Using the fact thâˆšat the diameter of the â„“1ball is 2, R= 2 and the Lipschitz associated
with our Ï•-risk isL NwhereLis the Lipschitz constant for Ï•. Our stochastic termRâˆšL
k
becomes 2 L/radicalBig
N
k. Imposing the same1
nerror as before we ï¬nd that kâˆ¼N2n, which is very
bad especially since we want log N.
2.4 Mirror Descent
Boosting is an example of when we want to do gradient descent on a non-Euclidean space,
in particular a â„“1space. While the dual of the â„“2-norm is itself, the dual of the â„“1norm is
theâ„“or sup norm. We want this appear if we have an â„“1constraint. The reason for this âˆ
is not intuitive because we are taking about measures on the same space IRd, but when we
consider optimizations on other spaces we want a procedure that does is not indiï¬€erent to
the measure we use. Mirror descent accomplishes this.
2.4.1 Bregman Projections
Deï¬nition: If/baâˆ‡dblÂ·/baâˆ‡dblis some norm on IRd, then/baâˆ‡dblÂ·/baâˆ‡dblis its dual norm.âˆ—
Example: If dual norm of the â„“pnorm/baâˆ‡dblÂ·/baâˆ‡dblpis theâ„“qnorm/baâˆ‡dblÂ·/baâˆ‡dblq, then1
p+1
q= 1. This is the
limiting case of HÂ¨ olderâ€™s inequality.
In general we can also reï¬ne our bounds on inner products in IRdtoxâŠ¤yâ‰¤ /baâˆ‡dblx/baâˆ‡dbl/baâˆ‡dbly/baâˆ‡dblifâˆ—
we consider xto be the primal and yto be the dual. Thinking like this, gradients live in
the dual space, e.g. in gsâŠ¤(xâˆ’xâˆ—),xâˆ’xâˆ—is in the primal space, so gsis in the dual. The
transpose of the vectors suggest that these vectors come from spaces with diï¬€erent measure,
even though all the vectors are in IRd.
Deï¬nition: Convex function Î¦ on a convex set Dis said to be
(i) L-Lipschitz with respect to /baâˆ‡dblÂ·/baâˆ‡dblif/baâˆ‡dblg/baâˆ‡dblâˆ—â‰¤Lâˆ€gâˆˆâˆ‚Î¦(x)âˆ€xâˆˆD
(ii)Î±-strongly convex with respect to /baâˆ‡dblÂ·/baâˆ‡dblif
Î±Î¦(y)â‰¥Î¦(x)+gâŠ¤(yâˆ’x)+2/baâˆ‡dblyâˆ’x/baâˆ‡dbl2
for allx,yâˆˆDand forgâˆˆâˆ‚f(x)
Example: If Î¦ is twice diï¬€erentiable with Hessian Hand/baâˆ‡dblÂ·/baâˆ‡dblis theâ„“2norm, then all
eig(H)â‰¥Î±.
Deï¬nition (Bregman divergence): For a given convex function Î¦ on a convex set
Dwithx,yâˆˆ D, the Bregman divergence of yfromxis deï¬ned as
DÎ¦(y,x) = Î¦(y)âˆ’Î¦(x)âˆ’âˆ‡Î¦(x)âŠ¤(yâˆ’x)
69
This divergence is the error of the function Î¦( y) from the linear approximation at x.
Also note that this quantity is not symmetric with respect to xandy. If Î¦ is convex then
DÎ¦(y,x)â‰¥0 because the Hessian is positive semi-deï¬nite. If Î¦ is Î±-strongly convex then
DÎ¦(y,x)â‰¥Î±
2/baâˆ‡dblyâˆ’x/baâˆ‡dbl2and if the quadratic approximation is good then this approximately
holds in equality and this divergence behaves like Euclidean norm.
Proposition: Given convex function Î¦ on Dwithx,y,zâˆˆ D
(âˆ‡Î¦(x)âˆ’âˆ‡Î¦(y))âŠ¤(xâˆ’z) =DÎ¦(x,y)+DÎ¦(z,x)âˆ’DÎ¦(z,y)
Proof.Looking at the right hand side
= Î¦(x)âˆ’Î¦(y)âˆ’âˆ‡Î¦(y)âŠ¤(xâˆ’y)+Î¦(z)âˆ’Î¦(x)âˆ’âˆ‡Î¦(x)âŠ¤(zâˆ’x)
âˆ’/bracketleftBig
Î¦(z)âˆ’Î¦(y)âˆ’âˆ‡Î¦(y)âŠ¤(zâˆ’y)/bracketrightBig
=âˆ‡Î¦(y)âŠ¤(yâˆ’x+zâˆ’y)âˆ’âˆ‡Î¦(x)âŠ¤(zâˆ’x)
= (âˆ‡Î¦(x)âˆ’âˆ‡Î¦(y))âŠ¤(xâˆ’z)
Deï¬nition (Bregman projection): GivenxâˆˆIRd, Î¦aconvex diï¬€erentiablefunction
onD âŠ‚ DÂ¯ IRdand convex CâŠ‚, the Bregman projection of xwith respect to Î¦ is
Ï€Î¦(x)âˆˆargminDÏ†(x,z)
zâˆˆC
70
18.657: Mathematics of Machine Learning
Lecturer: Philippe Rigollet Lecture 13
Scribe:Mina Karzand Oct. 21, 2015
Previously, we analyzed the convergence of the projected gradient descent algorithm.
We proved that optimizing the convex L-Lipschitz function fon a closed, convex set Cwith
diam(C)â‰¤Rwith step sizes Î·s=Râˆšwould give us accuracy of f(x)â‰¤f(xâˆ—)+LR
L kâˆšafterk
kiterations.
Although it might seem that projected gradient descent algorithm provides dimension-
free convergence rate, it is not always true. Reviewing the proof of convergence rate, we
realize that dimension-free convergence is possible when the objective function fand the
constraint set Care well-behaved in Euclidean norm (i.e., for all xâˆˆ Candgâˆˆâˆ‚f(x), we
have that |x|2and|g|2are independent of the ambient dimension). We provide an examples
of the cases that these assumptions are not satisï¬ed.
â€¢Consider the diï¬€erentiable, convex function fon the Euclidean ball B2,nsuch that
/baâˆ‡dblâˆ‡f(x)/baâˆ‡dbl â‰¤1,âˆ€xâˆˆB2,n. This implies that f(x)âˆšnand the projected âˆ |âˆ‡ | 2â‰¤
gradient descent converges to the minimum of finBn2,/radicalï£¬ignat ratek. Using the
log(n)method of mirror descent we can get convergence rate of/radicalbig
k
To get better rates of convergence in the optimization problem, we can use the Mirror
Descent algorithm. The idea is to change the Euclidean geometry to a more pertinent
geometry to a problem at hand. We will deï¬ne a new geometry by using a function which
is sometimes called potential function Î¦( x). We will use Bregman projection based on
Bregman divergence to deï¬ne this geometry.
The geometric intuition behind the mirror Descent algorithm is the following: The
projected gradient described in previous lecture works in any arbitrary Hilbert space Hso
that thenormof vectors isassociated withaninnerproduct. Now, supposeweareinterested
in optimization in a Banach space D. In other words, the norm (or the measure of distance)
that we use does not derive from an inner product. In this case, the gradient descent does
not even make sense since the gradient âˆ‡f(x) are elements of dual space. Thus, the term
xâˆ’Î·âˆ‡f(x) cannot be performed. (Note that in Hilbert space used in projected gradient
descent, the dual space of His isometric to H. Thus, we didnâ€™t have any such problems.)
The geometric insight of the Mirror Descent algorithm is that to perform the optimiza-
tion in the primal space D, one can ï¬rst map the point xâˆˆ Din primal space to the dual
spaceDâˆ—, then perform the gradient update in the dual space and ï¬nally map the optimal
point back to the primal space. Note that at each update step, the new point in the primal
spaceDmight be outside of the constraint set C âŠ‚ D, in which case it should be projected
into the constraint set C. The projection associate with the Mirror Descent algorithm is
Bergman Projection deï¬ned based on the notion of Bergman divergence.
Deï¬nition (Bregman Divergence): Forgivendiï¬€erentiable, Î±-stronglyconvexfunc-
tion Î¦(x) :D â†’R, we deï¬ne the Bregman divergence associated with Î¦ to be:
DÎ¦(y,x) = Î¦(y)âˆ’Î¦(x)âˆ’âˆ‡Î¦(x)T(yâˆ’x)
71
We will usetheconvex openset D âŠ‚nRwhoseclosure contains theconstraint set C âŠ‚ D.
Bregman divergence is the error term of the ï¬rst order Taylor expansion of the function Î¦
inD.
Also, note that the function Î¦( x) is said to be Î±-strongly convex w.r.t. a norm /baâˆ‡dbl./baâˆ‡dblif
Î¦(y)âˆ’Î¦(x)âˆ’âˆ‡Î¦(x)T Î±(yâˆ’x)â‰¥2/baâˆ‡dblyâˆ’x/baâˆ‡dbl2.
We used the following property of the Euclidean norm:
2aâŠ¤b=/baâˆ‡dbla/baâˆ‡dbl2+/baâˆ‡dblb/baâˆ‡dbl2âˆ’/baâˆ‡dblaâˆ’b/baâˆ‡dbl2
in the proof of convergence of projected gradient descent, where we chose a=xsâˆ’ys+1and
b=xsâˆ’xâˆ—.
To prove the convergence of the Mirror descent algorithm, we use the following property
oftheBregmandivergence inasimilarfashion. Thispropositionshowsthat theBregmandi-
vergenceessentially behaves astheEuclideannormsquaredintermsof projections:
Proposition: GivenÎ±-strongly diï¬€erentiable convex function Î¦ : D â†’R, for all
x,y,zâˆˆ D,
[âˆ‡Î¦(x)âˆ’âˆ‡Î¦(y)]âŠ¤(xâˆ’z) =DÎ¦(x,y)+DÎ¦(z,x)âˆ’DÎ¦(z,y).
As described previously, the Bregman divergence is used in each step of the Mirror descent
algorithm to project the updated value into the constraint set.
Deï¬nition (Bregman Projection): GivenÎ±-strongly diï¬€erentiable convex function
Î¦ :D â†’Rand for all xâˆˆ Dand closed convex set C âŠ‚ D
Î Î¦(x) = argmin DC Î¦(z,x)
zâˆˆCâˆ©D
2.4.2 Mirror Descent Algorithm
Algorithm 1 Mirror Descent algorithm
Input:x1âˆˆargmin Î¦( x),Î¶:d dR R such that Î¶(x) = Î¦(x)Câˆ©D â†’ âˆ‡
fors= 1,Â·Â·Â·,kdo
Î¶(ys+1) =Î¶(xs)âˆ’Î·gsforgsâˆˆâˆ‚f(xs)
xs+1= Î Î¦(yCs+1)
end for
return Eitherx=1
k/summationtextk
s=1xsorxâ—¦âˆˆargminx x1, ,xkf(x)âˆˆ{ Â·Â·Â· }
Proposition: Letzâˆˆ C âˆ©D, thenâˆ€yâˆˆ D,
(âˆ‡Î¦(Ï€(y)âˆ’âˆ‡Î¦(y))âŠ¤(Ï€(y)âˆ’z)â‰¤0
72
Moreover, DÎ¦(z,Ï€(y))â‰¤DÎ¦(z,y).
Proof.Deï¬neÏ€= Î Î¦(y) andh(t) =DÎ¦(Ï€+t(zâˆ’Ï€),y).Sinceh(t) is minimized at t= 0C
(due to the deï¬nition of projection), we have
hâ€²(0) =âˆ‡xDÎ¦(x,y)|x=Ï€(zâˆ’Ï€)â‰¥0
where suing the deï¬nition of Bregman divergence,
âˆ‡xDÎ¦(x,y) =âˆ‡Î¦(x)âˆ’âˆ‡Î¦(y)
Thus,
(âˆ‡Î¦(Ï€)âˆ’âˆ‡Î¦(y))âŠ¤(Ï€âˆ’z)â‰¤0.
Using proposition 1, we know that
(âˆ‡Î¦(Ï€)âˆ’âˆ‡Î¦(y))âŠ¤(Ï€âˆ’z) =DÎ¦(Ï€,y)+DÎ¦(z,Ï€)âˆ’DÎ¦(z,y)â‰¤0,
and since DÎ¦(Ï€,y)â‰¥0, we would have DÎ¦(z,Ï€)â‰¤DÎ¦(z,y).
Theorem: Assume that fis convex and L-Lipschitz w.r.t. /baâˆ‡dbl./baâˆ‡dbl. Assume that Î¦ is
Î±-strongly convex on C âˆ© Dw.r.t./baâˆ‡dbl./baâˆ‡dbland
R2= sup Î¦( x) m
xâˆ’in Î¦(x)
âˆˆCâˆ©D xâˆˆCâˆ©D
ta/radicalï£¬igkex1= argminxÎ¦(x) (assume that it exists). Then, Mirror Descent with Î·=âˆˆCâˆ©D
R2Î±
L Rgives,
2 2f(x)âˆ’f(xâˆ—)â‰¤RL/radicalbigg
andf(xâ—¦)Î±kâˆ’f(xâˆ—)â‰¤RL/radicalbigg
,Î±k
Proof.Takexâ™¯âˆˆ C âˆ©D. Similar to the proof of the projected gradient descent, we have:
(i)
f(xs)âˆ’f(xâ™¯)â‰¤gsâŠ¤(xsâˆ’xâ™¯)
(ii)1= (Î¶(xâ™¯s)âˆ’Î¶(ys+1))âŠ¤(xs)Î·âˆ’x
(iii)1= ( Î¦( xs) Î¦(ys+1))âŠ¤(xsxâ™¯)Î·âˆ‡ âˆ’âˆ‡ âˆ’
(iv)1=/bracketleftï£¬ig
Dâ™¯Î¦(xs,ys+1)+Dâ™¯Î¦(x ,xs)Î·âˆ’DÎ¦(x ,ys+1)/bracketrightï£¬ig
(v)1â‰¤/bracketleftï£¬ig
DÎ¦(xs,ys+1)+DÎ¦(xâ™¯,xs)âˆ’DÎ¦(xâ™¯,xs+1)Î·/bracketrightï£¬ig
(vi)Î·L21â‰¤+/bracketleftï£¬ig
DÎ¦(xâ™¯,xs)2Î±2Î·âˆ’DÎ¦(xâ™¯,xs+1)/bracketrightï£¬ig
Where (i) is due to convexity of the function f.
73
Equations (ii) and (iii) are direct results of Mirror descent algorithm.
Equation (iv) is the result of applying proposition 1.
Inequality (v) is a result of the fact that x= Î Î¦ â™¯s+1 (yCs+1), thus for x
â™¯ â™¯âˆˆ C âˆ© D, we have
DÎ¦(x ,ys+1)â‰¥DÎ¦(x ,xs+1).
We will justify the following derivations to prove inequality (vi):
(a)DÎ¦(xs,ys+1) = Î¦(xs)âˆ’Î¦(ys+1)âˆ’âˆ‡Î¦(ys+1)âŠ¤(xsâˆ’ys+1)
(b) Î±â‰¤[âˆ‡Î¦(x2s)âˆ’âˆ‡Î¦(ys+1)]âŠ¤(xsâˆ’ys+1)âˆ’2/baâˆ‡dblys+1âˆ’xs/baâˆ‡dbl
(c) Î±â‰¤Î·/baâˆ‡dblgs/baâˆ‡dblâˆ—/baâˆ‡dblxsâˆ’ys+1/baâˆ‡dblâˆ’2/baâˆ‡dblys+1âˆ’xs/baâˆ‡dbl2
(d)Î·2L2
â‰¤.2Î±
Equation (a) is the deï¬nition of Bregman divergence.
To show inequality (b), we used the fact that Î¦ is Î±-strongly convex which implies that
Î¦(ys+1)âˆ’Î¦(xs)â‰¥ âˆ‡Î¦(xs)T(ys+1âˆ’xs)Î±
2/baâˆ‡dbly2s+1âˆ’xs/baâˆ‡dbl.
According to the Mirror descent algorithm, âˆ‡Î¦(xs)âˆ’ âˆ‡Î¦(ys+1) =Î·gs. We use HÂ¨ olderâ€™s
inequality to show that gsâŠ¤(xsâˆ’ys+1)â‰¤ /baâˆ‡dblgs/baâˆ‡dblâˆ—/baâˆ‡dblxsâˆ’ys+1/baâˆ‡dbland derive inequality (c).
Lookingatthequadraticterm axâˆ’bx2fora,b >0,itisnothardtoshowthatmax ax
aâˆ’bx2=
2. We use this statement with x=/baâˆ‡dblys+1âˆ’xs/baâˆ‡dbl,a=Î· gb/baâˆ‡dbls/baâˆ‡dblL4 âˆ—â‰¤andb=Î±to derive2
inequality (d).
Again, we use telescopic sum to get
k1/summationdisplay Î·L2D(xâ™¯Î¦,x1)[f(xs)f(xâ™¯)] + . (2.1)k 2Î± kÎ·s=1âˆ’ â‰¤
We use the deï¬nition of Bregman divergence to get
DÎ¦(xâ™¯,x1) = Î¦(xâ™¯)âˆ’Î¦(x1)âˆ’âˆ‡Î¦(x1)(xâ™¯âˆ’x1)
â‰¤Î¦(xâ™¯)âˆ’Î¦(x1)
â‰¤sup Î¦(x) min Î¦( x)
x xâˆ’
âˆˆCâˆ©D âˆˆCâˆ©D
â‰¤R2.
Where we used the fact x1âˆˆargmin Î¦( x) in the description of the Mirror Descent
â™¯Câˆ©D
algorithm to prove âˆ‡Î¦(x1)(xâˆ’x1)â‰¥0. We optimize the right hand side of equation (2.1)
forÎ·to get
k1/summationdisplay
(xâ™¯ 2[f(xs)âˆ’f)]ks=â‰¤RL
1/radicalbigg
.Î±k
To conclude the proof, let xâ™¯â†’xâˆ—âˆˆ C.
Note that with the right geometry, we can get projected gradient descent as an instance
the Mirror descent algorithm.
74
2.4.3 Remarks
The Mirror Descent is sometimes called Mirror Prox. We can write xs+1as
xs+1= argmin DÎ¦(x,ys+1)
xâˆˆCâˆ©D
= argminÎ¦( x)
xâˆ’âˆ‡Î¦âŠ¤(ys+1)x
âˆˆCâˆ©D
= argminÎ¦( x) xs
xâˆ’[âˆ‡Î¦( )âˆ’Î·gs]âŠ¤x
âˆˆCâˆ©D
= argmin Î·(gsâŠ¤x)+Î¦(x)
xâˆ’âˆ‡Î¦âŠ¤(xs)x
âˆˆCâˆ©D
= argmin Î·(gsâŠ¤x)+DÎ¦(x,xs)
xâˆˆCâˆ©D
Thus, we have
xs+1= argmin Î·(gsâŠ¤x)+DÎ¦(x,xs).
xâˆˆCâˆ©D
To getxs+1, in the ï¬rst term on the right hand side we look at linear approximations
close toxsin the direction determined by the subgradient gs. If the function is linear, we
would just look at the linear approximation term. But if the function is not linear, the
linear approximation is only valid in a small neighborhood around xs. Thus, we penalized
by adding the term DÎ¦(x,xs). We can penalized by the square norm when we choose
DÎ¦(x,xs) =/baâˆ‡dblxâˆ’xs/baâˆ‡dbl2. In this case we get back the projected gradient descent algorithm
as an instance of Mirror descent algorithm.
But if we choose a diï¬€erent divergence DÎ¦(x,xs), we are changing the geometry and we
can penalize diï¬€erently in diï¬€erent directions depending on the geometry.
Thus, using the Mirror descent algorithm, we could replace the 2-norm in projected
gradient descent algorithm by another norm, hoping to get less constraining Lipschitz con-
stant. On the other hand, the norm is a lower bound on the strong convexity parameter.
Thus, there is trade oï¬€ in improvement of rate of convergence.
2.4.4 Examples
Euclidean Setup:
Î¦(x) =1x2, =dR, Î¦(x) =Î¶(x) =x. Thus, the updates will be similar to2/baâˆ‡dbl /baâˆ‡dbl D âˆ‡
the gradient descent.
1DÎ¦(y,x) =/baâˆ‡dbly/baâˆ‡dbl21âˆ’ /baâˆ‡dblx2
2/baâˆ‡dbl2x2âˆ’âŠ¤y+/baâˆ‡dblx/baâˆ‡dbl
1=/baâˆ‡dblxâˆ’y/baâˆ‡dbl2.2
Thus, Bregman projection with this potential function Î¦( x) is the same as the usual Eu-
clidean projection and the Mirror descent algorithm is exactly the same as the projected
descent algorithm since it has the same update and same projection operator.
Note that Î±= 1 since D1Î¦(y,x)â‰¥2/baâˆ‡dblxâˆ’y/baâˆ‡dbl2.
â„“1Setup:
We look at D=dR+\{0}.
75
Deï¬ne Î¦( x) to be the negative entropy so that:
d
Î¦(x) =/summationdisplay
xilog(xi), Î¶(x) =âˆ‡Î¦(x) ={1+log(xdi)
i=1}i=1
(s+1)âˆ‡(s)âˆ’(s+1)Thus, looking at the update function y= Î¦(x)Î·gs, we get log( yi) =
(s)log(xi)âˆ’(s) (s+1) ( s) (s)Î·giand for all i= 1,Â·Â·Â·,d, we have yi=xiexp(âˆ’Î·gi). Thus,
y(s)=x(s)exp(âˆ’Î·g(s)).
We call this setup exponential Gradient Descent or Mirror Descent with multiplicative
weights.
The Bregman divergence of this mirror map is given by
DÎ¦(y,x) = Î¦(y)âˆ’Î¦(x)âˆ’âˆ‡Î¦âŠ¤(x)(yâˆ’x)
/summationdisplayd /summationdisplayd d
=yilog(yi)âˆ’xilog(xi) (1+log( xi))(yixi)
i 1/summationdisplay
iâˆ’
=1 iâˆ’
= =1
/summationdisplaydyi=yilog( )+xii=1/summationdisplayd
(yi
i=1âˆ’xi)
Note that/summationtextd
i=1yilog(yi
i) is call the Kullback-Leibler divergence (KL-div) between yx
andx.
We show that the projection with respect to this Bregman divergence on the simplex
âˆ†d={xâˆˆdR:d
i=1xi= 1,xiâ‰¥0}amounts to a simple renormalization y/maâˆšstoâ†’y/|y|1. To
prove so, we prov/summationtext
ide the Lagrangian:
/summationdisplayd /summationdisplayd dyLi=yilog( )+ ( xixii=1 i=âˆ’yi)+Î»(
1/summationdisplay
xi
i=1âˆ’1).
To ï¬nd the Bregman projection, for all i= 1,Â·Â·Â·,dwe write
âˆ‚ yL âˆ’i= +1+ Î»= 0âˆ‚xixi
Thus, for all i, we have xi=Î³yi. We know that/summationtextd
i=1xi= 1. Thus, Î³=1/summationtextyi.
Thus, we have Î Î¦ y
âˆ†d(y) =
1. The Mirror Descent algorithm with this update and|y|
projection would be:
ys+1=xsexp(âˆ’Î·gs)
yxs+1=.|y|1
To analyze the rate of convergence, we want to study the â„“1norm on âˆ† d. Thus, we have
to show that for some Î±, Î¦ isÎ±-strongly convex w.r.t |Â·|1on âˆ†d.
76
DÎ¦(y,x) =KL(y,x)+/summationdisplay
(xi
iâˆ’yi)
=KL(y,x)
1â‰¥2|xâˆ’y|2
1
Where we used the fact that x,yâˆˆâˆ†dto showi(xiâˆ’yi) = 0 and used Pinsker
inequality show the result. Thu/summationtexts, Î¦ is 1-strongly conve/summationtext
x w.r.t.|Â·|1on âˆ†d.
Remembering that Î¦( x) =d
i=1xilog(xi) was deï¬ned to be negative entropy, we know
thatâˆ’log(d)â‰¤Î¦(x)â‰¤0 forxâˆˆâˆ†d. Thus,
R2= maxÎ¦( x)
xâˆˆâˆ†dâˆ’min Î¦(x) = log(d).
xâˆˆâˆ†d
Corollary: Letfbe a convex function on âˆ† dsuch that
/baâˆ‡dblg/baâˆ‡dblâˆâ‰¤L,âˆ€gâˆˆâˆ‚f(x),âˆ€xâˆˆâˆ†d.
2log(d)Then, Mirror descent with Î·=1/radicalï£¬ig
givesL k
2log(d) 2log(d)f(xk)âˆ’f(xâˆ—)â‰¤L/radicalbigg
, f(xâ—¦
k)âˆ’f(xâˆ—)kâ‰¤L/radicalbigg
k
Boosting: For weak classiï¬ers f1(x),Â·Â·Â·,fN(x) andÎ±âˆˆâˆ†n, we deï¬ne
N f1(x)
.fÎ±=ï£¶
/summationdisplay
Î±jfjandF(x) =ï£«
j=1ï£¬ï£­..
fN(x)ï£·ï£¸
so thatfÎ±(x) is the weighted majority vote classiï¬er. Note that |F|âˆâ‰¤1.
As shown before, in boosting, we have:
n
g=âˆ‡R/hatwide1
n,Ï†(fÎ±) =/summationdisplay
Ï†â€²(âˆ’yifÎ±(xi))(âˆ’yi)F(xi),ni=1
Since|F| â‰¤1 and|y| â‰¤1, then|g| â‰¤LwhereLis the Lipschitz constant of Ï† âˆ âˆ âˆ
(e.g., a constant like eor 2).
/radicalbigg
/hatwide /hatwide2log(N)Rn,Ï†(fÎ±â—¦
k)âˆ’minRn,Ï†(fÎ±)L
Î±âˆˆâˆ†nâ‰¤k
We need the number of iterations kâ‰ˆn2log(N).
The functions fjâ€™s could hit all the vertices. Thus, if we want to ï¬t them in a ball, the
/radicalï£¬igball has to be radiusâˆš
N. This is why the projected gradient descent would give the rate of
N
k. But by looking at the gradient we can determine the right geometry. In this case, the
gradient is bounded by sup-norm which is usually the most constraining norm in projected
77
gradient descent. Thus, using Mirror descent would be most beneï¬cial.
Other Potential Functions:
There are other potential functions which are strongly convex w.r.t â„“1norm. In partic-
ular, for
1Î¦(x) =|x|p 1
p, p= 1+p log(d)
then Î¦ is c/radicalbig
log(d)-strongly convex w.r.t â„“1norm.
78
18.657: Mathematics of Machine Learning
Lecturer: Philippe Rigollet Lecture 14
Scribe:Sylvain Carpentier Oct. 26, 2015
In this lecture we will wrap up the study of optimization techniques with stochastic
optimization. The tools that we are going to develop will turn out to be very eï¬ƒcient in
minimizing the Ï•-risk when we can bound the noise on the gradient.
3. STOCHASTIC OPTIMIZATION
3.1 Stochastic convex optimization
Weareconsideringrandomfunctions x/maâˆšstoâ†’â„“(x,Z)wherexistheoptimization parameterand
Za random variable. Let PZbe the distribution of Zand let us assume that x/maâˆšstoâ†’â„“(x,Z) is
convexPZa.s. In particular, IE[ â„“(x,Z)] will also be convex. The goal of stochastic convex
optimization is to approach min xIE[â„“(x,Z)] when is convex. For our purposes, will âˆˆC C C
be a deterministic convex set. However, stochastic convex optimization can be deï¬ned more
broadly. The constraint can be itself stochastic :
C={x,IE[g(x,Z)]â‰¤0}, gconvexPZa.s.
C={x,IP[g(x,Z)â‰¤0]â‰¥1âˆ’Îµ},â€œchance constraintâ€
The second constraint is not convex a priori but remedies are possible (see [NS06, Nem12]).
In the following, we will stick to the case where Xis deterministic. A few optimization
problems we tackled can be interpreted in this new framework.
3.1.1 Examples
Boosting. Recall that the goal in Boosting is to minimize the Ï•-risk:
minIE[Ï•(f
âˆˆÎ›âˆ’YÎ±(X))],
Î±
where Î› is the simplex of IRd. Deï¬ne Z= (X,Y) and the random function â„“(Î±,Z) =
Ï•(âˆ’YfÎ±(X)), convex PZa.s.
Linear regression. Here the goal is the minimize the â„“2risk:
min IE[(Yâˆ’f2Î±(X)) ].
Î±âˆˆIRd
Deï¬neZ= (X,Y) and the random function â„“(Î±,Z) = (Yâˆ’fÎ±(X))2, convex PZa.s.
Maximum likelihood. We consider samples Z1,...,Z niid with density pÎ¸,Î¸âˆˆÎ˜. For
instance, ZN(Î¸,1). The likelihood functions associated to this set of samples is Î¸ âˆ¼ /maâˆšstoâ†’/producttextn
=1pÎ¸(Zi). Letpâˆ—(Z) denote the true density of Zi (it does not have to be of the form pÎ¸
forsomeÎ¸âˆˆÎ˜. Then
n1/productdisplay/integraldisplaypâˆ—(z)IE[log pÎ¸(Zi)] =âˆ’log( ) pâˆ—(z)dz+C=n pÎ¸(z)i=1âˆ’KL(pâˆ—,pÎ¸)+C
79
whereCis a constant in Î¸. Hence maximizing the expected log-likelihood is equivalent to
minimizing the expected Kullback-Leibler divergence:
n
maxIE[log/productdisplay
pÎ¸(Zi)]
Î¸i=1â‡â‡’KL(pâˆ—,pÎ¸)
External randomization. Assume that we want to minimize a function of the form
1f(x) =n/summationdisplay
fi(x),ni=1
where the functions f1,...,fnare convex. As we have seen, this arises a lot in empirical
risk minimization. In this case, we treat this problem as deterministic problem but inject
artiï¬cial randomness as follows. Let Ibe a random variable uniformly distributed on
[n] =:{1,...,n}. We have the representation f(x) = IE[fI(x)], which falls into the context
of stochastic convex optimization with Z=Iandâ„“(x,I) =fI(x).
Important Remark :There is a key diï¬€erence between the case where we assume that
wearegivenindependentrandomvariablesandthecasewherewegenerateartiï¬cialran-
domness. Letusillustratethisdiï¬€erenceforBoosting. Wearegiven( X1,Y1),...,(Xn,Yn)
i.i.d from some unknown distribution. In the ï¬rst example, our aim is to minimize
IE[Ï•(âˆ’YfÎ±(X))] based on these nobservations and we will that the stochastic gradient
allows to do that by take one pair ( Xi,Yi) in each iteration. In particular, we can use
each pair at most once. We say that we do one pass on the data.
We could also leverage our statistical analysis of the empirical risk minimizer from
previous lectures and try to minimize the empirical Ï•-risk
1RË†n,Ï•(fÎ±) =n/summationdisplay
Ï•(Î±
i=1âˆ’Yif(Xi))n
by generating kindependent random variables I1,...,Ikuniform over [ n] and run the
stochasticgradientdescenttousonerandomvariable Ijineachiteration. Thediï¬€erence
hereisthat kcanbearbitrarylarge, regardlessofthenumber nofobservations(wemake
multiple passes onthedata). However, minimizingIE I[Ï•(âˆ’YIfÎ±(XI))|X1,Y1,...,X n,Yn]
will perform no better than the empirical risk minimizer whose statistical performance
is limited by the number nof observations.
3.2 Stochastic gradient descent
If the distribution of Zwas known, then the function x/maâˆšstoâ†’IE[â„“(x,Z)] would be known and
we could apply gradient descent, projected gradient descent or any other optimization tool
seen before in the deterministic setup. However this is not the case in reality where the
true distribution PZis unknown and we are only given the samples Z1,...,Z nand the
random function â„“(x,Z). In what follows, we denote by âˆ‚â„“(x,Z) the set of subgradients of
the function y/maâˆšstoâ†’â„“(y,Z) at point x.
80
Algorithm 1 Stochastic Gradient Descent algorithm
Input:x1âˆˆ C, positive sequence {Î·s}s1, independent random variables Z ,...,Z â‰¥ 1 k
with distribution PZ.
fors= 1 tokâˆ’1do
ys+1=xsâˆ’Î·sgËœs, gËœsâˆˆâˆ‚â„“(xs,Zs)
xs+1=Ï€(yCs+1)
end for
1return xÂ¯k=k
k/summationdisplay
xs
s=1
Note the diï¬€erence here with the deterministic gradient descent which returns either
xÂ¯korxâ—¦
k= argmin f(x). In the stochastic framework, the function f(x) = IE[â„“(x,Î¾)] is
x1,...,xn
typically unknown and xËškcannot be computed.
Theorem: LetCbea closed convex subset of IRdsuch that diam( C)â‰¤R. Assume that
he convex function f(x) = IE[â„“(x,Z)] attains its minimum on Catxâˆ—âˆˆIRd. Assume
thatâ„“(x,Z) is convex PZa.s. and that IE /baâˆ‡dblgËœ/baâˆ‡dbl2â‰¤L2for allgËœâˆˆâˆ‚â„“(x,Z) for allx. Then
ifÎ·sâ‰¡Î·=R
Lâˆš,k
LRIE[f(xÂ¯k)]âˆ’f(xâˆ—)â‰¤âˆš
k
Proof.
f xsf x g sxsx
= IE[gËœsâŠ¤(xsâˆ’xâˆ—)|xs]
1= IE[(ys+1xs)âŠ¤(xsxâˆ—)xs]Î·âˆ’ âˆ’ |
1= IE[/baâˆ‡dblx2 2sâˆ’y2s+1/baâˆ‡dbl+Î·/baâˆ‡dblxsâˆ’xâˆ—
2/baâˆ‡dbl âˆ’/baâˆ‡dblys+1âˆ’xâˆ—/baâˆ‡dbl |xs]
1â‰¤(Î·2IE[/baâˆ‡dblgËœs/baâˆ‡dbl2|xs]+IE[/baâˆ‡dblx2sâˆ’xâˆ—/baâˆ‡dbl |xs]âˆ’IE[/baâˆ‡dblxs+1âˆ’xâˆ—xÎ·/baâˆ‡dbl2
2|s]
Taking expectations and summing over swe get
k1/summationdisplay Î·L2R2
f(xs) (
s=1âˆ’f xâˆ—)kâ‰¤+.2 2Î·k
Using Jensenâ€™s inequality and chosing Î·=R
Lâˆš, we getk
LRIE[f(xÂ¯k)]âˆ’f(xâˆ—)â‰¤âˆš
k( )âˆ’(âˆ—)â‰¤âŠ¤(âˆ’âˆ—)
81
3.3 Stochastic Mirror Descent
We can also extend the Mirror Descent to a stochastic version as follows.
Algorithm 2 Mirror Descent algorithm
Input: x1âˆˆargmin Î¦( x),Î¶:dRâ†’dRsuch that Î¶(x) =âˆ‡Î¦(x), independentCâˆ©D
random variables Z1,...,Z kwith distribution PZ.
fors= 1,Â·Â·Â·,kdo
Î¶(ys+1) =Î¶(xs)âˆ’Î·gËœsforgËœsâˆˆâˆ‚â„“(xs,Zs)
xÎ¦s+1= Î  (yCs+1)
end for
return x=1k
k/summationtext
s=1xs
Theorem: Assume that Î¦ is Î±-strongly convex on C âˆ© Dw.r.t./baâˆ‡dblÂ·/baâˆ‡dbland
R2= sup Î¦( x) Î¦ x)
xâˆ’min (
âˆˆCâˆ©D xâˆˆCâˆ©D
takex1= argminxÎ¦(x) (assume that it exists). Then, Stochastic Mirror DescentâˆˆCâˆ©D
withÎ·=R
L/radicalBig
2Î±xRoutputs Â¯ k, such that
IE[f(xÂ¯k)]âˆ’f(xâˆ—)â‰¤RL/radicalbigg
2.Î±k
Proof.We essentially reproduce the proof for the Mirror Descent algorithm.
Takexâ™¯âˆˆ C âˆ©D. We have
f(xs ss
IE[gËœsâŠ¤(xsâˆ’xâˆ—)|xs]
1= IE[(Î¶(xs)Î·âˆ’Î¶(ys+1))âŠ¤(xsâˆ’xâ™¯)|xs]
1= IE[(âˆ‡Î¦(xs)âˆ’âˆ‡Î¦(ys+1))âŠ¤(xsÎ·âˆ’xâ™¯)|xs]
1= IE/bracketleftBig
Dâ™¯Î¦(xs,ys+1)+DÎ¦(xâ™¯,xs)âˆ’DÎ¦(x ,ys+1)Î·/vextendsingle/vextendsinglexs/bracketrightBig
1â‰¤IE/bracketleftBig
DÎ¦(xâ™¯s,ys+1)+DÎ¦(x ,xs)âˆ’DÎ¦(xâ™¯,xs+1)xÎ·/vextendsingle/vextendsingles/bracketrightBig
Î·â‰¤2IE[/baâˆ‡dblgËœs/baâˆ‡dbl21|xs]+ IE2Î±âˆ—Î·/bracketleftBig
DÎ¦(xâ™¯,xs)âˆ’DÎ¦(xâ™¯,xs+1)/vextendsingle/vextendsinglexs/bracketrightBig)âˆ’f(xâ™¯)â‰¤gâŠ¤(xâˆ’xâ™¯)
82
where the last inequality comes from
DÎ¦(xs,ys+1) = Î¦(xs)âˆ’Î¦(ys+1)âˆ’âˆ‡Î¦(ys+1)âŠ¤(xsâˆ’ys+1)
Î±â‰¤[âˆ‡Î¦(x2s)âˆ’âˆ‡Î¦(ys+1)]âŠ¤(xsâˆ’ys+1)âˆ’2/baâˆ‡dblys+1âˆ’xs/baâˆ‡dbl
Î±â‰¤Î·/baâˆ‡dblgËœs/baâˆ‡dblx y y x2âˆ—/baâˆ‡dblsâˆ’s+1/baâˆ‡dblâˆ’2/baâˆ‡dbls+1âˆ’s/baâˆ‡dbl
Î·2/baâˆ‡dblgËœâ‰¤s/baâˆ‡dbl2
âˆ—.2Î±
Summing and taking expectations, we get
k1/summationdisplay Î·L2DÎ¦(xâ™¯,x1)[f(xâ™¯s) ]
s=1â‰¤+kâˆ’f(x) . (3.1)2Î± kÎ·
We conclude as in the previous lecture.
3.4 Stochastic coordinate descent
Letfbe a convex L-Lipschitz and diï¬€erentiable function on IRd. Let us denote by âˆ‡ifthe
partial derivative of fin the direction ei. One drawback of the Gradient Descent Algorithm
is that at each step one has to update every coordinate âˆ‡ifof the gradient. The idea of
the stochastic coordinate descent is to pick at each step a direction ejuniformly and to
choose that ejto be the direction of the descent at that step. More precisely, of Iis drawn
uniformly on [ d], then IE[ dâˆ‡If(x)eI] =âˆ‡f(x). Therefore, the vector dâˆ‡If(x)eIthat has
only one nonzero coordinate is an unbiased estimate of the gradient âˆ‡f(x). We can use
this estimate to perform stochastic gradient descent.
Algorithm 3 Stochastic Coordinate Descent algorithm
Input: x1âˆˆ C, positive sequence {Î·s}s1, independent random variables I ,...,I â‰¥ 1 k
uniform over [ d].
fors= 1 tokâˆ’1do
ys+1=xsâˆ’Î·sdâˆ‡If(x)eI, gËœsâˆˆâˆ‚â„“(xs,Zs)
xs+1=Ï€(ys+1) C
end for
k1return xÂ¯k=k/summationdisplay
xs
s=1
If we apply Stochastic Gradient Descent to this problem for Î·=R/radicalBig
2, we directlyL dk
obtain
2dIE[f(xÂ¯k)]âˆ’f(xâˆ—)â‰¤RL/radicalbigg
k
We are in a trade-oï¬€ situation where the updates are much easier to implement but where
we need more steps to reach the same precision as the gradient descent alogrithm.
83
18.657: Mathematics of Machine Learning
Lecturer: Philippe Rigollet Lecture 15
Scribe:Zach Izzo Oct. 27, 2015
Part III
Online Learning
It is often the case that we will be asked to make a sequence of predictions, rather
than just one prediction given a large number of data points. In particular, this situa-
tion will arise whenever we need to perform online classiï¬cation: at time t, we have
(X1,Y1),...,(Xtâˆ’1,Ytâˆ’1) iid random variables, and given Xt, we are asked to predict
Ytâˆˆ {0,1}. Consider the following examples.
Online Shortest Path: We have a graph G= (V,E) with two distinguished vertices
sandt, and we wish to ï¬nd the shortest path from stot. However, the edge weights
E1,...,E tchange with time t. Our observations after time tmay be all of the edge weights
E1,...,E t; or our observations may only be the weights of edges through which our path
traverses; orourobservationmayonlybethesumoftheweightsoftheedgesweâ€™vetraversed.
Dynamic Pricing: We have a sequence of customers, each of which places a value vt
on some product. Our goal is to set a price ptfor thetth customer, and our reward for
doing so is ptifptâ‰¤vt(in which case the customer buys the product at our price) or 0
otherwise (in which case the customer chooses not to buy the product). Our observations
after time tmay bev1,...,vt; or, perhaps more realistically, our observations may only be
1I(p1< v1),...,1I(pt< vt). (In this case, we only know whether or not the customer bought
the product.)
Sequential Investment: GivenNassets, a portfolio is Ï‰âˆˆâˆ†N={xâˆˆIRn:xiâ‰¥
0,/summationtextNxi=1i= 1}. (Ï‰tells what percentage of our funds to invest in each stock. We could
also allow for negative weights, which would correspond to shorting a stock.) At each time
t, we wish to create a portfolio Ï‰tâˆˆâˆ†Nto maximize Ï‰T
tzt, whereztâˆˆIRNis a random
variable which speciï¬es the return of each asset at time t.
There are two general modelling approaches we can take: statistical or adversarial.
Statistical methods typically require that the observations are iid, and that we can learn
somethingaboutfuturepointsfrompastdata. Forexample, inthedynamicpricingexample,
we could assume vtâˆ¼N(v,1). Another example is the Markowitz model for the sequential
investment example, in which we assume that log( zt)âˆ¼ N(Âµ,Î£).
In this lecture, we will focus on adversarial models. We assume that ztcan be any
bounded sequence of numbers, and we will compare our predictions to the performance of
some benchmark. In these types of models, one can imagine that we are playing a game
against an opponent, and we are trying to minimize our losses regardless of the moves he
plays. In this setting, we will frequently use optimization techniques such as mirror descent,
as well as approaches from game theory and information theory.
84
1. PREDICTION WITH EXPERT ADVICE
1.1 Cumulative Regret
LetAbe a convex set of actions we can take. For example, in the sequential investment
example, A= âˆ†N. If our options are discreteâ€“for instance, choosing edges in a graphâ€“then
think ofAas the convex hull of these options, and we can play one of the choices randomly
according to some distribution. We will denote our adversaryâ€™s moves by Z. At time t,
we simultaneously reveal atâˆˆ Aandztâˆˆ Z. Denote by â„“(at,zt) the loss associated to the
player/decision maker taking action atand his adversary playing zt.
Inthegeneral case,/summationtextn
tâ„“=1(at,zt)canbearbitrarilylarge. Therefore, ratherthanlooking
at theabsoluteloss foraseriesof nsteps, wewill compareourloss totheloss ofabenchmark
called an expert. An expert is simply some vector bâˆˆ An,b= (b1,...,bt,...,bTn) . If we
chooseKexpertsb(1),...,b(K), then our benchmark value will be the minimum cumulative
loss amongst of all the experts:
n
(j)benchmark = min
â‰¤jâ‰¤K/summationdisplay
â„“(bt,zt).
1t=1
Thecumulative regret is then deï¬ned as
n n
Rn=/summationdisplay(j)â„“(at,zt)âˆ’min â„“(bt,zt).
1â‰¤jâ‰¤Kt=1/summationdisplay
t=1
At timet, we have access to the following information:
1. All of our previous moves, i.e. a1,...,atâˆ’1,
2. all of our adversaryâ€™s previous moves, i.e. z1,...,ztâˆ’1, and
3. All of the expertsâ€™ strategies, i.e. b(1),...,b(K).
Naively, one might try a strategy which chooses a=bâˆ— âˆ—tt, where bis the expert which
has incurred minimal total loss for times 1 ,...,tâˆ’1. Unfortunately, this strategy is easily
exploitable by the adversary: he can simply choose an action which maximizes the loss for
that move at each step. To modify our approach, we will instead take a convex combination
of the expertsâ€™ suggested moves, weighting each according to the performanceof that expert
thus far. To that end, we will replace â„“(at,zt) byâ„“(p,(bt,zt)), where pâˆˆâˆ†Kdenotes a
(1) ( K)convex combination, bt= (bt,...,bt)Tâˆˆ AKis the vector of the expertsâ€™ moves at time
t, andztâˆˆ Zis our adversaryâ€™s move. Then
n n
Rn=/summationdisplay
â„“(pt,zt)âˆ’min â„“(ej,zt)
1â‰¤jâ‰¤Kt=1/summationdisplay
t=1
whereejis the vector whose jth entry is 1 and the rest of the entries are 0. Since we are
restricting ourselves to convex combinations of the expertsâ€™ moves, we can write A= âˆ†K.
We can now reduce our goal to an optimization problem:
K n
min
âˆˆâˆ†K/summationdisplay
Î¸j
Î¸j=1/summationdisplay
â„“(ej,zt).
t=1
85
From here, one option would be to use a projected gradient descent type algorithm: we
deï¬ne
qt+1=ptâˆ’Î·(â„“(eT1,zt),...,â„“(eK,zT))
Kand then pâˆ†t+1=Ï€(pt) to be the projection of qt+1onto the simplex.
1.2 Exponential Weights
Suppose we instead use stochastic mirror descent with Î¦ = negative entropy. Then
qtqt+1,j=pt+1,jexp(âˆ’Î·â„“(ej,zt)), pt+1,j= ,/summationtextK
lq=1t+1,l
where we have deï¬ned
K/parenleftBigg
wt,jpt= ej, expKwj=1 l=1,l/parenrightBigg
wt,j=
t/parenleftBiggtâˆ’1 /summationdisplay
âˆ’Î·/summationdisplay
â„“(ej,zs)
s=1/parenrightBigg
.
This process looks at the los/summationtext
s from each expert and downweights it exponentially according
to the fraction of total loss incurred. For this reason, this method is called an exponential
weighting (EW) strategy .
Recall the deï¬nition of the cumulative regret Rn:
n n
Rn=/summationdisplay
â„“(pt,zt)âˆ’min
1â‰¤jâ‰¤Kt=1/summationdisplay
â„“(ej,zt).
t=1
Then we have the following theorem.
Theorem: Assume â„“(Â·,z) is convex for all zâˆˆ Zand that â„“(p,z)âˆˆ[0,1] for all pâˆˆ
âˆ†K,zâˆˆ Z. Then the EW strategy has regret
logK Î·nRnâ‰¤ +.Î·2
In particular, for Î·=/radicalBig
2logK,n
Rnâ‰¤/radicalbig
2nlogK.
Proof.We will recycle much of the mirror descent proof. Deï¬ne
K
ft(p) =/summationdisplay
pjâ„“(ej,zt).
j=1
Denote/bardblÂ·/bardbl:=|Â·|1. Then
n n1/summationdisplay Î·1/bardblg/bardbl2tlogKft( (âˆ—pt)âˆ’fâˆ—t)â‰¤n/summationtext
t=1p +,n 2 Î·nt=1
86
wheregtâˆˆâˆ‚ft(pt) and/bardbl Â· /bardblâˆ—is the dual norm (in this case /bardbl Â· /bardblâˆ—=| Â· |âˆ). The 2 in the
denominator of the ï¬rst term of this sum comes from setting Î±= 1 in the mirror descent
proof. Now,
gtâˆˆâˆ‚ft(pt)â‡’gt= (â„“(e1,zt),...,â„“(eTK,zt)).
Furthermore, since â„“(p,z)âˆˆ[0,1], we have /bardblgt/bardblâˆ—=|gt|âˆâ‰¤1 for all t. Thus
nÎ·1
n/summationtext
t=1/bardblgt/bardbl2
âˆ—logK Î· logK+ â‰¤+.2 nÎ·2Î·n
Substituting for ftyields
n K K n/summationdisplay/summationdisplay Î·nlogKpt,jâ„“(ej,zt)âˆ’min/summationdisplay
pjâ„“(ej,zt)â‰¤+.
pâˆˆâˆ†K 2Î·t=1j=1 j=1/summationdisplay
t=1
Note that the boxed term is actually min 1â‰¤jâ‰¤K/summationtextnâ„“t=1(ej,zt). Furthermore, applying
Jensenâ€™s to the unboxed term gives
n K n /summationdisplay/summationdisplay
pt,jâ„“(ej,zt)â‰¥/summationdisplay
â„“(pt,zt).
t=1j=1 t=1
Substituting these expressions then yields
Î·nlogKRnâ‰¤+.2Î·
We optimize over Î·to reach the desired conclusion.
We now oï¬€er a diï¬€erent proof of the same theorem which will give us the optimal
constant in the error bound. Deï¬ne
/parenleftBiggtâˆ’1/parenrightBiggK K/summationdisplay /summationdisplay/summationtextwt,jej=1 jwt,j= expâˆ’Î· â„“(ej,zs), Wt=wt,j, pt= .Wts=1 j=1
Fort= 1, we initialize w1,j= 1, soW1=K. It should be noted that the starting values for
w1,jare uniform, so weâ€™re starting at the correct point (i.e. maximal entropy) for mirrored
descent. Now we have
/parenleftbigg Kexpâˆ’tâˆ’1Î· â„“ ,zW j=1 s=1(ej s) exp(âˆ’Î·â„“(ej,zt))t+1log gWï£«/summationtext
ï£­
K tt/parenrightbigg
= lo/parenleftBig
Â· /summationtextâˆ’1
l â„“ e=e/summationtext
xp/parenleftBig
âˆ’Î·1/summationtext
j,z=1/parenrightBig
(ls)ï£¶
= log(IE Jâˆ¼pt[exp(âˆ’Î·â„“(eJ,zt))])/parenrightBigï£¸
12Hoeï¬€dingâ€™s lemma â‡’ â‰¤log/parenleftBig
Î·IEe eâˆ’Î·J J8â„“(e ,zt)
Î·2/parenrightBig
=âˆ’Î·IEJâ„“(eJ,zt)8
Î·2Î·2
Jensenâ€™s â‡’ â‰¤ âˆ’ Î·â„“(IEJeJ,zt) =âˆ’Î·â„“(pt,zt)8 8
87
since IE Jej=/summationtextKpj=1t,jej. If we sum over t, the sum telescopes. Since W1=K, we are left
withnnÎ·2
log(Wn+1)âˆ’log(K)â‰¤ âˆ’ Î·/summationdisplay
â„“(pt,zt).8t=1
We have
K n
log(Wn+1) = logï£«
/summationdisplay
exp/parenleftBigg
âˆ’Î·/summationdisplay
â„“(ej,zs)
j=1 s=1/parenrightBiggï£¶
,
so settingnjâˆ—= argmin1â‰¤jâ‰¤K/summationtextâ„“(eï£­ ï£¸
t=1j,zt), we obtain
n n
log(Wn+1)â‰¥log/parenleftBigg
exp/parenleftBigg
âˆ’Î·/parenrightBigg/parenrightBigg/summationdisplay
â„“(ejâˆ—,zs) =âˆ’Î·/summationdisplay
â„“(ejâˆ—,zt).
s=1 t=1
Rearranging, we have
n n /summationdisplay Î·nlogKâ„“(pt,zt)âˆ’â„“
t=/summationdisplay
(ejâˆ—,zt)â‰¤+.8Î·1 t=1
Finally, we optimize over Î·to arrive at
Î·=/radicalbigg
8logK oRnâ‰¤n/radicalbigg
nl gKâ‡’ .2
The improved constant comes from the assumption that our loss lies in an interval of size
1 (namely [0 ,1]) rather than in an interval of size 2 (namely [ âˆ’1,1]).
88
18.657: Mathematics of Machine Learning
Lecturer: Philippe Rigollet Lecture 16
Scribe:Haihao (Sean) Lu Nov. 2, 2015
Recall that in last lecture, we talked about prediction with expert advice. Remember
thatl(ej,zt) means the loss of expert jat timet, whereztis one adversaryâ€™s move. In this
lecture, for simplexity we replace the notation ztand denote by ztthe loss associated to all
experts at time t:
zt=ï£«
â„“(e1,zt)ï£¶
ï£¬... ,
â„“(eK,zt)ï£·
whereby for pâˆˆâˆ†K,pï£­ ï£¸
âŠ¤zt=Kp ej=1jâ„“(j,zt). This gives an alternative deï¬nition of ft(p)
in last lecture. Actually it is easy to check ft(p) =pâŠ¤zt, thus we can rewrite the theorem
for exponential weighting(EW/summationtext
) strategy as
n n
Rnâ‰¤/summationdisplay
pâŠ¤
tztâˆ’minp
pt=1âˆˆâˆ†k/summationdisplayâŠ¤zt2nlogK,
t=1â‰¤/radicalbig
where the ï¬rst inequality is Jensen inequality:
n n/summationdisplay
pâŠ¤
zzt
t=1â‰¥/summationdisplay
â„“(pz,zt).
t=1
We consider EW strategy for bounded convex losses. Without loss of generality, we
assumeâ„“(p,z)âˆˆ[0,1], for all ( p,z)âˆˆâˆ†KÃ—Z, thus in notation here, we expect ptâˆˆâˆ†K
andzK Â¯tâˆˆ[0,1] . Indeed if â„“(p,z)âˆˆ[m,M] then one can work with a rescaled loss â„“(a,z) =
â„“(a,z)âˆ’m. Note that now we have bounded gradient on pt, sinceztis bounded.Mâˆ’m
2. FOLLOW THE PERTURBED LEADER (FPL)
In this section, we consider a diï¬€erent strategy, called Follow the Perturbed Leader.
At ï¬rst, we introduce Follow the Leader strategy, and give an example to show that
Follow the Leader can be hazardous sometimes. At time t, assume that choose
tâˆ’1
pt= argmin
pâˆˆâˆ†K/summationdisplay
pâŠ¤zs.
s=1
Notethat thefunctiontobeoptimized islinear in p, wherebytheoptimal solutionshould
be a vertex of the simplex. This method can be viewed as a greedy algorithm, however, it
might not be a good strategy.
Consider the following example. Let K= 2,z1= (0,Îµ)âŠ¤,z2= (0,1)âŠ¤,z3= (1,0)âŠ¤,
z4= (0,1)âŠ¤and so on (alternatively having (0 ,1)âŠ¤and (1,0)âŠ¤whentâ‰¥2), where Îµis small
enough. Then with Following the Leader Strategy, we have that p1is arbitrary and in the
best case p1= (1,0)âŠ¤, andp2= (1,0)âŠ¤,p3= (0,1)âŠ¤,p4= (1,0)âŠ¤and so on (alternatively
having (0 ,1)âŠ¤and (1,0)âŠ¤whentâ‰¥2).
89
In the above example, we have
n n/summationdisplay n npâŠ¤
tztminpâŠ¤ztn1 1,
pâˆ†k 2 2t=1âˆ’ â‰¤ âˆ’ âˆ’ â‰¤
âˆˆ/summationdisplay
t=1âˆ’
which gives raise to linear regret.
Now letâ€™s consider FPL. FPL regularizes FL by adding a small amount of noise, which
can guarantee square root regret under oblivious adversary situation.
Algorithm 1 Follow the Perturbed Leader (FPL)
Input:LetÎ¾be a random variables uniformly drawn on [0 ,1]K.Î·
fort= 1 tondo
tâˆ’1
pt= argmin
pâˆˆâˆ†K/summationdisplay
s=1/parenleftbig
pâŠ¤zs+Î¾/parenrightbig
.
end for
We analyze this strategy in oblivious adversaries, which means the sequence ztis chosen
ahead of time, rather than adaptively given. The following theorem gives a bound for regret
of FPL:
Theorem: FPL with Î·=âˆš1yields expected regret:kn
IEÎ¾[Rn]â‰¤2âˆš
2nK .
Before proving the theorem, we introduce the so-called Be-The-Leader Lemma at ï¬rst.
Lemma: (Be-The-Leader)
For all loss function â„“(p,z), let
t
pâˆ—
t= arg min â„“(p,zs),
pâˆˆâˆ†K/summationdisplay
s=1
then we haven n /summationdisplay
â„“(pâˆ—
t,zt)
t=1â‰¤/summationdisplay
â„“(pnâˆ—,zt)
t=1
Proof.The proof goes by induction on n. Forn= 1, it is clearly true. From nton+1, it
90
follows from:
n+1 n /summationdisplay
â„“(ptâˆ—,zt) =/summationdisplay
â„“(pâˆ—
t,zt)+â„“(pnâˆ—
+1,zn+1)
t=1 i=1
n
â‰¤/summationdisplay
â„“(pâˆ—
n,zt)+â„“(pâˆ—
n+1,zn+1)
i=1
n
â‰¤/summationdisplay
â„“(pâˆ—
n+1,zt)+â„“(pâˆ—
n+1,zn+1),
i=1