<?xml version="1.0" encoding="UTF-8"?>
<CONTENT id="_800671_1"><TITLE value="Topic 9.2: Activation functions"/><TITLECOLOR value="#000000"/><DESCRIPTION
   value=""/><BODY><TEXT>&lt;div data-layout-row="d5da4c74-4a25-46e9-96cd-a4e377df1c73"&gt;&lt;div data-layout-column="da0524fc-d60d-4e96-93d6-bea4cc9a6c74" data-layout-column-width="12"&gt;&lt;div data-bbid="bbml-editor-id_75fd5f88-7bb0-454a-bc5a-d7fb22f5160a"&gt;&lt;h4&gt;Topic 9.2: Activation functions&lt;/h4&gt;&lt;br&gt;&lt;h5&gt;&lt;span style="color: #1c8845"&gt;Perceptron step function&lt;/span&gt;&lt;/h5&gt;&lt;br&gt;&lt;p&gt;As we indicated in the previous topic, neural networks use non linear continuous functions.&lt;/p&gt;&lt;p&gt;Watch this video to learn about three famous ones then continue reading.&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;a href="https://mediasite.centennialcollege.ca/Mediasite/Play/3e2cd2ea30374cd490897357ee84d5411d" data-bbtype="video" data-bbfile="{&amp;quot;src&amp;quot;:&amp;quot;https://mediasite.centennialcollege.ca/Mediasite/Play/3e2cd2ea30374cd490897357ee84d5411d&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;https://mediasite.centennialcollege.ca/Mediasite/Play/3e2cd2ea30374cd490897357ee84d5411d&amp;quot;}"&gt;https://mediasite.centennialcollege.ca/Mediasite/Play/3e2cd2ea30374cd490897357ee84d5411d&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;Let's remember the step function for the perceptron, as illustrated below:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693154_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Step function&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M9_perceptron_step.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M9_perceptron_step.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;This function is difficult to optimize for a number of reasons:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;It is not continuous.&lt;/li&gt;&lt;li&gt;It has a derivative of zero at all points.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;This means that we cannot know in which direction to look for a local minima of the function, which makes it difficult to minimize loss in a smooth way. So, instead of using a step function, we will use some continuous function.&lt;/p&gt;&lt;h5&gt;&lt;span style="color: #1c8845"&gt;Activation functions&lt;/span&gt;&lt;/h5&gt;&lt;br&gt;&lt;h6&gt;&lt;span style="color: #1c8845"&gt;Sigmoid&lt;/span&gt;&lt;/h6&gt;&lt;br&gt;&lt;p&gt;This&amp;nbsp;is the same function used for logistic regression the below diagram illustrates the function and the derivative formula:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693155_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Sigmoid activation function&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M9_Act_Sigmoid.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M9_Act_Sigmoid.png&lt;/a&gt;&lt;/p&gt;&lt;h6&gt;&lt;span style="color: #1c8845"&gt;Hyperbolic Tangent (Tanh)&lt;/span&gt;&lt;/h6&gt;&lt;br&gt;&lt;p&gt;The range for the Tanh function is (-1 to 1) Tanh is a scaled and shifited version of the Sigmoid function. The below diagram illustrates the function and the derivative formula:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693156_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Tanh activation function&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M9_act_tanh.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M9_act_tanh.png&lt;/a&gt;&lt;/p&gt;&lt;h6&gt;&lt;span style="color: #1c8845"&gt;Rectified linear unit (ReLU)&lt;/span&gt;&lt;/h6&gt;&lt;br&gt;&lt;p&gt;The ReLU activation function is a simple function, if the value is less than 0 it is zero else it is the value itself. The below diagram illustrates the function and the derivative formula:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693157_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;ReLU activation function&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M9_act_Relu.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:366.375,&amp;quot;height&amp;quot;:462.14551124825385}"&gt;M9_act_Relu.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Another version of the ReLU function is the sftplus function, which gives a smooth version of the ReLU as per the below diagram:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693158_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Softplus activation function&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M9_act_Relusoftplus.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:405,&amp;quot;height&amp;quot;:285.88235294117646}"&gt;M9_act_Relusoftplus.png&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h5&gt;References&lt;/h5&gt;&lt;p&gt;1- Chapter 21 Artificial intelligence a modern approach by Stuart J. Russell and Peter Norvig.&lt;/p&gt;&lt;p&gt;2- FA18 CS188 Lecture 22 -- Optimization and Neural Nets&lt;/p&gt;&lt;p&gt;3- Chapter 19 &lt;a href="https://learning.oreilly.com/library/view/artificial-intelligence-with/9781786464392/"&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-family: Open Sans;"&gt;&lt;span style="font-size: 0.875rem;"&gt;&lt;span style="text-decoration:underline;"&gt;Artificial Intelligence with Python second edition&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style="font-family: Open Sans;"&gt;&lt;span style="font-size: 0.875rem;"&gt;by Prateek Joshi &lt;/span&gt;&lt;/span&gt;&amp;nbsp;by Prateek Joshi&lt;/p&gt;&lt;p&gt;4- &lt;a href="https://dzone.com/articles/the-very-basic-introduction-to-feed-forward-neural"&gt;https://dzone.com/articles/the-very-basic-introduction-to-feed-forward-neural&lt;/a&gt;&lt;/p&gt;&lt;p&gt;5- &lt;a href="https://en.wikipedia.org/wiki/Feedforward_neural_network"&gt;https://en.wikipedia.org/wiki/Feedforward_neural_network&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</TEXT><TYPE
   value="H"/></BODY><DATES><CREATED value="2024-10-04 18:31:47 EDT"/><UPDATED value="2024-11-13 23:57:18 EST"/><START
   value=""/><END value=""/></DATES><FLAGS><ISAVAILABLE value="true"/><ISFROMCARTRIDGE value="false"/><ISFOLDER
   value="false"/><ISDESCRIBED value="false"/><ISTRACKED value="true"/><ISLESSON value="false"/><ISSEQUENTIAL
   value="false"/><ALLOWGUESTS value="true"/><ALLOWOBSERVERS value="true"/><LAUNCHINNEWWINDOW
   value="false"/><ISREVIEWABLE value="false"/><ISGROUPCONTENT value="false"/><ISSAMPLECONTENT
   value="false"/><PARTIALLYVISIBLE value="false"/><HASTHUMBNAIL value="false"/></FLAGS><CONTENTHANDLER
  value="resource/x-bb-document"/><RENDERTYPE value="REGULAR"/><FOLDERTYPE value=""/><URL value=""/><VIEWMODE
  value="TEXT_ICON_ONLY"/><OFFLINENAME value=""/><OFFLINEPATH value=""/><LINKREF value=""/><PARENTID
  value="_800531_1"/><REVIEWABLEREASON value="NONE"/><VERSION value="3"/><THUMBNAILALT value=""/><AISTATE
  value="No"/><AIACCEPTINGUSER value=""/><EXTENDEDDATA/><FILES/></CONTENT>
