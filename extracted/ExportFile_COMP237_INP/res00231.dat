<?xml version="1.0" encoding="UTF-8"?>
<CONTENT id="_800685_1"><TITLE value="Topic 2.3: Types of agents"/><TITLECOLOR value="#000000"/><DESCRIPTION
   value=""/><BODY><TEXT>&lt;div data-layout-row="3357e7b8-7ba0-41c2-9ac0-c694f8d4a7f7"&gt;&lt;div data-layout-column="a8eedfa2-44bf-4a6a-a0fb-46e904d781c9" data-layout-column-width="12"&gt;&lt;div data-bbid="bbml-editor-id_dda60c22-f176-4530-a21f-732eeafcb10c"&gt;&lt;h4&gt;Topic 2.3: Types of agents&lt;/h4&gt;&lt;br&gt;&lt;h5&gt;&lt;span style="color: #1c8845"&gt;Agents&lt;/span&gt;&lt;/h5&gt;&lt;br&gt;&lt;p&gt;Now we will focus on the artificial intelligence agents.&lt;/p&gt;&lt;h6&gt;Watch this lecture video then read through the topic material:&lt;/h6&gt;&lt;p style="text-align: center;"&gt;&lt;a href="https://mediasite.centennialcollege.ca/Mediasite/Play/3c14de0de9fb46d1a1dca969ba46d69a1d" data-bbtype="video" data-bbfile="{&amp;quot;src&amp;quot;:&amp;quot;https://mediasite.centennialcollege.ca/Mediasite/Play/3c14de0de9fb46d1a1dca969ba46d69a1d&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;https://mediasite.centennialcollege.ca/Mediasite/Play/3c14de0de9fb46d1a1dca969ba46d69a1d&amp;quot;}"&gt;https://mediasite.centennialcollege.ca/Mediasite/Play/3c14de0de9fb46d1a1dca969ba46d69a1d&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Let us recap on what we know so far:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;An &lt;span style="color: #000000"&gt;&lt;strong&gt;agent&lt;/strong&gt;&lt;/span&gt; is an entity that perceives and acts.&lt;/li&gt;&lt;li&gt;A &lt;span style="color: #000000"&gt;&lt;strong&gt;rational agent&lt;/strong&gt;&lt;/span&gt; selects actions that &lt;span style="color: #000000"&gt;&lt;strong&gt;maximize its (expected) utility&lt;/strong&gt;&lt;/span&gt;.&amp;nbsp;&lt;/li&gt;&lt;li&gt;Characteristics of the precepts, environment, and action space dictate techniques for selecting rational actions.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1692614_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;An agent is an entity that perceives and acts&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M2_agent_general.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M2_agent_general.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 0.875rem;"&gt;Think of an agent as:&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;span style="color: #1c8845"&gt;&lt;span style="font-size: 1.125rem;"&gt;agent = architecture + program&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 0.875rem;"&gt;The &lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 0.875rem;"&gt;&lt;strong&gt;architecture&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="font-size: 0.875rem;"&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 0.875rem;"&gt;reflects the sensors and actuators and the &lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 0.875rem;"&gt;&lt;strong&gt;program &lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 0.875rem;"&gt;implements the agent function.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;The job of AI is to design an agent program that implements the agent function—the mapping from precepts to actions. We assume this program will run on some sort of computing device with physical sensors and actuators—we call this the architecture.&lt;/p&gt;&lt;h4&gt;&lt;span style="color: #1c8845"&gt;Simple Reflex Agents&lt;/span&gt;&lt;/h4&gt;&lt;br&gt;&lt;p&gt;This type of agent programs chooses actions based on the&amp;nbsp;&lt;span style="color: #000000"&gt;&lt;strong&gt;current precept&lt;/strong&gt;&lt;/span&gt; and ignores the rest of &lt;span style="color: #000000"&gt;&lt;strong&gt;the precept history&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;.&amp;nbsp;&lt;/span&gt;They do not consider the future consequences of their actions.&lt;/p&gt;&lt;p&gt;The image below reflects an agent that is trying to grab an apple, just based on the precept of seeing the apple.&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1692615_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;agent with no plan&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M2_agent_no_plan.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:327.1875,&amp;quot;height&amp;quot;:272.5636337849156}"&gt;M2_agent_no_plan.png&lt;/a&gt;&lt;/p&gt;&lt;h6&gt;Simple reflex agents consider:&lt;/h6&gt;&lt;p style="text-align: center;"&gt;&lt;span style="color: #1c8845"&gt;&lt;span style="font-size: 1.5rem;"&gt;&lt;strong&gt;How the world IS&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;For example, the simple Vacuum program we explained in the previous topic is considered a simple reflex agent because its decisions are based on the current location and whether that location contains dirt or not.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;The agent programs use a &lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;Condition - action rule&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;If&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt; location-is-dirty &lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;then&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt; suck.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1692607_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Vacuum agent &amp;amp; environment&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M2_vacum.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:855.4000244140625,&amp;quot;height&amp;quot;:222.2029116405298}"&gt;M2_vacum.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1692608_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Vacuum simple reflex agent program&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M2_agent_vacum_funtion.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:937.4000244140625,&amp;quot;height&amp;quot;:293.98033621377533}"&gt;M2_agent_vacum_funtion.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;Think of the Pac-Man game:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;If&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt; Pac-Man&amp;nbsp;&lt;/span&gt;&lt;span style="font-size: 0.875rem;"&gt;sees the food &lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 0.875rem;"&gt;&lt;strong&gt;then&amp;nbsp;&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="font-size: 0.875rem;"&gt; the action is to eat&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;This would be considered a simple reflex agent.&lt;/p&gt;&lt;h6&gt;Watch the following two videos and notice the behaviour:&lt;/h6&gt;&lt;p style="text-align: center;"&gt;&lt;span style="font-size: 0.875rem;"&gt; &lt;/span&gt;&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1692617_1" data-bbtype="attachment" data-bbfile="{&amp;quot;linkName&amp;quot;:&amp;quot;pacman_demo_reflix.mp4&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;video/mp4&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;pacman_demo_reflix.mp4&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p style="text-align: center;"&gt;&lt;span style="font-size: 0.875rem;"&gt; &lt;/span&gt;&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;span style="font-size: 0.875rem;"&gt; &lt;/span&gt;&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1692618_1" data-bbtype="attachment" data-bbfile="{&amp;quot;linkName&amp;quot;:&amp;quot;pacman_simple_relfex_gone_bad.mp4&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;video/mp4&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;pacman_simple_relfex_gone_bad.mp4&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p style="text-align: center;"&gt;&lt;span style="font-size: 0.875rem;"&gt; &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Notice in the second video what happened when Pac-Man didn't find food.&lt;/p&gt;&lt;p&gt;Simple reflex agent programs act according to a rule whose condition matches the current state, as defined by the precept. The below figure illustrates the simple reflex agents:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1692619_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Simple Reflex agent&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M2_simple_reflix.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:597.7125244140625,&amp;quot;height&amp;quot;:380.35387302472094}"&gt;M2_simple_reflix.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Let's take a look at the pseudo code below:&lt;/p&gt;&lt;pre&gt;function SIMPLE-REFLEX-AGENT(precept) returns an action&lt;/pre&gt;&lt;pre&gt;persistent: rules, a set of condition-action rules&lt;/pre&gt;&lt;pre&gt;state &amp;lt;- INTERPRET-INPUT(precept )&lt;/pre&gt;&lt;pre&gt;rule &amp;lt;- RULE-MATCH(state, rules)&lt;/pre&gt;&lt;pre&gt;action &amp;lt;- rule.ACTION&lt;/pre&gt;&lt;pre&gt;return action&lt;/pre&gt;&lt;p&gt;Returning back to our vacuum agent program example, let's look at the Python code below:&lt;/p&gt;&lt;pre&gt;def ReflexVacuumAgent():&lt;/pre&gt;&lt;pre&gt;  #A reflex agent for the two-state vacuum environment&lt;/pre&gt;&lt;pre&gt;    &amp;gt;&amp;gt;&amp;gt; agent = ReflexVacuumAgent()&lt;/pre&gt;&lt;pre&gt;    &amp;gt;&amp;gt;&amp;gt; environment = TrivialVacuumEnvironment()&lt;/pre&gt;&lt;pre&gt;    &amp;gt;&amp;gt;&amp;gt; environment.add_thing(agent)&lt;/pre&gt;&lt;pre&gt;    &amp;gt;&amp;gt;&amp;gt; environment.run()&lt;/pre&gt;&lt;pre&gt;    &amp;gt;&amp;gt;&amp;gt; environment.status == {(1,0) : 'Clean' , (0,0) : 'Clean'}&lt;/pre&gt;&lt;pre&gt;    True&lt;/pre&gt;&lt;pre&gt;    """&lt;/pre&gt;&lt;pre&gt; &lt;/pre&gt;&lt;pre&gt;def program(precept):&lt;/pre&gt;&lt;pre&gt;  location, status = precept&lt;/pre&gt;&lt;pre&gt;  #Rules&lt;/pre&gt;&lt;pre&gt;  if status == 'Dirty':&lt;/pre&gt;&lt;pre&gt;    return 'Suck'&lt;/pre&gt;&lt;pre&gt;  elif location == loc_A:&lt;/pre&gt;&lt;pre&gt;    return 'Right'&lt;/pre&gt;&lt;pre&gt;  elif location == loc_B:&lt;/pre&gt;&lt;pre&gt;    return 'Left'&lt;/pre&gt;&lt;pre&gt; return Agent(Program)&lt;/pre&gt;&lt;br&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1692619_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Simple Reflex agent&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M2_simple_reflix.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:474.7125244140625,&amp;quot;height&amp;quot;:302.0829242472923}"&gt;M2_simple_reflix.png&lt;/a&gt;&lt;/p&gt;&lt;h4&gt;&lt;span style="color: #1c8845"&gt;Model-based reflex agent&lt;/span&gt;&lt;/h4&gt;&lt;br&gt;&lt;p&gt;A &lt;span style="color: #000000"&gt;&lt;strong&gt;model-based&lt;/strong&gt;&lt;/span&gt; reflex agent keeps track of the current state of the world, using an &lt;span style="color: #000000"&gt;&lt;strong&gt;internal model&lt;/strong&gt;&lt;/span&gt;. It then chooses an action in the same way as the reflex agent. The figure below illustrates the model based agent program:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1692620_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Model based reflex agent&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M2_model_based_reflex.jpg&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/jpeg&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:494,&amp;quot;height&amp;quot;:314.7743902439024}"&gt;M2_model_based_reflex.jpg&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this type of agent programs, note the following:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The agent should maintain an internal state that depends on the &lt;span style="color: #000000"&gt;&lt;strong&gt;precept history &lt;/strong&gt;&lt;/span&gt;and thereby reflects at least&lt;span style="color: #000000"&gt;&lt;strong&gt; some of the unobserved aspects of the current state.&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;Updating this internal state information as time goes by requires two kinds of knowledge to be encoded in the agent program:&lt;/li&gt;&lt;/ul&gt;&lt;ol&gt;&lt;li&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;how the &lt;span style="color: #000000"&gt;&lt;strong&gt;world evolves&lt;/strong&gt;&lt;/span&gt; independently of the agent&lt;/li&gt;&lt;li&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;how the agent’s&lt;span style="color: #000000"&gt;&lt;strong&gt; own actions&lt;/strong&gt;&lt;/span&gt; affect the world&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;Regardless of the kind of representation used, it is seldom possible for the agent to determine the current state of a partially observable environment exactly. Instead, the agent makes a &lt;span style="color: #000000"&gt;&lt;strong&gt;“best guess”&lt;/strong&gt;&lt;/span&gt; of the present environment state.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Let's look at the pseudo code below for the model based reflex agents:&lt;/p&gt;&lt;pre&gt;function MODEL-BASED-REFLEX-AGENT(precept) returns an action&lt;/pre&gt;&lt;pre&gt;persistent: state, the agent’s current conception of the world state&lt;/pre&gt;&lt;pre&gt;model: a description of how the next state depends on current state and action&lt;/pre&gt;&lt;pre&gt;rules: a set of condition-action rules&lt;/pre&gt;&lt;pre&gt;action: the most recent action, initially none&lt;/pre&gt;&lt;pre&gt;state &amp;lt;- UPDATE-STATE(state, action, percept ,model)&lt;/pre&gt;&lt;pre&gt;rule &amp;lt;- RULE-MATCH(state, rules)&lt;/pre&gt;&lt;pre&gt;action &amp;lt;- rule.ACTION&lt;/pre&gt;&lt;pre&gt;return action&lt;/pre&gt;&lt;p&gt;Going back to our vacuum agent program example, let's look at the Python code below:&lt;/p&gt;&lt;pre&gt;def ModelBasedVacuumAgent():&lt;/pre&gt;&lt;pre&gt;  #An agent that keeps track of what locations are clean or dirty&lt;/pre&gt;&lt;pre&gt;    &amp;gt;&amp;gt;&amp;gt; agent = ModelBasedVacuumAgent()&lt;/pre&gt;&lt;pre&gt;    &amp;gt;&amp;gt;&amp;gt; environment = TrivialVacuumEnvironment()&lt;/pre&gt;&lt;pre&gt;    &amp;gt;&amp;gt;&amp;gt; environment.add_thing(agent)&lt;/pre&gt;&lt;pre&gt;    &amp;gt;&amp;gt;&amp;gt; environment.run()&lt;/pre&gt;&lt;pre&gt;    &amp;gt;&amp;gt;&amp;gt; environment.status == {(1,0) : 'Clean' , (0,0) : 'Clean'}&lt;/pre&gt;&lt;pre&gt;    True&lt;/pre&gt;&lt;pre&gt;    """&lt;/pre&gt;&lt;pre&gt; &lt;/pre&gt;&lt;pre&gt;model = {loc_A: None, loc_B: None}&lt;/pre&gt;&lt;pre&gt;def program(precept):&lt;/pre&gt;&lt;pre&gt;  #Same as ReflexVacuumAgent, except if everything is clean, do NoOp&lt;/pre&gt;&lt;pre&gt;  location, status = precept&lt;/pre&gt;&lt;pre&gt;  model[location] = status&lt;/pre&gt;&lt;pre&gt;  #Update the model here&lt;/pre&gt;&lt;pre&gt;  if model[loc_A] == model[loc_B] == 'Clean':&lt;/pre&gt;&lt;pre&gt;    return 'NoOp'&lt;/pre&gt;&lt;pre&gt;  elif status == 'Dirty':&lt;/pre&gt;&lt;pre&gt;    return 'Suck'&lt;/pre&gt;&lt;pre&gt;  elif location == loc_A:&lt;/pre&gt;&lt;pre&gt;    return 'Right'&lt;/pre&gt;&lt;pre&gt;  elif location == loc_B: &lt;/pre&gt;&lt;pre&gt;    return 'Left' &lt;/pre&gt;&lt;pre&gt; &lt;/pre&gt;&lt;pre&gt;return Agent(Program)&lt;/pre&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1692620_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Model based reflex agent&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M2_model_based_reflex.jpg&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/jpeg&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M2_model_based_reflex.jpg&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Notice that we added status and model to the code.&lt;/p&gt;&lt;h4&gt;&lt;span style="color: #1c8845"&gt;Goal based agents&lt;/span&gt;&lt;/h4&gt;&lt;br&gt;&lt;p&gt;Knowing something about the current state of the environment is not always enough to decide what to do.&lt;/p&gt;&lt;p&gt;Therefore, the agent needs some sort of &lt;span style="color: #000000"&gt;&lt;strong&gt;goal information&lt;/strong&gt;&lt;/span&gt; that describes situations that are desirable. The figure below illustrates the model based agent program:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1692621_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Goal based agents&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M2_goal_based.jpg&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/jpeg&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:559.5750122070312,&amp;quot;height&amp;quot;:356.1474299901152}"&gt;M2_goal_based.jpg&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The agent program can combine this with the model (the same information as was used in the model based reflex agent) to choose actions that achieve the goal.&lt;/p&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;Search and planning&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt; &lt;/strong&gt;are the sub fields of AI devoted to finding action sequences that achieve the agent’s goals.&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1692622_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Goal based search agents -example google maps&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M2_maps.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:859.4000244140625,&amp;quot;height&amp;quot;:316.3359129164301}"&gt;M2_maps.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In the &lt;strong&gt;next module &lt;/strong&gt;we will cover these &lt;strong&gt;search agents&lt;/strong&gt; in more details.&lt;/p&gt;&lt;h4&gt;&lt;span style="color: #1c8845"&gt;Utility-based agents&lt;/span&gt;&lt;/h4&gt;&lt;br&gt;&lt;p style="text-align: center;"&gt;Many sequences of actions may lead to a goal, but some are &lt;span style="color: #000000"&gt;&lt;strong&gt;quicker, safer, more reliable, or cheaper&lt;/strong&gt;&lt;/span&gt; than others. Here is where the dimension of &lt;span style="color: #000000"&gt;&lt;strong&gt;utility &lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;comes.&lt;/span&gt;&amp;nbsp;The figure below illustrates the utility based agent program:&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1692623_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Utility based agent&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M2_utility.jpg&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/jpeg&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:555.1124877929688,&amp;quot;height&amp;quot;:350.39128839910694}"&gt;M2_utility.jpg&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;ul&gt;&lt;li&gt;Recall that a performance measure assigns a score to any given sequence of environment states, so it can easily distinguish between more and less desirable ways of reaching the goal.&lt;/li&gt;&lt;li&gt;An agent’s utility function is essentially an internalization of the performance measure. If the internal utility function and the external performance measure are in agreement, then an agent that chooses actions to maximize its utility will be rational according to the external performance measure.&lt;/li&gt;&lt;li&gt;Partially observed and stochastic environments are ubiquitous in the real world, and so, they dictate the choice of the action that maximizes the expected utility of the action outcome.&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;&lt;span style="color: #1c8845"&gt;Learning agents&lt;/span&gt;&lt;/h4&gt;&lt;br&gt;&lt;p&gt;Any type of agent ( model based, goal based, utility based, etc.) can be built as a learning agent. This allows agents to operate in unknown environments. The figure below illustrates the learning based agent program:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1692624_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Learning agents&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M2_learning.jpg&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/jpeg&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:537.25,&amp;quot;height&amp;quot;:377.1404622145894}"&gt;M2_learning.jpg&lt;/a&gt;&lt;/p&gt;&lt;p&gt;As noted from the above figure, a learning agent can be divided into four main components:&lt;/p&gt;&lt;h6&gt;Learning element&lt;/h6&gt;&lt;p&gt;Responsible for making improvements.&lt;/p&gt;&lt;h6&gt;Performance element&lt;/h6&gt;&lt;p&gt;Responsible for selecting external actions, this is what we previously considered as the entire agent, i.e. it takes in the precept and decides on the actions.&lt;/p&gt;&lt;h6&gt;Critic element&lt;/h6&gt;&lt;p&gt;Tells the learning element how well the agent is doing with respect to a fixed performance standard.&lt;/p&gt;&lt;h6&gt;Problem generator element&lt;/h6&gt;&lt;p&gt;Responsible for suggesting actions that will lead to new informative experiences.&lt;/p&gt;&lt;p&gt;Throughout the rest of the course we will be designing learning agents.&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;h5&gt;References&lt;/h5&gt;&lt;ol&gt;&lt;li&gt;Artificial intelligence a modern approach by Stuart J. Russell and Peter Norvig, Chapter 2&lt;/li&gt;&lt;li&gt;UC Berkeley CS188 Intro to AI&amp;nbsp;&amp;nbsp;&lt;a href="http://ai.berkeley.edu/home.html"&gt;http://ai.berkeley.edu/home.html&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</TEXT><TYPE
   value="H"/></BODY><DATES><CREATED value="2024-10-04 18:31:42 EDT"/><UPDATED value="2024-11-11 19:25:55 EST"/><START
   value=""/><END value=""/></DATES><FLAGS><ISAVAILABLE value="true"/><ISFROMCARTRIDGE value="false"/><ISFOLDER
   value="false"/><ISDESCRIBED value="false"/><ISTRACKED value="true"/><ISLESSON value="false"/><ISSEQUENTIAL
   value="false"/><ALLOWGUESTS value="true"/><ALLOWOBSERVERS value="true"/><LAUNCHINNEWWINDOW
   value="false"/><ISREVIEWABLE value="false"/><ISGROUPCONTENT value="false"/><ISSAMPLECONTENT
   value="false"/><PARTIALLYVISIBLE value="false"/><HASTHUMBNAIL value="false"/></FLAGS><CONTENTHANDLER
  value="resource/x-bb-document"/><RENDERTYPE value="REGULAR"/><FOLDERTYPE value=""/><URL value=""/><VIEWMODE
  value="TEXT_ICON_ONLY"/><OFFLINENAME value=""/><OFFLINEPATH value=""/><LINKREF value=""/><PARENTID
  value="_800493_1"/><REVIEWABLEREASON value="NONE"/><VERSION value="3"/><THUMBNAILALT value=""/><AISTATE
  value="No"/><AIACCEPTINGUSER value=""/><EXTENDEDDATA/><FILES/></CONTENT>
