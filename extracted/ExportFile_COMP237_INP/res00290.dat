<?xml version="1.0" encoding="UTF-8"?>
<CONTENT id="_800652_1"><TITLE value="Topic 9.1: More on ANN"/><TITLECOLOR value="#000000"/><DESCRIPTION
   value=""/><BODY><TEXT>&lt;div data-layout-row="28f5b61d-8785-4a96-9dfd-022d652919f0"&gt;&lt;div data-layout-column="c4e81a7c-9a48-4219-9db4-2e29849a9ad6" data-layout-column-width="12"&gt;&lt;div data-bbid="bbml-editor-id_d2d04a8a-69db-42a7-b876-1d5399c14f73"&gt;&lt;h4&gt;Topic 9.1: More on ANN&lt;/h4&gt;&lt;p&gt;In this module we build on the perception neuron concept and dig a bit deeper into Artificial neural networks (ANN).&lt;/p&gt;&lt;p&gt;Watch this video then continue reading:&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;a href="https://mediasite.centennialcollege.ca/Mediasite/Play/6c5b308be7684bf6b9b8f65530d7947c1d" data-bbtype="video" data-bbfile="{&amp;quot;src&amp;quot;:&amp;quot;https://mediasite.centennialcollege.ca/Mediasite/Play/6c5b308be7684bf6b9b8f65530d7947c1d&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;https://mediasite.centennialcollege.ca/Mediasite/Play/6c5b308be7684bf6b9b8f65530d7947c1d&amp;quot;}"&gt;https://mediasite.centennialcollege.ca/Mediasite/Play/6c5b308be7684bf6b9b8f65530d7947c1d&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h5&gt;&lt;span style="color: #1c8845"&gt;Artificial Neural Networks (ANN)&lt;/span&gt;&lt;/h5&gt;&lt;br&gt;&lt;p&gt;As noted from the video, the base for ANNs is the &lt;strong&gt;Perceptron&lt;/strong&gt;, which is the simplest form of neural networks. Having many perceptrons organized in many layers form the base for neural networks, when trained, as per the below figure:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693148_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Perceptrons are the building blocks for Neural networks&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M9_perceptron to ANN_cartoon.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:790.4000244140625,&amp;quot;height&amp;quot;:246.66249337386674}"&gt;M9_perceptron to ANN_cartoon.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;Deep learning&lt;/strong&gt;&lt;/span&gt; has its origins in early work that tried to model networks of neurons in the brain, remember 1943 (McCullach and Pitt) with the computation circuits. For this reason the networks trained by deep learning are often called neural networks.&lt;/p&gt;&lt;p&gt;The main advantage of these networks is the ability to:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Handle&lt;span style="color: #000000"&gt;&lt;strong&gt; high dimensional data&lt;/strong&gt;&lt;/span&gt; such as images.&lt;/li&gt;&lt;li&gt;Allows features to&lt;span style="color: #000000"&gt;&lt;strong&gt; interact&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt; &lt;/strong&gt;with each other along the computation paths.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Have a look at the below diagram, which illustrates the computation paths for linear and logistic regressions versus a neural network:&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693149_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Computation path comparison between Linear regression and neural  networks &amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M9_Linear_vesus_NN.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:980.4000244140625,&amp;quot;height&amp;quot;:352.46897618685273}"&gt;M9_Linear_vesus_NN.png&lt;/a&gt;&lt;/p&gt;&lt;h5&gt;&lt;span style="color: #1c8845"&gt;The universal approximation theorem&lt;/span&gt;&lt;/h5&gt;&lt;br&gt;&lt;p&gt;A&lt;span style="color: #000000"&gt;&lt;strong&gt; two-layer&lt;/strong&gt;&lt;/span&gt; neural network with just two layers of computational units, the first&lt;span style="color: #000000"&gt;&lt;strong&gt; non-linear&lt;/strong&gt;&lt;/span&gt; and the &lt;span style="color: #000000"&gt;&lt;strong&gt;second linear&lt;/strong&gt;&lt;/span&gt;, can approximate any continuous function to &lt;span style="color: #000000"&gt;&lt;strong&gt;an arbitrary degree of accuracy&lt;/strong&gt;&lt;/span&gt;.&lt;/p&gt;&lt;p&gt;The proof works by showing that an exponentially large network can represent many "bumps" of different heights at different locations in the input space, thereby approximating the desired function.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sufficiently large networks can implement a&lt;span style="color: #000000"&gt;&lt;strong&gt; lookup&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt; &lt;/strong&gt;table for continuous functions. One of the key ingredients that enable this, is having an activation function that is non linear and continuous.&lt;/p&gt;&lt;p&gt;This is one of the main differences between perceptron neurons and neurons in neural networks as per the below figure:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693150_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Activation function difference&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M9_diference between_perceptron and NN.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:878.4000244140625,&amp;quot;height&amp;quot;:324.9681296969061}"&gt;M9_diference between_perceptron and NN.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The fact that the activation function is &lt;span style="color: #000000"&gt;&lt;strong&gt;non-linear&lt;/strong&gt;&lt;/span&gt; is important because if it were not, any composition of units would still represent a linear function.&lt;/p&gt;&lt;p&gt;The &lt;span style="color: #000000"&gt;&lt;strong&gt;non-linearity&lt;/strong&gt;&lt;/span&gt; is what allows the sufficiently large networks of units to represent &lt;span style="color: #000000"&gt;&lt;strong&gt;arbitrary functions&lt;/strong&gt;&lt;/span&gt;.&lt;/p&gt;&lt;h5&gt;&lt;span style="color: #1c8845"&gt;Neural network units&lt;/span&gt;&lt;/h5&gt;&lt;br&gt;&lt;p&gt;Neural networks are composed of nodes or units connected by directed links.&lt;/p&gt;&lt;p&gt;A&lt;span style="color: #000000"&gt;&lt;strong&gt; link&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt; &lt;/strong&gt;from unit i to unit j serves to propagate the activation &lt;span style="color: #000000"&gt;&lt;strong&gt;a&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;&lt;sub&gt;i&lt;/sub&gt;&lt;/strong&gt;&lt;/span&gt; from i to j.&lt;/p&gt;&lt;p&gt;Each link also has a numeric weight&lt;span style="color: #000000"&gt;&lt;strong&gt; w&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;&lt;sub&gt;i,j&lt;/sub&gt;&lt;/strong&gt;&lt;/span&gt; associated with it, which determines the strength and sign of the connection.&lt;/p&gt;&lt;p&gt;Just as in linear regression models, each unit has a dummy input &lt;span style="color: #000000"&gt;&lt;strong&gt;a&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;&lt;sub&gt;0&lt;/sub&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt; =1&lt;/strong&gt;&lt;/span&gt; with an associated weight &lt;span style="color: #000000"&gt;&lt;strong&gt;w&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;&lt;sub&gt;0,j&lt;/sub&gt;&lt;/strong&gt;&lt;/span&gt; .&lt;/p&gt;&lt;p&gt;Each unit j first computes a weighted sum of its inputs, as per the following:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693151_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Formula weighted sums and unit representation&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M9_formula1_with_nueron.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:953,&amp;quot;height&amp;quot;:226.25179856115108}"&gt;M9_formula1_with_nueron.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Then it applies an activation function g to this sum to derive the output:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693152_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Formula activation function&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M9_formula2_activation.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M9_formula2_activation.png&lt;/a&gt;&lt;/p&gt;&lt;h6&gt;&lt;span style="color: #000000"&gt;Example:&lt;/span&gt;&lt;/h6&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;Have a look at the below simple network, which has&amp;nbsp;t&lt;/span&gt;wo inputs, one hidden layer of two units, and one output unit.&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693153_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;neural network with two inputs, one hidden layer of two units, and one output unit.&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M9_example.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M9_example.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;In order to calculate the output Y1^ we would do the following:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;Y&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;^&amp;nbsp;= a&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;5&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; = g&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;5&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;(in)=(w&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;0,5&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;+w&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;3,5&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; a&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;3&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; + w&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;4,5&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; a&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;4&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="font-size: 1.125rem;"&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;= g(w&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;0,5&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;,+w&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;3,5&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; g(w&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;0,3&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; + w&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;1,3&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; a&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; + w&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;2,3&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; a&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;) + w&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;4,5&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; g(w&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;04&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; + w&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;1,4&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; a&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; + w&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;2,4&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; a&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;))&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="font-size: 1.125rem;"&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;= g(w&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;0,5&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;+w&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;3,5&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; g(w&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;0,3&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; + w&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;1,3&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; x&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; + w&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;2,3&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; x&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;) + w&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;4,5&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; g(w&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;04&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; + w&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;1,4&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; x&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; + w&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;2,4&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt; x&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;)).&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Accordingly we have the output expressed as a function h&lt;sub&gt;w&lt;/sub&gt;(x) of the inputs and the weights.&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;h5&gt;References&lt;/h5&gt;&lt;p&gt;1- Chapter 21 Artificial intelligence a modern approach by Stuart J. Russell and Peter Norvig.&lt;/p&gt;&lt;p&gt;2- FA18 CS188 Lecture 22 -- Optimization and Neural Nets&lt;/p&gt;&lt;p&gt;3- Chapter 19 &lt;a href="https://learning.oreilly.com/library/view/artificial-intelligence-with/9781786464392/"&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-family: Open Sans;"&gt;&lt;span style="font-size: 0.875rem;"&gt;&lt;span style="text-decoration:underline;"&gt;Artificial Intelligence with Python second edition&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style="font-family: Open Sans;"&gt;&lt;span style="font-size: 0.875rem;"&gt;by Prateek Joshi &lt;/span&gt;&lt;/span&gt;&amp;nbsp;by Prateek Joshi&lt;/p&gt;&lt;p&gt;4- &lt;a href="https://dzone.com/articles/the-very-basic-introduction-to-feed-forward-neural"&gt;https://dzone.com/articles/the-very-basic-introduction-to-feed-forward-neural&lt;/a&gt;&lt;/p&gt;&lt;p&gt;5- &lt;a href="https://en.wikipedia.org/wiki/Feedforward_neural_network"&gt;https://en.wikipedia.org/wiki/Feedforward_neural_network&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</TEXT><TYPE
   value="H"/></BODY><DATES><CREATED value="2024-10-04 18:31:46 EDT"/><UPDATED value="2024-11-13 23:55:52 EST"/><START
   value=""/><END value=""/></DATES><FLAGS><ISAVAILABLE value="true"/><ISFROMCARTRIDGE value="false"/><ISFOLDER
   value="false"/><ISDESCRIBED value="false"/><ISTRACKED value="true"/><ISLESSON value="false"/><ISSEQUENTIAL
   value="false"/><ALLOWGUESTS value="true"/><ALLOWOBSERVERS value="true"/><LAUNCHINNEWWINDOW
   value="false"/><ISREVIEWABLE value="false"/><ISGROUPCONTENT value="false"/><ISSAMPLECONTENT
   value="false"/><PARTIALLYVISIBLE value="false"/><HASTHUMBNAIL value="false"/></FLAGS><CONTENTHANDLER
  value="resource/x-bb-document"/><RENDERTYPE value="REGULAR"/><FOLDERTYPE value=""/><URL value=""/><VIEWMODE
  value="TEXT_ICON_ONLY"/><OFFLINENAME value=""/><OFFLINEPATH value=""/><LINKREF value=""/><PARENTID
  value="_800530_1"/><REVIEWABLEREASON value="NONE"/><VERSION value="3"/><THUMBNAILALT value=""/><AISTATE
  value="No"/><AIACCEPTINGUSER value=""/><EXTENDEDDATA/><FILES/></CONTENT>
