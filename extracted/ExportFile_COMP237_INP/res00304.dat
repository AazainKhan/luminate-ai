<?xml version="1.0" encoding="UTF-8"?>
<CONTENT id="_800666_1"><TITLE value="Topic 6.2 Logistic regression loss function"/><TITLECOLOR
  value="#000000"/><DESCRIPTION
   value=""/><BODY><TEXT>&lt;div data-layout-row="001bcdee-8297-46bb-80a7-56ace628ed51"&gt;&lt;div data-layout-column="24864736-e11e-45a6-a5ac-44de17e1f9b1" data-layout-column-width="12"&gt;&lt;div data-bbid="bbml-editor-id_46cc41a7-2884-43cb-b0e0-135879a51fc1"&gt;&lt;h4&gt;Topic 6.2 Logistic regression loss function&lt;/h4&gt;&lt;br&gt;&lt;h5&gt;&lt;span style="color: #1c8845"&gt;Maximum Likelihood&lt;/span&gt;&lt;/h5&gt;&lt;br&gt;&lt;p&gt;Unlike linear regression, there is &lt;span style="color: #1c8845"&gt;&lt;strong&gt;&lt;span style="text-decoration:underline;"&gt;not an analytical solution&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #1c8845"&gt;&lt;span style="text-decoration:underline;"&gt; &lt;/span&gt;&lt;/span&gt;to solving this optimization problem. As such, an iterative optimization algorithm must be used.&lt;/p&gt;&lt;p&gt;Watch this video then continue reading the topic:&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;a href="https://mediasite.centennialcollege.ca/Mediasite/Play/e16ad684af4e4e7a8c3bc921d01e37891d" data-bbtype="video" data-bbfile="{&amp;quot;src&amp;quot;:&amp;quot;https://mediasite.centennialcollege.ca/Mediasite/Play/e16ad684af4e4e7a8c3bc921d01e37891d&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;https://mediasite.centennialcollege.ca/Mediasite/Play/e16ad684af4e4e7a8c3bc921d01e37891d&amp;quot;}"&gt;https://mediasite.centennialcollege.ca/Mediasite/Play/e16ad684af4e4e7a8c3bc921d01e37891d&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;As noted from the video, we need to treat the&amp;nbsp;problem as an &lt;span style="text-decoration:underline;"&gt;optimization or search problem,&lt;/span&gt; where we seek a set of parameters that results in the best fit for the &lt;span style="color: #1c8845"&gt;&lt;strong&gt;joint probability of the data sample (X)&lt;/strong&gt;&lt;/span&gt;. The main purpose is to find the best fitting line on the log scale so as to get the best &lt;span style="color: #1c8845"&gt;&lt;strong&gt;fitting &lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #1c8845"&gt;&lt;strong&gt;&lt;em&gt;sigmoid&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt; function as per the below figure:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693068_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;various lines for Logistic&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M6__best_line.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M6__best_line.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We will use the Maximum likelihood approach. In statistics,&amp;nbsp;&lt;span style="color: #1c8845"&gt;&lt;strong&gt;maximum likelihood estimation&amp;nbsp;(MLE)&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #1c8845"&gt; &lt;/span&gt;is a method of estimating the &lt;span style="color: #000000"&gt;&lt;strong&gt;parameters&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;of a &lt;span style="color: #000000"&gt;&lt;strong&gt;probability distribution&lt;/strong&gt;&lt;/span&gt;&amp;nbsp;by maximizing&amp;nbsp;a &lt;span style="color: #000000"&gt;&lt;strong&gt;likelihood function&lt;/strong&gt;&lt;/span&gt;, so that under the assumed statistical model the &lt;span style="color: #000000"&gt;&lt;strong&gt;observed data&lt;/strong&gt;&lt;/span&gt;&amp;nbsp;is most probable.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Watch this video to learn more about maximum likelihood:&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;a href="https://www.youtube.com/embed/XepXtl9YKwc?wmode=opaque" data-bbtype="video" data-bbfile="{&amp;quot;src&amp;quot;:&amp;quot;https://www.youtube.com/embed/XepXtl9YKwc?wmode=opaque&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;https://www.youtube.com/embed/XepXtl9YKwc?wmode=opaque&amp;quot;}"&gt;https://www.youtube.com/embed/XepXtl9YKwc?wmode=opaque&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Now watch this video for an example of applying maximum likelihood to estimate the parameters of a logistic regression problem:&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;a href="https://www.youtube.com/embed/BfKanl1aSG0?feature=oembed&amp;amp;wmode=opaque&amp;amp;rel=0" data-bbtype="video" data-bbfile="{&amp;quot;src&amp;quot;:&amp;quot;https://www.youtube.com/embed/BfKanl1aSG0?feature=oembed&amp;amp;wmode=opaque&amp;amp;rel=0&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;https://www.youtube.com/embed/BfKanl1aSG0?feature=oembed&amp;amp;wmode=opaque&amp;amp;rel=0&amp;quot;}"&gt;https://www.youtube.com/embed/BfKanl1aSG0?feature=oembed&amp;amp;wmode=opaque&amp;amp;rel=0&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Hopefully by now we have an idea about the optimization process, let us dig a bit into the math and nature of the probability distribution used.&lt;/p&gt;&lt;h5&gt;&lt;span style="color: #1c8845"&gt;Math&lt;/span&gt;&lt;/h5&gt;&lt;br&gt;&lt;p&gt;We can frame the problem of fitting a machine learning model as the problem of &lt;strong&gt;probability density estimation&lt;/strong&gt;. Specifically, the choice of model and model parameters is referred to as a modeling&lt;span style="color: #000000"&gt;&lt;strong&gt; hypothesis&amp;nbsp;h&lt;/strong&gt;&lt;/span&gt;, and the problem involves finding&amp;nbsp;&lt;span style="color: #000000"&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;/span&gt;&amp;nbsp;that best explains the &lt;span style="color: #000000"&gt;&lt;strong&gt;data&amp;nbsp;X&lt;/strong&gt;&lt;/span&gt;. We can, therefore, find the modeling hypothesis that maximizes the&lt;span style="color: #000000"&gt;&lt;strong&gt; likelihood function&lt;/strong&gt;&lt;/span&gt;. Have a look at the below figure and formula:&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693069_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Hypothesis to fit the X data&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M6_hypothesis.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M6_hypothesis.png&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693070_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;maximize the likelihood&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M6_formula1.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M6_formula1.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Supervised learning can be framed as a &lt;span style="color: #000000"&gt;&lt;strong&gt;conditional probability&lt;/strong&gt;&lt;/span&gt; problem of predicting the probability of the output given the input, as per the below:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693071_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Conditional probability&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M6_formula2.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M6_formula2.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In order to use maximum likelihood, we need to assume a probability distribution. In the case of logistic regression, a &lt;span style="color: #1c8845"&gt;&lt;strong&gt;Binomial probability distribution&lt;/strong&gt;&lt;/span&gt; is assumed for the data sample, where each example is one outcome of a &lt;span style="color: #1c8845"&gt;&lt;strong&gt;Bernoulli trial&lt;/strong&gt;&lt;/span&gt;. The Bernoulli distribution has a single parameter: the probability of a successful outcome (p).&lt;/p&gt;&lt;p&gt;P(y=1) = p&lt;/p&gt;&lt;p&gt;P(y=0) = 1 – p&lt;/p&gt;&lt;p&gt;Watch this short video to refresh the concept of Bernoulli trials:&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;a href="https://www.youtube.com/embed/nl9WiZMZnYs?feature=oembed&amp;amp;wmode=opaque&amp;amp;rel=0" data-bbtype="video" data-bbfile="{&amp;quot;src&amp;quot;:&amp;quot;https://www.youtube.com/embed/nl9WiZMZnYs?feature=oembed&amp;amp;wmode=opaque&amp;amp;rel=0&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;https://www.youtube.com/embed/nl9WiZMZnYs?feature=oembed&amp;amp;wmode=opaque&amp;amp;rel=0&amp;quot;}"&gt;https://www.youtube.com/embed/nl9WiZMZnYs?feature=oembed&amp;amp;wmode=opaque&amp;amp;rel=0&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Now we can replace&amp;nbsp;&lt;span style="color: #000000"&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;/span&gt;&amp;nbsp;with our &lt;span style="color: #000000"&gt;&lt;strong&gt;logistic regression model.&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;The expected value of the Bernoulli distribution can be calculated as follows:&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;span style="color: #000000"&gt;Expected value = P(y=1) * 1 + P(y=0) * 0&lt;/span&gt;&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;span style="color: #000000"&gt;Or, given p:&lt;/span&gt;&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;span style="color: #000000"&gt;Expected value = p * 1 + (1 – p) * 0&lt;/span&gt;&lt;/p&gt;&lt;p&gt;This would be the basis for the likelihood function for a specific input, where the probability is given by the model (&lt;strong&gt;&lt;em&gt;yhat&lt;/em&gt;&lt;/strong&gt;) and the actual label is given from the data set.&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;span style="color: #000000"&gt;likelihood = yhat * y + (1 – yhat) * (1 – y)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;This function will always return a large probability when the model is close to the matching class value, and a small value when it is far away, for both&amp;nbsp;y=0&amp;nbsp;and&amp;nbsp;y=1&amp;nbsp;cases.&lt;/p&gt;&lt;p&gt;We can verify the above equation by running the following code:&lt;/p&gt;&lt;pre&gt;def likelihood(y, yhat):&lt;/pre&gt;&lt;pre&gt;    return yhat * y + (1 - yhat) * (1-y)&lt;/pre&gt;&lt;pre&gt; &lt;/pre&gt;&lt;pre&gt;# test for y=1&lt;/pre&gt;&lt;pre&gt;y, yhat = 1, 0.9&lt;/pre&gt;&lt;pre&gt;print('y=%.1f, yhat=%.1f, likelihood: %.3f' %(y, yhat, likelihood(y, yhat)))&lt;/pre&gt;&lt;pre&gt;y, yhat = 1, 0.1&lt;/pre&gt;&lt;pre&gt;print('y=%.1f, yhat=%.1f, likelihood: %.3f' %(y, yhat, likelihood(y, yhat)))&lt;/pre&gt;&lt;pre&gt; &lt;/pre&gt;&lt;pre&gt;# test for y=0&lt;/pre&gt;&lt;pre&gt;y, yhat = 0, 0.1&lt;/pre&gt;&lt;pre&gt;print('y=%.1f, yhat=%.1f, likelihood: %.3f' %(y, yhat, likelihood(y, yhat)))&lt;/pre&gt;&lt;pre&gt;y, yhat = 0, 0.0&lt;/pre&gt;&lt;pre&gt;print('y=%.1f, yhat=%.1f, likelihood: %.3f' %(y, yhat, likelihood(y, yhat)))&lt;/pre&gt;&lt;p&gt;This function will always return a large probability when the model is close to the matching class value, and a small value when it is far away, for both&amp;nbsp;y=0&amp;nbsp;and&amp;nbsp;y=1&amp;nbsp;cases. We can take the logarithm of the equation and then sum over all the training observation to reach the following maximum likelihood equation:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693072_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;maximum likelihood&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M6_formula3.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M6_formula3.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Remember in machine learning we need to &lt;strong&gt;minimize the cost function&lt;/strong&gt;, so take the inverse of the above equation, as follows:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693073_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;minimize&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M6_formula4.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M6_formula4.png&lt;/a&gt;&lt;/p&gt;&lt;h5&gt;&lt;span style="color: #1c8845"&gt;Gradient Descent&lt;/span&gt;&lt;/h5&gt;&lt;br&gt;&lt;p&gt;Another way to estimate the best fit parameters for a logistic regression function is an approach named Gradient Descent. We will not cover the full details but will present the main ideas on how we can apply it with regard to Logistic regression.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Watch this video then continue reading:&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;a href="https://mediasite.centennialcollege.ca/Mediasite/Play/0d82cd3bdd094924a6281bb91b2ccb921d" data-bbtype="video" data-bbfile="{&amp;quot;src&amp;quot;:&amp;quot;https://mediasite.centennialcollege.ca/Mediasite/Play/0d82cd3bdd094924a6281bb91b2ccb921d&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;https://mediasite.centennialcollege.ca/Mediasite/Play/0d82cd3bdd094924a6281bb91b2ccb921d&amp;quot;}"&gt;https://mediasite.centennialcollege.ca/Mediasite/Play/0d82cd3bdd094924a6281bb91b2ccb921d&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;In linear regression we used the &lt;span style="color: #000000"&gt;&lt;strong&gt;L&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;/strong&gt;&lt;/span&gt; loss function. We noted that the loss function is a &lt;span style="color: #000000"&gt;&lt;strong&gt;convex&lt;/strong&gt;&lt;/span&gt; and that allows to find the &lt;span style="color: #000000"&gt;&lt;strong&gt;Global minimum.&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt; As per the below figure:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693074_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Linear regression&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M6_linear_versus_logistic.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M6_linear_versus_logistic.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;If we simplify and plot the loss function in 2 dimensions and do the same for a logistic regression we will note that the logistic function does not follow the same convex shape as per the below figure:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693075_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;loss function for linear and logistic&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M6_linear_logistic.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:1050.4000244140625,&amp;quot;height&amp;quot;:393.793776500006}"&gt;M6_linear_logistic.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We might not be able to reach the global minimum and get stuck at a local minimum.&lt;/p&gt;&lt;p&gt;Since we only have two possible outcomes 1 and 0 we want to assign &lt;span style="color: #000000"&gt;&lt;strong&gt;more punishment&lt;/strong&gt;&lt;/span&gt; when predicting 1 while the actual is 0 and when predict 0 while the actual is 1.&lt;/p&gt;&lt;p&gt;The loss function of logistic regression is doing this exactly which is called&amp;nbsp;Logistic Loss.&amp;nbsp;&lt;/p&gt;&lt;p&gt;If y = 1, when prediction = 1, the cost = 0, when prediction = 0, the learning algorithm is punished by a very large cost.&lt;/p&gt;&lt;p&gt;if y = 0, predicting 0 has no punishment but predicting 1 has a large value of cost.&lt;/p&gt;&lt;p&gt;Let us split the loss in this case into two graphs, one for the case y=1 and one for the case y =0, as per below:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693076_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Split the loss graph into two&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M6_loss_gradient.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:965.4000244140625,&amp;quot;height&amp;quot;:360.1916664251562}"&gt;M6_loss_gradient.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We can now write the loss equation as follows:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693077_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;loss function formula based on y&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M6_gr_formula1.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M6_gr_formula1.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Although we are looking at it by y = 1 and y = 0 separately, it can be written as one single formula which brings convenience for calculation.&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693078_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;One formula for logistic&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M6_gr_formula2.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M6_gr_formula2.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Multiplying by y and (1−y) in the above equation is a trick that let’s us use the same equation to solve for both y=1 and y=0 cases.&lt;/p&gt;&lt;p&gt;This is also inline,&lt;span style="font-size: 0.875rem;"&gt;&amp;nbsp;if we bring the two parts together we have a &lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 0.875rem;"&gt;&lt;strong&gt;convex !!&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="font-size: 0.875rem;"&gt; as per the below figure:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693079_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Both parts together&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M6_gradient2.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M6_gradient2.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Now, this looks like the &lt;strong&gt;L&lt;/strong&gt;&lt;strong&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;/strong&gt; loss function.&amp;nbsp;&lt;img src="https://s.brightspace.com/lib/emoticons/1.0.0/blush-light.svg" alt="blush with light skin tone emoticon"&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Great, we have overcome the problem of&lt;span style="color: #000000"&gt;&lt;strong&gt; local minimums&lt;/strong&gt;&lt;/span&gt; and can use the concept of the &lt;span style="color: #000000"&gt;&lt;strong&gt;L&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;/strong&gt;&lt;/span&gt; loss function but using the &lt;em&gt;sigmoid&lt;/em&gt; equations. What is key is the differential equations for the &lt;em&gt;sigmoid&lt;/em&gt; function which are as follows:&lt;/p&gt;&lt;p style="text-align: center;"&gt;g’(z) = g(z)(1-g(z))&lt;/p&gt;&lt;p style="text-align: center;"&gt;g’(w.x) = g(w.x)(1-g(w.x))&amp;nbsp;&lt;/p&gt;&lt;p&gt;We can then apply the gradient descent to reach the global minimum. Gradient descent will be covered in a future video.&lt;/p&gt;&lt;h5&gt;Summary&lt;/h5&gt;&lt;p&gt;Logistic regression is considered a &lt;span style="color: #000000"&gt;&lt;strong&gt;classification&lt;/strong&gt;&lt;/span&gt; algorithm.&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693080_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Sigmoid function for Logistic regression&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M6_sigmoid_with_eq.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M6_sigmoid_with_eq.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;If the model returns 0.4 it believes there is only a 40% chance of passing. If our decision boundary was 0.5, we would categorize this observation as “Fail”.&lt;/p&gt;&lt;p&gt;For Linear regression we used the sum of least squares to estimate the parameters of the model.&lt;/p&gt;&lt;p&gt;For Logistic regression we use Gradient descent or maximum likelihood estimation (MLE).&lt;/p&gt;&lt;p&gt;Both Gradient descent and maximum likelihood estimation (MLE) approaches can be used for other machine learning algorithms.&lt;/p&gt;&lt;p&gt;The below table illustrates the main differences between Linear regression and Logistic regression:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693081_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Comparission between linear and logistic&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M6_comparision.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M6_comparision.png&lt;/a&gt;&lt;/p&gt;&lt;h5&gt;References&lt;/h5&gt;&lt;ol&gt;&lt;li&gt;Chapter 19 &lt;span style="color: #000000"&gt;&lt;span style="font-family: Open Sans;"&gt;&lt;span style="font-size: 0.875rem;"&gt;Artificial intelligence a modern approach&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="font-family: Open Sans;"&gt;&lt;span style="font-size: 0.875rem;"&gt;by Stuart J. Russell and Peter Norvig&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;Chapter 6 &lt;span style="font-family: Open Sans;"&gt;&lt;span style="font-size: 0.875rem;"&gt;Python: Advanced Predictive Analytics, by Joseph Babcock and Ashish Kumar.&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://machinelearningmastery.com/what-is-maximum-likelihood-estimation-in-machine-learning/"&gt;Ref: https://machinelearningmastery.com/what-is-maximum-likelihood-estimation-in-machine-learning/&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</TEXT><TYPE
   value="H"/></BODY><DATES><CREATED value="2024-10-04 18:31:45 EDT"/><UPDATED value="2024-11-13 23:20:15 EST"/><START
   value=""/><END value=""/></DATES><FLAGS><ISAVAILABLE value="true"/><ISFROMCARTRIDGE value="false"/><ISFOLDER
   value="false"/><ISDESCRIBED value="false"/><ISTRACKED value="true"/><ISLESSON value="false"/><ISSEQUENTIAL
   value="false"/><ALLOWGUESTS value="true"/><ALLOWOBSERVERS value="true"/><LAUNCHINNEWWINDOW
   value="false"/><ISREVIEWABLE value="false"/><ISGROUPCONTENT value="false"/><ISSAMPLECONTENT
   value="false"/><PARTIALLYVISIBLE value="false"/><HASTHUMBNAIL value="false"/></FLAGS><CONTENTHANDLER
  value="resource/x-bb-document"/><RENDERTYPE value="REGULAR"/><FOLDERTYPE value=""/><URL value=""/><VIEWMODE
  value="TEXT_ICON_ONLY"/><OFFLINENAME value=""/><OFFLINEPATH value=""/><LINKREF value=""/><PARENTID
  value="_800517_1"/><REVIEWABLEREASON value="NONE"/><VERSION value="3"/><THUMBNAILALT value=""/><AISTATE
  value="No"/><AIACCEPTINGUSER value=""/><EXTENDEDDATA/><FILES/></CONTENT>
