<?xml version="1.0" encoding="UTF-8"?>
<CONTENT id="_800670_1"><TITLE value="Fairness and Bias"/><TITLECOLOR value="#000000"/><DESCRIPTION
   value=""/><BODY><TEXT>&lt;div data-layout-row="8d16e8b7-95cf-445d-8052-911328982fa9"&gt;&lt;div data-layout-column="24d8650d-eea0-4d11-8265-5954fbfd2162" data-layout-column-width="12"&gt;&lt;div data-bbid="bbml-editor-id_860cff24-e70e-4445-89fd-f5242a8a8094"&gt;&lt;h4&gt;Topic 13.2: Fairness and Bias&lt;/h4&gt;&lt;br&gt;&lt;h5&gt;&lt;span style="color: #1c8845"&gt;&lt;span style="font-size: 1.125rem;"&gt;Fairness and Bias&lt;/span&gt;&lt;/span&gt;&lt;/h5&gt;&lt;br&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;Machine learning&lt;/strong&gt;&lt;/span&gt; is augmenting and replacing human &lt;span style="color: #000000"&gt;&lt;strong&gt;decision making&lt;/strong&gt;&lt;/span&gt; in some important situations:&lt;/p&gt;&lt;p&gt;Let us take some examples:&lt;/p&gt;&lt;p&gt;•Who gets a loan?&lt;/p&gt;&lt;p&gt;•To what neighborhoods police officers are deployed?&lt;/p&gt;&lt;p&gt;•Who gets parole?&lt;/p&gt;&lt;p&gt;But we have to be careful because training data examples might have social/demographic data that will lead the algorithm to learn bias.&amp;nbsp;&lt;/p&gt;&lt;h6&gt;Important:&lt;/h6&gt;&lt;p&gt;&lt;span style="font-size: 0.875rem;"&gt;The machine learning algorithms can &lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 0.875rem;"&gt;perpetuate&lt;/span&gt;&lt;/span&gt;&lt;span style="font-size: 0.875rem;"&gt; &lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 0.875rem;"&gt;&lt;em&gt;societal bias.&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Let us take an example:&lt;/p&gt;&lt;h6&gt;Example:&lt;/h6&gt;&lt;p&gt;If you were asked to develop a model to &lt;span style="color: #000000"&gt;predict&lt;/span&gt; whether criminal defendants &lt;span style="color: #000000"&gt;are likely to reoffend&lt;/span&gt;. Based on model output a decision would be made to weather to release before trial or not.&lt;/p&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;Societal bias:&lt;/span&gt; the algorithm might pick-up from the training examples could be:&lt;/p&gt;&lt;p&gt;• Gender&lt;/p&gt;&lt;p&gt;• Race&lt;/p&gt;&lt;p&gt;• Prejudices of human judges&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693261_1" data-bbtype="attachment" data-bbfile="{&amp;quot;linkName&amp;quot;:&amp;quot;File_dtqubk6fga7djr0hqsx0r5bhlycpbab1001116265532.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;File_dtqubk6fga7djr0hqsx0r5bhlycpbab1001116265532.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="font-size: 0.625rem;"&gt;Image source : Prepared by Mayy Habayeb&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Designers of machine learning systems have a moral responsibility to ensure that their systems are in fact fair.&lt;/p&gt;&lt;p&gt;But what is Fairness?&amp;nbsp;&lt;/p&gt;&lt;p&gt;The figure below illustrates the most commonly six-concepts for fairness:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693262_1" data-bbtype="attachment" data-bbfile="{&amp;quot;linkName&amp;quot;:&amp;quot;File_n4zlphf84ugqgyxnmuw0v1xfgqel0sog001116265532.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;File_n4zlphf84ugqgyxnmuw0v1xfgqel0sog001116265532.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="font-size: 0.625rem;"&gt;Image source : Prepared by Mayy Habayeb&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Expand the below to learn more about each criteria:&lt;/p&gt;&lt;h6&gt;Individual fairness&lt;/h6&gt;&lt;p&gt;A requirement that individuals are treated similarly to other similar individuals regardless of what class they are in.&lt;/p&gt;&lt;h6&gt;Group fairness&lt;/h6&gt;&lt;p&gt;A requirement that two classes be treated similarly, as measured by some summary statistics.&lt;/p&gt;&lt;h6&gt;Equal outcome&lt;/h6&gt;&lt;p&gt;The idea that each demographic class gets the same results; they have demographic parity.&lt;/p&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;p&gt;We are asked to develop a system to decide whether to approve a loan or not.&amp;nbsp;The goal is to approve the applicants that will not default, i.e. the applicants who will pay back.&lt;/p&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;Demographic parity states in this case both males and females have the same approval percentage of loans approved. This is a group fairness criteria. This approach favors redress (fixing) of past biases over accuracy.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-family: Comic Sans MS;"&gt;Let us think a minute&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693263_1" data-bbtype="attachment" data-bbfile="{&amp;quot;linkName&amp;quot;:&amp;quot;File_pct50nvebx0istka5zjqugeavztx4vr3001116265532.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;File_pct50nvebx0istka5zjqugeavztx4vr3001116265532.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 0.625rem;"&gt;Image source : Prepared by Mayy Habayeb&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;If a man and a woman are equal in every way, except the woman receives a lower salary for the same job (How unfair&#x1f616;), Should she be approved because she would be equal if not for historical biases! or should she be rejected because the lower salary might cause her to default??&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;h6&gt;Equal opportunity&lt;/h6&gt;&lt;p&gt;The idea that the people who truly have the &lt;span style="color: #000000"&gt;ability to pay back&lt;/span&gt; the loan should have an &lt;span style="color: #000000"&gt;equal chance &lt;/span&gt;of being correctly classified.&lt;/p&gt;&lt;p&gt;This approach is also called &lt;span style="color: #000000"&gt;&lt;strong&gt;"Balance"&lt;/strong&gt;&lt;/span&gt;.&lt;/p&gt;&lt;p&gt;It can lead to unequal outcomes and ignores the effect of historical biases.&lt;/p&gt;&lt;h6&gt;Equal impact&lt;/h6&gt;&lt;p&gt;People with similar likelihood to pay back the loan should have the same expected utility, regardless of the class they belong to.&lt;/p&gt;&lt;p&gt;This goes beyond "Equal opportunity" in that it considers both the benefits of a true prediction and the cost of a false prediction.&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;h5&gt;References&lt;/h5&gt;&lt;ol&gt;&lt;li&gt;Chapter 27 &lt;span style="color: #000000"&gt;&lt;span style="font-family: Open Sans;"&gt;&lt;span style="font-size: 0.875rem;"&gt;Artificial intelligence a modern approach&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="font-family: Open Sans;"&gt;&lt;span style="font-size: 0.875rem;"&gt;by Stuart J. Russell and Peter Norvig,&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</TEXT><TYPE
   value="H"/></BODY><DATES><CREATED value="2024-10-04 18:31:49 EDT"/><UPDATED value="2024-11-14 19:10:53 EST"/><START
   value=""/><END value=""/></DATES><FLAGS><ISAVAILABLE value="true"/><ISFROMCARTRIDGE value="false"/><ISFOLDER
   value="false"/><ISDESCRIBED value="false"/><ISTRACKED value="true"/><ISLESSON value="false"/><ISSEQUENTIAL
   value="false"/><ALLOWGUESTS value="true"/><ALLOWOBSERVERS value="true"/><LAUNCHINNEWWINDOW
   value="false"/><ISREVIEWABLE value="false"/><ISGROUPCONTENT value="false"/><ISSAMPLECONTENT
   value="false"/><PARTIALLYVISIBLE value="false"/><HASTHUMBNAIL value="false"/></FLAGS><CONTENTHANDLER
  value="resource/x-bb-document"/><RENDERTYPE value="REGULAR"/><FOLDERTYPE value=""/><URL value=""/><VIEWMODE
  value="TEXT_ICON_ONLY"/><OFFLINENAME value=""/><OFFLINEPATH value=""/><LINKREF value=""/><PARENTID
  value="_800554_1"/><REVIEWABLEREASON value="NONE"/><VERSION value="3"/><THUMBNAILALT value=""/><AISTATE
  value="No"/><AIACCEPTINGUSER value=""/><EXTENDEDDATA/><FILES/></CONTENT>
