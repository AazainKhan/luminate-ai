<?xml version="1.0" encoding="UTF-8"?>
<CONTENT id="_800692_1"><TITLE value="Topic 9.4: Learning networks and back propagation algorithm"/><TITLECOLOR
  value="#000000"/><DESCRIPTION
   value=""/><BODY><TEXT>&lt;div data-layout-row="613e0580-c313-40ec-9715-ff22ef481c77"&gt;&lt;div data-layout-column="8a200c4d-554f-428a-8133-5e7c05bb43c3" data-layout-column-width="12"&gt;&lt;div data-bbid="bbml-editor-id_513e8312-503d-4fbe-aa10-cb8531d6d58b"&gt;&lt;h4&gt;Topic 9.4: Learning networks and back propagation algorithm&lt;/h4&gt;&lt;br&gt;&lt;h5&gt;&lt;span style="color: #1c8845"&gt;Learning the network&lt;/span&gt;&lt;/h5&gt;&lt;br&gt;&lt;p&gt;From the previous topics it is obvious that we need to learn the best weights and bias from the training data.&lt;/p&gt;&lt;p&gt;Watch this video then continue reading:&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;a href="https://mediasite.centennialcollege.ca/Mediasite/Play/c37b8c13026a42a4b504911d8a06bf781d" data-bbtype="video" data-bbfile="{&amp;quot;src&amp;quot;:&amp;quot;https://mediasite.centennialcollege.ca/Mediasite/Play/c37b8c13026a42a4b504911d8a06bf781d&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;https://mediasite.centennialcollege.ca/Mediasite/Play/c37b8c13026a42a4b504911d8a06bf781d&amp;quot;}"&gt;https://mediasite.centennialcollege.ca/Mediasite/Play/c37b8c13026a42a4b504911d8a06bf781d&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;The key is weights update and once we feed forward a network we need to move backward from the output layer to the input layer passing through each layer and recalculating&amp;nbsp;weights and biases at each step.&lt;/p&gt;&lt;p&gt;This method is called &lt;span style="color: #000000"&gt;&lt;strong&gt;Back propagation&lt;/strong&gt;&lt;/span&gt; for the way the that the error at the output is passed back through the network.&lt;/p&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;Stochastic gradient descent&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt; &lt;/strong&gt;is usually used for these problems especially when the training data is&amp;nbsp;large and the number of dimensions is high.&lt;/p&gt;&lt;h6&gt;To visualize the concept of the back propagation please watch the following video:&lt;/h6&gt;&lt;p style="text-align: center;"&gt;&lt;a href="https://www.youtube.com/embed/Ilg3gGewQ5U?feature=oembed&amp;amp;wmode=opaque&amp;amp;rel=0" data-bbtype="video" data-bbfile="{&amp;quot;src&amp;quot;:&amp;quot;https://www.youtube.com/embed/Ilg3gGewQ5U?feature=oembed&amp;amp;wmode=opaque&amp;amp;rel=0&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;https://www.youtube.com/embed/Ilg3gGewQ5U?feature=oembed&amp;amp;wmode=opaque&amp;amp;rel=0&amp;quot;}"&gt;https://www.youtube.com/embed/Ilg3gGewQ5U?feature=oembed&amp;amp;wmode=opaque&amp;amp;rel=0&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;As long as we can calculate the derivatives of such expressions with respect to the weights, we can use the gradient-descent loss-minimization method to train the network. Now that we have a good idea about how the back propagation works let us see an example of calculating two weights from our previous simple example.&lt;/p&gt;&lt;h6&gt;To follow you will need to review the &lt;span style="color: #000000"&gt;"Chain rule"&lt;/span&gt; from calculus. If you need to refresh on the "Chain rule" watch this short video:&lt;/h6&gt;&lt;p style="text-align: center;"&gt;&lt;a href="https://www.youtube.com/embed/0T0QrHO56qg?feature=oembed&amp;amp;wmode=opaque&amp;amp;rel=0" data-bbtype="video" data-bbfile="{&amp;quot;src&amp;quot;:&amp;quot;https://www.youtube.com/embed/0T0QrHO56qg?feature=oembed&amp;amp;wmode=opaque&amp;amp;rel=0&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;https://www.youtube.com/embed/0T0QrHO56qg?feature=oembed&amp;amp;wmode=opaque&amp;amp;rel=0&amp;quot;}"&gt;https://www.youtube.com/embed/0T0QrHO56qg?feature=oembed&amp;amp;wmode=opaque&amp;amp;rel=0&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h6&gt;Example&lt;/h6&gt;&lt;p&gt;Let us go back to the simple network introduced in topic 9.3, as per the below figure.&lt;/p&gt;&lt;p&gt;Let us first calculate the weight and calculate the weight &lt;span style="color: #000000"&gt;&lt;strong&gt;w&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;&lt;sub&gt;3,5&lt;/sub&gt;&lt;/strong&gt;&lt;/span&gt; and the calculate the weight &lt;span style="color: #000000"&gt;&lt;strong&gt;w&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;&lt;sub&gt;1,3 &lt;/sub&gt;&lt;/strong&gt;&lt;/span&gt;using the back propagation algorithm&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693162_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;simple ANN&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;PastedImage_fgc5ma57x25jwvt1zcqtdy1mdfkhl0ds001102388000.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;PastedImage_fgc5ma57x25jwvt1zcqtdy1mdfkhl0ds001102388000.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The figure below shows the mathematical steps for calculating the weight&amp;nbsp;&lt;span style="color: #000000"&gt;&lt;strong&gt;w&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;&lt;sub&gt;3,5&lt;/sub&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693163_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Formulas to calculate the weight w35&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M9_backw35.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M9_backw35.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The figure below shows the mathematical steps for calculating the weight&amp;nbsp;&lt;span style="color: #000000"&gt;&lt;strong&gt;w&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;&lt;sub&gt;1,3&lt;/sub&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693164_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Back propagation mathematical solution for calculating the weight w1,3&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M9_back13.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M9_back13.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;If we define &lt;span style="color: #000000"&gt;∆&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;sub&gt;5&lt;/sub&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt; &lt;/span&gt;as the perceived error at the point where unit 5 receives its input : &lt;span style="color: #000000"&gt;∆&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;sub&gt;5&lt;/sub&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&amp;nbsp;&lt;/span&gt; = 2(y^-y)g’5(in&lt;sub&gt;5&lt;/sub&gt;), then the gradient is &lt;span style="color: #000000"&gt;∆&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;sub&gt;5&lt;/sub&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;a&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;sub&gt;3&lt;/sub&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;If &lt;span style="color: #000000"&gt;∆&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;sub&gt;5&lt;/sub&gt;&lt;/span&gt; is positive that means y^ is too big.&lt;/p&gt;&lt;p&gt;If &lt;span style="color: #000000"&gt;a&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;sub&gt;3&lt;/sub&gt;&lt;/span&gt; is also positive then increasing &lt;span style="color: #000000"&gt;w&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;sub&gt;3,5&lt;/sub&gt;&lt;/span&gt; will make things &lt;span style="color: #000000"&gt;worse&lt;/span&gt;.&lt;/p&gt;&lt;p&gt;If &lt;span style="color: #000000"&gt;a&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;sub&gt;3&lt;/sub&gt;&lt;/span&gt; is negative then increasing w3,5 will make things &lt;span style="color: #000000"&gt;better&lt;/span&gt; i.e. decrease the error &lt;span style="color: #000000"&gt;∆&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;sub&gt;5.&lt;/sub&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693165_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Back propogation&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M9_back_propogation.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M9_back_propogation.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Notice that all activation functions are monotonically non-decreasing, which means there derivatives are non negative.&lt;/p&gt;&lt;br&gt;&lt;h5&gt;References&lt;/h5&gt;&lt;p&gt;1- Chapter 21 Artificial intelligence a modern approach by Stuart J. Russell and Peter Norvig.&lt;/p&gt;&lt;p&gt;2- FA18 CS188 Lecture 22 -- Optimization and Neural Nets&lt;/p&gt;&lt;p&gt;3- Chapter 19&amp;nbsp;&lt;a href="https://learning.oreilly.com/library/view/artificial-intelligence-with/9781786464392/"&gt;Artificial Intelligence with Python second edition&lt;/a&gt;&amp;nbsp;by Prateek Joshi&amp;nbsp;&amp;nbsp;by Prateek Joshi&lt;/p&gt;&lt;p&gt;4-&amp;nbsp;&lt;a href="https://dzone.com/articles/the-very-basic-introduction-to-feed-forward-neural"&gt;https://dzone.com/articles/the-very-basic-introduction-to-feed-forward-neural&lt;/a&gt;&lt;/p&gt;&lt;p&gt;5-&amp;nbsp;&lt;a href="https://en.wikipedia.org/wiki/Feedforward_neural_network"&gt;https://en.wikipedia.org/wiki/Feedforward_neural_network&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</TEXT><TYPE
   value="H"/></BODY><DATES><CREATED value="2024-10-04 18:31:47 EDT"/><UPDATED value="2024-11-14 00:00:37 EST"/><START
   value=""/><END value=""/></DATES><FLAGS><ISAVAILABLE value="true"/><ISFROMCARTRIDGE value="false"/><ISFOLDER
   value="false"/><ISDESCRIBED value="false"/><ISTRACKED value="true"/><ISLESSON value="false"/><ISSEQUENTIAL
   value="false"/><ALLOWGUESTS value="true"/><ALLOWOBSERVERS value="true"/><LAUNCHINNEWWINDOW
   value="false"/><ISREVIEWABLE value="false"/><ISGROUPCONTENT value="false"/><ISSAMPLECONTENT
   value="false"/><PARTIALLYVISIBLE value="false"/><HASTHUMBNAIL value="false"/></FLAGS><CONTENTHANDLER
  value="resource/x-bb-document"/><RENDERTYPE value="REGULAR"/><FOLDERTYPE value=""/><URL value=""/><VIEWMODE
  value="TEXT_ICON_ONLY"/><OFFLINENAME value=""/><OFFLINEPATH value=""/><LINKREF value=""/><PARENTID
  value="_800533_1"/><REVIEWABLEREASON value="NONE"/><VERSION value="3"/><THUMBNAILALT value=""/><AISTATE
  value="No"/><AIACCEPTINGUSER value=""/><EXTENDEDDATA/><FILES/></CONTENT>
