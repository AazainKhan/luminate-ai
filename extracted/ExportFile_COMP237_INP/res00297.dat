<?xml version="1.0" encoding="UTF-8"?>
<CONTENT id="_800659_1"><TITLE value="Topic 10.2: Bayes rule"/><TITLECOLOR value="#000000"/><DESCRIPTION
   value=""/><BODY><TEXT>&lt;div data-layout-row="a6899fb9-114f-460d-9a77-b405b349b722"&gt;&lt;div data-layout-column="562d7abe-cdfb-456f-82ca-e9613d383809" data-layout-column-width="12"&gt;&lt;div data-bbid="bbml-editor-id_081c8d5c-7f8f-4c8e-8aeb-9d6404fd45e1"&gt;&lt;h4&gt;Topic 10.2: Bayes rule&lt;/h4&gt;&lt;h4&gt;&lt;span style="color: #1c8845"&gt;Language models&lt;/span&gt;&lt;/h4&gt;&lt;br&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;From the previous topic we notice that a Language model is "A&amp;nbsp;probability distribution&amp;nbsp;describing the&amp;nbsp;likelihood&amp;nbsp;of any&amp;nbsp;string". Bayes Theorem comes in play here. &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Watch this video then continue reading:&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;a href="https://mediasite.centennialcollege.ca/Mediasite/Play/5331704e5ee74637958378ec01f560b61d" data-bbtype="video" data-bbfile="{&amp;quot;src&amp;quot;:&amp;quot;https://mediasite.centennialcollege.ca/Mediasite/Play/5331704e5ee74637958378ec01f560b61d&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;https://mediasite.centennialcollege.ca/Mediasite/Play/5331704e5ee74637958378ec01f560b61d&amp;quot;}"&gt;https://mediasite.centennialcollege.ca/Mediasite/Play/5331704e5ee74637958378ec01f560b61d&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;h5&gt;Bayes Theorem&lt;/h5&gt;&lt;p&gt;Thomas Bayes (/beɪz/; c. 1701 – 7 April 1761) was an English statistician, philosopher and Presbyterian minister who is known for formulating a specific case of the theorem that bears his name: Bayes' theorem.&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693184_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Thomas Bayes&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M10_Thomas.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M10_Thomas.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Bayes' Theorem&amp;nbsp;is a way of finding a&lt;span style="color: #000000"&gt;&lt;strong&gt; probability&lt;/strong&gt;&lt;/span&gt;&amp;nbsp;when we know certain other probabilities.&lt;/p&gt;&lt;p&gt;The formula is:&lt;/p&gt;&lt;p style="text-align: center;"&gt;P(A|B) =&amp;nbsp;P(A)&amp;nbsp;*P(B|A)&lt;em&gt; /&lt;/em&gt;P(B)&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Which tells us:&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;how often A happens&amp;nbsp;&lt;em&gt;given that B happens&lt;/em&gt;, written&amp;nbsp;&lt;strong&gt;P(A|B)&lt;/strong&gt;,&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;When we know:&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;how often B happens&amp;nbsp;&lt;em&gt;given that A happens&lt;/em&gt;, written&amp;nbsp;&lt;strong&gt;P(B|A)&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;and how likely A is on its own, written&amp;nbsp;&lt;strong&gt;P(A)&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;and how likely B is on its own, written&amp;nbsp;&lt;strong&gt;P(B)&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br&gt;&lt;p&gt;&lt;span style="color: #046ef6"&gt; &lt;/span&gt;&lt;span style="color: #046ef6"&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Let us say P(Fire) means how often there is fire, and &lt;strong&gt;P(Smoke)&lt;/strong&gt; means how often we see smoke, then:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;P(Fire|Smoke)&lt;/strong&gt; means how often there is fire when we can see smoke&lt;/li&gt;&lt;li&gt;&lt;strong&gt;P(Smoke|Fire)&lt;/strong&gt; means how often we can see smoke when there is fire&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;So, the formula tells us "forwards"&amp;nbsp;&lt;strong&gt;P(Fire|Smoke)&lt;/strong&gt;&amp;nbsp;when we know "backwards"&amp;nbsp;&lt;strong&gt;P(Smoke|Fire)&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;dangerous fires are rare (1%)&lt;/li&gt;&lt;li&gt;but smoke is fairly common (10%) due to barbecues,&lt;/li&gt;&lt;li&gt;and 90% of dangerous fires make smoke&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We can then discover the&amp;nbsp;&lt;strong&gt;probability of dangerous Fire when there is Smoke&lt;/strong&gt;:&lt;/p&gt;&lt;p&gt;P(Fire|Smoke) = &lt;em&gt;P(Fire) P(Smoke|Fire) /&amp;nbsp;&lt;/em&gt;P(Smoke)&lt;/p&gt;&lt;p&gt;=1% x 90% /&amp;nbsp;10%&lt;/p&gt;&lt;p&gt;=9%&lt;/p&gt;&lt;h5&gt;Naive Bayes Classifier&lt;/h5&gt;&lt;br&gt;&lt;p&gt;The &lt;span style="color: #000000"&gt;&lt;strong&gt;Naive Bayes Classifier&lt;/strong&gt;&lt;/span&gt; technique is based on the so-called Bayesian theorem and is particularly suited when the&lt;span style="color: #000000"&gt;&lt;strong&gt; dimensionality&lt;/strong&gt;&lt;/span&gt; of the &lt;strong&gt;inputs is high&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;Despite its simplicity, &lt;span style="color: #000000"&gt;&lt;strong&gt;Naive Bayes&lt;/strong&gt;&lt;/span&gt; can often outperform more sophisticated classification methods.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Naive Bayes:&lt;/strong&gt; a popular &lt;span style="color: #000000"&gt;&lt;strong&gt;generative&lt;/strong&gt;&lt;/span&gt; model for &lt;span style="color: #000000"&gt;&lt;strong&gt;classification&lt;/strong&gt;&lt;/span&gt;.&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;​&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-2830379_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;teacher_small.png&amp;quot;,&amp;quot;displayName&amp;quot;:&amp;quot;teacher_small.png&amp;quot;,&amp;quot;fileName&amp;quot;:&amp;quot;teacher_small.png&amp;quot;,&amp;quot;fileSize&amp;quot;:17210,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inlineOnly&amp;quot;,&amp;quot;resourceUrl&amp;quot;:&amp;quot;@X@EmbeddedFile.requestUrlStub@X@sessions/CE/CEA4C703B41DAEBA0198FCB9EA1A6F9F/b2ab92b71044480db30a3107118c4e92/teacher_small.png&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;teacher_small.png&amp;quot;}"&gt;teacher_small.png&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;​&lt;/p&gt;&lt;p&gt;&lt;span style="color: #1c8845"&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="color: #1c8845"&gt;&lt;strong&gt;The difference between discriminative and generative models is discriminative models (logistic regression, Support vector machines (SVM), etc.) try to model the conditional probability distribution while generative models (Naive Bayes, HMM..etc) try to model the joint probability.&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br&gt;&lt;p&gt;Naive Bayes Performance is competitive to most of state-of-the-art classifiers even in presence of violating independence assumption. Many successful applications, e.g. spam mail filtering.&lt;/p&gt;&lt;p&gt;To demonstrate the concept of&lt;span style="color: #000000"&gt;&amp;nbsp;Naive&amp;nbsp;Bayes&lt;/span&gt; Classification, consider the example displayed in the below figure.&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693185_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Green and red data points&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M10_green_red.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M10_green_red.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The objects can be classified as either Green or Red. Our task is to classify new cases as they arrive, i.e., decide to which class label they belong, based on the currently exiting objects.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Since there are twice as many Green points as Red, it is reasonable to believe that a new case (which hasn't been observed yet) is twice as likely to have membership Green rather than Red. In the &lt;span style="color: #000000"&gt;&lt;strong&gt;Bayesian analysis&lt;/strong&gt;&lt;/span&gt;, this belief is known as the prior&lt;span style="color: #000000"&gt;&lt;strong&gt; probability&lt;/strong&gt;&lt;/span&gt;.&lt;/p&gt;&lt;h6&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;Prior probabilities&lt;/strong&gt;&lt;/span&gt;&lt;/h6&gt;&lt;p&gt;Prior probabilities are based on previous experience, in this case the percentage of GREEN and RED points, and often used to predict outcomes before they actually happen.&lt;/p&gt;&lt;p&gt;Since there is a total of 60 points, 40 of which are GREEN and 20 RED, our prior probabilities for class membership are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Prior probability of Green points ≈&amp;nbsp;Number of Green points / Total number of points&amp;nbsp;≈ 40/60&lt;/li&gt;&lt;li&gt;Prior probability of Red points ≈&amp;nbsp;Number of Red points / Total number of points&amp;nbsp;≈ 20/60&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Having calculated our prior probability, we are now ready to classify a new point (White circle) as per the below diagram:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693186_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;data points green and red in addition to a new white point&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M10_green_red_white.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M10_green_red_white.png&lt;/a&gt;&lt;/p&gt;&lt;h6&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;Likelihood&lt;/strong&gt;&lt;/span&gt;&lt;/h6&gt;&lt;p&gt;Since the points are well clustered, it is reasonable to assume that the more Green (or Red) points in the vicinity (the area near or surrounding a particular place ) of the white point (X), the more likely that the new cases belong to that particular color. To measure this &lt;span style="color: #000000"&gt;&lt;strong&gt;likelihood&lt;/strong&gt;&lt;/span&gt;, we draw a circle around the white point (X) which encompasses a number&amp;nbsp;of points irrespective of their class labels. Then we calculate the number of points in the circle belonging to each class label. From this we calculate the likelihood:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Likelihood X given Green ≈&amp;nbsp;Number of Green in the vicinity of X/ Total number of green points&amp;nbsp;&amp;nbsp;≈&amp;nbsp;Probability of X given Green&amp;nbsp;≈&amp;nbsp;1/40&lt;/li&gt;&lt;li&gt;Likelihood X given Red&amp;nbsp;&amp;nbsp;≈&amp;nbsp;Number of&amp;nbsp;Red&lt;span style="font-size: 0.875rem;"&gt;&amp;nbsp;in the &lt;/span&gt;vicinity&lt;span style="font-size: 0.875rem;"&gt;&amp;nbsp;of X/ Total number of green points&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;≈&amp;nbsp;Probability of X given Red&amp;nbsp;&amp;nbsp;&amp;nbsp;≈ 3/20&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Although the prior probabilities indicate that X may belong to Green (given that there are twice as many Green compared to Red) the likelihood indicates otherwise; that the class membership of X is Red (given that there are more Red objects in the vicinity of X than Green).&lt;/p&gt;&lt;p&gt;In the Bayesian analysis, the final classification is produced by combining &lt;strong&gt;both sources&lt;/strong&gt; of information:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;The prior&lt;/li&gt;&lt;li&gt;The likelihood&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;to form a posterior probability using the Bayes' rule:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Posterior probability of X being Green ≈ Prior probability of Green points * Likelihood X given Green ≈ 4/6&amp;nbsp;* 1/40 ≈ 1/60&lt;/li&gt;&lt;li&gt;Posterior probability of X being Red ≈ Prior probability of Red points * Likelihood X given Red ≈ 2/6&amp;nbsp;* 3/20 ≈ 1/20&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We classify X as Red since its class membership achieves the largest posterior probability.&lt;/p&gt;&lt;h5&gt;Bayes classification&lt;/h5&gt;&lt;p&gt;We can extend the concept to a dataset &lt;span style="color: #000000"&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/span&gt; containing a vector of features &lt;span style="color: #000000"&gt;&lt;strong&gt;F&lt;/strong&gt;&lt;/span&gt; containing many input features &lt;span style="color: #000000"&gt;&lt;strong&gt;x&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;....x&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;&lt;sub&gt;j&lt;/sub&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/span&gt;and a vector of classes&lt;span style="color: #000000"&gt;&lt;strong&gt; L&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt; &lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;c&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;....c&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;sub&gt;L&lt;/sub&gt;&lt;/span&gt; i.e. instead of two classes red and green mentioned in the above example, we can have many classes.&lt;/p&gt;&lt;p&gt;The most important thing is to assume these features are independent, and we tend to learn (estimate) the class probability as per the below:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693187_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Learning Naive Bayes&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M10_Bayes_f1.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M10_Bayes_f1.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;Looks complicated &lt;/span&gt;&lt;img src="https://s.brightspace.com/lib/emoticons/1.0.0/wow-light.svg" alt="wow with light skin tone emoticon"&gt;&lt;span style="color: #000000"&gt;.&amp;nbsp;&amp;nbsp;Don't worry if the math looks complicated we will do an example, as per below:&lt;/span&gt;&lt;/p&gt;&lt;h4&gt;&lt;span style="color: #1c8845"&gt;Example&lt;/span&gt;&lt;/h4&gt;&lt;p&gt;Assume a group of friends play tennis every day in the Park &lt;span style="color: #000000"&gt;&lt;strong&gt;only&amp;nbsp;if the weather conditions are favorable&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;.&lt;/span&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693188_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;friends play tennis Ref: https://freesvg.org/tennis-players&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M10_image_play_tennis.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M10_image_play_tennis.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;A researcher visited the park for &lt;span style="color: #000000"&gt;&lt;strong&gt;14&lt;/strong&gt;&lt;/span&gt; days and &lt;span style="color: #000000"&gt;&lt;strong&gt;recorded&lt;/strong&gt;&lt;/span&gt; the details of the weather condition (Outlook, Temperature, Humidity and wind), in addition to if the friends&lt;span style="color: #000000"&gt;&lt;strong&gt; showed up to play&lt;/strong&gt;&lt;/span&gt; or not that day (Yes or No), the table below show the results that the researcher recorded:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693189_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;14 day observations of weather condition and if teams played at the park or not&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M10_play_t1.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M10_play_t1.png&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;span style="color: #1c8845"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;strong&gt;First: Learning phase&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;From the above table we can first calculate the prior probabilities&amp;nbsp;as follows:&lt;/p&gt;&lt;p style="text-align: center;"&gt;P(Play=Yes) = 9/14&lt;/p&gt;&lt;p style="text-align: center;"&gt;P(Play=No) = 5/14&lt;/p&gt;&lt;p&gt;We can then calculate the likelihood probabilities by generating the probability distribution tables i.e frequencies&amp;nbsp;for each feature, as follows:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693190_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;probability tables&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M10_play_t2.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M10_play_t2.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Accordingly, the conditional probabilities are:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693191_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Conditional probabilities&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M10_play_conditional.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:947.4000244140625,&amp;quot;height&amp;quot;:221.8925154853937}"&gt;M10_play_conditional.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="color: #1c8845"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;strong&gt;Second: Testing phase&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Given a &lt;span style="color: #000000"&gt;new instance x’ &lt;/span&gt;, predict its class(label)&lt;/p&gt;&lt;p style="text-align: center;"&gt;x’=(Outlook=Sunny, Temperature=Cool, Humidity=High, Wind=Strong)&lt;/p&gt;&lt;p&gt;From the tables we can get the relevant likelihoods for each feature as follows:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693192_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;conditional probabilities related to x'&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M10_test_conditional.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:435.9000244140625,&amp;quot;height&amp;quot;:396.33490045853586}"&gt;M10_test_conditional.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Finally, we can calculate(estimate) the possible two class&amp;nbsp;probabilities:&lt;/p&gt;&lt;p style="text-align: left;"&gt;P(Yes|x’) ≈ [P(Sunny|Yes)P(Cool|Yes)P(High|Yes)P(Strong|Yes)]P(Play=Yes) = 0.0053&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2/9&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;* 3/9&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;* 3/9&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;* 3/9&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;*&amp;nbsp;&amp;nbsp;&amp;nbsp;9/14&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;=0.0053&lt;/p&gt;&lt;p&gt;P(No|x’) ≈ [P(Sunny|No) P(Cool|No)P(High|No)P(Strong|No)]P(Play=No) = 0.0206&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3/5&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;* 1/5&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;* 4/5&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;* 3/5&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;*&amp;nbsp;&amp;nbsp;&amp;nbsp;5/14&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;=0.0206&lt;/p&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;Given the fact P(Yes|x’) &amp;lt; P(No|x’), we label x’ to be “No”.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;h5&gt;References&lt;/h5&gt;&lt;ol&gt;&lt;li&gt;Chapter 12 Artificial intelligence a modern approach by Stuart J. Russell and Peter Norvig.&lt;/li&gt;&lt;li&gt;Chapter 5&amp;amp;15 &lt;a href="https://learning.oreilly.com/library/view/artificial-intelligence-with/9781786464392/"&gt;Artificial Intelligence with Python&lt;/a&gt;&amp;nbsp;second edition by Prateek Joshi&lt;/li&gt;&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Thomas_Bayes"&gt;https://en.wikipedia.org/wiki/Thomas_Bayes&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.mathsisfun.com/data/bayes-theorem.html"&gt;https://www.mathsisfun.com/data/bayes-theorem.html&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</TEXT><TYPE
   value="H"/></BODY><DATES><CREATED value="2024-10-04 18:31:47 EDT"/><UPDATED value="2024-11-14 16:12:43 EST"/><START
   value=""/><END value=""/></DATES><FLAGS><ISAVAILABLE value="true"/><ISFROMCARTRIDGE value="false"/><ISFOLDER
   value="false"/><ISDESCRIBED value="false"/><ISTRACKED value="true"/><ISLESSON value="false"/><ISSEQUENTIAL
   value="false"/><ALLOWGUESTS value="true"/><ALLOWOBSERVERS value="true"/><LAUNCHINNEWWINDOW
   value="false"/><ISREVIEWABLE value="false"/><ISGROUPCONTENT value="false"/><ISSAMPLECONTENT
   value="false"/><PARTIALLYVISIBLE value="false"/><HASTHUMBNAIL value="false"/></FLAGS><CONTENTHANDLER
  value="resource/x-bb-document"/><RENDERTYPE value="REGULAR"/><FOLDERTYPE value=""/><URL value=""/><VIEWMODE
  value="TEXT_ICON_ONLY"/><OFFLINENAME value=""/><OFFLINEPATH value=""/><LINKREF value=""/><PARENTID
  value="_800537_1"/><REVIEWABLEREASON value="NONE"/><VERSION value="3"/><THUMBNAILALT value=""/><AISTATE
  value="No"/><AIACCEPTINGUSER value=""/><EXTENDEDDATA/><FILES/></CONTENT>
