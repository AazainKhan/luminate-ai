<?xml version="1.0" encoding="UTF-8"?>
<CONTENT id="_800663_1"><TITLE value="Topic 5.2 Supervised Learning"/><TITLECOLOR value="#000000"/><DESCRIPTION
   value=""/><BODY><TEXT>&lt;div data-layout-row="e409c506-fa71-4ac0-b317-abaa25685394"&gt;&lt;div data-layout-column="9fce3484-864d-4cd0-be97-1da623b585e6" data-layout-column-width="12"&gt;&lt;div data-bbid="bbml-editor-id_cb23e8d4-88c6-47f1-9697-8d18c74294c4"&gt;&lt;h4&gt;&lt;span style="color: #1c8845"&gt;Supervised learning&lt;/span&gt;&lt;/h4&gt;&lt;h5&gt;Overview&lt;/h5&gt;&lt;p&gt;In supervised learning the main concept is to let the machine learn from &lt;span style="color: #000000"&gt;experiences and examples&lt;/span&gt; rather than &lt;span style="color: #1c8845"&gt;&lt;strong&gt;hard-coded rules.&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Watch this video before continuing to read the topic:&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;a href="https://mediasite.centennialcollege.ca/Mediasite/Play/1cec67d039344395bf8f67e0926feb901d" data-bbtype="video" data-bbfile="{&amp;quot;src&amp;quot;:&amp;quot;https://mediasite.centennialcollege.ca/Mediasite/Play/1cec67d039344395bf8f67e0926feb901d&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;https://mediasite.centennialcollege.ca/Mediasite/Play/1cec67d039344395bf8f67e0926feb901d&amp;quot;}"&gt;https://mediasite.centennialcollege.ca/Mediasite/Play/1cec67d039344395bf8f67e0926feb901d&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;For example, if you were given an problem that involved the computer to decided if an object is a tree or flower, as per the below figure, you have two choices:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Either write code to examine certain aspects of each object for example the height and code an "if then else", or&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Use machine learning.&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693007_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Trees and flowers&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M5_tree_flower.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:704,&amp;quot;height&amp;quot;:315.42346368715084}"&gt;M5_tree_flower.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The first option will work for most of the cases, but what if you encounter the following object which is a very short tree like Salix herbacea, “dwarf willow”?&lt;/p&gt;&lt;p&gt;Your code would incorrectly classify this object into a flower.&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693008_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Salix herbacea, “dwarf willow”&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M5_Salix_herbacea.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:664,&amp;quot;height&amp;quot;:218.29000000000002}"&gt;M5_Salix_herbacea.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Instead, we&amp;nbsp;use algorithms to &lt;span style="color: #000000"&gt;&lt;strong&gt;figure out the rules &lt;/strong&gt;&lt;/span&gt;and train the algorithms using &lt;span style="color: #000000"&gt;&lt;strong&gt;historical data&lt;/strong&gt;&lt;/span&gt;. As per the below figure, each observation is labeled and the machine learns from the labeled historical data:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693009_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Supervised labeled data&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M5_super_tree_flower.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:790.4000244140625,&amp;quot;height&amp;quot;:350.7726210128512}"&gt;M5_super_tree_flower.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The more data we give the better results we get.&lt;/p&gt;&lt;p&gt;All supervised learning algorithms are based on the following:&lt;/p&gt;&lt;p&gt;Given a training set (data) of &lt;strong&gt;N&lt;/strong&gt; examples (x1,y1), (x2,y2),… (xN,yN), where each pair was generated by an unknown function y = f(x), we try to discover a function &lt;strong&gt;&lt;em&gt;h&lt;/em&gt;&lt;/strong&gt; that approximates the true function &lt;strong&gt;&lt;em&gt;f&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;The proper functioning of such algorithms depends significantly on the input data:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;If there are only a &lt;strong&gt;few &lt;/strong&gt;training inputs, the algorithm might not have enough experience to provide a correct output&lt;/li&gt;&lt;li&gt;If there are m&lt;span style="text-decoration:underline;"&gt;any&lt;/span&gt; inputs may make it excessively slow since the derivative function generated by a large number of inputs could be very complicated&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693010_1" data-bbtype="attachment" data-bbfile="{&amp;quot;linkName&amp;quot;:&amp;quot;PastedImage_ddgjm62pyu1gh6squo502r46h2z9h64s001100762417.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;PastedImage_ddgjm62pyu1gh6squo502r46h2z9h64s001100762417.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The below figure illustrates different models:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693011_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Different models to fit the data&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M5_different_models.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:878,&amp;quot;height&amp;quot;:237.90967741935484}"&gt;M5_different_models.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;/span&gt; is the model for the data, drawn from a model class&lt;span style="color: #000000"&gt;&lt;strong&gt; Ɦ&lt;/strong&gt;&lt;/span&gt;. The output &lt;span style="color: #000000"&gt;&lt;strong&gt;yi&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt; &lt;/strong&gt;is the&lt;span style="color: #000000"&gt;&lt;strong&gt; Ground truth&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt; &lt;/span&gt;we are asking our model to predict.&lt;/p&gt;&lt;p&gt;The target is a &lt;span style="color: #000000"&gt;&lt;strong&gt;consistent hypothesis&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt; &lt;/span&gt;where:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693012_1" data-bbtype="attachment" data-bbfile="{&amp;quot;linkName&amp;quot;:&amp;quot;PastedImage_yvw567s9gzzmu8pf5hojq5kd7p0dc7pv001100762417.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;PastedImage_yvw567s9gzzmu8pf5hojq5kd7p0dc7pv001100762417.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Look for the best-fit function where&lt;span style="color: #000000"&gt;&lt;strong&gt; h(xi)&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt; &lt;/strong&gt;is close to&lt;span style="color: #000000"&gt;&lt;strong&gt;&amp;nbsp;yi&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;h5&gt;&lt;span style="color: #1c8845"&gt;Main steps of Supervised Learning&lt;/span&gt;&lt;/h5&gt;&lt;h6&gt;Step #1&lt;/h6&gt;&lt;p&gt;Split the data into training and testing as per the below figure. Keep the testing data aside:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693013_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Split the data into training and testing&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M5_split.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:912.4000244140625,&amp;quot;height&amp;quot;:397.31928505496853}"&gt;M5_split.png&lt;/a&gt;&lt;/p&gt;&lt;h6&gt;Step #2&lt;/h6&gt;&lt;p&gt;Use the training data on &lt;strong&gt;&lt;em&gt;h&lt;/em&gt;&lt;/strong&gt; to come up with parameters (weights) of &lt;strong&gt;&lt;em&gt;h&lt;/em&gt;&lt;/strong&gt;, as per the below figure:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693014_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Train the model&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M5_train.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:691.625,&amp;quot;height&amp;quot;:348.0255425320346}"&gt;M5_train.png&lt;/a&gt;&lt;/p&gt;&lt;h6&gt;Step #3&lt;/h6&gt;&lt;p&gt;Test the model as per the below figure:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693015_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Test the model using the test data&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M5_test.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:824.4000244140625,&amp;quot;height&amp;quot;:316.78334480879136}"&gt;M5_test.png&lt;/a&gt;&lt;/p&gt;&lt;h6&gt;Step #4:&lt;/h6&gt;&lt;p&gt;Compare the predicated results with the actual test data labels (&lt;strong&gt;&lt;em&gt;y&lt;/em&gt;&lt;/strong&gt; actual), as per the below figure:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693016_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Compare the predicted with the actual&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M5_compare.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:756.0499877929688,&amp;quot;height&amp;quot;:200}"&gt;M5_compare.png&lt;/a&gt;&lt;/p&gt;&lt;h5&gt;&lt;span style="color: #1c8845"&gt;Supervised learning Model validation – (Bias and Variance)&lt;/span&gt;&lt;/h5&gt;&lt;p&gt;Let us learn a bit more about bias and variance:&lt;/p&gt;&lt;h5&gt;&lt;span style="color: #1c8845"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;strong&gt;Bias&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h5&gt;&lt;br&gt;&lt;p&gt;Any predictive&amp;nbsp;model needs to be &lt;strong&gt;validated&lt;/strong&gt; to see how it is performing on &lt;span style="color: #000000"&gt;&lt;strong&gt;different sets of data&lt;/strong&gt;&lt;/span&gt;, whether the accuracy of the model is constant over all the sources of similar data or not.&lt;/p&gt;&lt;p&gt;This checks the problem of&lt;span style="color: #000000"&gt;&lt;strong&gt; underfitting&lt;/strong&gt;&lt;/span&gt;, wherein the model fits very well in one set of data but &lt;span style="color: #000000"&gt;&lt;strong&gt;doesn't fit&lt;/strong&gt;&lt;/span&gt; that well in&amp;nbsp;&lt;span style="color: #000000"&gt;&lt;strong&gt;another dataset&lt;/strong&gt;&lt;/span&gt;. We call this the &lt;strong&gt;Bias&lt;/strong&gt; of the Model &lt;span style="color: #000000"&gt;&lt;strong&gt;h.&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;This is the tendency of a&lt;span style="color: #000000"&gt;&lt;strong&gt; predictive hypothesis&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt; &lt;/strong&gt;to deviate from the &lt;span style="color: #000000"&gt;&lt;strong&gt;expected results&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt; &lt;/strong&gt;when averaged over different training sets, as per the figure below:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693017_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Different training datasets&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M5_biaspng.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:595,&amp;quot;height&amp;quot;:218.68094701240136}"&gt;M5_biaspng.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="color: #1c8845"&gt;&lt;span style="font-size: 1.5rem;"&gt;&lt;strong&gt;Variance&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;By &lt;strong&gt;Variance&lt;/strong&gt; we mean the amount of change in the hypothesis due to the&lt;strong&gt; fluctuation&lt;/strong&gt;&amp;nbsp;in &lt;span style="color: #000000"&gt;&lt;strong&gt;the training dataset.&lt;/strong&gt;&lt;/span&gt; If the &lt;span style="color: #000000"&gt;&lt;strong&gt;fluctuation&lt;/strong&gt;&lt;/span&gt; is high and the model fits, that means the model is &lt;span style="color: #000000"&gt;&lt;strong&gt;overfitting&lt;/strong&gt;&lt;/span&gt; and has high variance. The below figure illustrates an overfitting scenario, notice the green line tries to encompass all the blue data points:&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693018_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Overfitting - Variance&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M5_overfitting.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:597.0499877929688,&amp;quot;height&amp;quot;:381.7115866805371}"&gt;M5_overfitting.png&lt;/a&gt;&lt;/p&gt;&lt;h5&gt;&lt;span style="color: #1c8845"&gt;Generalizing the model&lt;/span&gt;&lt;/h5&gt;&lt;br&gt;&lt;p&gt;&lt;span style="color: #000000"&gt;Have a look at the below figure. The same data points are fitted into two models: &lt;/span&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;a noisy (roughly linear) linear function. (black)&lt;/li&gt;&lt;li&gt;A polynomial function.&lt;/li&gt;&lt;/ol&gt;&lt;p style="text-align: center;"&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693019_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Two models on same data Ref: https://en.wikipedia.org/wiki/Overfitting#/media/File:Overfitting.svg&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M5_bias_versus_variance.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M5_bias_versus_variance.png&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Although the polynomial function is a &lt;strong&gt;perfect fit&lt;/strong&gt;, the&lt;span style="color: #000000"&gt;&lt;strong&gt; linear function can be expected to generalize better&lt;/strong&gt;&lt;/span&gt;: if the two functions were used to extrapolate beyond the fitted data, the linear function should &lt;span style="color: #000000"&gt;&lt;strong&gt;make better predictions&lt;/strong&gt;&lt;/span&gt;.&lt;/p&gt;&lt;h5&gt;&lt;span style="color: #1c8845"&gt;Bias – Variance trade off&lt;/span&gt;&lt;/h5&gt;&lt;p style="text-align: center;"&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693020_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Albert Einstein Ref:https://cs.wikipedia.org/wiki/Albert_Einstein&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M5_Albert.jpg&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/jpeg&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M5_Albert.jpg&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span style="color: #046ef6"&gt;&lt;span style="font-size: 1.125rem;"&gt;The supreme goal of all theory is to make the irreducible basic elements as simple and&amp;nbsp;as few as possible without having to surrender the adequate representation of a single datum of experience&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;                                                                                                                                                                                          &lt;/strong&gt;   &lt;span style="color: #046ef6"&gt;&lt;strong&gt; [Albert Einstein]&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;span style="color: #1c8845"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;em&gt;Our goal in machine learning is to select the hypothesis that will optimally fit future examples.&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;h6&gt;&lt;span style="color: #1c8845"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;strong&gt;The loss function &lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h6&gt;&lt;br&gt;&lt;p&gt;In machine learning, we want to &lt;span style="color: #1c8845"&gt;&lt;strong&gt;minimize&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #1c8845"&gt; &lt;/span&gt;the loss of the utility function that the agents try to maximize.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;span style="font-size: 1.125rem;"&gt;Loss function &lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 1.125rem;"&gt;&lt;strong&gt;L:&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="font-size: 1.125rem;"&gt;L (x,y,&lt;/span&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mstyle mathsize="24px"&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;/mstyle&gt;&lt;/math&gt;&lt;span style="font-size: 1.125rem;"&gt;) = Utility (result of using a&amp;nbsp;given input x) – Utility (result of using &lt;/span&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mstyle mathsize="24px"&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;/mstyle&gt;&lt;/math&gt;&lt;span style="font-size: 1.125rem;"&gt; given input x)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="font-size: 1.125rem;"&gt;L(y,y) is always zero&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="font-size: 1.125rem;"&gt;Many types of loss functions:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mstyle mathsize="24px"&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfenced open="|" close="|"&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mstyle displaystyle="true"&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mstyle&gt;&lt;/math&gt;&lt;/p&gt;&lt;p&gt;&lt;math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mstyle mathsize="24px"&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mfenced&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mstyle displaystyle="true"&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mstyle&gt;&lt;/math&gt;&lt;/p&gt;&lt;p&gt;&lt;math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mstyle mathsize="24px"&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mstyle&gt;&lt;/math&gt;&lt;span style="font-size: 1.125rem;"&gt;&amp;nbsp;if&amp;nbsp;&amp;nbsp;y= &lt;/span&gt;&lt;math xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mstyle mathsize="24px"&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;/mstyle&gt;&lt;/math&gt;&lt;span style="font-size: 1.125rem;"&gt; else 1&lt;/span&gt;&lt;/p&gt;&lt;p&gt;There are many other loss functions but for now we will focus on the concept.&lt;/p&gt;&lt;h6&gt;Watch this video:&lt;/h6&gt;&lt;p style="text-align: center;"&gt;&lt;a href="https://www.youtube.com/embed/BmNKbnF69eY?feature=oembed&amp;amp;wmode=opaque&amp;amp;rel=0" data-bbtype="video" data-bbfile="{&amp;quot;src&amp;quot;:&amp;quot;https://www.youtube.com/embed/BmNKbnF69eY?feature=oembed&amp;amp;wmode=opaque&amp;amp;rel=0&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;https://www.youtube.com/embed/BmNKbnF69eY?feature=oembed&amp;amp;wmode=opaque&amp;amp;rel=0&amp;quot;}"&gt;https://www.youtube.com/embed/BmNKbnF69eY?feature=oembed&amp;amp;wmode=opaque&amp;amp;rel=0&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h5&gt;References&lt;/h5&gt;&lt;ol&gt;&lt;li&gt;Artificial intelligence a modern approach&amp;nbsp;by Stuart J. Russell and Peter Norvig. Chapter 19.&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</TEXT><TYPE
   value="H"/></BODY><DATES><CREATED value="2024-10-04 18:31:44 EDT"/><UPDATED value="2024-11-11 22:50:48 EST"/><START
   value=""/><END value=""/></DATES><FLAGS><ISAVAILABLE value="true"/><ISFROMCARTRIDGE value="false"/><ISFOLDER
   value="false"/><ISDESCRIBED value="false"/><ISTRACKED value="true"/><ISLESSON value="false"/><ISSEQUENTIAL
   value="false"/><ALLOWGUESTS value="true"/><ALLOWOBSERVERS value="true"/><LAUNCHINNEWWINDOW
   value="false"/><ISREVIEWABLE value="false"/><ISGROUPCONTENT value="false"/><ISSAMPLECONTENT
   value="false"/><PARTIALLYVISIBLE value="false"/><HASTHUMBNAIL value="false"/></FLAGS><CONTENTHANDLER
  value="resource/x-bb-document"/><RENDERTYPE value="REGULAR"/><FOLDERTYPE value=""/><URL value=""/><VIEWMODE
  value="TEXT_ICON_ONLY"/><OFFLINENAME value=""/><OFFLINEPATH value=""/><LINKREF value=""/><PARENTID
  value="_800510_1"/><REVIEWABLEREASON value="NONE"/><VERSION value="3"/><THUMBNAILALT value=""/><AISTATE
  value="No"/><AIACCEPTINGUSER value=""/><EXTENDEDDATA/><FILES/></CONTENT>
