<?xml version="1.0" encoding="UTF-8"?>
<CONTENT id="_800651_1"><TITLE value="Topic 12.1: Introduction to Computer Vision"/><TITLECOLOR
  value="#000000"/><DESCRIPTION
   value=""/><BODY><TEXT>&lt;div data-layout-row="2a57399f-5829-400d-a5f7-953d2c51b8e8"&gt;&lt;div data-layout-column="05f54a9b-23f2-40cf-b1b1-9e814a4a3275" data-layout-column-width="12"&gt;&lt;div data-bbid="bbml-editor-id_035708b2-dc0b-4216-befb-9531bb173529"&gt;&lt;h4&gt;Topic 12.1: Introduction to Computer Vision&lt;/h4&gt;&lt;br&gt;&lt;h5&gt;&lt;span style="color: #1c8845"&gt;Introduction to Computer Vision&lt;/span&gt;&lt;/h5&gt;&lt;br&gt;&lt;p&gt;Vision is a perceptual channel that accepts stimulus and reports representation of the world.&lt;/p&gt;&lt;p&gt;Most &lt;span style="color: #000000"&gt;&lt;strong&gt;agents&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt; &lt;/span&gt;that use vision use passive sensing they &lt;span style="color: #000000"&gt;&lt;strong&gt;do not need&lt;/strong&gt;&lt;/span&gt; to send out light to see.&lt;/p&gt;&lt;p&gt;Active sensing involves sending out a signal such as a radar or ultra sound:&lt;/p&gt;&lt;p&gt;Examples of active sensing are bats and dolphins.&lt;/p&gt;&lt;p&gt;A &lt;span style="color: #000000"&gt;&lt;strong&gt;feature&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt; &lt;/span&gt;is a number obtained by applying a simple computation to an image.&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693223_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Features&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;01_Intro.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;01_Intro.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Watch this video, then continue reading the material&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;a href="https://mediasite.centennialcollege.ca/Mediasite/Play/f2b583b1293e438388bc8801fda2f4201d" data-bbtype="video" data-bbfile="{&amp;quot;src&amp;quot;:&amp;quot;https://mediasite.centennialcollege.ca/Mediasite/Play/f2b583b1293e438388bc8801fda2f4201d&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;https://mediasite.centennialcollege.ca/Mediasite/Play/f2b583b1293e438388bc8801fda2f4201d&amp;quot;}"&gt;https://mediasite.centennialcollege.ca/Mediasite/Play/f2b583b1293e438388bc8801fda2f4201d&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h5&gt;&lt;span style="color: #1c8845"&gt;Computer Vision Applications&lt;/span&gt;&lt;/h5&gt;&lt;p&gt;Being able to "see" is&amp;nbsp;a critical component of the learning process in humans. Similarly, even though they use different method to "see," capturing images and recognizing what is&amp;nbsp;contained in those images is of paramount importance to computers in order to create datasets to feed machine learning pipelines and obtain insight from that data.&lt;/p&gt;&lt;h6&gt;Autonomous driving&lt;/h6&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693224_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Autonomous Driving&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;02_Autonomous Driving.jpg&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/jpeg&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;02_Autonomous Driving.jpg&lt;/a&gt;Computers, just like their human counterparts, need to be able to ingest gigabytes worth of data in any given second, analyze that data, and make life altering decisions in real time.&lt;/p&gt;&lt;h6&gt;Linking picture and words&lt;/h6&gt;&lt;p&gt;Many people create pictures and videos and share them in the internet. People need to search using words.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Solutions:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Tagging systems and image classification.&lt;/li&gt;&lt;li&gt;Another solution is building captioning systems i.e. systems that can write a caption of one or more sentences per image.&lt;/li&gt;&lt;li&gt;The COCO (Common objects in context) dataset is a comprehensive collection with over 200,000 captioned images.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693225_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Linking pictures and words&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;03_LinkingPicture.PNG&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;03_LinkingPicture.PNG&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h5&gt;Some application areas:&lt;/h5&gt;&lt;br&gt;&lt;p&gt;&lt;span style="color: #1c8845"&gt;&lt;strong&gt;1- Reconstruction from many views - model rebuilding&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Might build a 3 D from a collection of images.&lt;/p&gt;&lt;p&gt;The state of the art in multiple-view reconstruction is now highly advanced.&lt;/p&gt;&lt;p&gt;This figure outlines a system built by Michael Goesele and colleagues from the Universityof Washington, TU Darmstadt, and Microsoft Research. From a collection of pictures of a monument taken by a large community of users and posted on the Internet (a), their system can determine the viewing directions for those pictures, shown by the small black pyramids in (b) and a comprehensive 3D reconstruction shown in (c).&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693226_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Model Building&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;04_modelbuilding.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:821,&amp;quot;height&amp;quot;:366.3170254403131}"&gt;04_modelbuilding.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;span style="color: #1c8845"&gt;&lt;strong&gt;2- Mix animation with live actors in a video&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;To place computer graphic data into real video.&lt;/p&gt;&lt;p&gt;&lt;span style="color: #1c8845"&gt;&lt;strong&gt;3- Path reconstruction&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Robots need to know where they have been, if there is a camera we can build a model of the camera’s path of the world.&lt;/p&gt;&lt;p&gt;Construction management:&lt;/p&gt;&lt;p&gt;Keeping track of what is happening using Drones filming the current state then building a 3D model and exploring the difference between the plans and the reconstructions using visualization techniques.&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693227_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Path Reconstruction&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;05_PathReconstruction.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;05_PathReconstruction.png&lt;/a&gt;&lt;/p&gt;&lt;h6&gt;Geometry from a single view&lt;/h6&gt;&lt;p&gt;If you have seen many pictures of some category—say, birds—you can use them to produce a &lt;span style="color: #000000"&gt;&lt;strong&gt;3D reconstruction&lt;/strong&gt;&lt;/span&gt; from a single new view. You need to be sure that all objects have a fairly similar geometry (so a picture of an ostrich won’t help if you’re looking at a sparrow), but &lt;span style="color: #000000"&gt;&lt;strong&gt;classification methods&lt;/strong&gt;&lt;/span&gt; can sort this out. From the many images you can estimate how texture values in the image are distributed across the object, and thus complete the texture for parts of the bird you haven’t seen yet.&lt;/p&gt;&lt;h6&gt;Depth maps&lt;/h6&gt;&lt;p style="text-align: center;"&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693228_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Depth Maps&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;06_DepthMaps.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;06_DepthMaps.png&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;   An array giving a depth to each pixel in the image&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; &lt;span style="font-size: 3rem;"&gt;→&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; Obtain a dataset of images and depth maps.&lt;/p&gt;&lt;p&gt;Train a network to predict depth.&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;   If there are many images of a bird (single views). Using past data and geometry an optimization procedure can be used to build the &lt;strong&gt;parameters&lt;/strong&gt; for a 3D geometry of the bird.&lt;/p&gt;&lt;h6&gt;Making Pictures&lt;/h6&gt;&lt;p&gt;Using depth maps, then estimating the lightning in the image by matching to other images with known lighting.&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693229_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Image Transformation&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;07_ImageTransoformation.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;07_ImageTransoformation.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Neural networks can be trained to do image transformation.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Paired images&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Paired image translation where the input consists of aerial images and the corresponding map tiles, and the goal is to train a network to produce a map tile from an aerial image. (The system can also learn to generate aerial images from map tiles.) The network is trained by comparing yˆi (the output for example xi of type X) to the right output yi of type Y . Then at test time, the network must make new images of type Y from new inputs of type X.&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693230_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Paired Images&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;08_pairedimages.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;08_pairedimages.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Unpaired images&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Example: Train two transformation networks to transform images of horses automatically to Zebras.&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693231_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Unpaired Images&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;09_unpairedImages.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;09_unpairedImages.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Unpaired image translation: given two populations of images (here type X is horses and type Y is zebras), but no corresponding pairs, learn to translate a horse into a zebra. The method trains two predictors: one that maps type X to type Y, and another that maps type Y to type X. If the first network maps a horse x&lt;sub&gt;i&lt;/sub&gt; to a zebra yˆ&lt;sub&gt;i&lt;/sub&gt; , the second network should map yˆ&lt;sub&gt;i&lt;/sub&gt; back to the original x&lt;sub&gt;i&lt;/sub&gt; . The difference between x&lt;sub&gt;i&lt;/sub&gt; and xˆ&lt;sub&gt;i&lt;/sub&gt; is what trains the two networks. The cycle from Y to X and back must be closed. Such networks can successfully impose rich transformations on images.&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;h5&gt;References&lt;/h5&gt;&lt;ol&gt;&lt;li&gt;Chapter 25 Artificial intelligence a modern approach by Stuart J. Russell and Peter Norvig.&lt;/li&gt;&lt;li&gt;Chapter 15 Artificial intelligence with python Second edition by Patrik&lt;/li&gt;&lt;li&gt;&lt;a href="https://victorzhou.com/blog/bag-of-words/"&gt;https://victorzhou.com/blog/bag-of-words/&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</TEXT><TYPE
   value="H"/></BODY><DATES><CREATED value="2024-10-04 18:31:48 EDT"/><UPDATED value="2024-11-14 16:48:45 EST"/><START
   value=""/><END value=""/></DATES><FLAGS><ISAVAILABLE value="true"/><ISFROMCARTRIDGE value="false"/><ISFOLDER
   value="false"/><ISDESCRIBED value="false"/><ISTRACKED value="true"/><ISLESSON value="false"/><ISSEQUENTIAL
   value="false"/><ALLOWGUESTS value="true"/><ALLOWOBSERVERS value="true"/><LAUNCHINNEWWINDOW
   value="false"/><ISREVIEWABLE value="false"/><ISGROUPCONTENT value="false"/><ISSAMPLECONTENT
   value="false"/><PARTIALLYVISIBLE value="false"/><HASTHUMBNAIL value="false"/></FLAGS><CONTENTHANDLER
  value="resource/x-bb-document"/><RENDERTYPE value="REGULAR"/><FOLDERTYPE value=""/><URL value=""/><VIEWMODE
  value="TEXT_ICON_ONLY"/><OFFLINENAME value=""/><OFFLINEPATH value=""/><LINKREF value=""/><PARENTID
  value="_800548_1"/><REVIEWABLEREASON value="NONE"/><VERSION value="3"/><THUMBNAILALT value=""/><AISTATE
  value="No"/><AIACCEPTINGUSER value=""/><EXTENDEDDATA/><FILES/></CONTENT>
