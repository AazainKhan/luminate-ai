<?xml version="1.0" encoding="UTF-8"?>
<CONTENT id="_800648_1"><TITLE value="Topic 11.1 : Bag of words"/><TITLECOLOR value="#000000"/><DESCRIPTION
   value=""/><BODY><TEXT>&lt;div data-layout-row="dadec99a-e6a6-468e-8c79-5a73f80c234e"&gt;&lt;div data-layout-column="7d4562d6-0f68-45ab-9f1d-0f869a53f6d2" data-layout-column-width="12"&gt;&lt;div data-bbid="bbml-editor-id_3344e29e-24e7-4ce1-9b5a-556c35f87cf3"&gt;&lt;h4&gt;Topic 11.1 : Bag of words&lt;/h4&gt;&lt;br&gt;&lt;h4&gt;&lt;span style="color: #1c8845"&gt;Language models&lt;/span&gt;&lt;/h4&gt;&lt;br&gt;&lt;p&gt;Recall from the previous module we defined a language model as:&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;span style="color: #1c8845"&gt;&lt;span style="font-size: 1.125rem;"&gt;A probability distribution describing the likelihood of any string.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;In this module we will focus on the models and will start by explaining the first simple model &lt;span style="color: #000000"&gt;&lt;strong&gt;The Bag of words (BoW)&lt;/strong&gt;&lt;/span&gt;.&lt;/p&gt;&lt;p&gt;Watch this video then continue reading:&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;a href="https://mediasite.centennialcollege.ca/Mediasite/Play/2e6f5586621e4a968233e9a2d50956d21d" data-bbtype="video" data-bbfile="{&amp;quot;src&amp;quot;:&amp;quot;https://mediasite.centennialcollege.ca/Mediasite/Play/2e6f5586621e4a968233e9a2d50956d21d&amp;quot;,&amp;quot;alt&amp;quot;:&amp;quot;https://mediasite.centennialcollege.ca/Mediasite/Play/2e6f5586621e4a968233e9a2d50956d21d&amp;quot;}"&gt;https://mediasite.centennialcollege.ca/Mediasite/Play/2e6f5586621e4a968233e9a2d50956d21d&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;One of the main&amp;nbsp;goals of text analysis&amp;nbsp;with the Bag of Words model is to convert text into a numerical form so that we can use machine learning on it.&amp;nbsp;&lt;/p&gt;&lt;p&gt;It is a generative model that describes a process for generating sentence.&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693206_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Language models&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M11_language probability.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M11_language probability.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The&amp;nbsp;bag-of-words&amp;nbsp;(BOW)&lt;/strong&gt; model is a representation that turns arbitrary text into vectors&amp;nbsp;by counting how many times each word appears. This process is often referred to as&amp;nbsp;vectorization.&lt;/p&gt;&lt;p&gt;Have a look at the below image, if we cut into small pieces and describe i.e.write on&amp;nbsp;&lt;span style="font-size: 0.875rem;"&gt;each piece the&amp;nbsp;description, then put all the pieces of papers in&amp;nbsp;a bag we will end up with a bag of words representing the picture.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693207_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;Bag of words example Ref:https://commons.wikimedia.org/wiki/File:Bag_of_words.JPG:&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M11_bag_of_words.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;,&amp;quot;width&amp;quot;:555,&amp;quot;height&amp;quot;:382.2009202453988}"&gt;M11_bag_of_words.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The application of the Naïve Bayes to strings is called the bag of words classification model.&lt;/p&gt;&lt;p&gt;&lt;span style="font-size: 0.875rem;"&gt;In other words, we want to look at the joint probability distribution over all sentences and categories. That suggest we should consider all the words.&lt;/span&gt;&lt;/p&gt;&lt;h6&gt;Vectorization&lt;/h6&gt;&lt;p&gt;As mentioned earlier, The (BOW) model is a representation that turns arbitrary text into vectors&amp;nbsp;by counting how many times each word appears.&lt;/p&gt;&lt;p&gt;Let us look at two simple examples:&lt;/p&gt;&lt;h5&gt;Example 1:&lt;/h5&gt;&lt;p&gt;Consider the following sentences:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Sentence 1: &lt;/strong&gt;the cat sat&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Sentence 2: &lt;/strong&gt;the cat sat in the hat&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Sentence 3: &lt;/strong&gt;the cat with the hat&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We’ll refer to each of these as a text&amp;nbsp;document and each is a bag of words.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Determine the Vocabulary&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;the, cat, sat, in, hat,&amp;nbsp;with&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Count&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;   Document&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; the&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; cat&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; sat&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; in&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; hat&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; with&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;   the cat sat&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; 1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; 1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; 1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; 0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; 0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; 0&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;   the cat sat in the hat&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; 2&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; 1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; 1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; 1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; 1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; 0&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;   the cat with the hat&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; 2&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; 1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; 0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; 0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; 1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt; 1&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;   Now we have 6 numeric vectors for each document!&lt;/p&gt;&lt;ul&gt;&lt;li&gt;sentence 1: &lt;span style="color: #000000"&gt;&lt;strong&gt;[1, 1, 1, 0, 0, 0]&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;sentence 2:&lt;span style="color: #000000"&gt;&lt;strong&gt; [2, 1, 1, 1, 1, 0]&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;sentence 3: &lt;span style="color: #000000"&gt;&lt;strong&gt;[2, 1, 0, 0, 1, 1]&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h5&gt;Example 2:&lt;/h5&gt;&lt;p&gt;Consider the following sentences:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Sentence 1: &lt;/strong&gt;The children are playing in the hall&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Sentence 2: &lt;/strong&gt;The hall has a lot of space&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Sentence 3: &lt;/strong&gt;Lots of children like playing in an open space&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We’ll refer to each of these as a text&amp;nbsp;document and each is a bag of words.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; Determine the Vocabulary&lt;/p&gt;&lt;p style="text-align: center;"&gt;The, children, are, playing, in, hall, has, a, lot, of, space, like, an, open&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Count&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Sentence 1:&lt;span style="color: #000000"&gt;&lt;strong&gt; [2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;Sentence 2:&lt;span style="color: #000000"&gt;&lt;strong&gt; [1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0]&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;Sentence 3: &lt;span style="color: #000000"&gt;&lt;strong&gt;[0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1]&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;&lt;span style="color: #1c8845"&gt;Issues &amp;amp; solutions&lt;/span&gt;&lt;/h4&gt;&lt;p&gt;Some issues:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;With&lt;span style="color: #000000"&gt;&lt;strong&gt; large text&lt;/strong&gt;&lt;/span&gt; bodies &lt;span style="color: #000000"&gt;&lt;strong&gt;sparsity&lt;/strong&gt;&lt;/span&gt;.&lt;/li&gt;&lt;li&gt;Repeated word that are not&amp;nbsp;&lt;span style="color: #000000"&gt;&lt;strong&gt;useful&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;Loose &lt;span style="color: #000000"&gt;&lt;strong&gt;meaning&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt; &lt;/strong&gt;and grammar.&lt;/li&gt;&lt;li&gt;Common words with same meaning in different context.&lt;/li&gt;&lt;/ol&gt;&lt;br&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693208_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;issues Ref: :https://www.pxfuel.com/en/search?q=problem&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M11_issue.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M11_issue.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Some solutions:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Remove stop words (Common words)&lt;/li&gt;&lt;li&gt;Drop words that are very rare, but that might have high variance on our predictive model)&lt;/li&gt;&lt;li&gt;Stemming &amp;amp; Lemmatization&lt;/li&gt;&lt;li&gt;Hashing&lt;/li&gt;&lt;li&gt;We can add other features to the vectors like time message sent, sender e-mail in e-mails.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a href="@X@EmbeddedFile.requestUrlStub@X@bbcswebdav/xid-1693209_1" data-bbtype="attachment" data-bbfile="{&amp;quot;alternativeText&amp;quot;:&amp;quot;issue solution Ref:https://www.pxfuel.com/en/free-photo-qhhjt&amp;quot;,&amp;quot;linkName&amp;quot;:&amp;quot;M11_issue_solution.png&amp;quot;,&amp;quot;mimeType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;render&amp;quot;:&amp;quot;inline&amp;quot;}"&gt;M11_issue_solution.png&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Despite the issues Bag of Word models work very well in classification using machine algorithms such as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Naive Bayes&lt;/li&gt;&lt;li&gt;Logistic regression&lt;/li&gt;&lt;li&gt;Neural networks&lt;/li&gt;&lt;li&gt;Support vector machines&lt;/li&gt;&lt;/ul&gt;&lt;h6&gt;Text classifications&lt;/h6&gt;&lt;p&gt;Despite the issues it is still useful to do classification using &lt;span style="color: #000000"&gt;&lt;strong&gt;Naive Bayes&lt;/strong&gt;&lt;/span&gt;.&lt;/p&gt;&lt;p&gt;We can learn the prior probabilities required for the algorithm via a corpus of text, where each segment of text is labeled with a class.&lt;/p&gt;&lt;p&gt;A corpus consists of at least a million words and at least of tenth of thousand distinct vocabularies.&lt;/p&gt;&lt;p&gt;From the corpus we can estimate the Prior probabilities of each class &lt;strong&gt;P(Class)&lt;/strong&gt;.&lt;/p&gt;&lt;h5&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/h5&gt;&lt;p&gt;If we would like to see if a body of text relates to Business or Sports:&lt;/p&gt;&lt;p&gt;Class is:&amp;nbsp;&lt;span style="color: #000000"&gt;&lt;strong&gt;Business&lt;/strong&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;, &lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;Sport&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;So we can work out the prior probabilities:&lt;/p&gt;&lt;p style="text-align: center;"&gt;&lt;span style="color: #000000"&gt;&lt;strong&gt;P(Business),&amp;nbsp;P(Sport)&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;We can also use counts to learn the conditional probabilities of each word given the category&amp;nbsp;&lt;span style="color: #000000"&gt;P(w&lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;sub&gt;j&lt;/sub&gt;&lt;/span&gt;&lt;span style="color: #000000"&gt;|Class).&lt;/span&gt;&lt;/p&gt;&lt;p&gt;For example if we have seen &lt;span style="color: #000000"&gt;&lt;strong&gt;3000 texts&lt;/strong&gt;&lt;/span&gt;&amp;nbsp;&lt;span style="font-size: 0.875rem;"&gt;and &lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 0.875rem;"&gt;&lt;strong&gt;300&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="font-size: 0.875rem;"&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/span&gt;&lt;span style="font-size: 0.875rem;"&gt;were classified as business then we can estimate &lt;/span&gt;&lt;span style="color: #000000"&gt;&lt;span style="font-size: 0.875rem;"&gt;P(Business) ≈ 300/3000 ≈ 0.1&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;And if within the business category we have seen &lt;span style="color: #000000"&gt;100,000 words&lt;/span&gt; and in them the word &lt;span style="color: #000000"&gt;“stocks”&lt;/span&gt; appeared 700 times, then we can estimate &lt;span style="color: #000000"&gt;P(stocks|Class = Business) ≈ 700/100,000 ≈ 0.007&lt;/span&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;h5&gt;References&lt;/h5&gt;&lt;ol&gt;&lt;li&gt;Chapter 23 Artificial intelligence a modern approach by Stuart J. Russell and Peter Norvig.&lt;/li&gt;&lt;li&gt;Chapter 15 Artificial Intelligence with Python&amp;nbsp;&amp;nbsp;second edition by Prateek Joshi&lt;/li&gt;&lt;li&gt;&lt;a href="https://victorzhou.com/blog/bag-of-words/"&gt;https://victorzhou.com/blog/bag-of-words/&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</TEXT><TYPE
   value="H"/></BODY><DATES><CREATED value="2024-10-04 18:31:48 EDT"/><UPDATED value="2024-11-14 16:22:31 EST"/><START
   value=""/><END value=""/></DATES><FLAGS><ISAVAILABLE value="true"/><ISFROMCARTRIDGE value="false"/><ISFOLDER
   value="false"/><ISDESCRIBED value="false"/><ISTRACKED value="true"/><ISLESSON value="false"/><ISSEQUENTIAL
   value="false"/><ALLOWGUESTS value="true"/><ALLOWOBSERVERS value="true"/><LAUNCHINNEWWINDOW
   value="false"/><ISREVIEWABLE value="false"/><ISGROUPCONTENT value="false"/><ISSAMPLECONTENT
   value="false"/><PARTIALLYVISIBLE value="false"/><HASTHUMBNAIL value="false"/></FLAGS><CONTENTHANDLER
  value="resource/x-bb-document"/><RENDERTYPE value="REGULAR"/><FOLDERTYPE value=""/><URL value=""/><VIEWMODE
  value="TEXT_ICON_ONLY"/><OFFLINENAME value=""/><OFFLINEPATH value=""/><LINKREF value=""/><PARENTID
  value="_800542_1"/><REVIEWABLEREASON value="NONE"/><VERSION value="3"/><THUMBNAILALT value=""/><AISTATE
  value="No"/><AIACCEPTINGUSER value=""/><EXTENDEDDATA/><FILES/></CONTENT>
