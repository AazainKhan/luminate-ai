{"id": "9af33ce6-157d-4da1-aa43-9bfa8307b98a", "text": "Introduction to Data Science Using Python Afrand Agah, Ph.D.\nA Member of The Pennsylvania Alliance for Design of Open Textbooks\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License (CC BY-NC 4.0) as a part of PA-ADOPT, except where otherwise noted. Cover image by Artturi Jalli on Unsplash. The contents of this eTextbook were developed under a grant from the Fund for the Improvement of Postsecondary Education, (FIPSE), U.S. Department of Education. However, those contents do not necessarily represent the policy of the Department of Education, and you should not assume endorsement by the Federal Government. The Verdana (© 2006 Microsoft Corporation) and Courier New (© 2006 The Monotype Corporation) fonts have been used throughout this book, which is permitted by their licenses: License: You may use this font as permitted by the EULA for the product in which this font is included to display and print content. You may only (i) embed this font in content as permitted by the embedding restrictions included in this font; and (ii) temporarily download this font to a printer or other output device to help print content. Embedding: Editable embedding. This font may be embedded in documents and temporarily loaded on the remote system. Documents containing this font may be editable (Apple Inc. (2021). Font Book (Version 10.0 (404)) [App].).", "topic": "Math for ML", "subtopic": "erm", "section_heading": "Introduction to Data Science Using Python Afrand Agah, Ph.D.", "source_title": "Introduction-to-Data-Science-AAgah-20240620-1", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "270b8302-5a03-45b9-b98f-e6336cb25db6", "text": "as permitted by the embedding restrictions included in this font; and (ii) temporarily download this font to a printer or other output device to help print content. Embedding: Editable embedding. This font may be embedded in documents and temporarily loaded on the remote system. Documents containing this font may be editable (Apple Inc. (2021). Font Book (Version 10.0 (404)) [App].).\n\nAbout PA-ADOPT The Pennsylvania Alliance for Design of Open Textbooks (PA-ADOPT) is made up of four participating institutions from the Pennsylvania State System of Higher Education (PASSHE) that are all regional and primarily undergraduate institutions, situated in Southeastern Pennsylvania. The PA-ADOPT project addresses gaps in the open textbook marketplace, improves student learning, and mitigates rising student costs. PA-ADOPT was made possible by the US Department of Education Open Textbook Pilot Program. \nPreface Data science is the process of representing models that fit data. Its goal is to predict future output based on past observations of inputs. In data science, one collects information and interprets it to make decisions. A data scientist must have programming skills and an understanding of mathematics and statistical concepts. The first part of this book is an introduction to Python programming, which is a highly used language by data scientists, and the second part is an introduction to machine learning and statistical knowledge required for data science. Python is a popular programming language because of its scalability, readability, and strong community support. But perhaps the most important aspect is its extensive libraries and frameworks. \nAbout OER Open Educational Resources (OER) are instructional, learning, and research materials, digital or non, that are open-source and in the public domain or that are licensed so that users have free and perpetual permission to engage in the following activities: •Retain: the right to make, own, and control copies of the content •Reuse: the right to use the content in a wide range of ways •Revise: the right to adapt, adjust, modify, or alter the content itself •Remix: the right to combine the original or revised content with other open content to create something new •Redistribute: the right to share copies of the original content, revisions, and remixes with others. \nTable of Contents About PA-ADOPT 3Preface 4About OER 5Table of Contents 61. Installing Python 82. Introduction to Programming 102.1. Variables 112.1.1. Boolean Variables 132.1.2. Random Variables 152.2. Strings 152.3. ASCII Code 162.4. Practice Questions 213. Decision Structures 223.1. Nested Decision Structures 233.2. Practice Questions 254. Repetitions 274.1. While Loops 274.2. For Loops 294.3. Practice Questions 324.4. Nested Loops 334.5. Practice Questions 345. Functions 375.1. Void and Value Returning Functions 385.2. Passing Data to and From Functions 395.3. Mathematical Built-in Functions 405.4. Practice Questions 426. Recursion 437. File Access 457.1. Read From a File 467.2. Write to a File 467.2.1. New File 477.2.2. An Existing File 477.3. Notable Built-in Functions 477.4. Practice Questions 49\n8. Lists 508.1. Practice Questions 559. Arrays 569.1 Practice Questions 5710. Plotting Graphs 5811. Object Oriented Programming 6511.1 Constructor 6611.2 Inheritance 6611.3 Polymorphism 6712. Using Python Packages 6913. Python and Graph Theory 7113.1 Networkx 7213.2 Matplotlib 7514. Python and Machine Learning 7714.1 Supervised Learning 7814.1.2 Regression 7814.1.3 Linear Functions 7814.1.4 Polynomial Functions 7914.2. Unsupervised Learning 8115. Python and Statistics 8415.1 Standardizing Data by Scaling 8615.2 T-Test 87References 89Appendix 90Solutions for Practice Questions (2.4) 90Solutions for Practice Questions (3.2) 93Solutions for Practice Questions (4.3) 99Solutions for Practice Questions (4.5) 103Solutions for Practice Questions (5.4) 106Solutions for Practice Questions (7.4) 110Solutions for Practice Questions (8.1) 114Solutions for Practice Questions (9.1) 115\n1. Installing Python A computer program is a list of step-by-step instructions to solve a given problem. Imagine you need to have a typewritten essay. The first issue is in what language the essay should be written: English, French, German, etc. Then, you need to use software to type in your essay. Programming languages recognize only plain text files that use a character encoding American Standard Code for Information Interchange (ASCII) to represent numbers, letters, and symbols. Here, we will use Python as our programming language, so you will need to install Python 3.12 (the latest version of this writing) or a later version, as depicted in Figure 1-1. \nPython is available for Windows, Mac, and other platforms. When you install the Python interpreter, IDLE will also automatically be installed. IDLE is an integrated development environment for Python—a Python shell is depicted in Figure 1-2. \n8\nFigure 1-1 Python Interpreter\nFigure 1-2 Screenshot of Python shell\nUsing a Python shell, you can test your Python program in an interactive mode. Or you can use the text editor to type in your Python program; this is called script mode. Python keywords are displayed in orange, comments are shown in red, string literals are indicated in green, and built-in functions are displayed in purple. The syntax of Python is easy so beginners can learn Python with no difficulty. (Python is named after a comedy show, Monty Python.) Python has several applications, including web development, machine learning, and data analysis. A high-level language lets you create programs without knowing how the internal units of a computer work. A compiler translates a high-level language into a machine language program. Machine language programs can then be executed at any time. Python uses an interpreter, which translates and runs in a high-level language. Download Python from their website. Installation steps are simple: accept the agreement and finish the installation. There are many helpful Python resources on the web, such as: •Python Software Foundation •Python Documentation Running Python programs: Mistakes in programs are called errors. IDLE catches syntax errors; run time errors only occur while a program runs. When an error occurs, Python stops executing and instead displays several lines of text containing helpful information about the error. \n9\n2. Introduction to Programming Programming is the process of writing computer programs. Python is a straightforward but powerful object-oriented programming language. A built-in function in Python is called a function, which is a prewritten program. Let us begin with print(argument), which is a widely used function. Consider the Python statement of print(“west”); the argument is the message you write inside parentheses. The word “west” is a word or a phrase but in Python, it is considered a string literal and is the program's output, which will be displayed on the screen. By default, the operating system connects standard output to the terminal window. String literal is any text that is enclosed in quotation marks. The quotes surrounding a string are called delimiters because they tell Python where a string begins and ends. To display a string literal, you can use single-quote marks (‘’) or double-quote marks (“”). If you have multiple arguments in a print () function, they are separated by a space when they are displayed. •Use sep to specify how to separate multiple items. •Python inserts a space between each of the arguments of the print function. •Use \\n to display the output on the following line. •Use \\’ or \\” to display single-quote or double-quote marks. •The print( ) function automatically advances to the following line. •Use end to keep the print function from advancing to the following line. •To deal with long strings, break them into multiple lines and put a backslash at the end of every line except for the last line. Comments improve the readability of a program, but they do not change the outcome of a program. Use the # symbol to mark the start of a line as a comment line. For comments that span several lines, use triple quotes. 10\n2.1. Variables The variable is a symbolic name representing a value whose associated value may change. A variable must first be created to be utilized. Variables keep values accessible. Values are assigned to variable names using the assignment operator =. The assignment operator takes the value to the right side of the operation and gives it to the name on the left side of the operator. A variable can be overwritten for any number of times, but only the latest one will be used in the program. Consider the following statements: name= ‘West’ name= ‘Chester’ name= ‘University’ The variable name is being overwritten, the latest value assigned to the variable name is university, and the program cannot access the old values of the variable name. To name a variable, you must follow specific rules: •Names are case-sensitive, so a variable named temp differs from one named Temp. •The name of a variable cannot contain space. •The first symbol must be a letter between a to z or A to Z. •After the first symbol, you may use any letter, digit, or underscore symbol. •The first symbol can be the underscore symbol. •Descriptive names are better than short names. •Limiting variable names to a maximum of three or four is a good rule of thumb. •Do not use Python keywords; Here is a list of some Python keywords: and, def, elif, else, FALSE, for, if, import, in, not, or, pass, return, TRUE, and while. There are different numerical types in Python, such as integers and float. Consider 7 and 7.1234, both are numerical values, the first one is an integer and the second one is a float. int() and float() can produce numbers of a specific 11\ntype. Int() returns an integer constructed from a number or string. Float() returns a floating-point number built from a number or string. Floating-point numbers always have at least one decimal place of precision. Python supports mixed arithmetic: when a binary arithmetic operator has operands of different numeric types, the operand with the narrower type is widened, where the integer is more limited than the floating point. To read data from the keyboard as input, use the input(prompt) function, which reads a line from input and converts it to a string (stripping a trailing new line). If the prompt argument is present, it is written to standard output. Consider the following two statements: name= input (‘Please enter a name’) print (name) This is a Python program to get input from the user. The string literal is the prompt displayed on the screen, and the print statement will show the input on the screen. One primary reason to use a variable is to remember a value from one part of a program to be used in another part. Let's assume that we need to have three variables, and we call them x, y, and z. If x = 10 y = 100 z = -2 y = 10 x = x + y + z Then, variable x is initialized with the value of 10; variable y is initially 100, then overwritten by 10, and the final value of x is 18. All numeric types support the four primary mathematical operations: sum (+), difference (-), product (*), and quotient (/). Additionally, you can use the following operations: exponent (**), floored quotient (//), and the remainder (%). 12\n2.1.1. Boolean Variables Boolean variables: Any object can be tested for truth value in conditional statements. A Boolean variable is either true or false. Boolean returns zero for false and one for true. You can use and, or and not to combine Boolean variables. Table 2-1. depicts the logical meaning of using and, or operators. \nAND operator takes two expressions and generates one new expression that is true if both expressions are true. OR operator takes two expressions and generates one new expression that is true if at least one of the expressions is true. Precedence of operation: To determine which operation needs to be evaluated first, an operator with the highest precedence will be considered first if an expression has more than one operator. The highest priority is given to the following: •exponentiation •remainder, multiplication, division •sum and difference Python provides several ways to format a text. F-string embeds expressions inside string literals. F-string is an expression that will be evaluated at run time and is not a constant value. A string must end with the same character; if it begins with a single quote, it must end with a single quote. F-strings are string literals that have an f before the opening quotation mark. Include Python expressions enclosed in curly braces. Python will replace those expressions with their resulting values. PQP AND QP OR QTRUETRUETRUETRUETRUEFALSEFALSETRUEFALSETRUEFALSETRUEFALSEFALSEFALSEFALSETable 2-1 Logical And, Or\n13\nConsider the following statements in Python: number = 1234.56 print (f’{number}’) print (f’{number:10.4f}’) print (f’{number:.1f}’) print (f’{number:,.5f}’) This example illustrates formatting a given number in four different ways. The number after the “:” represents the width of the displayed output, the number after the “.” means the total number of digits after the decimal point and will be rounded up or down accordingly, “,” is used to separate every three digits from the integer part of the number. A Python program to convert a degree from Fahrenheit to Celsius can be written as follows: F = 212 C = (F – 32) * 5/9 print (C) Another example is to write a Python program to convert seconds into hours, minutes, and seconds. We may have the following program: Num = 86400 H = Num / 3600 M = (Num % 3600) // 60 S = Num % 60 print (H, “:”, M, “:”, S) \n14\n2.1.2. Random Variables There are several ways to generate random numbers in Python using its functions. The built-in random() function generates a random float number between 0.0 and 1.0, with 53-bit precision. Each time this function is called, it returns values uniformly distributed in the given range. A Python program can gain access to another Python program by importing. By importing, you bind two or more programs together. The interpreter has built-in functions, such as the print() function. If a function is not part of the standard library of Python, then you need to use import to bind it to your Python program; for example, you may write it as the following: import random number = random.randint (5,10) print (number) which generates a random number in the given range; the randomly generated number includes five and ten. The built-in randint(a,b) function generates a random number that is an integer and is between a and b, including a and b. 2.2. Strings The string is a primary data type in Python. Strings are used to represent texts. A string contains individual symbols or elements. Another built-in function in Python is len(), which returns the total number of elements in each string. One can access each string element in Python by referring to its index. Indices start from zero and proceed with increments of one at a time. Therefore, every element of a string has a numbered position. Consider a string such as Name = “ABCD”; here, Name [0] is A, Name [1] is B, and so on. Sometimes, we may need to select more than one element of a string, technically a span of elements is called a slice. To get a slice of the above Name variable in Python, one can have Name [start: end], which returns elements starting at position start, up to (not including) the element at position end. Strings are immutable, which means they cannot be changed once they have been created. The “+” operator concatenates two strings; this means “1” + “2” is “12” and not “3”. If the element on either side of “+” is a string, Python performs concatenation; if both sides are numbers, then it is a mathematical addition. 15\nIn Python, one can multiply a string by an integer, which will result in displaying the string for an integer number of times. However, you cannot multiply two strings. One can convert strings to numeral values by use of int(input(prompt)) or float(input(prompt)). 2.3. ASCII Code The American Standard Code for Information Interchange is called ASCII and is used for communication between computers. In this coding system, each character and symbol is translated into a code readable by computers. ASCII uses one byte of memory to display symbols, therefore it only displays 256 possible characters. This includes numbers, capital and lowercase letters, and a few more symbols. This is not enough and a new standard has been created and is called Unicode, which utilizes more than one byte supports more than 65,000 characters, and includes all letters from all international languages and symbols. Table 2-2 depicts the ASCII table. DEC.HEX.SymbolSymbol Description000hNULL(Null character)101hSOH(Start of Header)202hSTX(Start of Text)303hETX(End of Text)404hEOT(End of Transmission)505hENQ(Enquiry)606hACK(Acknowledgement)707hBEL(Bell)808hBS(Backspace)909hHT(Horizontal Tab)100AhLF(Line feed)110BhVT(Vertical Tab)120ChFF(Form feed)130DhCR(Carriage return)140EhSO(Shift Out)150FhSI(Shift In)1610hDLE(Data link escape)1711hDC1(Device control 1)1812hDC2(Device control 2)16\n1913hDC3(Device control 3)2014hDC4(Device control 4)2115hNAK(Negative acknowledgement)2216hSYN(Synchronous idle)2317hETB(End of transmission block)2418hCAN(Cancel)2519hEM(End of medium)261AhSUB(Substitute)271BhESC(Escape)281ChFS(File separator)291DhGS(Group separator)301EhRS(Record separator)311FhUS(Unit separator)3220h (Space)3321h!(Exclamation mark)3422h\"(Quotation mark; quotes)3523h#(Number sign)3624h$(Dollar sign)3725h%(Percent sign)3826h&(Ampersand)3927h'(Apostrophe)4028h((round brackets or parentheses)4129h)(round brackets or parentheses)422Ah*(Asterisk)432Bh+(Plus sign)442Ch,(Comma)452Dh-(Hyphen)462Eh.(Dot, full stop)472Fh/(Slash)4830h0(number zero)4931h1(number one)5032h2(number two)5133h3(number three)5234h4(number four)DEC.HEX.SymbolSymbol Description\n17\n5335h5(number five)5436h6(number six)5537h7(number seven)5638h8(number eight)5739h9(number nine)583Ah:(Colon)593Bh;(Semicolon)603Ch<(Less-than sign)613Dh=(Equals sign)623Eh>(Greater-than sign; Inequality) 633Fh?(Question mark)6440H@(At sign)6541hA(Capital A)6642hB(Capital B)6743hC(Capital C)6844hD(Capital D)6945hE(Capital E)7046hF(Capital F)7147hG(Capital G)7248hH(Capital H)7349hI(Capital I)744AhJ(Capital J)754BhK(Capital K)764ChL(Capital L)774DhM(Capital M)784EhN(Capital N)794FhO(Capital O)8050hP(Capital P)8151hQ(Capital Q)8252hR(Capital R)8353hS(Capital S)8454hT(Capital T)8555hU(Capital U)8656hV(Capital V)DEC.HEX.SymbolSymbol Description\n18\n8757hW(Capital W)8858hX(Capital X)8959hY(Capital Y)905AhZ(Capital Z)915Bh[(square brackets or box brackets)925Ch\\(Backslash)935Dh](square brackets or box brackets)945Eh^(Caret or circumflex accent)955Fh_(underscore, understrike, underbar or low line)9660h`(Grave accent)9761ha(Lowercase a)9862hb(Lowercase b)9963hc(Lowercase c)10064hd(Lowercase d)10165he(Lowercase e)10266hf(Lowercase f)10367hg(Lowercase g)10468hh(Lowercase h)10569hi(Lowercase i)1066Ahj(Lowercase j)1076Bhk(Lowercase k)1086Chl(Lowercase l)1096Dhm(Lowercase m)1106Ehn(Lowercase n)1116Fho(Lowercase o)11270hp(Lowercase p)11371hq(Lowercase q)11472hr(Lowercase r)11573hs(Lowercase s)11674ht(Lowercase t)11775hu(Lowercase u)11876hv(Lowercase v)11977hw(Lowercase w)12078hx(Lowercase x)DEC.HEX.SymbolSymbol Description\n19\nEvery character in the Unicode System has a numerical equivalent. Python utilizes built-in functions such as chr() and ord() for these coding systems. Use chr() to get the numerical value of a character and use ord() to get the character equivalent to a numerical value. Table 2-3 depicts Python print() commands, which use some of the above built-in functions. 12179hy(Lowercase y)1227Ahz(Lowercase z)1237Bh{(curly brackets or braces)1247Ch|(vertical-bar, vbar, vertical line or vertical slash)1257Dh}(curly brackets or braces)1267Eh~(Tilde; swung dash)12720hDEL(Delete)DEC.HEX.SymbolSymbol Description\nTable 2-2 ASCII Characters\nCodeOutputprint(ord('A'))An Integer representing character A is 65print(ord('a'))An Integer representing character a is 97print(chr(65))A character representing unicode 65 is Aprint(\"\\u221a\")√ (The symbol of square root)print(\"\\u270E\")✎ (A symbol of a pen pointing at right)print(“\\u2709\")✉ (A symbol of an envelope)Table 2-3 Some special symbols using ASCII extended characters\n20\n2.4. Practice Questions 1.Write a Python program to get a number, then calculate its squared and cubed value. 2.Write a Python program to get a number such as n as input and then calculate . 3.Write a Python program to get a number that contains three digits and then display each digit on a single line. 4.Write a Python program to get a circle's radius and calculate its area. 5.Write a Python program to find the roots of a quadratic equation. 6.Write a Python program to get a name as input and display it five times. 7.Write a Python program to get today’s date as three integers and display it as m/day/year. 8.Write a Python program to simulate the roll of a die. 9.Write a Python program to simulate the roll of a pair of dice. 10.Write a Python program to generate an even random number between one and one hundred. Go to 2.4 Practice Question Solutions. nn\n21\n3. Decision Structures Decision is the process of deciding something. This decision is always based on a logical question or condition in programming languages. Decision structures, also known as if-then-else or conditional statements, change the path of executing a program. A program is a sequence of several lines of code. Anytime we have a decision structure in a program, the program flow could go to different paths, and it is all based on the given logical condition. Many times, a computer problem can not be solved by executing sequential lines of codes, and alternative paths of executions are needed, where based on a condition, the program could take alternative paths. We use decision structures to handle these alternative paths of programs. For example, whether an integer is even or odd, we would like to display a message; accordingly, we must have two separate printing messages in this case, only one will execute based on the given input. Therefore, the control of the program goes to different paths. Let us consider the following program: Num=12 if (Num %2 ==0): print (“Even”) else: print (“Odd”) If the condition in front of the if statement is true, the program displays “Even”; otherwise, it shows “Odd.” It would be impossible to show both messages. In Python, the structure of a conditional statement is as follows: If (question/condition): Action 1 else: Action 2 The indentation is required, and control of the program goes either to Action 1 or Action 2. After both if and else statements, you need to include a colon. The if and else statements must be aligned together. Spaces matter at the beginning of lines but not elsewhere. 22\nComparison operations: A logical condition that uses numerical values can incorporate operations such as less, less or equal, more, more and equal, equal, and not equal to compare several operands, as depicted in Table 3-1. \nConditional statements can be combined by use of relational operators, as discussed in Table 2-1. 3.1. Nested Decision Structures Conditional statements can be nested, as it is possible to have a program with a series of conditions that need testing. In the example given in section 3, Action 1 and Action 2 each could be another conditional statement such as the following: if (Q1): if (Q2): Action 1 else: Action 2 else: if (Q3): Action 3 else: Action 4 OperationMeaning<Less than<=Less or Equal>Greater>=Greater or Equal!=Not Equal = =EqualTable 3-1 Comparison Operators\n23\nSince the logic of nested conditional statements can be complex, a more straightforward approach is available, which is called an if-elif-else statement. if(Q1): Action 1 elif(Q2): Action2 elif(Q3): Action 3 else: Action 4 In each conditional statement, if the condition is true, the statements following if are executed, and if the condition is false, the statements following else are executed. The following program finds the largest value among three numbers. num1=0 num2=-1 num3=1 if(num1>=num2) and (num1>=num3): maximum=num1 elif(num2>=num1) and (num2>=num3): maximum=num2 else: maximum=num3 print(\"The maximum value is\",maximum) \n24\n3.2. Practice Questions 1.Write a Python program to get a number and verify if it is divisible by five. 2.Write a Python program to get a number and verify if it is divisible by five or by three. 3.Write a Python program to get a year and verify whether it is a leap or a common year. A year is a leap year if it is divisible by four, except those years divisible by a hundred are not leap years unless they are also divisible by four hundred. 4.Write a Python program to get a date and check if it is a magic date. A date is magical when you multiply month by day and get the year. For example, February 12th, 2024 is a magical date: . 5.Write a Python program to get a number between one and five and convert it to a Roman numeral. 6.Write a Python program to check if a given input is odd or even without using remainder (%). 7.Write a Python program to get a word that contains three symbols and display it in reverse order. 8.Write a Python program to get a word with a length of four as input and then display each string element on one line. 9.Write a Python program to get a word with a length of four as input, then switch the first and last elements and redisplay the new word. 10.Write a Python program to get a word with a length of four as input and verify if it is a palindrome. 11.Write a Python program to get a word with a length of four as input and then reverse it. 12.Write a Python program to get a word with a length of four as input, and if it starts with ‘a’, replace ‘a’ with ‘A’ and redisplay the name; otherwise, display the last two elements of the name. 13.Write a Python program to get a word, and if the length of the word is even, extract the first element and display it; otherwise, display the last element of the string. 2×12=24\n25\n14.Write a Python program to get a word with a length of four as input and count the total number of times it contains ‘e’ or ‘E.’ 15.Write a Python program to get a word with a length of four as input and then replace every ‘e’ with ‘X.’ Go to 3.2 Practice Question Solutions. \n26\n4. Repetitions Repetition is the recurrence of an action. In programming, it means repeating something that has already been written. Imagine you need to write a program to get a student’s name, last name, and ID and then display these on the screen. A Python program to do so would be like the following example: Name=input(“Please enter a name”) Last=input(“Please enter a last name”) Number=input(“Please enter an ID#”) print(Name,Last,Number) 4.1. While Loops Now imagine that you must write the above program for five hundred students. One way is to copy the above program five hundred times, which is impractical. And what if we had an even more significant number of students? A feasible way is incorporating repetitions, and that means utilizing loops. There are several ways that you can use loops. The first approach is called a while loop. Let us have a closer look at a while loop. Generally, the structure of the while loop is as follows: while (condition): Here is the body of the loop The loop's body is the part of the program that will be repeated; this part should be indented. The total time of the repetition depends on the given condition at the beginning of the while loop. This condition is either true or false. •If the condition is true, the body of the while loop will be repeated. •As soon as the condition is not valid anymore, the program's control goes out of the loop and directly to the first line right after the while loop, and repetition stops. •If the condition is not valid, the while loop never begins. 27\n•If the condition is always true, the program goes into an infinite loop, and the programmer must halt the program by pressing external keys on the keyboard to stop the loop. •The condition must be updated inside the loop's body to ensure that the while loop would not stick in an infinite loop. To write a Python program to display numbers between one and five, we have: n=1 while(n<6): print(n) n=n+1 Here, n begins at one, and if n is less than six, it prints the current value of n and increases the value of n at each iteration of the while loop by one. To write a Python program to display numbers between five and one, we have the following: n=5 while(n>0): print(n) n=n-1 Here, n begins at five, and if n is greater than zero, it prints the current value of n and decreases the value of n at each iteration of the while loop by one. A Python program to display the sum of numbers from one to ten would be: n=1 total=0 while(n<=10): total=total+ n n=n+1 print(total) Here, we need to have two variables: total, which represents the total summation of numbers, and n, which represents the total ten numbers that we 28\nare adding together. At each iteration inside the while loop, we need to add the current value of n to the total summation. In our last example, we write a Python program to display the following sum . As you can see, the only difference is the increments of n, which go by increments of ten at each iteration. n=0 total=0 while (n<=100): total=total+ n n=n+10 print (total) 4.2. For Loops There are several ways that you can utilize loops. The second approach is called a for loop. Generally, the structure of the for loop is as follows: for (condition): Here is the body of the loop The condition can be written as a variable name in range (number of times to repeat). As we saw in the while loop, the body of the loop is the part of the program that will be repeated; this part should be indented. The total time of the repetition depends on the given condition at the beginning of the for loop. This condition is always a Boolean variable. •The condition should be given as a range. •Elements of the range should be separated by commas and listed inside a set of brackets. Python has a built-in function, which is called range. The range () function returns a sequence of numbers. By default, it begins at zero, with increments of one. •range (m) ✦Generates numbers from 0 up to and including m-1. 0+10+20+30+...+100\n29\n•range (m, n) ✦Generates numbers from m up to and including n-1. •range (m, n, k) ✦Generates numbers from m up to and including n-1 with the increments of k. Now, let us rewrite the programs that we discussed in section 4-1, by using for loops. To write a Python program to display numbers between one and five, we have the following: for n in range (1,6) print(n) Here, n begins at one, and increases by one at each iteration of the loop, the last value of n is five. To write a Python program to display numbers between five and one, we have the following: for n in range (5,0,-1) print(n) Here, n begins at five and decreases by one at each iteration, the last value of n is 1. A Python program to display the sum of numbers from one to ten would be as follows: total=0 for n in range (1,11) total=total+ n print(total) Here, we need to have two variables: total, which represents the total summation of numbers, and n, which represents the total ten numbers that we are adding together. At each iteration inside the for loop, we need to add the current value of n to the total summation. 30\nIn our last example, we write a Python program to display the following sum . As you can see, the only difference is the increments of n, which go by increments of ten at each iteration. total=0 for n in range (0,101,10) total=total+ n print(total) 0+10+20+30+...+100\n31\n4.3. Practice Questions 1.Write a Python program to display numbers between one and five. 2.Write a Python program to display numbers between five and one. 3.Write a Python program to show at which temperature Fahrenheit and Centigrade have the same reading. 4.Write a Python program to calculate the following sum: . 5.Write a program to calculate the following sum: . 6.Write a program to display numbers between one and five using a for loop. 7.Write a program to display numbers between five and one using a for loop. 8.Write a program to simulate rolling a pair of dice and show how many tries it takes to get a pair. 9.Write a program to get five numbers as input and calculate their average. 10.Write a program to get inputs from the keyboard, where “-1” indicates the end of inputs, and then calculate their average. 11.Write a program to check whether a given word is a palindrome. 12.Write a program to get a word as input and replace every string character with its first character; for example,“abcd” would change to “aaaa”. Go to 4.3 Practice Question Solutions. 1+2+3+...+1011+12+13+...+110\n32\n4.4. Nested Loops A nested loop means a loop inside another loop, and these are typically used for working with two dimensions. When a loop is nested inside another loop, the inner loop runs many times inside the outer loop. The inner loop will be re-started in each iteration of the outer loop. The inner loop must finish all its iterations before the outer loop can continue to its next iteration. Consider the following program: for i in range (1,4): for j in range (1,7): print (\"*\", end=\" \") print (\"\\n\") Here, for every iteration of the outer loop, the program must finish all iterations of the inner loop. The outer loop iterates three times, and the inner loop iterates six times. The program generates three lines; on each line, it prints six asterisks, (as depicted in Figure 4-1). \n33\nFigure 4-1\n4.5. Practice Questions 1.Write a Python program to display the pattern in Figure 4-2. \n2.Write a Python program to display the pattern in Figure 4-3. \n3.Write a Python program to display the pattern in Figure 4-4. \n34\nFigure 4-2\nFigure 4-3\nFigure 4-4\n4.Write a Python program to display the pattern in Figure 4-5. \n5.Write a Python program to display the pattern in Figure 4-6. \n6.Write a Python program to display the pattern in Figure 4-7. \n7.Write a Python program to display the pattern as depicted in Figure 4-8. \n35\nFigure 4-5\nFigure 4-6\nFigure 4-7\nFigure 4-8\n8.Write a Python program to display the pattern as depicted in Figure 4-9. \n9.Write a Python program to display the pattern as depicted in Figure 4-10. \n10.Write a Python program to display the pattern as depicted in Figure 4-11. \nGo to 4.5 Practice Question Solutions. \n36\nFigure 4-9\nFigure 4-10\nFigure 4-11\n5. Functions A function is a block of code that only runs when called. You can pass data into a function. Therefore, you can write a code block, call it a function, and then use it many times. A large Python program can be broken into several functions by isolating each task into one function. To call a function, write the function’s name followed by two parentheses (). The same set of restrictions for a variable’s name is applied while naming a function. The structure of a function in Python is as follows: def function’s name(): Body of the function The first line of a function is its definition and is called the function header; it begins with the keyword def. Every line of code that is part of a block of codes is indented. To run a function, it must be called. To call a function, write its name followed by a set of parentheses. Let us consider the following program: def func1(): print(\"you just called function1\") def func2 (): print(\"you just called function2\") for m in range(0,3): func1() func2() Here func1 and func2 are two functions we defined; each prints a literal string. In the for loop, each function is called three times. When you define a function, it is created, but you still need to call it. When a function is called, the program's control is transferred to that function. \n37\nNow let’s consider the following program: m=20 def func1(): m=100 func1() print (m) variable m is initialized with a value of twenty; in function func1(), there is also a variable m, two different variables with the same name. Variable m in func1() is a local variable, and any changes that happen to m in func1() do not affect variable m that is outside of fucn1(). A local variable is any variable that is created inside of a function and is only accessible from inside of the function. Several functions can have variables with the same name, and they do not affect each other, as they cannot access each other’s variables. 5.1. Void and Value Returning Functions One way to categorize functions is based on the value that they return. A void function is a function that does not return any value; it executes its block of code and then terminates. On the other hand, a value-returning function executes its block of code and then returns a value to the program that called it. In the following example, func1() is the function that prints a variable and terminates; it is a void function. def func1(): print(m) A value-returning function must have a return statement as the last line in the function. In the following example, func1() is the function that returns a value stored in variable m; since it is returning a variable, func1() is a value-returning function. A function may return more than one value; in Python, if you have more than one value to return, the result is displayed as a list, such as (a,b,c), members in a set of parentheses separated by commas. def func1(): return(m) 38\n5.2. Passing Data to and From Functions Data passed to a function when called is referred to as an argument, and the variable that receives an argument is called a parameter. Consider the following example: def func1(word): m=word[0] return (m) func1(name) In this example, the name is data passed to func1() and is an argument, and the word receives this argument and is called a parameter. In this example, func1() extracts and returns the first symbol of the argument word. In the following example, func1() has two parameters as indicated in its header, it will append them together and display the result. When we call this function we need to list two parameters, otherwise it would be a compile error. def func1(fname,lname): print(fname+\" \"+lname) func1(“Steven\",\"Jobs\") In the following example, func1(), we use a loop to print all elements of a given list. dir=[\"West\",\"East\",\"North\",\"South\"] def func1(dir): for d in dir: print(d) func1(dir) \n39\n5.3. Mathematical Built-in Functions In addition to writing your defined functions, Python has built-in mathematical functions that you can use. And, like using random numbers, you need to import a standard library into your Python program. Some built-in functions and variables in Python that you can incorporate into your program are as follows: •sqrt(x): to display the square root of a number. import math print(math.sqrt(64)) •floor(x): to round down the number to the nearest integer. import math x=123.45 print(math.floor(x)) •ceil(x): to round up the number to the nearest integer. import math x=123.45 print(math.ceil(x)) •exp(x): to return E raised to the power of x.E is the base of the natural system of logarithms or the Euler’s number (2.71). import math x=2 print(math.exp(x)) •math.e: to return E, the base of the natural system of logarithms or the Euler’s number. import math print(math.e) 40\n•math.pi: to return the value of PI (3.14149). import math print(math.pi) •sin(x): to return the sine of the x, which should be in radians. import math x=90 print(math.sin(x)) •cos(x): to return the cosine of the x, which should be in radians. import math x=0 print(math.cos(x)) •tan(x): to return the tangent of the x, which should be in radians. import math x=90 print(math.tan(x)) •log(x): to return the logarithm base 2 of x. import math x=2 print(math.log(x)) \n41\n5.4. Practice Questions 1.Write a Python program to generate a random number between zero and five, then simulate a magic ball. You may select your message from these options: without a doubt, better not tell you now, my sources say no, ask again later, outlook hazy. 2.Write a Python program to get a word from a user through the keyboard and then display the first character of that word. 3.Write a Python program to get a word from a user through the keyboard and then display the first character of that word. Extracting the character should be done in a function, but displaying the character should be done in the main program. 4.Write a Python program to get two numbers as input and calculate their average. 5.Write a Python program to get a degree in Fahrenheit and convert it to Celsius. 6.Write a Python program to get an input (integer) and calculate its factorial, where the factorial of a number is denoted by n! and is the product of all positive integers less than or equal to n. For example: . 7.Write a Python program to generate the first ten Fibonacci numbers, where each is the sum of the two preceding ones. These are numbers in the Fibonacci series: . 8.Write a Python program to get a word from a user through the keyboard and then reverse the word through the use of a function. Go to 5.4 Practice Question Solutions. 5!=5×4×3×2×11,1,2,3,5,8,...\n42\n6. Recursion When a function calls itself, it is called a recursive function. Making a function call itself is a way to break down complicated problems into more straightforward problems that are identical in structure to the original problem. In recursive programming, there is always one base case, which does not require recursion and stops the chain of recursive calls. Any problem that can be solved recursively can be solved by using loops. Repetitive problems are more easily solved using recursion. Example 1: a recursive program to calculate the summation of the first ten positive integers. n=10 def Sum (n): if (n==0): return 0 else: return Sum(n-1)+n print (Sum(n)) To calculate the summation of the first ten positive integers, we need summation of the first nine positive integers and then add ten to that summation; however, to calculate the summation of the first nine positive integers, we need summation of the first eight positive integers and then add nine to that, and inductively for each new summation you need the previous summation. The base case here is the summation of numbers from zero to zero, which is zero. \n43\nExample 2: a recursive program to calculate the factorial of five, where the factorial of a non-negative integer n is a product of all positive integers less than or equal to n. n=5 def Fact(n): if (n==1): return 1 else: return n*Fact(n-1) print (Fact(n)) The n factorial also equals the product of n with the next smaller factorial. Example 3: a recursive program to generate the first ten elements of the Fibonacci sequence. The Fibonacci sequence is a sequence in which each number is the sum of the two preceding ones. The sequence starts with two ones. def Fib (n): if ((n==1) or (n==0)): return 1 else: return Fib(n-1) +Fib(n-2) for i in range (0,10): print (Fib(i), \" \", end= \"\"). \n44\n7. File Access A file is a series of bytes used to store data; this data is organized in a specific format; some are easy to read by a human, like text files. However, all files are then translated into binary for processing by the computer. Every time that you use Word processing programs or image editors, you work with files. For every program that we wrote, that required the user to enter data, as the input data was not retained at a permanent memory location, we needed to re-enter the data for each run of the program. To save the data between the runs, one way is to save the data on a file, which means saving the data on the computer’s disk, one form of permanent memory. You can read and write all types of files; in this chapter, we only work with text files. In a text file, data is encoded as a text using ASCII or Unicode formatting, therefore even numerical values are stored as a series of characters. A text file can be opened and viewed by any text editor. There are two different ways to access data in a file, sequential access and random access. In sequential access, you must access data from the beginning of the file to the end of the file; you can not skip any piece of data; you have to read all the data that comes before the desired data. On the other hand, in random access, direct access to any piece of data in the file is possible. This is like accessing different tracks on a CD or DVD. In a text file, each line of the text is ended with the new line character (‘\\n’). You can open a file, read from it, or write on it; if the file does not exist, it will be created; you can also append data to a file if it already exists. Remember that at the end of a Python program, you should permanently close every open file, freeing up the memory space used by the opened files. When a file is closed, the connection between the file and the program is removed, and should you need to access the same file, you need to reopen the file. \n45\n7.1. Read From a File Retrieving data from a file is known as reading from a file. Suppose you need to access a text file to read it; the following statement reads the first line of the file and copies it into a string line. line=open('filefortest.txt',‘r’) If the text file is not in the current subdirectory, then you must specify the entire path to your file. line=open('c:/users/Agah/desktop/filefortest.txt',‘r’) •You can use readline(), which reads a line from a file and returns it as a string. •You can use read(), which reads the entire file. •You can use read(n), which reads n bytes of a file. •‘\\n’ is considered two bytes. It marks the location where a new line begins in a file. •When strings are printed, ‘\\n’ causes an extra blank line to appear. 7.2. Write to a File Every file has a name and a type; the type of a file is a series of three characters, which appear after the filename followed by a dot. The .txt indicates that the file is a text file. To write on a file in Python, first, you need to open the file for writing. To write on a file, you can use the same print () function we used previously. When you are done accessing a file, you must close the file. To convert a numerical value to a string, use the str () function. \n46\n7.2.1. New File To create a new file, you can use the following statements: file=open('filefortest.txt','w' ) file.write( “WCU \\n” ) file.write(f‘{“abc”}\\n’) file.close() Here ‘w’ indicates that the file is being accessed for writing, and a new file will be created. If the file already exists, it will be overwritten, and the data on the original file is not retrievable anymore. The statement open(), opens the file for writing, and write(), writes data on the file; every time that you write on a file, it is in the form of a string; you can either use string literals or use F-formatting to write the data. 7.2.2. An Existing File If the file already exists and you wish to append data to the end of the existing file, then use the following statements: file=open(‘filefortest.txt','a' ) file.write( “WCU \\n” ) file.write(f‘{“abc”}\\n’) file.close() Here, ‘a’ indicates that the data will be appended to the end of the existing file. 7.3. Notable Built-in Functions When a file is opened for reading for the first time, a read pointer is positioned at the beginning of the file, after the first read, it is maintained to mark the location of the next item that will be accessed from the file. As discussed in section 7.1, ‘\\n’ separates items in a file, and it causes an extra blank line to appear in a file. If you need to remove the extra line, you can utilize a built-in function called rstrip(). This function strips a specific character from the end of a given string. For example, name.rstrip(‘\\n’), removes the trailing ‘\\n’ from the end of the string name. 47\nEach time that you open a file for writing, data is written on the file as strings. Python has a built-in function str(), which converts a value into a string, which you can use to directly write numerical values as strings on a file. \n48\n7.4. Practice Questions 1.Write a Python program to read a file and print every line of the file on the screen by using a for loop. 2.Write a Python program to read a file and print every line of the file on the screen by using a while loop. 3.Write a Python program to read a file and print the total number of lines in the file. 4.Write a Python program to read a file and print every word in the file that begins with ‘A’. 5.Write a Python program to get an input from the keyboard and then add it to an existing file. 6.Write a program in Python to generate five random numbers and write them in a file. 7.Write a program in Python to read a file and check if it contains a specific word. 8.Write a program in Python to read a file and count the total number of e’s in the file. 9.Write a program in Python to write five numbers on a file and then display their summation. Go to 7.4 Practice Question Solutions. \n49\n8. Lists A list is an entity that contains multiple data items. Use square brackets to indicate the start and end of the list and separate items in a list with commas. [] is the empty list. You can change items that are in a list, which means a list is mutable, a programmer may add or remove items from a list. We use lists to store multiple items in a single variable. A list of items enables a programmer to keep related data values together. The Indexing and slicing that we used as built-in functions over strings can be utilized on lists too. Each item of a list is called an element of the list. Here is a list of four integers: num=[1,2,3,4] A list may contain elements that have different types. For example: num=[1,`east’,`west’,2.5] is a list that contains four different elements of three different types. You can use the print() function to print entire elements of a list. Similar to strings, num[0] is the first element of the list num, and also num[:3] returns the first three elements of the list, which is one slice of a list, the same concept that we had with strings. Negative indexing means to start from the end, therefore -1 refers to the last element of the list. You can use the “+” operator to add one list to the end of another list. And use the ‘*’ operator to repeat a list. For example [‘*’]*13, displays thirteen ‘*’. In Python you can use the following built-in functions on a list: •index(a): to return the index of the first occurrence of a in the list; consequently the output of the following program is 1. num=[1,2,3,4,5] print(num.index(2)) \n50\n•len: to return the total number of elements in a list; consequently the output of the following program is 5. num=[1,2,3,4,15] print(len(num)) •max: to return the largest elements in a list; consequently the output of the following program is 5. num=[1,2,3,4,5] print(max(num)) •min: to return the smallest element in a list; consequently the output of the following program is 1. num=[1,2,3,4,5] print(min(num)) •remove (a): to remove the first occurrence of the element a from the list; consequently the output of the following program is [1, 2, 4, 5]. num=[1,2,3,4,5] num.remove(num[2]) print(num) •sort: to sort elements in a list in ascending order; consequently the output of the following program is [-5, -2, 1, 4, 31]. num=[1,-2,31,4,-5] num.sort() print(num) •sum: to return the sum of the elements in a list; consequently the output of the following program is 1. num=[1,-2,3,4,-5] print(sum(num)) •append: to add an element to the end of an existing list. 51\n•clear: to remove all elements from a list. •copy: to return a copy of the list. •count: total number of elements. •reverse: to reverse the order of items in a list. Here is a Python program that utilizes the above five functions: num=[1,2,3,4,5] num.append(7) num.append(num[0]) num.append(num[-2]) print(num) num=[1,2,3,4,5] num.clear() print(num) num=[1,2,3,4,5,4,4] print(num.count(4)) num=[1,2,3,4,5] num.reverse() print(num) \n52\nYou can copy a list into a file. The following example creates a file for writing and by use of a for loop, it copies all elements of a list onto a file. num=[1,-2,3,4,-5] myfile=open(\"data.txt\",\"w\") for n in num: myfile.write(str(n)) myfile.write(\"\\n\") myfile.close() You can copy a file into a list. The following example reads a file and copies all lines of the file into a list. myfile=open(\"data.txt\",\"r\") newlist=myfile.readlines() myfile.close() print(newlist) The following example checks to see if an item is an element of a given list. mylist=[\"West\",\"East\",\"North\",\"South\"] if \"West\" in mylist: print(\"Yes, West is in the list\") else: print(\"No it is not in the list”) The following example inserts a new element into an existing list. mylist=[“West\",\"East\",\"North\",\"South\"] mylist.insert(2,”Ocean”) As a result, the new list contains five elements and the third element is Ocean. 53\nThe following example appends an item as the last element to a given list. mylist=[“West\",\"East\",\"North\",\"South\"] mylist.append(”Ocean”) As a result, the new list contains five elements and the fifth element is Ocean. The following example appends an entire list to the end of another list. mylist=[\"West\",\"East\",\"North\",\"South\"] newlist=[\"Ocean\",\"Sea\",\"Creek\"] mylist.extend(newlist) \n54\n8.1. Practice Questions 1.Write a Python program to replace every element of a list with its square. 2.Write a Python program to find the smallest and the second smallest elements in a list. 3.Write a Python program to remove the first and last elements of a list and redisplay the new list. 4.Write a Python program to calculate the average of the elements in a list. 5.Write a Python program to replace every even element of a list with “*”. Go to 8.1 Practice Question Solutions. \n55\n9. Arrays As mentioned in chapter eight, Numpy is a library in Python that can generate lists, matrices, linear algebra, and so on. Using Numpy, you can generate arrays. Arrays are used to keep the related data at the same location in the memory. And since arrays are stored at one continuous location in memory, it is quicker to access elements of an array than elements of a list. Therefore by using arrays, programmers store multiple values as one variable. And by using each element’s index we can quickly retrieve a particular element of an array. Consider the following Python program: import numpy MyArray=numpy.array([-1,0,1]) print(MyArray) In the above Python program, MyArray is the name of the array that contains three elements: -1,0,1. The following Python program generates a two-dimensional array. An array can have any number of dimensions. import numpy B=numpy.array([[-1,0,1],[-2,0,2]]) print(B) To access each element of an array, you need to refer to its index, and as with lists, you begin from index zero. In the above program, B[0][0]=-1, is the element on the first row and the first column. You can select more than one element of an array at a time. A[m:n] means several elements from array A, from index m to (not including) index n. \n56\n9.1 Practice Questions 1. Write a Python program to generate an array and reverse it, without changing the order of elements in the original array. 2. Write a Python program to generate an array as input and reverse it. The original array will be reversed. 3. Write a Python program to find the common elements in two arrays. 4. Write a Python program to find repeated items in an array. 5. Write a Python program to find the second largest value in an array. 6. Write a Python program to check if an array contains a value. Go to 9.1 Practice Question Solutions. \n57\n10. Plotting Graphs Matplotlib is a library in Python that you can utilize for the visualization of a graph. However, it is not part of the standard Python library, and you need to install it. To do so you need to open a terminal and on a Mac type: sudo pip3 install matplotlib and if you are using a Windows system, then enter the following command: pip install matplotlib Sudo is a command in Unix-based operating systems that grants administrator privileges to a user. PIP is a package manager for all the libraries that you may need to import into your Python programs. Once matplotlib is installed, you can import it into your Python programs. Plot(x,y) is a built-in function to draw lines in a graph, where x is the horizontal axis and y is the vertical axis. The plot () function creates a line that connects a series of data points with straight lines. Let us consider the following Python program: import matplotlib.pyplot as plt x=(1,2,3,4,5,6,7,8) y=(0,50,100,50,-50,0,100,-50) plt.plot(x,y,marker='s',linestyle=\":\",color=\"g\",linewidth=2.0) plt.show() Here, import matplotlib.pyplot as plt means that you can refer to that library as plt. When you use plot(), markers are used to emphasize each point on a graph with a specified marker, line style indicates the style of the line, color indicates the color of the line, and line width indicates the width of the line, all of which are line properties that you can use as built-in functions. Finally, to display a graph on the screen, you need to use another built-in function show(), as plot() builds the graph in memory but does not display it by default. Figure 10-1 is the output of the above program. 58\nTable 10-1 indicates color choices for colors in any graph. \nYou can use the following built-in functions to draw graphs: •bar(): to draw bar graphs. •grid(): to add grid lines to the plot. •pie(): to draw pie charts. •subplot(m,n,p): to show multiple graphs, in m rows, n columns and the current plot is the plot number p. ColorMeaningBBlueGGreenRRedCCyanMMagentaYYellowKBlackWWhiteTable 10-1\n59\nFigure 10-1\n•title(): to add a title to a graph. •xlabel() and ylable(): to add a title to the x and y axes. •xlim() and ylim(): to change the lower and upper limits of numbers on the x and y axes. \nThe output of the following Python program is depicted in Figure 10-2, here we have used a subplot. import matplotlib.pyplot as plt x=(1,2,3,4,5) y=(0,50,100,50,-50) plt.subplot(2,1,2) plt.plot(x,y,marker='s',linestyle=\":\",color=\"g\",linewidth=2.0) plt.grid() x=(1,2,3,4,5) y=(-50,0,10,100,50) plt.subplot(2,1,1) plt.plot(x,y,marker='o',linestyle=\":\",color=\"r\",linewidth=2.0) 60\nFigure 10-2\nplt.grid() plt.show() The output of the following Python program is depicted in Figure 10-3. import matplotlib.pyplot as plt x=(1,2,3,4,5,6) y=(100,-10,10,50,20,0) plt.bar(x,y,width=0.1,color='r') plt.show() \nNumPy, which stands for Numerical Python is also another Python library, that you can use for working with linear algebra, matrices, and so on. Assume that you need to draw a graph for Sine(x). Since the Sine function is part of Numerical Python, you need to import it into your program. The output of the following Python program is depicted in Figure 10-4. import matplotlib.pyplot as plt import numpy as np x=np.arange(0,10,0.05) y=np.sin(x) 61\nFigure 10-3\nplt.plot(x, y) plt.show() The meaning of import numpy as np is that you can refer to that library as np. In this Python program, we used arange(n,m,p), which means n is the start of the \ninterval, m is the end of the interval and p is the spacing between the values. The output of the following Python program is depicted in Figure 10-5. Sine and Cosine functions are part of Numerical Python. import matplotlib.pyplot as plt import numpy as np z=np.arange(0,360,10) x=np.cos(z) y=np.sin(z) plt.plot(x,y) plt.show() 62\nFigure 10-4\nThe output of the following Python program is a pie chart, which is depicted in Figure 10-6. \nimport matplotlib.pyplot as plt x=(10,20,30,90) lab=(\"west\",\"east\",\"south\",\"north\") 63\nFigure 10-6\nFigure 10-5\nplt.pie(x,labels=lab) plt.show() \n64\n11. Object Oriented Programming Python is an object-oriented programming language. This model of programming is based on objects. An object is an entity that has predefined attributes and behavior. The Python program makes objects interact with other objects. Class is the template for an object, and an object is an instance of a class. A smartphone is an object. Examples of attributes are traits such as screen size, camera, weight, or number of external ports. Examples of behavior are taking a picture, recording a movie, or connecting to Wi-Fi. Class is a blueprint of the object. Every cell phone that is released by a particular manufacturer will come with certain predefined attributes and behavior, such as the examples above. The class provides all the common elements. The statement class Vehicle: creates one class, which is named Vehicle. The statement v1=Vehicle() creates one instance of the class vehicle, which is called v1. In programming, we refer to behavior as methods or functions. The following Python program consists of a class car. Every car has a model and a year associated with it. C1 and c2 are two objects and are instances of the class car. Class Car: def __init__(self,model,year): self.model=model self.year=year c1=Car(“TESLA”,2024) c2=Car(“FERRARI”,2023) print(c1.model,”***”,c1.year) print(c2.model,”****”,c2.year) 65\n11.1 Constructor A constructor is a method for initializing objects of a class. All classes have a constructor, which is executed when the class is being initiated. In Python, a constructor’s name is always init and it must have a prefix and suffix of double underscores. Since it is a method, you must include def before its name. As with any other method, a constructor may or may not accept any arguments. A default constructor does not accept any arguments. A parametrized constructor accepts arguments, and in this case, you can pass data during object creation, which is used to initialize the instance members. Objects can also have methods. The following Python program depicts one class called vehicle, v1 is one instance of the class, and Driver is one method that belongs to the object. Every object has three attributes, model, year, and number of drivers. class Vehicle: def __init__(self,Model,year,NoD): self.model=model self.year=year self.NoD=NoD def Driver(self): print(\"Number of Driver\" + self.NoD) v1=Vehicle(“TESLA\",2024,0) v1.driver() Self represents the instance of the class; it is a reference to the current instance of the class. Use self to create an attribute that belongs to an object that self is referencing. This is called an instance of an object. An object cannot be created without a constructor, if you do not declare a constructor in your program, Python generates one by default. 11.2 Inheritance Inheritance allows one class to inherit attributes and methods from another. Parent class or base class is the class being inherited from, and child class is the 66\nclass that is inherited from a parent class. The child class is a derived class from the base class; the child class creates an instance of the class. Child class may override the methods of its parent class. Overriding means that a child class has an implementation of a method that is already defined in the parent class. In the flowing Python program shown below, the class vehicle is the parent class, and the car is the child class. Therefore, the class car has the same properties and methods as the vehicle class. class vehicle: def __init__(self,model,year): self.model=model self.year=year def printcar(self): print(self.model,self.year) class car(vehicle): def __init__(self,model,year): v1=vehicle(“Toyota\",2000) v1.printcar() v1=car(\"Honda\",2020) v1.printcar() When you include the following in the child’s class: def __init__(self,model,year): then it no longer inherits this method and instead uses its own, which is called overriding. 11.3 Polymorphism Class polymorphism is when multiple classes have the same method name. For example, a class shape() by itself does not have any definition of area. However, if you consider class triangle, circle, or hexagon, they all have the concept of area in common, and you could have a separate method named area() for each 67\none of them, which calculates the area of that particular shape. This is depicted in the following Python program. class Shape: def area(self): print(\"Needs more specifications\") class Circle(Shape): def area(self): print(\"3.14*r*r\") class Triangle(Shape): def area(self): print(\"h*b/2\") s1=Shape() c1=Circle() t1=Triangle() s1.area() c1.area() t1.area() \n68\n12. Using Python Packages Python has several libraries that make analyzing data very easy. It also contains highly powerful machine-learning libraries. The libraries that we will utilize are for data analysis, visualization, machine learning, and data mining. To get ready to work in a Python environment, you need to install some packages. A package is an archived file that we download from the internet and install on your computer. To be able to run the programs that are provided in this book, you need to be sure that you are using the latest version of each released package. At the time of writing this book, the latest edition of pip is 23.3.1, and the latest edition of numpy is 1.26.3. Follow these directions to install these packages: To install a given package, you need to run this command: pip install name of the package. In a Python shell type-in the following commands: Pip install -U NumPy Pip install --upgrade pip Pip install network Pip install matplotlib Pip install fpgrowth_py •Pip is a management system written in Python, which you can use to install other packages. Pip connects to an online repository of packages. •Networkx is used for the creation and studying the structure of networks and graphs. It is free software, which means users can run the software, change it, and distribute it. The term “free” means users have the freedom to use the software in any way that they choose, but the source of the software is not openly available. •Matplotlib is a plotting library used for static and interactive visualizations in Python. It is designed to be like MATLAB, can use Python, is available for free, and is an open-source software. Open-source software is freely 69\navailable and can be modified and redistributed, by encouraging open collaborations. •Numpy is a library for Python programming, where the type and size of data, arrays, and mathematical functions are supported. Numpy is the fundamental package for scientific computing with Python and is an open-source software. •fpgrowth_py is a Python package used for frequent pattern mining and associations in data sets. •“-U” or “—upgrade” means upgrade the current edition to the latest version. Now that you have installed the above packages, you can import them into your Python programs. If the Python interpreter generates errors, it means one or more packages have not been installed correctly. All Python packages are available at the Python Package index. \n70\n13. Python and Graph Theory A graph is a structure, which contains vertices (nodes) and edges. Graphs can be used to represent acquaintances between people such as friendship relationships on Facebook, followers on X (Twitter), and so on. In each graph, each edge connects two vertices, and each of the vertices is called an endpoint. If there is an edge between two vertices, those two vertices are called adjacent nodes. A simple graph is a graph where no two edges connect the same pair of vertices. A railway system between two cities represents a simple graph. A graph with multiple edges connecting the same vertices is called a multigraph. For example, imagine a graph where vertices are cities in a country and edges are direct flights between two cities. A graph may be directed or undirected; if it is needed to assign a direction to an edge between two vertices, then the graph is directed. For example, being a follower of a person on an online social network makes a graph a directed graph. In some online social networks, certain people can influence others. Therefore, a directed graph can be used to represent when one node influences another node; there is a direction assigned to the edge between the two vertices. A directed multigraph can be utilized to represent text messages sent and received between two phone numbers. The degree of a vertex in an undirected graph is the total number of edges that it touches. In a directed graph the total number of edges that are incoming to a node is called its in-degree, and the number of edges that are outgoing from a node is called its out-degree. A path is a sequence of edges that begin at one node of a graph and traverses from one node to another node along the edges of the graph. For example, two people on an online social network are linked where there is a path between the two. This path could represent influence, such as being a follower of a person or retweeting messages in a network. In a graph, the shortest path is a path between two nodes such that it contains a minimized number of edges. Also, the closeness centrality of a node is the average length of the shortest path between the node and all other nodes in that graph. Therefore, a node’s centrality indicates the number of shortest paths that it is part of. The higher the number of centralities, the more important the node is. 71\n13.1 Networkx Networkx is a Python package for the creation and analysis of networks. A network could be an online social network, or a graph that represents friendships, sending the receiving messages such as tweets, and so on. You can use Networkx over large data sets; you can use it to design a new algorithm or build network models. To create a graph with no edges and no nodes, you can execute the following Python program: import networkx as nx G = nx.Graph() Using the kite_graph data set (a data set is any collection of data), which is part of the Networkx package, the output of the following Python program is depicted in Figure 13-1, where we have 10 vertices and 18 edges. In Networkx, you can use graph objects to generate graphs. A directed graph is specified by the class DiGraph() and a multigraph is specified by the class MultiGraph(). import networkx as nx import matplotlib.pyplot as plt G = nx.krackhardt_kite_graph() nx.draw_networkx(G) plt.show() \n72\nFigure 13-1\nIn the above program, we can examine the degree of each node; for example, print(G.degree(3)) will display five as it is the degree of node number three. Or print(G.adj[3]) will generate the following output, which is the list of all of the adjacent nodes to node number three. {0: {}, 1: {}, 2: {}, 4: {}, 5: {}, 6: {}} The output of the following Python program is depicted in Figure 13-2. Since G is a directed graph, add.edge(m,n) adds an edge from node m to node n. import networkx as nx import matplotlib.pyplot as plt G = nx.DiGraph() G.add_edge(1,2) G.add_edge(1,3) G.add_edge(2,1) G.add_edge(2,4) nx.draw_networkx(G) plt.show() \n73\nFigure 13-2\nThe following Python program prints the node centrality of each node in a kite graph, as given in Figure 13-1. import networkx as nx import matplotlib.pyplot as plt G = nx.krackhardt_kite_graph() print(nx.betweenness_centrality(G)) And as you can see the node with the highest centrality is node number seven. {0: 0.023148148148148143, 1: 0.023148148148148143, 2: 0.0, 3: 0.10185185185185183, 4: 0.0, 5: 0.23148148148148148, 6: 0.23148148148148148, 7: 0.38888888888888884, 8: 0.2222222222222222, 9: 0.0} The following Python program displays the degree centrality of each node in a kite graph, given in Figure 13-1. import networkx as nx import matplotlib.pyplot as plt G = nx.krackhardt_kite_graph() print(nx.degree_centrality(G)) And as you can see the node with the highest node centrality is node number three. {0: 0.4444444444444444, 1: 0.4444444444444444, 2: 0.3333333333333333, 3: 0.6666666666666666, 4: 0.3333333333333333, 5: 0.5555555555555556, 6: 0.5555555555555556, 7: 0.3333333333333333, 8: 0.2222222222222222, 9: 0.1111111111111111} 74\nG.number_of_nodes() and G.number_of_edges() respectively, display the total number of nodes and the total number of edges in a graph. G.remove_edge(m,n) removes the edge between node m and node n. 13.2 Matplotlib Many utilities that you can use in Matplotlib are imported under the plt alias. Use the following command to use the package as plt in your Python programs: import matplotlib.pyplot as plt. •Plot() is used to draw a line between two points. •Use marker() to mark each point on a line with a specified marker and use ms to set the size of the marker. \n75\nFigure 13-3 \nThe following program draws a line graph, as depicted in Figure 13-3: import matplotlib.pyplot as plt import numpy as np y=np.array([-1,1,-0,-4]) plt.plot(y,marker='*',c=\"red\" ,ms=15) plt.show() \n76\n14. Python and Machine Learning When you improve your performance based on your observations of past events, learning occurs. Learning is needed because you cannot anticipate all possible situations that can occur in the future. Machine Learning is the study of statistical algorithms that can learn from data; It is the process of analyzing data and predicting the possible outcomes. Perhaps classification is the most basic form of analyzing data. An example of this is an incoming email being marked as spam or not spam, and this marking classifies emails into two classes. There are three different types of data: numerical, categorical, and ordinal. Numerical data can be counted, such as the total number of emails that one receives, or it can be measured data, such as the size of an attachment to an email. Categorical data are not measured in numbers; for example, a person’s gender cannot be measured numerically. Ordinal numbers are non-numerical but with an implied order, such as a rating that you provide for an item listed on Amazon. There are different techniques that you may apply to data based on its type. There are three types of learning: supervised learning, unsupervised learning, and reinforcement learning. •In supervised learning through a computer program, a computer learns from the inputs to generate the desired output. A computer observes pairs of input-outputs and the correlation between them. The input could be an image, and the output be a traffic light. Therefore, when someone acts as a teacher, it is thus called supervised learning. •In unsupervised learning, a computer must find patterns within the data. There is no learning process; a spam-detecting system may use unsupervised learning to classify different emails into several possible spam categories, each with a different probability of being spam. Then, each category uses supervised learning to learn how many of those emails were a spam message. •In reinforcement learning, an agent must interact with the environment dynamically; interactions come in the form of rewards and punishments. The goal is to maximize the rewards. 77\n14.1 Supervised Learning Classification is one type of classification, where a program is trained on a dataset to predict the category of new data. Classification is either binary or multi-class. For example, consider if an incoming message is either a text or a picture. An incoming email may be spam or not. We can train the email system that if an email contains certain words, then with a high probability, it is a spam message. We can use linear classifiers to create a linear decision; this classification is based on a linear combination of characteristics of the data. 14.1.2 Regression Prediction is like classification; you predict a numerical value of a variable. Data mining is examining data where the classification will occur in the future. One approach for supervised learning is regression. Regression is predicting the behavior of one variable based on the behavior of another variable. The variable that is being predicted is the response variable. For example, if an influencer in an online social network purchases an item or gives a high approval to an object, several of their followers will purchase the same object. What makes a person an influencer depends on many factors, such as fame, degree centrality, betweenness, and so on. If you have a business, then you would like to recognize these influencers as they can influence others to buy your product, which in return could boost your business. Therefore, you need to be able to predict the future behavior of people many times and make business decisions accordingly, which means you need to apply regression. 14.1.3 Linear Functions In the equation, , you can calculate the value of y if the value of x is given. The relationship between x and y is linear. Regardless of the values of a and b, the graph representing this equation is always a straight line. The variable a is called slope as it defines the slope of the line and variable b is called the intercept. The intercept is the value where the plotted line intersects the y-axis. Slope and intercept are key values of linear regression. The output of the following program as depicted in Figure 14-1 is a linear regression between two variables, and the straight line is used to predict the future values of the two variables. y=a*x+b\n78\nimport matplotlib.pyplot as plt from scipy import stats x=[1,2,3,-2,-5,20,-10,100,2] y=[100,-2,20,-10,200,5,20,100,-11] slope,intercept,r,p,std_err=stats.linregress(x, y) def fun(x): return (x*slope-0.5) pred=list(map(fun,x)) plt.scatter(x,y,c=\"red\") plt.plot(x,pred) plt.show() \n14.1.4 Polynomial Functions The following Python program depicts a polynomial regression as depicted in Figure 14-2. Linear regression works on continuous data and when data are correlated. However, if the correlation is not linear, we can use polynomial regression that instead of a best-fit line will have a best-fit polynomial line. Polynomials fit a wide range of curvature. 79\nFigure 14-1\nimport numpy import matplotlib.pyplot as plt x=[1,2,3,4,5,6,7,8,9,10] y=[10,-2,3,4,5,-200,11,90,0,200] pred=numpy.poly1d(numpy.polyfit(x,y,5)) line=numpy.linspace(1,10,100) plt.scatter(x,y) plt.plot(line,pred(line)) plt.show() \nA linear regression model can determine the relationship between a variable and the response variable, where other variables are fixed. In polynomial regression, a nonlinear relationship between variables is modeled. \n80\nFigure 14-2\n14.2. Unsupervised Learning In unsupervised learning, input data variables are given with no corresponding output variables. Here the goal is to find patterns in the data. The machine must group information according to patterns and similarities but without any prior information. •Clustering is one approach to unsupervised learning. This is where data is divided into several groups, and data with similarities will be placed in the same cluster. For example, emails that contain a set of specific words will be classified as spam. First, each data point is assigned to one cluster, then we calculate the center point of each cluster, reassign each data to the cluster with the closest centroid, and repeat until no other cluster assignment is possible. The following Python program puts the data into two clusters, Figure 14–3 depicts the data: import numpy as np import matplotlib.pyplot as plt from sklearn.cluster import AgglomerativeClustering x=[1,-1,2,-2,3,-3,4,-4,5,-5] y=[10,20,30,-10,-20,30,50,100,90,1] clust=list(zip(x,y)) hierarchical_cluster=AgglomerativeClustering(n_clusters=2) labels=hierarchical_cluster.fit_predict(clust) plt.scatter(x,y,c=labels) plt.show() \n81\nAnd the following Python program puts the same data into four clusters. Figure 14-4 depicts the results. import numpy as np import matplotlib.pyplot as plt from sklearn.cluster import AgglomerativeClustering x=[1,-1,2,-2,3,-3,4,-4,5,-5] y=[10,20,30,-10,-20,30,50,100,90,1] clust=list(zip(x,y)) hierarchical_cluster=AgglomerativeClustering(n_clusters=2) labels=hierarchical_cluster.fit_predict(clust) plt.scatter(x,y,c=labels) plt.show() \n82\nFigure 14-3\n•Association is another approach to unsupervised learning, where you find rules that express your data. For example, people who are followers of person x will be followers of person y, based on the list of people whom they have followed in the past. Or, based on your past tweets, your future tweets will contain certain words. The following Python program uses a frequent pattern growth algorithm for finding associations, which is a data-mining technique. Here it finds the frequent items in a dataset, where the minimum support is 0.8, and the minimum confidence for generating association rule is 0.5. from fpgrowth_py import fpgrowth car= [['Ferrari','Tesla','Ford'],['Ford','Tesla','Toyota']] freqItemSet,rules=fpgrowth(car,minSupRatio=0.8, minConf=0.5) print(freqItemSet) The output is depicted below, as out of six data items, there are two “Ford” and two “Tesla”; here the minimum support is 0.8: [{‘Tesla’}, {‘Ford’}] [{‘Tesla’}, {‘Ford’}] 83\nFigure 14-4\n15. Python and Statistics Statistics is the science of collecting, organizing, and analyzing data. In statistics, we frequently use these values: •Mean: The average value of a data set is calculated by adding all numbers in the data set and then dividing it by the number of values in the set. ✦You can import Numpy and use numpy.mean() •Median: The mid-point value is calculated by sorting all the data and picking the one in the middle. ✦Data should be sorted and then use numpy.median() •Mode: The most common value is the number that occurs the highest number of times. ✦Use stats.mode(), which returns mode and how many times the mode appeared. •Standard deviation: This term refers to how much the data deviates from the typical values. ✦Returns the standard deviation of data, where a low number is an indication that most numbers are close to the mean. Use numpy.std(). •Variance: This term refers to how the values are spread out, and it shows how data points differ from the mean. ✦Returns the variance of data. Use numpy.var(). •Quantiles: This term is used to divide a population according to a distribution. ✦Divides the data into intervals with equal probability. Use numpy.quantiles(). \n84\nThe following Python program utilizes all the above functions. import numpy from scipy import stats A=[10,2,3,4,5,6,7,8,9,10] x=numpy.mean(A) print(\"mean\",x) x=numpy.median(A) print(\"median\",x) x=stats.mode(A) print(\"mode\",x) x=numpy.std(A) print(\"std\",x) x=numpy.var(A) print(\"var\",x) x=numpy.quantile(A,0.75) print(“75%\",x) And the output is as following: mean 6.4 median 6.5 mode ModeResult(mode=10, count=2) std 2.727636339397171 var 7.4399999999999995 75% 8.75 \n85\n15.1 Standardizing Data by Scaling Sometimes, your data has different numerical values and may even be in different units of measurement, so it is hard to compare them. Imagine the number of retweets and the number of followers of someone; both are numerical values but in different scales. You can scale data, which means data is standardized by scaling data to fit a normal distribution with a mean of zero and a standard deviation of one. If a is the original value, m is the mean and s is the standard deviation, a new standard value of a can be calculated as . In Python, you can use StandardScaler() to do so. The following Python program converts an array of old numbers into a scaled array of new numbers. from numpy import asarray from sklearn.preprocessing import StandardScaler old=asarray([[1,0.1],[2, 0.05],[10, 0.2],[5, 0.1],[-1, 0.01]]) print(old) print(\"the new scaled data\") scaler=StandardScaler() new=scaler.fit_transform(old) print(new) And the output is as the following: [[ 1. 0.1 ] [ 2. 0.05] [10. 0.2 ] [ 5. 0.1 ] [-1. 0.01]] the new scaled data [[-0.62725005 0.12561486] [-0.36589586 -0.65947801] [ 1.72493763 1.69580059] [ 0.4181667 0.12561486] [-1.14995842 -1.2875523 ]] a−ms\n86\n15.2 T-Test Many times, in statistics, we need to compare several samples to conclude the given data. A t-test is used to see if there is a significant difference between two groups of data. A t-test checks to see if the two groups of data are significantly different or if the difference is just due to random variation of data, and, hence, they are representing the same fact. Assumption means that the data in each group follows the normal distribution and that the observation of data in the two groups is independent of each other. In a t-test, we compare two means while considering a signification level, which is a predefined threshold used to decide whether to accept or reject results to be statistically significant. The degree of freedom gives us the number of independent variables used to calculate the estimate between two sample groups. In the following Python program, the significance level is 5%, and the degree of freedom is five, which depends on the size of the sample and how many are independent of each other. alpha=0.05 stats.t.ppf(1-alpha,5) Stats contains a large number of probability distributions, statistical tests, and estimations, which you can import into your Python program, with a command such as the following: import scipy.stats as stats.from scipy import stats import numpy as np A=np.array([1,2,3,4]) B=np.array([-12,100,8]) ts,pv=stats.ttest_ind(A,B) alpha=0.05 df=len(A)+len(B) ct=stats.t.ppf(1-alpha/4,df) print(ts,\"****\",pv,\"*****\",ct) print(“With T-value”) 87\nif (ts>ct): print(\"significant difference\") else: print(\"No significant difference”) \n88\nReferences 1.David Amos, Dan Bader, Joanna Jablonski, and Fletcher Heisler. A practical introduction to Python 3, ISBN: 9781775093336 (electronic). 2.Brian Heinold and John Prexy. A Practical Introduction to Python Programming, ISBN: 979-8848271577. 3.https://docs.python.org/3/ Python 3.12 documentation 4.https://www.python.org 5.https://en.wikipedia.org/wiki/Python_Software_Foundation 6.Tony Gaddis. Starting out with Python, 5th edition. Pearson, ISBN: 978-0-13-592903-2. 7.Douglas B. West. Introduction to Graph Theory, 2nd edition. Pearson, ISBN:978-0-13-1437371. 8.https://en.wikipedia.org/wiki/List_of_Unicode_characters 9.https://beginnersbook.com/2018/03/python-tutorial-learn-programming/ 10.https://www.w3resource.com/python/python-tutorial.php 11.Wes McKinney. Python for Data Analysis, O’Reilly, ISBN: 978-1-449-31979-3. 12.Alberto Boschetti and Luca Massaron. Python Data Science Essentials, Packt Publishing Ltd., ISBN: 978-1-78953-786-4. 13.Samir Madhavan. Mastering Python for Data Science, Packt Publishing Ltd., ISBN: 978-1-78439-015-0. 14.Andreas C. Müller and Sarah Guido. Introduction to Machine Learning with Python, A Guide for Data Scientists, O’Reilly, ISBN: 978-1-449-36941-5. 15.Frank Kane. Hands-On Data Science and Python Machine Learning, Packt Publishing Ltd., ISBN: 978-1-78728-074-8. 16.Joel Grus.Data Science from Scratch, O'Reilly, ISBN:978-1-492-04113-9. 89\nAppendix Solutions for Practice Questions (2.4) 1.Write a Python program to get a number, then calculate its squared and cubed value. number = int (input(\"Enter a number\")) print (number *number, \"***\", number*number*number) 2.Write a Python program to get a number such as n as input and then calculate . number= int (input(\"Enter a number\")) print (number ** number) 3.Write a Python program to get a number that contains three digits and then display each digit on a single line. number=int (input(\"Enter a number\")) d1 = number // 100 d2 = (number // 10) % 10 d3 = (number % 100) % 10 print (d1,\"***\", d2,\"***\", d3) 4.Write a Python program to get a circle's radius and calculate its area. R= float (input(\"Enter a radius\")) Area=3.14*R*R print (\"The area is \", Area) nn\n90\n5.Write a Python program to find the roots of a quadratic equation. a = float (input (“Enter a\")) b = float (input (\"Enter b\")) c = float (input (\"Enter c\")) d = (b**2) - (4*a*c) d1 = d**0.5 sol1 = ((-1*b) - d1)/(2*a) sol2 = ((-1*b) + d1)/(2*a) print (sol1,\"***\",sol2 ) 6.Write a Python program to get a name as input and display it five times. name= input (\"Enter a name\") print (name*5) 7.Write a Python program to get today’s date as three integers and display it as m/day/year. month= int(input(\"Enter a month\")) day= int(input(\"Enter a day\")) year= int(input(\"Enter a year\")) print(month,'/', day, '/', year) 8.Write a Python program to simulate the roll of a die. import random dice=random.randint(1,6) print (dice) \n91\n9.Write a Python program to simulate the roll of a pair of dice. import random dice1=random.randint(1,6) dice2=random.randint(1,6) print (dice1,\"****\",dice2) 10.Write a Python program to generate an even random number between one and one hundred. import random m=random.randint(1,50) m=2*m print (m) Return to Section 2.4 \n92\nSolutions for Practice Questions (3.2) 1.Write a Python program to get a number and verify if it is divisible by five. number= int(input(\"Enter a number\")) if (number %5==0): print(\"It is divisible by 5\") else: print(\"It is not divisible by 5\") 2.Write a Python program to get a number and verify if it is divisible by five or by three. number= int(input( \"Enter a number\")) if (number %5==0 or number %3==0): print (\"It is divisible either by 3 or by 3\") else: print (\"It is not divisible by 3 and 5\") 3.Write a Python program to get a year and verify whether it is a leap or a common year. A year is a leap year if it is divisible by four, except those years divisible by a hundred are not leap years unless they are also divisible by four hundred. year= int (input ( \"Enter a year\" )) if ((year%4==0 and year%100!=0)or(year %400==0)): print (\"It is a leap year\") else: print (\"It is a common year\") \n93\n4.Write a Python program to get a date and check if it is a magic date. A date is magical when you multiply month by day and get the year. For example, February 12th of 2024 is a magical date as 2*12=24. month= int(input(\"Enter a month\")) day= int(input(\"Enter a day\")) year= int(input(\"The last two digits of a year?”)) if (month*day ==year): print (\"It is a magic date\") else: print (\"It is NOT a magic date\") 5.Write a Python program to get a number between one and five and convert it to a Roman numeral. number= int(input (\"Enter a number\")) if (number==1): print (\"I\") elif (number==2): print (\"II\") elif (number==3): print (\"III\") elif (number==4): print (\"IV\") elif (number==5): print (\"V\") else: print (\"Invalid input\") 94\n6.Write a Python program to check if a given input is odd or even without using remainder (%). number=int (input(\"Enter a number\")) if ((number//2)*2==number): print (\"Even\") else: print (\"Odd\") 7.Write a Python program to get a word that contains three symbols and display it in reverse order. name= input (\"Enter a name\") print (name[2], name[1], name[0]) 8.Write a Python program to get a word with a length of four as input and then display each string element on one line. name= input (\"Enter a name\") print(name[3],\"\\n\",name[2],\"\\n\",name[1],\"\\n\",name[0],end=\"\") 9.Write a Python program to get a word with a length of four as input, then switch the first and last elements and redisplay the new word. name= input (\"Enter a name\") print (name[3], name[1],name[2],name[0],end= \"\" ) 10.Write a Python program to get a word with a length of four as input and verify if it is a palindrome. name= input (\"Enter a name\") if (name[3]==name[0] and name[2]==name[1]): print (\"It is a palindrome\") else: print (\"It is not a palindrome\") 95\n11.Write a Python program to get a word with a length of four as input and then reverse it. name= input ( \"Enter a name\" ) print (name[3],name[2],name[1],name[0], sep= \"\" ) 12.Write a Python program to get a word with a length of four as input, and if it starts with ‘a’, replace ‘a’ with ‘A’ and redisplay the name; otherwise, display the last two elements of the name. name= input (\"Enter a name\") if (name[0]== 'a' ): print ('A' ,name[1],name[2],name[3], sep=\"\" ) else: print (name[2],name[3], sep=\"\" ) 13.Write a Python program to get a word, and if the length of the word is even, extract the first element and display it; otherwise, display the last element of the string. word = input (\"Enter a name\") if (len (word) %2 ==0): print (word[0]) else: print (word[ len ( word) -1] ) \n96\n14.Write a Python program to get a word with a length of four as input and count the total number of times it contains ‘e’ or ‘E.’ word= input (\"Enter a word\") total=0 if (word[0]== 'e' or word[0]== 'E' ): total=total+1 if (word[1]== 'e' or word[1]== 'E' ): total=total+1 if (word[2]== 'e' or word[2]== 'E' ): total=total+1 if (word[3]== 'e' or word[3]== 'E' ): total=total+1 print (\"The total number of E's \" ,total) \n97\n15.Write a Python program to get a word with a length of four as input and then replace every ‘e’ with ‘X.’ name= input (\"Enter a name\") if (name[0]== 'e' ): temp1= 'x' else : temp1=name[0] if (name[1]== 'e' ): temp2= 'x' else : temp2=name[1] if (name[2]== 'e' ): temp3= 'x' else : temp3=name[2] if (name[3]== 'e' ): temp4= 'x' else : temp4=name[3] print (temp1,temp2,temp3,temp4,sep= “\") Return to Section 3.2 \n98\nSolutions for Practice Questions (4.3) 1.Write a Python program to display numbers between one and five. n=1 while (n<6): print (n) n=n+1 2.Write a Python program to display numbers between five and one. n=5 while (n>0): print (n) n=n-1 3.Write a Python program to show at which temperature Fahrenheit and Centigrade have the same reading. F=-100 C=-99 while (F!=C): print (F, C) C=(F-32)*5//9 F=F+1 4.Write a Python program to calculate the following sum: . n=1 total=0 while (n<=10): total=total+n n=n+1 print (total) 1+2+3+...+10\n99\n5.Write a program to calculate the following sum: . n=1 total=0 while (n<=10): total=total+(1/n) n=n+1 print (total) 6.Write a program to display numbers between one and five using a for loop. for n in [1,2,3,4,5]: print (n) 7.Write a program to display numbers between five and one using a for loop. for n in [5,4,3,2,1]: print (n) 8.Write a program to simulate rolling a pair of dice and show how many tries it takes to get a pair. import random total=0 flag= True while (flag): dice1=random.randint(1,6) dice2=random.randint(1,6) total=total+1 print (total, \"***\",dice1, \"****\",dice2) if (dice1 ==dice2): flag= False 11+12+13+...+110\n100\n9.Write a program to get five numbers as input and calculate their average. total=0 for n in range (1,6): number=float (input (\"Please enter a number\")) total=total+number print (total/5) 10.Write a program to get inputs from the keyboard, where “-1” indicates the end of inputs, and then calculate their average. total=0 counter=0 flag= True while (flag): number=float(input(\"Enter a number,-1 to exit\")) if (number !=-1): total=total+number counter=counter+1 else : flag= False print (total/counter) \n101\n11.Write a program to check whether a given word is a palindrome. flag= True word= input (\"please input a word\") for n in range (0, len (word)): if (word[n] !=word[ len (word)-1-n]): flag= False if (flag== True): print (\"It is a palindrome\") else: print (\"It is NOT a palindrome\") 12.Write a program to get a word as input and replace every string character with its first character; for example,“abcd” would change to “aaaa”. word= input (\"please input a word\") for n in range (0, len(word)): print (word[0],end= \"\" ) Return to Section 4.3 \n102\nSolutions for Practice Questions (4.5) 1.Write a Python program to display the pattern in Figure 4-2. for i in range (1,5): for j in range (1,i+1): print ( \"*\", end=\" \" ) print ( \"\\n\" ) 2.Write a Python program to display the pattern in Figure 4-3. for i in range (1,5): for j in range (1,i+1): print (i, end= \" \") print (\"\\n\") 3.Write a Python program to display the pattern in Figure 4-4. for i in range (1,5): for j in range (1,i+1): print (j, end= \" \") print(“\\n\") 4.Write a Python program to display the pattern in Figure 4-5. for i in range (5,1,-1): for j in range (1, i): print (j, end= \" \") print (“\\n”) \n103\n5.Write a Python program to display the pattern in Figure 4-6. for i in range (1,5): for j in range (5, i,-1): print (\" \" , end= \" \") for k in range (1,j): print (k, end= \" \") print (\"\\n\" ,end= \"\" ) 6.Write a Python program to display the pattern in Figure 4-7. for n in range (0,5): for m in range (n, 4): print (\" \" , end= \"\") for m in range (n + 1): print (\"* \" , end=\"\") print () 7.Write a Python program to display the pattern as depicted in Figure 4-8. for n in range (0,5): for m in range (n, 4): print (\" \", end= \"\") for m in range (n + 1): print (n, end=\"\") print() \n104\n8.Write a Python program to display the pattern as depicted in Figure 4-9. for n in range (0,5): for m in range (n, 4): print (\" \", end= \"\") for m in range (n + 1): print (n, end=\" \") print () 9.Write a Python program to display the pattern depicted in Figure 4-10. for n in range (0,5): for m in range (n, 4): print (\" \", end= \"\") for m in range (n + 1): print (m, end=\" \") print () 10.Write a Python program to display the pattern depicted in Figure 4-11. for n in range (0,5): for m in range (n, 4): print (\" \", end= \"\") for m in range (n+1,0,-1): print (m, end=\" \") print () Return to Section 4.5 \n105\nSolutions for Practice Questions (5.4) 1.Write a Python program to generate a random number between zero and five, then simulate a magic ball. You may select your message from these options: without a doubt, better not tell you now, my sources say no, ask again later, outlook hazy. import random def magic(rand1): if (rand1==1): print (“ Reply Hazy\") if (rand1==2): print (\" Without a doubt\") if (rand1==3): print (\" Better not tell you now\") if (rand1==4): print (\" My sources say no\") if (rand1==5): print (\" Ask again later\") rand1=random.randint(1,5) magic(rand1) 2.Write a Python program to get a word from a user through the keyboard and then display the first character of that word. word= input(\"Please enter a word:\") def Extract(word): print(word[0]) Extract(word) 106\n3.Write a Python program to get a word from a user through the keyboard and then display the first character of that word. Extracting the character should be done in a function, but displaying the character should be done in the main program. word= input (\"Please enter a word:\") def Extract(word): return(word[0]) print (Extract(word)) 4.Write a Python program to get two numbers as input and calculate their average. val1= float(input(\"1st number?\")) val2= float(input(\"2nd number?\")) def Avg (num1,num2): return ((num1+num2)/2) print (Avg(val1,val2)) 5.Write a Python program to get a degree in Fahrenheit and convert it to Celsius. F= float (input(\"Please input a degree in F:\")) def FtoC(F): C=(F-32)*5/9 return C print (FtoC(F)) \n107\n6.Write a Python program to get an input (integer) and calculate its factorial, where the factorial of a number is denoted by n! and is the product of all positive integers less than or equal to n. For example, . number=int (input(\"Input a value:\")) def Fact (m): fact=1 for i in range (1,m+1): fact=fact*i print (m, \"!=\" ,fact) Fact(number) 7.Write a Python program to generate the first ten Fibonacci numbers, where each is the sum of the two preceding ones. These are numbers in the Fibonacci series: number= int(input(\"How many elements of Fibonacci should I display?\")) def Fib (m): i=0 fib1=1 fib2=1 total=0 print(\"Fibonacci series: \",fib1,\",\",fib2,end= \"\") for i in range (1,m+1): total=fib1+fib2 print (\",\", total,end= \"\" ) fib1=fib2 fib2=total Fib(number) 5!=5×4×3×2×1\n1,1,2,3,5,8,...\n108\n8.Write a Python program to get a word from a user through the keyboard and then reverse the word through the use of a function. word= input (\"Please enter a word:\") def Rev(word): newname=“” for i in range (0, len(word)): newname=word[m]+newname return(new name) print (Rev(word)) Return to Section 5.4 \n109\nSolutions for Practice Questions (7.4) 1.Write a Python program to read a file and print every line of the file on the screen by using a for loop. myfile=open(\"output11.txt\",'r') for n in myfile: str1=int(n) print(str1) myfile.close() 2.Write a Python program to read a file and print every line of the file on the screen by using a while loop. myfile=open(\"output11.txt\",'r') str1=myfile.readline() while str1 !='': str1=myfile.readline() print(str1) myfile.close() 3.Write a Python program to read a file and print the total number of lines in the file. myfile=open(\"output11.txt\",'r') str1=myfile.readline() m=0 while str1 !='' : print(str1.rstrip( '\\n' )) m=m+1 str1=myfile.readline() myfile.close() print(\"The total number of lines is :\",m) 110\n4.Write a Python program to read a file and print every word in the file that begins with ‘A’. myfile=open(\"output2.txt\",'r') line=myfile.readline() while(line !=''): if(line[0] ==\"A\"): print(line) line=myfile.readline() myfile.close() 5.Write a Python program to get an input from the keyboard and then add it to an existing file. myfile=open(\"output2.txt\",'a') line=input(\"please input the word to be added to file\") myfile.write(line) myfile.close() 6.Write a Python program to generate five random numbers and write them in a file. import random randfile=open( \"output2.txt\",'w') for n in range(1,6): rand1=random.randint(-10,10) randfile.write(f'{rand1} \\n') randfile.close() \n111\n7.Write a Python program to read a file and check if it contains a specific word. myfile=open(\"input00.txt\",'r') flag=True while(flag or line !='' ): line=myfile.readline() if line.find('Chester') != -1: flag=False if(line =='' ): break myfile.close() if(flag==False): print(\"Found\") else: print(\"Not Found\") \n112\n8.Write a Python program to read a file and count the total number of e’s in the file. myfile=open(\"input00.txt\",'r') counter=0 line=\"...\" while(line !='' ): line=myfile.readline() for n in range(0,len(line)): if(line[n] ==\"e\"): counter=counter+1 myfile.close() print(counter) 9.Write a Python program to write five numbers on a file and then display their summation. myfile=open(\"data.txt\",\"w\") for n in range(1,6): myfile.write(str(n)) myfile.write(\"\\n\") myfile.close() myfile=open(\"data.txt\",\"r\") sum=0 for n in myfile: sum=sum+int(n) print(sum) myfile.close() Return to Section 7.4 113\nSolutions for Practice Questions (8.1) 1.Write a Python program to replace every element of a list with its square. for i in range(len(num)): num[i]=num[i]**2 2.Write a Python program to find the smallest and the second smallest elements in a list. num=[12,-3,100,-67] num.sort() print(num[0],num[1]) 3.Write a Python program to remove the first and last elements of a list and redisplay the new list. num=[12,-3,100,-67,200,5] m=len(num) num.remove(num[0]) num.remove(num[len(num)-1]) print(num) 4.Write a Python program to calculate the average of the elements in a list. num=[-5,4,5,1,-4] print(sum(num)/len(num)) 5.Write a Python program to replace every even element of a list with “*”. num=[12,-6,3,200,11,21,0] for n in range(0,len(num)): if(num[n] %2==0): num[n]=\"*\" print(num) Return to Section 8.1 114\nSolutions for Practice Questions (9.1) 1.Write a Python program to generate an array and reverse it, without changing the order of elements in the original array. def rev(A): NewA=A[::-1] print(\"Reversed Array is: \",end=\" \") for i in NewA: print(i, end=\" \") A=[1,2,3,4,5] rev(A) 2.Write a Python program to generate an array as input and reverse it. The original array will be reversed. A=[1,2,3,4,5,6] print(A) def rev(A,start,end): while start < end: A[start],A[end]=A[end],A[start] start=start+1 end=end-1 rev(A,0,5) print(\"Reversed array is: \") print(A) \n115\n3.Write a Python program to find the common elements in two arrays. A=[1,-1,0,100,2] B=[100,12,-1] def com(A,B): com=[i for i in A if i in B] return com print(com(A,B)) 4.Write a Python program to find repeated items in an array. A=[1,-1,0,1,2,200,1,0] def Repeat(A): size=len(A) rep=[] for i in range(size): k=i+1 for j in range(k,size): if (A[i]==A[j] and A[i] not in rep): rep.append(A[i]) return rep print(Repeat(A)) \n116\n5.Write a Python program to find the second largest value in an array. A=[1,0,4,-1,100] n=len(A) def Max(A,size): A.sort(reverse=True) for i in range(1,size): if (A[i] != A[0]): print(\"The 2nd largest \",A[i]) return Max(A,n) 6.Write a Python program to check if an array contains a value. A=[1,-1,100,2,-100] flag=False for i in A: if(i==100): flag=True break if( flag==True): print(\"Found\") else: print(\"!Found\") Return to Section 9.1\n117", "topic": "Classical Search", "subtopic": "a*", "section_heading": "as permitted by the embedding restrictions included in this font; and (ii) temporarily download this font to a printer o", "source_title": "Introduction-to-Data-Science-AAgah-20240620-1", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "72501b06-24da-46bb-aac1-00d9a02adb09", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 1\nScribe:Philippe Rigollet Sep. 9, 2015\n1. WHAT IS MACHINE LEARNING (IN THIS COURSE) ?\nThis course focuses on statistical learning theory , which roughly means understanding the\namount of data required to achieve a certain prediction accuracy. To better understand\nwhat this means, we ﬁrst focus on stating some diﬀerences between statistics andmachine\nlearning since the two ﬁelds share common goals.\nIndeed,bothseemtotrytousedatatoimprovedecisions. Whiletheseﬁeldshaveevolved\nin the same direction and currently share a lot of aspects, they were at the beginning quite\ndiﬀerent. Statistics was around much before machine learning and statistics was already\na fully developed scientiﬁc discipline by 1920, most notably thanks to the contributions of\nR. Fisher, who popularized maximum likelihood estimation (MLE) as a systematic tool for\nstatistical inference. However, MLErequiresessentially knowingtheprobabilitydistribution\nfromwhichthedataisdraw,uptosomeunknownparameterofinterest. Often, theunknown\nparameter has a physical meaning and its estimation is key in better understanding some\nphenomena. Enabling MLE thus requires knowing a lot about the data generating process:\nthis is known as modeling . Modeling can be driven by physics or prior knowledge of the\nproblem. In any case, it requires quite a bit of domain knowledge.\nMore recently (examples go back to the 1960’s) new types of datasets (demographics,\nsocial, medical,...) have become available. However, modeling the data that they contain\nis much more hazardous since we do not understand very well the input/output process\nthus requiring a distribution free approach. A typical example is image classiﬁcation where\nthe goal is to label an image simply from a digitalization of this image. Understanding\nwhat makes an image a cat or a dog for example is a very complicated process. However,\nfor the classiﬁcation task, one does not need to understand the labelling process but rather\nto replicate it. In that sense, machine learning favors a blackbox approach (see Figure 1).\ninputX outputYblackbox\n y=f(x)+εinputX outputY\nFigure 1: The machine learning blackbox (left) where the goal is to replicate input/output\npairs from past observations, versus the statistical approach that opens the blackbox and\nmodels the relationship.\nThese diﬀerences between statistics and machine learning have receded over the last\ncouple of decades. Indeed, on the one hand, statistics is more and more concerned with\nﬁnite sample analysis, model misspeciﬁcation and computational considerations. On the\nother hand, probabilistic modeling is now inherent to machine learning. At the intersection\nof the two ﬁelds, lies statistical learning theory , a ﬁeld which is primarily concerned with\nsample complexity questions, some of which will be the focus of this class.\n1\n2. STATISTICAL LEARNING THEORY\n2.1 Binary classiﬁcation\nA large partof this class will bedevoted tooneof thesimplest problemof statistical learning\ntheory: binary classiﬁcation (aka pattern recognition [DGL96]). In this problem, weobserve\n(X1,Y1),...,(Xn,Yn) that are nindependent random copies of ( X,Y)∈ X×{0,1}. Denote\nbyPX,Ythe joint distribution of ( X,Y). The so-called featureXlives in some abstract\nspaceX(think IRd) andY∈ {0,1}is called label. For example, Xcan be a collection of\ngene expression levels measured on a patient and Yindicates if this person suﬀers from\nobesity. The goal of binary classiﬁcation is to build a rule to predict YgivenXusing\nonly the data at hand. Such a rule is a function h:X → {0,1}called a classiﬁer . Some\nclassiﬁers are better than others and we will favor ones that have low classiﬁcation error\nR(h) = IP(h(X) =Y). Let us make some important remarks.\nFist of all, since Y∈ {0,1}thenYhas a Bernoulli distribution: so much for distribution\nfree assumptions! However, we will not make assumptions on the marginal distribution of\nXor, what matters for prediction, the conditional distribution of YgivenX. We write,\nY|X∼Ber(η(X)), where η(X) = IP(Y= 1|X) = IE[Y|X] is called the regression function\nofYontoX.\nNext, note that we did not write Y=η(X). Actually we have Y=η(X) +ε, where\nε=Y−η(X) is a“noise” randomvariablethat satisﬁes IE[ ε|X] = 0. Inparticular, this noise\naccounts for the fact that Xmay not contain enough information to predict Yperfectly.\nThis is clearly the case in our genomic example above: it not whether there is even any\ninformation about obesity contained in a patient’s genotype. The noise vanishes if and only\nifη(x)∈ {0,1}for allx∈ X. Figure 2.1 illustrates the case where there is no noise and the\nthe more realistic case where there is noise. When η(x) is close to .5, there is essentially no\ninformation about YinXas theYis determined essentially by a toss up. In this case, it\nis clear that even with an inﬁnite amount of data to learn from, we cannot predict Ywell\nsince there is nothing to learn. We will see what the eﬀect of the noise also appears in the\nsample complexity./ne}ationslash\nFigure 2: The thick black curve corresponds to the noiseless case where Y=η(X)∈ {0,1}\nand the thin red curve corresponds to the more realistic case where η∈[0,1]. In the latter\ncase, even full knowledge of ηdoes not guarantee a perfect prediction of Y.\nIn the presence of noise, since we cannot predict Yaccurately, we cannot drive the\nclassiﬁcation error R(h) to zero, regardless of what classiﬁer hwe use. What is the smallest\nvalue that can beachieved? As a thought experiment, assume to begin with that we have all\nxη(x)\n1\n.5\n2\nthe information that we may ever hope to get, namely we know the regression function η(·).\nFor a given Xto classify, if η(X) = 1/2 we may just toss a coin to decide our prediction\nand discard Xsince it contains no information about Y. However, if η(X) = 1/2, we have\nan edge over random guessing: if η(X)>1/2, it means that IP( Y= 1|X)>IP(Y= 0|X)\nor, in words, that 1 is more likely to be the correct label. We will see that the classiﬁer\nh∗(X) = 1I(η(X)>1/2) (called Bayes classiﬁer ) is actually the best possible classiﬁer in\nthe sense that\nR(h∗) = infR(h),\nh(·)\nwhere the inﬁmum is taken over all classiﬁers, i.e. functions from Xto{0,1}. Note that\nunlessη(x)∈ {0,1}for allx∈ X(noiseless case), we have R(h∗) = 0. However, we can\nalways look at the excess risk E(h) of a classiﬁer hdeﬁned by\nE(h) =R(h)−R(h∗)≥0.\nIn particular, we can hope to drive the excess risk to zero with enough observations by\nmimicking h∗accurately.\n2.2 Empirical risk\nThe Bayes classiﬁer h∗, while optimal, presents a major drawback: we cannot compute it\nbecause we do not know the regression function η. Instead, we have access to the data\n(X1,Y1),...,(Xn,Yn), which contains some (but not all) information about ηand thus h∗.\nIn order to mimic the properties of h∗recall that it minimizes R(h) over all h. But the\nfunction R(·) is unknown since it depends on the unknown distribution PX,Yof (X,Y). We\nˆ estimate it by the empirical classiﬁcation error, or simply empirical risk Rn(·) deﬁned for\nany classiﬁer hby\nn1ˆRn(h) =/summationdisplay\n1I(h(Xi) =Yi).ni=1\nˆ ˆ Since IE[1I( h(Xi) =Yi)] = IP(h(Xi) =Yi) =R(h), we have IE[ Rn(h)] =R(h) soRn(h) is\nanunbiased estimator of R(h). Moreover, for any h, by the law of large numbers, we have\nˆ ˆ Rn(h)→R(h) asn→ ∞, almost surely. This indicates that if nis large enough, Rn(h)\nshould be close to R(h).\nAs a result, in order to mimic the performance of h∗, let us use the empirical risk\nˆ ˆ minimizer (ERM) hdeﬁned to minimize Rn(h) over all classiﬁers h. This is an easy enough\nˆ ˆ task: deﬁne hsuchh(Xi) =Yifor alli= 1,...,nandh(x) = 0 ifx∈/{X1,...,X n}. We\nˆˆ haveRn(h) = 0, which is clearly minimal. The problem with this classiﬁer is obvious: it\ndoes not generalize outside the data. Rather, it predicts the label 0 for any xthat is not in\nˆˆ the data. We could have predicted 1 or any combination of 0 and 1 and still get Rn(h) = 0.\nˆ In particular it is unlikely that IE[ R(h)] will be small./ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash /ne}ationslash\n3\nImportant Remark :Recall that R(h) = IP(h(X)/ne}ationslash=Y).\nˆ ˆ ˆ Ifh(·) =h({(X1,Y1),...,(Xn,Yn)};·) is c\nonstructed from the data, R(h) denotes\ntheconditional probability\nˆ ˆR(h) = IP(h(X)/ne}ationslash=Y|(X1,Y1),...,(Xn,Yn)).\nˆ ˆ rathert\nhanIP(h(X)/ne}ationslash=Y). As aresult R(h)is arandomva r iablesinceit dependsonthe\nrandomness of the data ( X1,Y1),...,(Xn,Yn). One way to view this is to observe that\nˆ we compute the deterministic function R(·) and then plug in the random classiﬁer h.\nThis problem is inherent to any method if we are not willing to make any assumption\non the distribution of ( X,Y) (again, so much for distribution freeness!). This can actually\nbe formalized in theorems, known as no-free-lunch theorems.\nTheorem: ˆ For any integer n≥1, any classiﬁer hbuilt from ( X1,Y1),...,(Xn,Yn) and\nanyε >0, there exists a distribution PX,Yfor (X,Y) such that R(h∗) = 0 and\nˆIER(hn)≥1/2−ε.\nTo be fair, note that here the distribution of the pair ( X,Y) is allowed to depend on\nnwhich is cheating a bit but there are weaker versions of the no-free-lunch theorem that\nessentially imply that it is impossible to learn without further assumptions. One such\ntheorem is the following.\nTheorem: ˆ For any classiﬁer hbuilt from ( X1,Y1),...,(Xn,Yn) and any sequence\n{an}n>0 that converges to 0, there exists a distribution PX,Yfor (X,Y) such that\nR(h∗) = 0 and\nˆIER(hn)≥an,for alln≥1\nIn the above theorem, the distribution of ( X,Y) is allowed to depend on the whole sequence\n{an}n>0 but not on a speciﬁc n. The above result implies that the convergence to zero of\nthe classiﬁcation error may be arbitrarily slow.\n2.3 Generative vs discriminative approaches\nBoth theorems above imply that we need to restrict the distribution PX,Yof (X,Y). But\nisn’t that exactly what statistical modeling is? The is answer is not so clear depending on\nhow we perform this restriction. There are essentially two schools: generative which is the\nstatistical modeling approach and discriminative which is the machine learning approach.\nGenerative: This approach consists in restricting the set of candidate distributions PX,Y.\nThis is what is done in discriminant analysis1where it is assumed that the condition dis-\n1Amusingly, the generative approach is called discriminant analysis but don’t let the terminology fool\nyou.\n4\ntributions of XgivenY(there are only two of them: one for Y= 0 and one for Y= 1) are\nGaussians on X= IRd(see for example [HTF09] for an overview of this approach).\nDiscriminative: This is the machine learning approach. Rather than making assumptions\ndirectly on the distribution, one makes assumptions on what classiﬁers are likely to perform\ncorrectly. In turn, this allows to eliminate classiﬁers such as the one described above and\nthat does not generalize well.\nWhile it is important to understand both, we will focus on the discriminative approach\nin this class. Speciﬁcally we are going to assume that we are given a class Hof classiﬁers\nsuch that R(h) is small for some h∈ H.\n2.4 Estimation vs. approximation\nAssumethat we aregiven a class Hin which weexpect to ﬁnda classiﬁer that performswell.\nThisclassmaybeconstructedfromdomainknowledgeorsimplycomputational convenience.\nˆ We will see some examples in the class. For any candidate classiﬁer hnbuilt from the data,\nwe can decompose its excess risk as follows:\nˆ ˆ ˆ E(hn) =R(hn)−R(h∗) =R(hn)−infR(h)+ infR(h)−R(h∗).\nh∈H h∈H/bracehtipupleft\nestimat/bracehtipdownright\nio/bracehtipdownleft\nn error/bracehtipupright /bracehtipupleft\napproxim/bracehtipdownright\na/bracehtipdownleft\ntion error/bracehtipupright\nOn the one hand, estimation error accounts for the fact that we only have a ﬁnite\namount of observations and thus a partial knowledge of the distribution PX,Y. Hopefully\nwe can drive this error to zero as n→ ∞. But we already know from the no-free-lunch\ntheorem that this will not happen if His the set of all classiﬁers. Therefore, we need to\ntakeHsmall enough. On the other hand, if His too small, it is unlikely that we will\nﬁnd classiﬁer with performance close to that of h∗. A tradeoﬀ between estimation and\napproximation can be made by letting H=Hngrow (but not too fast) with n.\nFor now, assume that His ﬁxed. The goal of statistical learning theory is to understand\nhow the estimation error drops to zero as a function not only of nbut also of H. For the\nﬁrst argument, we will use concentration inequalities such as Hoeﬀding’s and Bernstein’s\ninequalities that allow us to control how close the empirical risk is to the classiﬁcation error\nby bounding the random variable\n/vextendsinglen1/vextendsingle/summationdisplay\n1I(h(X (h /vextendsingle i) =Yi)−IP (X) =Y)/vextendsingle\nn/vextendsingle\ni=1/vextendsingle\nwith high probability. More generally we will be interested in results that allow to quantify\nhow close the average of independent and identically distributed (i.i.d) random variables is\nto their common expected value.\nˆˆˆ Indeed, since by deﬁnition, we have Rn(h)≤Rn(h) for allh∈ H, the estimation error\n¯ can be controlled as follows. Deﬁne h∈ Hto be any classiﬁer that minimizes R(·) overH\n(assuming that such a classiﬁer exist).\nˆ ˆ ¯ R(hn)−infR(h) =R(hn)−R(h)\nh∈H\nˆˆˆ¯ ˆ ˆ ¯ ¯ =Rn(hn)−Rn(h)+R(hn)−Rˆn(h)+Rˆn n(h)−R(h)/bracehtipupleft\n≤/bracehtipdownright/bracehtipdownleft\n0/bracehtipupright\n≤/vextendsingle/vextendsingleˆˆ ˆ ˆ¯ ¯ Rn(hn)−R(hn)/vextendsingle/vextendsingle+/vextendsingle/vextendsingleRn(h)−R(h)/vextendsingle/vextendsingle./ne}ationslash /ne}ationslash\n5\n¯ ˆ¯ ¯ Sincehis deterministic, we can use a concentration inequality to control/vextendsingle/vextendsingleRn(h)−R(h)/vextendsingle/vextendsingle.\nHowever,\nn1ˆˆ ˆ Rn(hn) =/summationdisplay\n1I(hn(Xi) =Yi)ni=1\nisnot ˆ the average of independent random variables since hndepends in a complicated\nmanner on all of the pairs ( Xi,Yi),i= 1,...,n. To overcome this limitation, we often use\nˆ a blunt, but surprisingly accurate tool: we “sup out” hn,\n/vextendsingle/vextendsingleˆˆ ˆ ˆˆ ˆ Rn(hn)−R(hn)/vextendsingle/vextendsingle≤sup/vextendsingle\nh∈/vextendsingleRn(hn)−R(hn)/vextendsingle\nH/vextendsingle.\nControlling this supremum falls in the scope of suprema of empirical processes that we will\nstudy in quite a bit of detail. Clearly the supremum is smaller as His smaller but Hshould\nbe kept large enough to have good approximation properties. This is the tradeoﬀ between\napproximation and estimation. It is also know in statistics as the bias-variance tradeoﬀ./ne}ationslash\n6\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 2\nScribe: Jonathan Weed Sep. 14, 2015\nPart I\nStatistical Learning Theory\n1. BINARY CLASSIFICATION\nIn the last lecture, we looked broadly at the problems that machine learning seeks to solve\nand the techniques we will cover in this course. Today, we will focus on one such problem,\nbinary classi\fcation , and review some important notions that will be foundational for the\nrest of the course.\nOur present focus on the problem of binary classi\fcation is justi\fed because both binary\nclassi\fcation encompasses much of what we want to accomplish in practice and because the\nresponse variables in the binary classi\fcation problem are bounded. (We will see a very\nimportant application of this fact below.) It also happens that there are some nasty surprises\nin non-binary classi\fcation, which we avoid by focusing on the binary case here.\n1.1 Bayes Classi\fer\nRecall the setup of binary classi\fcation: we observe a sequence ( X1;Y1);:::; (Xn;Yn) ofn\nindependent draws from a joint distribution PX;Y. The variable Y(called the label) takes\nvalues inf0;1g, and the variable Xtakes values in some space Xrepresenting \\features\" of\nthe problem. We can of course speak of the marginal distribution PXofXalone; moreover,\nsinceYis supported onf0;1g, the conditional random variable YjXis distributed according\nto a Bernoulli distribution. We write YjX\u0018Bernoulli(\u0011 (X)), where\n\u0011(X) = I P(Y = 1jX ) = I E[YjX]:\n(The function \u0011is called the regression function .)\nWe begin by de\fning an optimal classi\fer called the Bayes classi\fer. Intuitively, the\nBayes classi\fer is the classi\fer that \\knows\" \u0011|it is the classi\fer we would use if we had\nperfect access to the distribution YjX.\nDe\fnition: The Bayes classi\fer ofXgivenY, denotedh\u0003, is the function de\fned by the\nrule\n(h\u001a1 if\u0011 x)>1=2 \u0003(x) =0 if\u0011(x)\u00141=2.\nIn other words, h\u0003(X) = 1 whenever I P(Y = 1jX )>I P(Y = 0jX ).\nOur measure of performance for any classi\fer h(that is, any function mapping Xto\nf0;1g) will be the classi\fcation error :R(h) = I P(Y=h(X)). The Bayes risk is the value\nR\u0003=R(h\u0003) of the classi\fcation error associated with the Bayes classi\fer. The following\ntheorem establishes that the Bayes classi\fer is optimal with respect to this metric.6\n7\nTheorem: For any classi\fer h, the following identity holds:\nR(h)\u0000R(h\u0003) =Z\nj2\u0011(x)\u00001jPx(dx) = I E X[j2\u0011(X)\u00001j1(h(X ) =h\u0003(X))] (1.1)\nh=h\u0003\nWhereh=h\u0003is the (measurable) set fx2Xjh(x) =h\u0003(x)g.\nIn particular, since the integrand is nonnegative, the classi\fcation error R\u0003of the\nBayes classi\fer is the minimizer of R(h) over all classi\fers h.\nMoreover,\n1R(h\u0003) = I E[min(\u0011 (X);1\u0000\u0011(X))]\u0014: (1.2)2\nProof. We begin by proving Equation (1.2). The de\fnition of R(h) implies\nR(h) = I P(Y=h(X)) = I P(Y = 1;h(X ) = 0) + I P( Y= 0;h(X ) = 1);\nwhere the second equality follows since the two events are disjoint. By conditioning on X\nand using the tower law, this last quantity is equal to\nI E[I E[1(Y = 1;h(X ) = 0)jX ]] + I E[I E[1(Y = 0;h(X ) = 1)jX]]\nNow,h(X) is measurable with respect to X, so we can factor it out to yield\nI E[1(h(X ) = 0)\u0011(X) +1(h(X ) = 1)(1\u0000\u0011(X))]]; (1.3)\nwhere we have replaced I E[ YjX] by\u0011(X).\nIn particular, if h=h\u0003, then Equation 1.3 becomes\nI E[1(\u0011 (X)\u00141=2)\u0011 (X) +1(\u0011(x)>1=2)(1\u0000\u0011(X))]:\nBut\u0011(X)\u00141=2 implies \u0011(X)\u00141\u0000\u0011(X) and conversely, so we \fnally obtain\nR(h\u0003) = I E[1(\u0011 (X)\u00141=2)\u0011 (X) +1(\u0011(x)>1=2)(1\u0000\u0011(X))]\n= I E[(1(\u0011 (X)\u00141=2) + 1(\u0011(x)>1=2)) min(\u0011(X);1\u0000\u0011(X))]\n= I E[min(\u0011 (X);1\u0000\u0011(X))];\nas claimed. Since min(\u0011 (X);1\u0000\u0011(X))\u00141=2, its expectation is also certainly at most 1 =2\nas well.\nNow, given an arbitrary h, applying Equation 1.3 to both handh\u0003yields\nR(h)\u0000R(h\u0003) = I E[ 1(h(X ) = 0)\u0011 (X) +1(h(X ) = 1)(1\u0000\u0011(X))]\n\u00001(h\u0003(X) = 0)\u0011 (X) +1(h\u0003(X) = 1)(1\u0000\u0011(X))]];\nwhich is equal to\nI E[(1(h(X ) = 0)\u00001(h\u0003(X) = 0))\u0011 (X) + (1(h(X ) = 1)\u00001(h\u0003(X) = 1))(1\u0000\u0011(X))]:\nSinceh(X) takes only the values 0 and 1, the second term can be rewritten as \u0000(1(h(X ) =\n0)\u00001(h\u0003(X) = 0)). Factoring yields\nI E[(2\u0011 (X)\u00001)(1(h(X ) = 0)\u00001(h\u0003(X) = 0))]:66\n6\n6\n8\nThe term 1(h(X ) = 0)\u00001(h\u0003(X) = 0) is equal to \u00001, 0, or 1 depending on whether h\nandh\u0003agree. When h(X) =h\u0003(X), it is zero. When h(X) =h\u0003(X), it equals 1 whenever\nh\u0003(X) = 0 and\u00001 otherwise. Applying the de\fnition of the Bayes classi\fer, we obtain\nI E[(2\u0011 (X)\u00001)1(h(X ) =h\u0003(X)) sign(\u0011\u00001=2)] = I E[j2\u0011(X)\u00001j1(h(X ) =h\u0003(X))];\nas desired.\nWe make several remarks. First, the quantity R(h)\u0000R(h\u0003) in the statement of the\ntheorem above is called the excess risk ofhand denotedE(h). (\\Excess,\" that is, above\nthe Bayes classi\fer.) The theorem implies that E(h)\u00150.\nSecond, the risk of the Bayes classi\fer R\u0003equals 1=2 if and only if \u0011(X) = 1=2 almost\nsurely. This maximal risk for the Bayes classi\fer occurs precisely when Y\\contains no\ninformation\" about the feature variable X. Equation (1.1) makes clear that the excess risk\nweighs the discrepancy between handh\u0003according to how far \u0011is from 1=2. When \u0011is\nclose to 1=2, no classi\fer can perform well and the excess risk is low. When \u0011is far from\n1=2, the Bayes classi\fer performs well and we penalize classi\fers that fail to do so more\nheavily.\nAs noted last time, linear discriminant analysis attacks binary classi\fcation by putting\nsome model on the data. One way to achieve this is to impose some distributional assump-\ntions on the conditional distributions XjY= 0 andXjY= 1.\nWe can reformulate the Bayes classi\fer in these terms by applying Bayes' rule:\nI P(X =xY= 1)I P(Y= 1)\u0011(x) = I P(Y = 1jjX=x) = :I P(X =xjY= 1)I P(Y= 1) + I P(X=xjY= 0)I P(Y= 0)\n(In general, when PXis a continuous distribution, we should consider in\fnitesimal proba-\nbilities I P(X2dx).)\nAssume that XjY= 0 andXjY= 1 have densities p0andp1, and I P(Y = 1) =\u0019is\nsome constant re\necting the underlying tendency of the label Y. (Typically, we imagine\nthat\u0019is close to 1=2, but that need not be the case: in many applications, such as anomaly\ndetection,Y= 1 is a rare event.) Then h\u0003(X) = 1 whenever \u0011(X)\u00151=2, or, equivalently,\nwhenever\np1(x) 1\u0000\u0019:p0(x)\u0015\u0019\nWhen\u0019= 1=2, this rule amounts to reporting 1 or 0 by comparing the densities p1\nandp0. For instance, in Figure 1, if \u0019= 1=2 then the Bayes classi\fer reports 1 whenever\np1\u0015p0, i.e., to the right of the dotted line, and 0 otherwise.\nOn the other hand, when \u0019is far from 1=2, the Bayes classi\fer is weighed towards the\nunderlying bias of the label variable Y.\n1.2 Empirical Risk Minimization\nThe above considerations are all probabilistic , in the sense that they discuss properties of\nsome underlying probability distribution. The statistician does nothave access to the true\nprobability distribution PX;Y; she only has access to i.i.d. samples (X 1;Y1);:::; (Xn;Yn).\nWe consider now this statistical perspective. Note that the underlying distribution PX;Y\nstill appears explicitly in what follows, since that is how we measure our performance: we\njudge the classi\fers we produced on future i.i.d. draws from PX;Y.6\n6 6\n9\nFigure 1: The Bayes classi\fer when \u0019= 1=2.\nGiven dataDn=f ^ (X1;Y1);:::; (Xn;Yn)g, we build a classi\fer hn(X), which is random\nin two senses: it is a function of a random variable Xand also depends implicitly on the\n^ random dataDn. As above, we judge a classi\fer according to the quantity E(hn). This is\na random variable: though we have integrated out X, the excess risk still depends on the\ndataDn. We therefore will consider bounds both on its expected value and bounds that\n^ hold in high probability. In any case, the bound E(hn)\u00150 always holds. (This inequality\ndoes not merely hold \\almost surely,\" since we proved that R(h)\u0015R(h\u0003) uniformly over\nall choices of classi\fer h.)\nLast time, we proposed two di\u000berent philosophical approaches to this problem. In\nparticular, generative approaches make distributional assumptions about the data, attempt\nto learn parameters of these distributions, and then plug the resulting values into the model.\nThe discriminative approach|the one taken in machine learning|will be described in great\ndetail over the course of this semester. However, there is some middle ground, which is worth\nmentioning brie\ny. This middle ground avoids making explicit distributional assumptions\naboutXwhile maintaining some of the \navor of the generative model.\nThe central insight of this middle approach is the following: since by de\fnition h\u0003(x) =\n^ 1(\u0011(X)>1=2), we estimate \u0011by some\u0011^nand thereby produce the estimator hn=\n1(\u0011^n(X)>1=2). The result is called a plug-in estimator.\nOf course, achieving good performance with a plug-in estimator requires some assump-\ntions. (No-free-lunch theorems imply that we can't avoid making an assumption some-\nwhere!) One possible assumption is that \u0011(X) is smooth; in that case, there are many\nnonparamteric regression techniques available (Nadaraya-Watson kernel regression, wavelet\nbases, etc.).\nWe could also assume that \u0011(X) is a function of a particular form. Since \u0011(X) is only\nsupported on [0; 1], standard linear models are generally inapplicable; rather, by applying\nthe logit transform we obtain logistic regression , which assumes that \u0011satis\fes an identity\nof the form\nlog\u0012\u0011(X)\n1\u0000\u0011(X)\u0013\n=\u0012TX:\nPlug-in estimators are called \\semi-paramteric\" since they avoid making any assumptions\nabout the distribution of X. These estimators are widely used because they perform fairly\nwell in practice and are very easy to compute. Nevertheless, they will not be our focus here.\nIn what follows, we focus here on the discriminative framework and empirical risk min-\nimization. Our benchmark continues to be the risk function R(h) = I E1(Y =h(X)), which6\n10\nis clearly not computable based on the data alone; however, we can attempt to use a na \u0010ve\nstatistical \\hammer\" and replace the expectation with an average.\nDe\fnition: The empirical risk of a classi\fer his given by\nn1^Rn(h) =X\n1(Yi=h(Xi)):ni=1\nMinimizing the empirical risk over the family of all classi\fers is useless, since we can\nalways minimize the empirical risk by mimicking the data and classifying arbitrarily other-\nwise. We therefore limit our attention to classi\fers in a certain family H.\n^ De\fnition: The Empirical Risk Minimizer (ERM) overHis any element1hermof the set\n^ argminhRn(h).2H\nIn order for our results to be meaningful, the class Hmust be much smaller than the\n^ space of all classi\fers. On the other hand, we also hope that the risk of hermwill be close\nto the Bayes risk, but that is unlikely if His too small. The next section will give us tools\nfor quantifying this tradeo\u000b.\n1.3 Oracle Inequalities\nAn oracle is a mythical classi\fer, one that is impossible to construct from data alone but\n\u0016 whose performance we nevertheless hope to mimic. Speci\fcally, given Hwe de\fnehto be\nan element of argminhR(h)|a classi\fer in2H Hthat minimizes the true risk. Of course,\n\u0016 we cannot determine h, but we can hope to prove a bound of the form\n^R(h)\u0014 \u0016R(h) + something small: (1.4)\n\u0016 Sincehis the best minimizer in Hgiven perfect knowledge of the distribution, a bound of\n^ the form given in Equation 1.4 would imply that hhas performance that is almost best-in-\nclass. We can also apply such an inequality in the so-called improper learning framework,\n^ where we allow hto lie in a slightly larger class H0\n^\u001bH; in that case, we still get nontrivial\n\u0016 guarantees on the performance of hif we know how to control R(h)\nThere is a natural tradeo\u000b between the two terms on the right-hand side of Equation 1.4.\nWhenH \u0016 is small, we expect the performance of the oracle hto su\u000ber, but we may hope\n\u0016 to approximate hquite closely. (Indeed, at the limit where His a single function, the\n\\something small\" in Equation 1.4 is equal to zero.) On the other hand, as Hgrows the\noracle will become more powerful but approximating it becomes more statistically di\u000ecult.\n(In other words, we need a larger sample size to achieve the same measure of performance.)\n^ SinceR(h) is a random variable, we ultimately want to prove a bound in expectation\nor tail bound of the form\n^ I P(R(h)\u0014 \u0016R(h) + \u0001n;\u000e(H))\u00151\u0000\u000e;\nwhere \u0001n;\u000e(H) is some explicit term depending on our sample size and our desired level of\ncon\fdence.\n1In fact, even an approximate solution will do: our bounds will still hold whenever we produce a classi\fer\n^ ^^ hsatisfying Rn(h)\u0014infhR2H n(h) + \".6\n11\nIn the end, we should recall that\nE^ ^\u0000\u0003 ^\u0000 \u0016 \u0016 (h) =R(h)R(h) = (R(h)R(h)) + (R (h)\u0000R(h\u0003)):\nThe second term in the above equation is the approximation error, which is unavoidable\nonce we \fx the class H. Oracle inequalities give a means of bounding the \frst term, the\nstochastic error.\n1.4 Hoe\u000bding's Theorem\nOur primary building block is the following important result, which allows us to understand\nhow closely the average of random variables matches their expectation.\nTheorem (Hoe\u000bding's Theorem): LetX1;:::;Xnbenindependent random vari-\nables such that Xi2[0;1] almost surely.\nThen for any t>0,\nI P \f\fn1X\nXiI EXini=1\u0000\f\f\n\f>!\n\f\f\f2t\u00142e\u00002nt:\nIn other words, deviations from\f\nthe mean deca\f\ny exponentially fast in nandt.\nProof. De\fne centered random variables Zi=Xi\u0000I EXi. It su\u000eces to show that\n\u00121X\u0013\n\u0014\u00002nt2I PZi>t e ;n\nsince the lower tail bound follows analogously. (Exercise!)\nWe apply Cherno\u000b bounds. Since the exponential function is an order-preserving bijec-\ntion, we have for any s>0\nI P\u00121X\nZstni>t\u0013\n= I P\u0010\nexp\u0010\nsX\nZstn s Z ii ]n\u0011\n>e\u0011\n\u0014e\u0000I E[eP\n(Markov)\n=e\u0000stnI E[esZi]; (1.5)\nwhere in the last equality we have used the independence of theY\nZi.\nWe therefore need to control the term I E[ esZi], known as the moment-generating func-\ntion ofZi. If theZiwere normally distributed, we could compute the moment-generating\nfunction analytically. The following lemma establishes that we can do something similar\nwhen theZiare bounded.\nLemma (Hoe\u000bding's Lemma): IfZ2[a;b] almost surely and I EZ = 0, then\n2 2\n\u0014s(b\u0000a)I EesZe 8:\nProof of Lemma. Consider the log-moment generating function (s) = log I E[esZ], and note\nthat it su\u000eces to show that (s)\u0014s2(b\u0000a)2=8. We will investigate by computing the\n12\n\frst several terms of its Taylor expansion. Standard regularity conditions imply that we\ncan interchange the order of di\u000berentiation and integration to obtain\nI E[ZesZ] 0(s) = ;I E[esZ]\n2I E[Z2esZ]I E[esZ] I E[ZesZ]2esZesZ\n 00(s) =\u0000= I E\u0014\nZ2\u0015\n\u0000\u0012\nI EZI E[esZ]2 I E[esZ]\u0014\nI E[esZ]\u0015\u0013\n:\nSinceesZ\nsZintegrates to 1, we can interpret 00(s) as the variance of Zunder the probabilityI E[e]\nmeasuredF=esZ\nsZdI E. We obtainI E[e]\n 00(s) = var F(Z) = var F\u0012a+bZ\u00002\u0013\n;\nsince the variance is una\u000bected under shifts. But jZ\u0000a+b\n2j\u0014b\u0000aalmost surely since2\nZ2[a;b] almost surely, so\nvarF\u0012+bZ\u00002\u0013\n\u0014F\"\u00122a a+bZ\u00002\u0013#\n(b\u0000a)2\n\u0014:4\nFinally, the fundamental theorem of calculus yields\ns us2(b a)2\n (s) =Z\n(u du\n0Z\n 00)\u0000:\n0\u00148\nThis concludes the proof of the Lemma.\nApplying Hoe\u000bding's Lemma to Equation (1.5), we obtain\nI P\u00121X2Z >t\u0013\n\u0014e\u0000stnY\nes2=8=ens =8stni\u0000;n\nfor anys>0. Plugging in s= 4t> 0 yields\nI P\u00121X\nZi>t\u0013\n\u0014e\u00002nt2;n\nas desired.\nHoe\u000bding's Theorem implies that, for any classi\fer h, the bound\nlog(2=\u000e )j^Rn(h)\u0000R(h)j\u0014r\n2n\nholds with probability 1 \u0000\u000e. We can immediately apply this formula to yield a maximal\ninequality: ifHis a \fnite family, i.e., H=fh1;:::;hMg, then with probability 1 \u0000\u000e=M\nthe bound\nlogj^Rn(hj)\u0000R(hj)j\u0014r\n(2M=\u000e)\n2n\n13\n^ holds. The event that max jjRn(hj)\u0000R(hj)j ^ >tis the union of the events jRn(hj)\u0000R(hj)j>\ntforj= 1;:::;M , so the union bound immediately implies that\nlog(2M=\u000e )maxj^Rn(hj)\u0000R(hj)\njj\u0014r\n2n\nwith probability 1 \u0000\u000e. In other words, for such a family, we can be assured that the empirical\nrisk and the true risk are close. Moreover, the logarithmic dependence on Mimplies that\nwe can increase the size of the family Hexponentially quickly with nand maintain the\nsame guarantees on our estimate.\n14\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 3\nScribe:James Hirst Sep. 16, 2015\n1.5 Learning with a ﬁnite dictionary\nRecall from the end of last lecture our setup: We are working with a ﬁnite dictionary\nH={h1,...,h M}of estimators, andwewouldliketounderstandthescaling ofthis problem\nwith respect to Mand the sample size n. GivenH, one idea is to simply try to minimize\nˆ the empirical risk based on the samples, and so we deﬁnethe empirical risk minimizer, herm,\nby\nˆ ˆ herm∈argminRn(h).\nh∈H\nˆ ˆ In what follows, we will simply write hinstead of hermwhen possible. Also recall the\n¯ deﬁnition of the oracle, h, which (somehow) minimizes the true risk and is deﬁned by\n¯h∈argminR(h).\nh∈H\nˆ ¯ The following theorem shows that, although hcannot hope to do better than hin\ngeneral, the diﬀerence should not be too large as long as the sample size is not too small\ncompared to M.\nˆ Theorem: The estimator hsatisﬁes\nˆ ¯R(h)≤R(h)+/radicalbigg\n2log(2M/δ)\nn\nwith probability at least 1 −δ. In expectation, it holds that\n/radicalbigg\n2log(2M)ˆ ¯IE[R(h)]≤R(h)+ .n\nProof. ˆ ˆˆˆ¯ From the deﬁnition of h, we have Rn(h)≤Rn(h), which gives\nˆ ¯ ˆ¯ ¯ ˆ ˆˆ R(h)≤R(h)+[Rn(h)−R(h)]+[R(h)−Rn(h)].\nThe only term here that we need to control is the second one, but since we don’t have\n¯ any real information about h, we will bound it by a maximum over Hand then apply\nHoeﬀding:\nlog(2M/δ)ˆ¯ ¯ ˆ ˆˆ ˆ [Rn(h)−R(h)]+[R(h)−Rn(h)]≤2max|Rn(hj)−R(hj)| ≤2\nj/radicalbigg\n2n\nwith probability at least 1 −δ, which completes the ﬁrst part of the proof.\n15\nTo obtain the bound in expectation, we start with a standard trick from probability\nwhich bounds a max by its sum in a slightly more clever way. Here, let {Zj}jbe centered\nrandom variables, then\n/bracketleftbigg /bracketrightbigg1/parenleftbigg /bracketleftbigg1IE max|Zj|= logexp sIE max|Zj|/bracketrightbigg/parenrightbigg\n≤logIE/bracketleftbigg\nexp/parenleftbigg\nsmax|Zj|\nj s j s j/parenrightbigg/bracketrightbigg\n,\nwhere the last inequality comes from applying Jensen’s inequality to the convex function\nexp(·). Now we bound the max by a sum to get\n2M1 )≤log/summationdisplay 1 s2log(2M sIE[exp(sZj)]≤log/parenleftbigg\n2Mexp/parenleftbigg /parenrightbigg/parenrightbigg\n= + ,s s 8n s 8nj=1\nˆ where we used Zj=Rn(hj)−R(hj) in our case and then applied Hoeﬀding’s Lemma. Bal-\nancing terms by minimizing over s, this gives s= 2/radicalbig\n2nlog(2M) and plugging in produces\n/bracketleftbigglog(2M)ˆ IE max|Rn(hj)−R(hj)| ≤\nj/bracketrightbigg/radicalbigg\n,2n\nwhich ﬁnishes the proof.\n2. CONCENTRATION INEQUALITIES\nConcentration inequalities are results that allow us to bound the deviations of a function of\nrandomvariablesfromitsaverage. Theﬁrstofthesewewillconsiderisadirect improvement\nto Hoeﬀding’s Inequality that allows some dependence between the random variables.\n2.1 Azuma-Hoeﬀding Inequality\nGiven a ﬁltration {Fi}iof our underlying space X, recall that {∆i}iare called martingale\ndiﬀerences if, for every i, it holds that ∆ i∈ Fiand IE[∆ i|Fi] = 0. The following theorem\ngives a very useful concentration bound for averages of bounded martingale diﬀerences.\nTheorem (Azuma-Hoeﬀding): Suppose that {∆i}iare margingale diﬀerences with\nrespect to the ﬁltration {Fi}i, and let Ai,Bi∈ Fi−1satisfyAi≤∆i≤Bialmost surely\nfor every i. Then\nIP/bracketleftBigg\n1/summationdisplay 2n∆i> t/bracketrightBigg\n2t2\n≤expni/parenleftbigg\n−/summationtextn\ni=1/bardblBi−Ai/bardbl2∞/parenrightbigg\n.\nIn comparison to Hoeﬀding’s inequality, Azuma-Hoeﬀding aﬀords not only the use of\nnon-uniform boundedness, but additionally requires no independence of the random vari-\nables.\nProof.We start with a typical Chernoﬀ bound.\nIP/bracketleftBigg /bracketrightBigg/summationdisplay\n∆i> t≤IE/bracketleftBig\nes/summationtext∆i/bracketrightBig\ne−st= IE\ni/bracketleftBig\nIE/bracketleftBig\nes/summationtext∆i|Fn−1/bracketrightBig/bracketrightBig\ne−st\n16\nn−1 n−1 2 2= IE/bracketleftBig\nes/summationtext∆iIE[es∆n|Fn1]e−st− ≤IE[es/summationtext∆i·es(Bn−An)/8]e−st,\nwhere we have used the fact that the ∆/bracketrightBig\ni,i < n, are allFnmeasureable, and then applied\nHoeﬀding’s lemma on the inner expectation. Iteratively isolating each ∆ ilike this and\napplying Hoeﬀding’s lemma, we get\nIP/bracketleftBiggn/summationdisplay s2\n∆> t/bracketrightBigg\n≤exp/parenleftBigg/summationdisplay\n/bardblB−A/bardbl2/parenrightBigg\ne−sti i i8∞.\ni i=1\nOptimizing over sas usual then gives the result.\n2.2 Bounded Diﬀerences Inequality\nAlthough Azuma-Hoeﬀding is a powerful result, its full generality is often wasted and can\nbe cumbersome to apply to a given problem. Fortunately, there is a natural choice of the\n{Fi}iand{∆i}i, giving a similarly strong result which can be much easier to apply. Before\nwe get to this, we need one deﬁnition.\nDeﬁnition (Bounded Diﬀerences Condition): Letg:X →IR and constants cibe\ngiven. Then gis said to satisfy the bounded diﬀerences condition (with constants ci) if\nsup|g(x ,...,x )−g(x ,...,x′1 n 1 i,...,x n)| ≤ci\nx′1,...,xn,xi\nfor every i.\nIntuitively, gsatisﬁes the bounded diﬀerences condition if changing only one coordinate\nofgat a time cannot make the value of gdeviate too far. It should not be too surprising\nthat these types of functions thus concentrate somewhat strongly around their average, and\nthis intuition is made precise by the following theorem.\nTheorem (Bounded Diﬀerences Inequality): Ifg:X →IR satisﬁes the bounded\ndiﬀerences condition, then\n2t2\nIP[|g(X1,...,X n)−IE[g(X1,...,X n)|> t]≤2exp/parenleftbigg\n−/summationtext\nic2\ni/parenrightbigg\n.\nProof.Let{Fi}ibe given by Fi=σ(X1,...,X i), and deﬁne the martingale diﬀerences\n{∆i}iby\n∆i= IE[g(X1,...,X n)|Fi]−IE[g(X1,...,X n)|Fi−1].\nThen\nIP/bracketleftBigg\n|/summationdisplay\n∆i|> t/bracketrightBigg\n= IP/vextendsingle\ng(X1,...,X n)−IE[g(X1,...,X n)\ni/vextendsingle\n> t ,\nexactly the quantity we want to bou/bracketleftbig/vextendsingle\nnd. Now, note that/vextendsingle/bracketrightbig\n∆i≤IE/bracketleftbigg\nsupg(X1,...,x i,...,X n)|Fi−IE[g(X1,...,X n)|Fi−1]\nxi/bracketrightbigg\n17\n= IE/bracketleftbigg\nsupg(X1,...,x i,...,X n)−g(X1,...,X n)|Fi−1\nxi/bracketrightbigg\n=:Bi.\nSimilarly,\n∆i≥IE/bracketleftbigg\ninfg(X1,...,x i,...,X n)−g(X1,...,X n)|Fi−1=:Ai.\nxi/bracketrightbigg\nAt this point, our assumption on gimplies that /bardblBi−Ai/bardbl∞≤cifor every i, and since\nAi≤∆i≤BiwithAi,Bi∈ Fi−1, an application of Azuma-Hoeﬀding gives the result.\n2.3 Bernstein’s Inequality\nHoeﬀding’s inequality is certainly a powerful concentration inequality for how little it as-\nsumes about the random variables. However, one of the major limitations of Hoeﬀding is\njust this: Since it only assumes boundedness of the random variables, it is completely obliv-\nious to their actual variances. When the random variables in question have some known\nvariance, an ideal concentration inequality should capture the idea that variance controls\nconcentration to some degree. Bernstein’s inequality does exactly this.\nTheorem (Bernstein’s Inequality): LetX1,...,X nbe independent, centered ran-\ndom variables with |X| ≤cfor every i, and write σ2=n−1i iVar(Xi) for the average\nvariance. Then/summationtext\nIP/bracketleftBigg\n1/summationdisplay nt2\nXi> t/bracketrightBigg\n≤exp/parenleftBigg\n−n 2σ2+2tci 3/parenrightBigg\n.\nHere, one should think of tas being ﬁxed and relatively small compared to n, so that\nstrength of the inequality indeed depends mostly on nand 1/σ2.\nProof.The idea of the proof is to do a Chernoﬀ bound as usual, but to ﬁrst use our\nassumptions on the variance to obtain a slightly better bound on the moment generating\nfunctions. To this end, we expand\n∞(sk ∞X) skck−2iIE[esXi] = 1+IE[ sXi]+IE/bracketleftBigg /bracketrightBigg/summationdisplay\n≤1+Var(Xi)/summationdisplay\n,k! k!k=2 k=2\nwhere we have used IE[ Xk\ni]≤IE[X2\ni|Xi|k−2]≤Var(Xk−2i)c. Rewriting the sum as an\nexponential, we get\nesc\nsXi2 −sc−1IE[e]≤sVar(Xi)g(s), g(s) := .c2s2\nThe Chernoﬀ bound now gives\nIP/bracketleftBigg\n1/summationdisplay\nXi> t/bracketrightBigg\n≤exp/parenleftBigg\ninf[s2(/summationdisplay\nVar(Xi))g(s)−nst]/parenrightBigg\n= exp/parenleftbigg\nn·inf[s2σ2g(s)−st],n s>0 s>0i i/parenrightbigg\nand optimizing this over s(a fun calculus exercise) gives exactly the desired result.\n18\n3. NOISE CONDITIONS AND FAST RATES\nˆ To measure the eﬀectiveness of the estimator h, we would like to obtain an upper bound\nˆ ˆ on the excess risk E(h) =R(h)−R(h∗). It should be clear, however, that this must depend\nsigniﬁcantly on the amount of noise that we allow. In particular, if η(X) is identically equal\nˆ to 1/2, then we should not expect to be able to say anything meaningful about E(h) in\ngeneral. Understanding this trade-oﬀ between noise and rates will be the main subject of\nthis chapter.\n3.1 The Noiseless Case\nA natural (albeit somewhat na¨ ıve) case to examine is the completely noiseless case. Here,\nwe will have η(X)∈ {0,1}everywhere, Var( Y|X) = 0, and\nE(h) =R(h)−R(h∗) = IE[|2η(X)−1|1I(h(X) =h∗(X))] = IP[h(X) =h∗(X)].\nLet us now denote\n¯ ˆ Zi= 1I(h(Xi) =Yi)−1I(h(Xi) =Yi),\n¯ and write Zi=Zi−IE[Zi]. Then notice that we have\nˆ ¯ |Zi|= 1I(h(Xi) =h(Xi)),\nand\nVar(Zi)≤IE[Z2ˆ ¯i] = IP[h(Xi) =h(Xi)].\nˆ For any classiﬁer hj∈ H, we can similarly deﬁne Zi(hj) (by replacing hwithhjthrough-\nout). Then, to set up an application of Bernstein’s inequality, we can compute\nn1/summationdisplay¯ Var(Zi(hj))≤IP[hj(Xi) =h(Xi)] =:σ2\nnj.\ni=1\nAt this point, we will make a (fairly strong) assumption about our dictionary H, which\n¯ is thath∗∈ H, which further implies that h=h∗. Since the random variables Zicompare\n¯ ˆ toh, this will allow us to use them to bound E(h), which rather compares to h∗. Now,\n¯ applying Bernstein (with c= 2) to the {Zi(hj)}ifor every jgives\n/bracketleftBiggn1/bracketrightBigg/summationdisplay nt2δ¯ IP Zi(hj)> t≤exp/parenleftBigg\n− =2σ2\ni=1 j+4t3/parenrightBigg\n:,n M\nand a simple computation here shows that it is enough to take\n/radicalBigg\n2σ2\njlog(M/δ)4t≥max ,log(M/δ)n 3n\n=:t0(j)\n¯ for this to hold. From here, we may use the assumption h=h∗to conclude that\nˆ ˆ IP/bracketleftBig\nE(h)> t0(ˆj)/bracketrightBig\n≤δ, hˆ=h.j/ne}ationslash /ne}ationslash\n/ne}ationslash /ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n19\nˆ However, we also know that σ2\nˆ≤ E(h), which implies thatj\n/radicalBigg\nˆ2E(h)log(M/δ) 4ˆE(h)≤max ,log(M/δ)n 3n\n\nˆ with probability 1 −δ, and solving for E(h) gives the improved rate\nlog(M/δ)ˆE(h)≤2 .n\n20\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 4\nScribe:Cheng Mao Sep. 21, 2015\nIn this lecture, we continue to discuss the eﬀect of noise on the rate of the excess risk\nˆ ˆ ˆ E(h) =R(h)−R(h∗) wherehis the empirical risk minimizer. In the binary classiﬁcation\nmodel, noise roughly means how close the regression function ηis from1\n2. In particular, if\nη=1then we observe only noise, and if η∈ {0,1}we are in the noiseless case which has2\nbeen studied last time. Especially, we achieved the fast ratelogMin the noiseless case byn¯ assuming h∗∈ Hwhich implies that h=h∗. This assumption was essential for the proof\nand we will see why it is necessary again in the following section.\n3.2 Noise conditions\nThe noiseless assumption is rather unrealistic, so it is natural to ask what the rate of excess\nrisk is when the noise is present but can be controlled. Instead of the condition η∈ {0,1},\nwe can control the noise by assuming that ηis uniformly bounded away from1\n2, which is\nthe motivation of the following deﬁnition.\nDeﬁnition (Massart’s noise condition): The noise in binary classiﬁcation is said\nto satisfy Massart’s condition with constant γ∈(0,1\n2] if|η(X)−1| ≥γalmost surely.2\nOnce uniform boundedness is assumed, the fast rate simply follows from last proof with\nappropriate modiﬁcation of constants.\nˆ ˆ ˆ Theorem: LetcE(h) denote the excess risk of the empirical risk minimizer h=herm.\nIf Massart’s noise condition is satisﬁed with constant γ, then\nlog(M/δ)ˆE(h)≤γn\nwith probability at least 1 −δ. (In particular γ=1gives exactly the noiseless case.)2\nProof. ¯ ¯ DeﬁneZi(h) = 1I(h(Xi) =Yi)−1I(h(Xi) =Yi). By the assumption h=h∗and the\nˆ ˆ deﬁnition of h=herm,\nˆ ˆ ¯E(h) =R(h)−R(h)\nˆˆˆ¯ˆ¯ˆˆ ¯ ˆ =Rn(h)−Rn(h)+Rn(h)−Rn(h)−R(h)−R(h) (3.1)\nn1ˆ ≤ )/parenleftbig /parenrightbig\n/summationdisplay/parenleftbigˆ Zi(h)−IE[Zi(h]/parenrightbig\n. (3.2)ni=1\nHence it suﬃces to bound the deviation of/summationtext\niZifrom its expectation. To this end, we\nhope to apply Bernstein’s inequality. Since\nVar[Zi(h)]≤IE[Z2 ¯i(h) ] = IP[h(Xi) =h(Xi)],/ne}ationslash /ne}ationslash\n/ne}ationslash\n21\nwe have that for any 1 ≤j≤M,\nn1/summationdisplay¯ Var[Zi(hj)]≤IP[hj(X) =h(X)] =:σ2\nnj.\ni=1\nBernstein’s inequality implies that\nn/bracketleftbig1/summationdisplay /bracketrightbig /parenleftbig nt2\nIP ( Zi(hj)−IE[Zi(hj)])> t≤exp−n 2σ2\ni=1 j+2\n3t/parenrightbigδ=:.M\nApplying a union bound over 1 ≤j≤Mand taking\n2σ2\njlog(M/δ)2log(M/δ)t=t0(j) := max/radicalBigg\n/parenleftbig\n,n 3n/parenrightbig\n,\nwe get that\nn1/summationdisplay\n(Zi(hj)−IE[Zi(hj)])≤t0(j) (3.3)ni=1\nfor all 1≤j≤Mwith probability at least 1 −δ.\nˆ Suppose h=hˆ. It follows from (3.2) and (3.3) that with probability at least 1 −δ,j\nˆE(h)≤tˆ0(j).\n(Note that so far the proof is exactly the same as the noiseless case.) Since |η(X)−1\n2| ≥γ\n¯ a.s. and h=h∗,\nˆ ˆ ¯ E(h) = IE[|2η(X)−1|1I(h(X) =h∗(X))]≥2γIP[hˆ(X) =h(X)] = 2γσ2\nˆ.j j\nTherefore,/radicalBigg\nˆE(h)log(M/δ) 2log(M/δ)ˆE(h)≤max , , (3.4)γn 3n\nso we conclude that with probabilit/parenleftbig\ny at least 1 −δ,/parenrightbig\nlog(M/δ)ˆE(h)≤ .γn\n¯ The assumption that h=h∗was used twice in the proof. First it enables us to ignore\nthe approximation error and only study the stochastic error. More importantly, it makes\nthe excess risk appear on the right-hand side of (3.4) so that we can rearrange the excess\nrisk to get the fast rate.\nMassart’s noise condition is still somewhat strong because it assumes uniform bounded-\nness ofηfrom1\n2. Instead, we can allow ηto be close to1\n2but only with small probability,\nand this is the content of next deﬁnition./ne}ationslash\n/ne}ationslash /ne}ationslash\n22\nDeﬁnition (Tsybakov’s noise condition or Mammen-Tsybakov noise condi-\ntion):The noise in binary classiﬁcation is said to satisfy Tsybakov’s condition if there\nexistsα∈(0,1),C10>0 andt0∈(0,2] such that\n1 αIP[|η(X)− | ≤t]≤C10t−α\n2\nfor allt∈[0,t0].\nαIn particular, as α→1,t1−α→0\nα, so this recovers Massart’s condition with γ=t0and\nwe have the fast rate. As α→0,t1−α→1, so the condition is void and we have the slow\nrate. In between, it is natural to expect fast rate (meaning faster than slow rate) whose\norder depends on α. We will see that this is indeed the case.\nLemma: Under Tsybakov’s noise condition with constants α,C0andt0, we have\nIP[h(X) =h∗(X)]≤CE(h)α\nfor any classiﬁer hwhereC=C(α,C0,t0) is a constant.\nProof.We have\nE(h) = IE[|2η(X)−1|1I(h(X) =h∗(X))]\n1≥IE[|2η(X)−1|1I(|η(X)− |> t)1I(h(X) =h∗(X))]2\n1≥2tIP[|η(X)− |> t,h(X) =h∗(X)]2\n1≥2tIP[h(X) =h∗(X)]−2tIP[|η(X)− | ≤t]2\n1≥2tIP[h(X) =h∗(X)]−2C0t1−α\n1where Tsybakov’s condition was used in the last step. Take t=cIP[h(X) =h∗(X)]−α\nαfor\nsome positive c=c(α,C0,t0) to be chosen later. We assume that c≤t0to guarantee that\nt∈[0,t0]. Sinceα∈(0,1),\nE(h)≥2cIP[h(X) =h∗(X)]1/α1−2C c1−αIP[h(X) =h∗10 (X)]/α\n≥cIP[h(X) =h∗(X)]1/α\nby selecting csuﬃciently small depending on αandC0. Therefore\n1IP[h(X) =h∗(X)]≤ E(h)α\ncα\nand choosing C=C(α,C0,t0) :=c−αcompletes the proof.\nHaving established the key lemma, we are ready to prove the promised fast rate under\nTsybakov’s noise condition./ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash /ne}ationslash\n/ne}ationslash\n/ne}ationslash\n23\nTheorem: If Tsybakov’s noise condition is satisﬁed with constant α,C0andt0, then\nthere exists a constant C=C(α,C0,t0) such that\nl )ˆEh)≤C/parenleftbigog(M/δ(1\nn/parenrightbig\n2−α\nwith probability at least 1 −δ.\nThis rate of excess risk parametrized by αis indeed an interpolation of the slow ( α→0)\nˆ and the fast rate ( α→1). Futhermore, note that the empirical risk minimizer hdoes not\ndepend on the parameter αat all! It automatically adjusts to the noise level, which is a\nvery nice feature of the empirical risk minimizer.\nProof.The majority of last proof remains valid and we will explain the diﬀerence. After\nestablishing that\nˆE(h)≤t0(ˆj),\nwe note that the lemma gives\nˆσ2 ¯ ˆˆ= IP[h(X)/ne}ationslash=h(X)]≤CE(h)α.j\nIt follows\nthat /radicalBigg\nˆ /parenleftbig2CE(h)αlog(M/δ) 2log(M/δ)ˆE(h)≤max ,n 3n/parenrightbig\n/parenleftBig2ClogM2ˆ1\nE(h)≤max/parenleftbigδ/parenrightbig\n2−αlog(M/δ),/parenrightBig\n.n 3nand thus\n4. VAPNIK-CHERVONENKIS (VC) THEORY\nThe upper bounds proved so far are meaningful only for a ﬁnite dictionary H, because if\nM=|H|is inﬁnite all of the bounds we have will simply be inﬁnity. To extend previous\nresults to the inﬁnite case, we essentially need the condition that only a ﬁnite number of\nelements in an inﬁnite dictionary Hreally matter. This is the objective of the Vapnik-\nChervonenkis (VC) theory which was developed in 1971.\n4.1 Empirical measure\nRecall from previous proofs (see (3.1) for example) that the key quantity we need to control\nis\nˆ2sup/parenleftbig\nRn(h)−R(h).\nh∈H\nInstead of the union bound which would not work in th/parenrightbig\ne inﬁnite case, we seek some bound\nthat potentially depends on nand the complexity of the set H. One approach is to consider\nsome metric structure on Hand hope that if two elements in Hare close, then the quantity\nevaluated at these two elements are also close. On the other hand, the VC theory is more\ncombinatorial and does not involve any metric space structure as we will see.\n24\nBy deﬁnition\nn1ˆRn(h)−R(h) =/summationdisplay/parenleftbig\n1I(h(Xi) =Yi)−IE[1I(h(Xi) =Yi)]ni=1/parenrightbig\n.\nLetZ= (X,Y) andZi= (Xi,Yi), and let Adenote the class of measurable sets in the\nsample space X ×{0,1}. For a classiﬁer h, deﬁneAh∈ Aby\n{Zi∈Ah}={h(Xi) =Yi}.\nMoreover, deﬁne measures µnandµonAby\nn1µn(A) =/summationdisplay\n1I(Zi∈A) and µ(A) = IP[Zi∈A]ni=1\nforA∈ A. With this notation, the slow rate we proved is just\nlog(2|A|/δ)ˆsupRn(h)−R(h) = sup|µn(A)−µ(A)| ≤\nh∈H A∈A/radicalbigg\n.2n\nSince this is not accessible in the inﬁnite case, we hope to use one of the concentration\ninequalities to give an upperbound. Note that µn(A) is a sum of random variables that may\nnot be independent, so the only tool we can use now is the bounded diﬀerence inequality.\nIf we change the value of only one ziin the function\nz1,...,zn/mapsto→sup|µn(A)−µ(A)|,\nA∈A\nthe value of the function will diﬀer by at most 1 /n. Hence it satisﬁes the boundeddiﬀerence\nassumption with ci= 1/nfor all 1≤i≤n. Applying the bounded diﬀerence inequality, we\nget that\n/vextendsinglelog(2/δ) /vextendsinglesup|µn(A)−µ(A)|−IE[sup|µn(A)−µ(A)|]≤\nA∈A A∈A/radicalbigg\n2n\nwith probability/vextendsingle\nat least 1 −δ. Note that this already preclu/vextendsingle/vextendsingle\n/vextendsingle\ndes any fast rate (faster than\nn−1/2). Toachieve fast rate, weneedTalagrand inequality andlocalization techniques which\nare beyond the scope of this section.\nIt follows that with probability at least 1 −δ,\nlog(2/δ)sup|µn(A)−µ(A)| ≤IE[sup|µn(A)−µ(A)|]+\nA A∈A/radicalbigg\n.\nA∈ 2n\nWe will now focus on bounding the ﬁrst term on the right-hand side. To this end, we need\na technique called symmetrization, which is the subject of the next section.\n4.2 Symmetrization and Rademacher complexity\nSymmetrization is a frequently used technique in machine learning. Let D={Z1,...,Z n}\nbe the sample set. To employ symmetrization, we take another independent copy of the\nsample set D′={Z′\n1,...,Z′\nn}. This sample only exists for the proof, so it is sometimes\nreferred to as a ghost sample. Then we have\nn n1 1µ(A) = IP[Z∈A] = IE[/summationdisplay\n1I(Z′\ni∈A)] = IE[ 1I( Z′\ni∈A)|D] = IE[µ′\nn nn(A)|D]\ni=1/summationdisplay\ni=1/ne}ationslash /ne}ationslash\n/ne}ationslash\n25\nnwhereµ′\nn:=1/summationtext\ni=11I(Z′\ni∈A). Thus by Jensen’s inequality,n\nIE[sup|µn(A)−µ(A)|] = IE/bracketleftbig\nsup/vextendsingle/vextendsingle\nµn(A)−IE[µ′\nn(A)|D]\nA∈A A∈A\n≤IE sup IE[ |µn(A)−µ′\nn(A)||D/vextendsingle/vextendsingle\n]/bracketrightbig\n≤/bracketleftbig\nA∈A\nIE/bracketleftbig\nsup|µ′n(A)−µn(A)|/bracketrightbig\nA∈A\nn1/bracketrightbig\n= IE/bracketleftbig\nsup/vextendsingle/summationdisplay/parenleftbig\n1I(Z′i∈A)−1I(Z\nA∈Ani∈A)\ni=1/parenrightbig/vextendsingle/bracketrightbig\n.\nSinceD′has the same distribution of D, by sy/vextendsingle\nmmetry 1I( Zi∈A)−1I(Z′/vextendsingle\ni∈A) has the same\ndistribution as σi/parenleftbig\n1I(Zi∈A)−1I(Z′\ni∈A)/parenrightbig\nwhereσ1,...,σ nare i.i.d. Rad(1\n2), i.e.\n1IP[σi= 1] = IP[ σi=−1] =,2\nandσi’s are taken to be independent of both samples. Therefore,\nn\nIE[sup|µn(A)−µ(A)|]≤IE\nAA/bracketleftbig\nsup′\n∈ A∈A/vextendsingle/vextendsingle1/summationdisplay\nσi/parenleftbig\n1I(Zi∈A)−1I(Zni∈A)\ni=1\nn/parenrightbig/vextendsingle/bracketrightbig\n≤2IE/bracketleftbig1/vextendsingle\nsup/vextendsingle/summationdisplay\nσi1I(Zi∈A).\nA∈Ani=1/vextendsingle/bracketrightbig\n(4.5)\nUsingsymmetrization we have boundedIE[sup/vextendsingle\nA∈A|µn(A)−µ(A)|/vextendsingle\n] by amuch nicer quantity.\nYet we still need an upper bound of the last quantity that depends only on the structure\nofAbut not on the random sample {Zi}. This is achieved by taking the supremum over\nallzi∈ X ×{0,1}=:Y.\nDeﬁnition: The Rademacher complexity of a family of sets Ain a space Yis deﬁned\nto be the quantity\nn\nRn(A) = sup sup/vextendsingle1IE/bracketleftbig /summationdisplay\nσi1I(zi∈A)\nz1,...,zn∈YA∈Ani=1/vextendsingle/bracketrightbig\n.\nThe Rademacher complexity of a set B⊂I/vextendsingle\nRnis deﬁned to b/vextendsingle\ne\nn1Rn(B) = IE/bracketleftbig\nsup\nb∈B/vextendsingle/vextendsingle\nn/summationdisplay\nσibi\ni=1/vextendsingle/vextendsingle/bracketrightbig\n.\nWe conclude from (4.5) and the deﬁnition that\nIE[sup|µn(A)−µ(A)|]≤2Rn(A).\nA∈A\nnIn the deﬁnition of Rademacher complexity of a set, the quantity1sni=1σibimeasure\nhow well a vector b∈Bcorrelates with a random sign pattern {σi}. The more complex\nBis, the better some vector in Bcan replicate a sign pattern. In/vextendsingle/vextendsingle\npa/summationtext\nrticular, i/vextendsingle/vextendsingle\nfBis the\nfull hypercube [ −1,1]n, thenRn(B) = 1. However, if B⊂[−1,1]ncontains only k-sparse\n26\nvectors, then Rn(B) =k/n. Hence Rn(B) is indeed a measurement of the complexity of\nthe setB.\nThe set of vectors to our interest in the deﬁnition of Rademacher complexity of Ais\nT(z) :={(1I(z1∈A),...,1I(zn∈A))T,A∈ A}.\nThus the key quantity here is the cardinality of T(z), i.e., the number of sign patterns these\nvectors can replicate as Aranges over A. Although the cardinality of Amay be inﬁnite,\nthe cardinality of T(z) is bounded by 2n.\n27\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture\nScribe:Vira Semenova andPhilippe Rigollet Sep. 23, 2015\nIn this lecture, we complete the analysis of the performance of the empirical risk mini-\nmizer under a constraint on the VC dimension of the family of classiﬁers. To that end, we\nwill see how to control Rademacher complexities using shatter coeﬃcients. Moreover, we\nwill see how the problem of controlling uniform deviations of the empirical measure µnfrom\nthe true measure µas done by Vapnik and Chervonenkis relates to our original classiﬁcation\nproblem.\n4.1 Shattering\nRecall from the previous lecture that we are interested in sets of the form\nT(z) :=/braceleftbig\n(1I(z1∈A),...,1I(zn∈A)),A∈ A, z= (z1,...,zn). (4.1)\nIn particular, the cardinality of T(z), i.e., the numbe/bracerightbig\nr of binary patterns these vectors\ncan replicate as Aranges over A, will be of critical importance, as it will arise when\ncontrolling the Rademacher complexity. Although the cardinality of Amay be inﬁnite, the\ncardinality of T(z) is always at most 2n. When it is of the size 2n, we say that Ashatters\nthe setz1,...,zn. Formally, we have the following deﬁnition.\nDeﬁnition: A collection of sets Ashatters the set of points {z1,z2,...,zn}\ncard{(1I(z1∈A),...,1I(zn∈A)),A∈ A}= 2n.\nThe sets of points {z1,z2,...,zn}that we are interested are realizations of the pairs Z1=\n(X1,Y1),...,Z n= (Xn,Yn) and may, in principle take any value over the sample space.\nTherefore, we deﬁne the shatter coeﬃcient to be the largest cardinality that we may obtain.\nDeﬁnition: Theshatter coeﬃcients of a class of sets Ais the sequence of numbers\n{SA(n)}n≥1, where for any n≥1,\nSA(n) = sup card (1I( z1A),...,1I(znA)),A\nz1,...,zn{ ∈ ∈ ∈ A}\nand the suprema are taken over the whole sample space.\nBydeﬁnition,the nthshattercoeﬃcient SA(n)isequalto2nifthereexistsaset {z1,z2,...,zn}\nthatAshatters. The largest of such sets is precisely the Vapnik-Chervonenkis or VC di-\nmension.\nDeﬁnition: The Vapnik-Chervonenkis dimension, or VC-dimension of\ndVCAis the largest\nintegerdsuch that SA(d) = 2 . We write ( A) =d.5\n28\nIfSA(n) = 2nfor all positive integers n, thenVC(A) :=∞\nIn words, Ashatters someset of points of cardinality dbut shatters noset of points of\ncardinality d+1. In particular, Aalso shatters no set of points of cardinality d′> dso that\nthe VC dimension is well deﬁned.\nInthesequel, wewillseethattheVCdimensionwillplaytherolesimilartoofcardinality,\nbut on an exponential scale. For interesting classes Asuch that card( A) =∞, we also may\nhaveVC(A)<∞. For example, assume that Ais the class of half-lines ,A={(−∞,a],a∈\nIR}∪ {[a,∞),a∈IR}, which is clearly inﬁnite. Then, we can clearly shatter a set of size\n2 but we for three points z1,z2,z3,∈IR, if for example z1< z2< z3, we cannot create the\npattern (0 ,1,0) (see Figure 4.1). Indeed, half lines can can only create patterns with zeros\nfollowed by ones or with ones followed by zeros but not an alternating pattern like (0 ,1,0).\n00\n10\n01\n11000\n100\n110\n111\n001\n011\n101\nFigure 1: If A={halﬂines}, then any set of size n= 2 is shattered because we can\ncreate all 2n= 4 0/1 patterns (left); if n= 3 the pattern (0 ,1,0) cannot be reconstructed:\nSA(3) = 7<23(right). Therefore, VC(A) = 2.\n4.2 The VC inequality\nWe have now introducedall the ingredients necessary tostate themain result of this section:\nthe VC inequality.\nTheorem (VC inequality): For any family of sets Awith VC dimension VC(A) =d,\nit holds /radicalbigg\n2dlog(2en/d)IE sup|µn(A)−µ(A)| ≤2\nA∈A n\nNotethatthisresultholdsevenif AisinﬁniteaslongasitsVCdimensionisﬁnite. Moreover,\nobserve that log( |A|) has been replaced by a term of order dlog 2en/d.\nTo prove the VC inequality, we proceed in three steps:/parenleftbig /parenrightbig\n29\n1. Symmetrization, to bound the quantity of interest by the Rademacher complexity:\nIE[sup|µn(A)−µ(A)|]≤2Rn( )\nA∈AA.\nWe have already done this step in the previous lecture.\n2. Control of the Rademacher complexity using shatter coeﬃcients. We are going to\nshow that\ngR(A)≤/radicaligg\n2lo\nn/parenleftbig\n2SA(n)\nn/parenrightbig\n3. We are going to need the Sauer-Shelah lemma to bound the shatter coeﬃcients by\nthe VC dimension. It will yield\nS(n)≤/parenleftigen/parenrightigd\n, d=VC A (dA).\nPut together, these three steps yield the VC inequality.\nStep 2: Control of the Rademacher complexity\nWe prove the following Lemma.\nLemma: For anyB⊂IRn, such that |B|<∞:, it holds\nn/bracketleftbig/vextendsingle1 2σ/vextendsingle )B/bracketrightbig (Rn( ) = IE max /vextendsingle/summationdisplay log 2B\nibi/vextendsingle≤max| |\nb∈Bn b∈Bi=1|b|2/radicalbig\nn\nwhere|·|2denotes the Euclidean norm.\nProof.Note that\n1Rn(B) = IEn/bracketleftbig\nmaxZb,\nb∈B|\nwhereZb=/summationtextn\ni=1σibi. In particular, since/vextendsingle/vextendsingle/bracketrightbig\n−|bi| ≤σi|bi| ≤ |bi|, a.s., Hoeﬀding’s lemma\nimplies that the moment generating function of Zbis controlled by\nn n\nIE/bracketleftbig\nexp(sZb)/bracketrightbig\n=/productdisplay\nIE\ni=1/bracketleftbig\nexp(sσibi)/bracketrightbig\n≤/productdisplay\nexp(s2b2\ni/2) = exp( s2b2\n2/2) (4.2)\ni=1| |\nNext, to control IE max b∈BZb|, we use the same technique as in Lecture 3, section 1.5.\n¯ To that end, deﬁne/bracketleftbig\nB=B∪/vextendsingle/vextendsingle\n{−B/bracketrightbig\n}and observe that for any s >0,\nIE/bracketleftbigg1max|Zb|/bracketrightbigg\n= IE/bracketleftbigg\nmaxZb/bracketrightbigg\n= logexp/parenleftbigg\nsIE/bracketleftbigg\nmaxZb/bracketrightbigg/parenrightbigg1≤logIE exp smaxZb,\nb∈B b∈B¯ s b¯∈B s/bracketleftbigg /parenleftbigg\nb¯∈B/parenrightbigg/bracketrightbigg\nwhere the last inequality follows from Jensen’s inequality. Now we bound the max by a\nsum to get/bracketleftbigg /bracketrightbigg1/summationdisplay log|¯B|s b2\nIE max|Zb| ≤log IE[exp( sZb)]≤ +| |2,\nb∈B s s2n\nb∈B¯\nwhere in the last inequality, we used (4.2). Optimizing over s >0 yields the desired\nresult.\n30\nWe apply this result to our problem by observing that\nRn(A) = sup (\n,Rn(T z))\nz1,... zn\nwhereT(z) is deﬁned in (4.1). In particular, since T(z)⊂ {0,1}, we have |b√|2≤n\nfor allb∈T(z). Moreover, by deﬁnition of the shatter coeﬃcients, if B=T(z), then\n|¯B| ≤2|T(z)| ≤2SA(n). Together with the above lemma, it yields the desired inequality:\n/radicalbigg\n2log(2SA(n))Rn(A)≤ .n\nStep 3: Sauer-Shelah Lemma\nWe need to use a lemma from combinatorics to relate the shatter coeﬃcients to the VC\ndimension. A priori, it is not clear from its deﬁnition that the VC dimension may be at\nall useful to get better bounds. Recall that steps 1 and 2 put together yield the following\nbound\n2log(2S(n))IE[sup n(A)\nA−µ(A) ]\nA∈|µ | ≤A2/radicalbigg\n(4.3)n\nIn particular, if SA(n) is exponential in n, the bound (4.3) is not informative, i.e., it does\nnot imply that the uniform deviations go to zero as the sample size ngoes to inﬁnity. The\nVC inequality suggest that this is not the case as soon as VC(A)<∞but it is not clear a\npriori. Indeed, it may be the case that\nVCSA(n) = 2nforn≤dandSA(n) = 2n−1 forn > d,\nwhich would imply that ( A) =d <∞but that the right-hand side in (4.3) is larger than\n2 for alln. It turns our that this can never be the case: if the VC dimension is ﬁnite, then\nthe shatter coeﬃcients are at most polynomial inn. This result is captured by the Sauer-\nShelah lemma, whose proof is omitted. The reading section of the course contains pointers\nto various proofs, speciﬁcally the one based on shiftingwhich is an important technique in\nenumerative combinatorics.\nLemma (Sauer-Shelah): IfVC(A) =d, then∀n≥1,\nd\nSA(n)≤/summationdisplay/parenleftbiggn en d\n.k/parenrightbigg\n≤dk=0/parenleftig /parenrightig\nTogether with (4.3), it clearly yields the VC inequality. By applying the bounded diﬀerence\ninequality, we also obtain the following VC inequality that holds with high probability. This\nis often the preferred from for this inequality in the literature.\nCorollary (VC inequality): For any family of sets Asuch that VC(A) =dand any\nδ∈(0,1), it holds with probability at least 1 −δ,\n/radicalbigg\n2dlog(2en/d)/radicalbigg\nlog(2/δ)supµnA)−µ(A)| ≤2 +\nA∈A|( .n 2n\n31\nNote that the logarithmic term log(2 en/d) is actually superﬂuous and can be replaced\nby a numerical constant using a more careful bounding technique. This is beyond the scope\nof this class and the interested reader should take a look at the recommending readings.\n4.3 Application to ERM\nThe VC inequality provides an upper bound for supA∈A|µn(A)−µ(A)|in terms of the VC\ndimension of the class of sets A. This result translates directly to our quantity of interest:\n2VC( )log2en)ˆsup|Rnh)−VC(A)log(2/δ(R(h) 2n/parenrightbig\n+\nh∈H≤/radicaligg\nA\n|/parenleftbig /radicalbigg\n(4.4)2n\nwhereA={Ah:h∈ H}andAh={(x,y)∈ X ×{0,1}:h(x) =y}. Unfortunately, the\nVC dimension of this class of subsets of X ×{0,1}is not very natural. Since, a classiﬁer h\nis a{0,1}valued function, it is more natural to consider the VC dimension of the family\nA=/braceleftbig\n{h= 1}:h∈ H/bracerightbig\n.\nDeﬁnition: LetHbe a collection of classiﬁers and deﬁne\nA¯={h= 1}:h∈ H\nWe deﬁne the VC d/braceleftbig\nimension VC( ) o/bracerightbig\n=/braceleftbig\nA:∃h∈ H,h(·) = 1I(· ∈A).\nH ¯ fHto be the VC dimension of/bracerightbig\nA.\n¯ ¯ It is not clear how VC(A) relates to the quantity VC(A), where A={Ah:h∈ H}and\nAh={(x,y)∈ X ×{0,1}:h(x) =y}that appears in the VC inequality. Fortunately, these\ntwo are actually equal as indicated in the following lemma.\nLemma: Deﬁne the two families for sets: = AX×h:h 2{0,1}where\n{ ¯A { ∈ H} ∈\nAh= (x,y)∈ X ×{0,1}:h(x) =y}andA=/braceleftbig\n{h= 1 :h 2X.\nS S ≥ VCA¯} ∈ H ∈\nThen,A¯(n) =A¯(n) for alln1. It implies ( ) = VC(A/bracerightbig\n).\nProof.Fixx= (x1,...,xn)∈ Xnandy= (y1,y2,...,yn)∈ {0,1}nand deﬁne\nT(x,y) ={(1I(h(x1) =y1),...,1I(h(xn) =yn)),h∈ H}\nand\n¯T(x) ={(1I(h(x1) = 1),...,1I(h(xn) = 1)),h∈ H}\nTo that end, ﬁx v∈ {0,1}and recall the XOR (exclusive OR) boolean function from {0,1}\nto{0,1}deﬁned by u⊕v= 1I(u=v). It is clearly1a bijection since ( u⊕v)⊕v=u.\n1One way to see that is to introduce the “spinned” variables u˜ = 2u−1 andv˜ = 2v−1 that live in\n/tildewider {−1,1}. Thenu⊕v=u˜·v˜, and the claim follows by observing that ( u˜·v˜)·v˜ =u˜. Another way is to simply\nwrite a truth table./ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash /ne}ationslash\n/ne}ationslash\n32\nWhen applying XOR componentwise, we have\n\n1I(h(x1) =y1)\n1I(h(x1= 1) y\n ..\n) 1 ..\n .\n1I(h(xi) .\n\n=yi) = 1I(h(xi) = 1)\n..\n.⊕\n.\n .\n.\n1I(h(xn) =yn)\n \n \n1I(h(xn) = 1)...\nyi\n ...\nyn \n¯\nSince XOR is a bijection, we must have card[ T(x,y)] = card[ T(x)]. The lemma follows\nby taking the supremum on each side of the equality.\nIt yields the following corollary to the VC inequality.\nCorollary: LetHbe a family of classiﬁers with VC dimension d. Then the empirical\nˆ risk classiﬁer hermoverHsatisﬁes\nerm/radicalbigg\n2dlog(2en/d)ˆR(h)≤minR(h)+4 +\nh∈H n/radicalbigg\nlog(2/δ)\n2n\nwith probability 1 −δ.\nProof.Recall from Lecture 3 that\nˆR(herm)−min ) ≤ ˆ R(h2sup\nh∈H h∈H\nThe proof follows directly by applyi/vextendsingle/vextendsingleRn(h)−R(h)/vextendsingle\nng (4.4) and the above lemma./vextendsingle/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n33\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 6\nScribe:Ali Makhdoumi Sep. 28, 2015\n5. LEARNING WITH A GENERAL LOSS FUNCTION\nIn the previous lectures we have focused on binary losses for the classiﬁcation problem and\ndeveloped VCtheory forit. Inparticular, theriskforaclassiﬁcation function h:X → {0,1}\nand binary loss function the risk was\nR(h) = IP(h(X) =Y) = IE[1I( h(X) =Y)].\nIn this lecture we will consider a general loss function and a general regression model where\nYis not necessarily a binary variable. For the binary classiﬁcation problem, we then used\nthe followings:\n•Hoeﬀding’s inequality: it requires boundedness of the loss functions.\n•Bounded diﬀerence inequality: again it requires boundedness of the loss functions.\n•VC theory: it requires binary nature of the loss function.\nLimitations of the VC theory:\n•Hard to ﬁnd the optimal classiﬁcation: the empirical risk minimization optimization,\ni.e.,\nn1min\nhn/summationdisplay\n1I(h(Xi) =Yi)\ni=1\nis a diﬃcult optimization. Even though it is a hard optimization, there are some\nalgorithms that try to optimize this function such as Perceptron and Adaboost.\n•This is not suited for regression. We indeed know that classiﬁcation problem is a\nsubset of Regression problem as in regression the goal is to ﬁnd IE[ Y|X] for a general\nY(not necessarily binary).\nIn this section, we assume that Y∈[−1,1] (this is not a limiting assumption as all the\nresults can bederived for any bounded Y) and we have a regression problem where ( X,Y)∈\nX ×[−1,1]. Most of the results that we preset here are the analogous to the results we had\nin binary classiﬁcation. This would be a good place to review those materials and we will\nrefer to the techniques we have used in classiﬁcation when needed.\n5.1 Empirical Risk Minimization\n5.1.1 Notations\nLoss function: In binary classiﬁcation the loss function was 1I( h(X) =Y). Here, we\nreplace this loss function by ℓ(Y,f(X)) which we assume is symmetric, where f∈ F,\nf:X →[−1,1] is the regression functions. Examples of loss function include/\\e}atio\\slash /\\e}atio\\slash\n/\\e}atio\\slash\n/\\e}atio\\slash\n34\n•ℓ(a,b) = 1I( a=b) ( this is the classiﬁcation loss function).\n•ℓ(a,b) =|a−b|.\n•ℓ(a,b) = (a−b)2.\n•ℓ(a,b) =|a−b|p,p≥1.\nWe further assume that 0 ≤ℓ(a,b)≤1.\nRisk: risk is the expectation of the loss function, i.e.,\nR(f) = IEX,Y[ℓ(Y,f(X))],\nwhere the joint distribution is typically unknown and it must be learned from data.\nData: we observe a sequence ( X1,Y1),...,(Xn,Yn) ofnindependent draws from a joint\ndistribution PX,Y, where ( X,Y)∈ X ×[−1,1]. We denote the data points by Dn=\n{(X1,Y1),...,(Xn,Yn)}.\nEmpirical Risk : the empirical risk is deﬁned as\nn1ˆRn(f) =n/summationdisplay\nℓ(Yi,f(Xi)),\ni=1\nˆ ˆ and the empirical risk minimizer denoted by ferm(orf) is deﬁned as the minimizer of\nempirical risk, i.e.,\nˆargminRn(f).\nf∈F\nˆ In order to control the risk of fwe shall compare its performance with the following oracle:\n¯f∈argminR(f).\nf∈F\nNote that this is an oracle as in order to ﬁnd it one need to have access to PXYand then\nˆ optimize R(f) (we only observe the data Dn). Since fis the minimizer of the empirical\nˆˆˆ¯ risk minimizer, we have that Rn(f)≤Rn(f), which leads to\nˆR(f)≤ˆR(f)−ˆˆˆˆˆ¯ˆ¯ ¯ ¯ Rn(f)+Rn(f)−Rn(f)+Rn(f)−R(f)+R(f)\n≤¯ ˆ ˆˆˆ¯ ¯ ¯ ˆ R(f)+R(f)−Rn(f)+Rn(f)−R(f)≤R(f)+2sup\nf∈F|Rn(f)−R(f)|.\nTherefore, the quantity of interest that we need to bound is\nsup|ˆRn(f)−R(f)\nf∈F|.\nMoreover, from theboundeddiﬀerence inequality, we know that sincethe loss function ℓ(·,·)\nˆ is bounded by 1, supf∈F|Rn(f)−R(f)|has the bounded diﬀerence property with ci=1\nn\nfori= 1,...,n, and the bounded diﬀerence inequality establishes\nP/bracketleftigg\n2t2\nsup|ˆ ˆ Rn(f)−R(f)|−IE/bracketleftigg\nsup|Rn(f)−R(f)\nf∈F|/bracketrightigg\n≥t\nf∈F/bracketrightigg\n−≤exp/parenleftbigg\nx2\ni i/parenrightbigg\n= e pc−2nt2,\nwhich in turn yields/parenleftbig /parenrightbig/summationtext\nlog(1/delta)|ˆsupRn(f)−R(f)| ≤I|ˆE/bracketleftigg\nsupRn(f)−R(f) δ\nf∈F f|/bracketrightigg\n+\n∈/radicalbigg\n,w.p. 1\nF 2n−.\nˆ As a result we only need to bound the expectation IE[supf∈F|Rn(f)−R(f)|]./\\e}atio\\slash\n35\n5.1.2 Symmetrization and Rademacher Complexity\nSimilar to the binary loss case we ﬁrst use symmetrization technique and then intro-\nduce Rademacher random variables. Let Dn={(X1,Y1),...(Xn,Yn)}be the sample set\nand deﬁne an independent sample (ghost sample) with the same distribution denoted by\nD′\nn={(X′\n1,Y′\n1),...(X′\nn,Y′\nn)}( for each i, (X′\ni,Y′\ni) is independent from Dnwith the same\ndistribution as of ( Xi,Yi)). Also, let σi∈ {−1,+1}be i.i.d. Rad(1) random variables2\nindependent of DnandD′\nn. We have\nIE/bracketleftiggn1sup|ℓ i\nf∈Fn/summationdisplay\n(Yi,f(X))\ni=1−IE[ℓ(Yi,f(Xi))]|/bracketrightigg\nn n1 1= IE/bracketleftigg\nsup ℓ(Yi,f(X ℓ(Y′i)) IEi,f(X′\ni))Dn\nf∈F|n/summationdisplay\ni=1−/bracketleftigg\nn/summationdisplay\ni=1|/bracketrightigg\n|/bracketrightigg\nn n1 1= IE/bracketleftigg\nsup|IE/bracketleftigg/summationdisplay\nℓ(Yi,f(X′i)) ℓ(Yi,f(X′\ni))Dn\nf∈Fni=1−n/summationdisplay\ni=1|/bracketrightigg\n|/bracketrightigg\nn(a)\n≤IE/bracketleftiggn1 1supIE/bracketleftigg\n|/summationdisplay\nℓ(Yi,f(X′i))−/summationdisplay\nℓ(Y ,f(X′\ni i))| |Dn\nf∈Fn ni=1 i=1/bracketrightigg/bracketrightigg\n≤IE/bracketleftiggn n1sup|/summationdisplay 1ℓ(Yi,f(Xi)) ℓ(Y′\ni,f(X′\nf∈Fn ni))\ni=1−/summationdisplay\ni=1|/bracketrightigg\n(b) 1= IE/bracketleftiggn\nsup|/summationdisplay\nσi/parenleftbig\nℓ(Yi,f(Xi))−ℓ(Y′X\nfFni,f(′\ni))\n∈i=1/parenrightbig\n|/bracketrightigg\nn(c) 1≤2IE/bracketleftigg\nsup\nf∈F|n/summationdisplay\nσiℓ(Yi,f(Xi))\ni=1|/bracketrightigg\nn\n≤2supIE/bracketleftigg\n1sup|/summationdisplay\nσiℓ(yi,f(xi))\nDnf∈Fni=1|/bracketrightigg\n.\nwhere (a) follows from Jensen’s inequality with convex function f(x) =x, (b) follows from\nthe fact that ( X ,Y) and (X′ ′| |\ni i i,Yi) has the same distributions, and (c) follows from triangle\ninequality.\nRademacher complexity: of a class Fof functions for a given loss function ℓ(·,·) and\nsamplesDnis deﬁned as\nn1Rn(ℓ◦F) = supIE/bracketleftigg\nsup|/summationdisplay\nσiℓ(yi,f(xi)).\nDnf∈Fni=1|/bracketrightigg\nTherefore, we have\nIE/bracketleftiggn1sup|/summationdisplay\nℓ(Yi,f(Xi))\nf∈Fni=1−IE[ℓ(Yi,f(Xi))]|/bracketrightigg\n≤2Rn(ℓ◦F)\nand we only require to bound the Rademacher complexity.\n5.1.3 Finite Class of functions\nSuppose that the class of functions Fis ﬁnite. We have the following bound.\n36\nTheorem: Assume that Fis ﬁnite and that ℓtakes values in [0 ,1]. We have\n/radicalbigg\n2log(2Rn(ℓ◦F)|F|)≤ .n\nProof.From the previous lecture, for B⊆nR, we have that\nn1 2log(2B)Rn(B) = IE/bracketleftigg\nmax\nb∈B|n/summationdisplay\nσibi\ni=1|/bracketrightigg\n| |≤max\nb∈B|b|2/radicalbig\n.n\nHere, we have\nℓ(y(x 1,f1))\n.B= .,.\nℓ(yn,f(xn)f∈ F\n\n.\n)\nSinceℓtakes values in [0 ,1], this\nim\npliesB\n⊆ {b:|b|2√≤\nn}. Plugging this bound in the\nprevious inequality completes the proof.\n5.2 The General Case\nRecall that for the classiﬁcation problem, we had F ⊂ {0,1}X. We have seen that the\ncardinality of the set {(f(x1),...,f(xn)),f\nˆerm∈ F}plays an important role in bounding the\nrisk off(this is not exactly what we used but the XOR argument of the previous lecture\nallows us to show that the cardinality of this set is the same as the cardinality of the set\nthat interests us). In this lecture, this set might be uncountable. Therefore, we need to\nintroduce a metric on this set so that we can treat the close points in the same manner. To\nthis end we will deﬁne covering numbers (which basically plays the role of VC dimension\nin the classiﬁcation).\n5.2.1 Covering Numbers\nDeﬁnition: Given a set of functions Fand a pseudo metric donF((F,d) is a metric\nspace) and ε >0. Anε-netof (F,d) is a set Vsuch that for any f∈ F, there exists\ng∈Vsuch that d(f,g)≤ε. Moreover, the covering numbers of (F,d) are deﬁned by\nN(F,d,ε) = inf{|V|:Vis anε-net}.\nFor instance, for the Fshown in the Figure 5.2.1 the set of points {1,2,3,4,5,6}is a\ncovering. However, the covering number is 5 as point 6 can be removed from Vand the\nresulting points are still a covering.\nDeﬁnition: Givenx= (x1,...,x n), theconditional Rademacher average of a class of\n37\nfunctions Fis deﬁned as\nRˆx\nn= IE/bracketleftiggn1sup σ\nf∈F/vextendsingle/vextendsingle\nn/summationdisplay\nif(xi)\ni=1/bracketrightigg\n/vextendsingle/vextendsingle.\nNote that in what follows we consider a general class of functions F. However, for\napplying the results in order to bound empirical risk minimization, we take xito be (xi,yi)\nandFto beℓ◦F. We deﬁne the empirical l1distance as\nn\ndx 1\n1(f,g) =n/summationdisplay\ni\n=1|f(x)\ni−g(xi)|.\nTheorem: If 0≤f≤1 for all f∈ F, then for any x= (x1,...,x n), we have\nˆRx\nn(F)≤inf\nε≥0/radicalbigg\nx /braceleftbig2log(2N(F,dε+1,ε))\nn/bracerightbig\n.\nProof.Fixx= (x1,...,x n) andε >0. LetVbe a minimal ε-net of ( F,dx\n1). Thus,\nby deﬁnition we have that |V|=N(F,dx\n1,ε). For any f∈ F, deﬁnef◦∈Vsuch that6\n5 432 1 F\nǫ\n38\ndx\n1(f,f◦)≤ε. We have that\nn1Rx\nn(F) = IE/bracketleftigg\nsup σif(xi)\nf∈F|n/summationdisplay\ni=1|/bracketrightigg\n≤IE/bracketleftiggn n1 1sup|/summationdisplay\nσi(f(xi)f◦(xi)) +IE sup σif◦(xi)\nf∈Fn f∈Fni=1− |/bracketrightigg /bracketleftigg\n|/summationdisplay\ni=1|/bracketrightigg\n≤ε+IE/bracketleftiggn1max σif(xi)\nf∈V|n/summationdisplay\ni=1|/bracketrightigg\n/radicalbigg\n2log(2≤ε+|V|)\nn/radicalbigg\n2log(2N(=ε+F,dx\n1,ε)).n\nSince the previous bound holds for any ε, we can take the inﬁmum over all ε≥0 to obtain\nx/radicalbigg\n/braceleftbig2log(2N(F,dx\nRn(F)≤infε+1,ε))\nε≥0 n/bracerightbig\n.\nThe previous bound clearly establishes a trade-oﬀ because as εdecreases N(F,dx\n1,ε) in-\ncreases.\n5.2.2 Computing Covering Numbers\nAs a warm-up, we will compute the covering number of the ℓ2ball of radius 1 indRdenoted\nbyB2. We will show that the covering is at most (3\nε)d. There are several techniques to\nprove this result: one is based on a probabilistic method argument and one is based on\ngreedily ﬁnding an ε-net. We will describe the later approach here. We select points in V\none after another so that at step k, we have uk∈B2\\∪k\nj=1B(uj,ε). We will continue this\nprocedure until we run out of points. Let it be step N. This means that V={u1,...,u N}\nis anε-net. We claim that the balls B(ui,ε) andB(uj,ε) for any i,j12 2∈ {,...,N}are\ndisjoint. The reason is that if v∈B(ui,ε)∩B(uj,ε), then we would have2 2\nε ε/bardblui−uj/bardbl2≤ /bardblui−v/bardbl2+/bardblv−uj/bardbl2≤+ =ε,2 2\nwhich contradicts the way we have chosen the points. On the other hand, we have that\n∪N\nj=1B(uj,ε)⊆(1+ε)B2. Comparing the volume of these two sets leads to2 2\nε ε|V|( )dvol(B2)≤(1+ )dvol(B2),2 2\nwherevol(B2) denotes the volume of the unit Euclidean ball in ddimensions. It yields,\n|V| ≤/parenleftbig\n1+εd\n22d3d\n= +1 . /parenleftbigε ε\n2/parenrightbig/parenrightbigg\n/parenrightbigd/parenleftbigg\n≤/parenleftbigg\nε/parenrightbigg\n39\nFor anyp≥1, deﬁne\n1\ndx\np(f,g) =/parenleftiggn1p /summationdisplay\n|f(xi)g(x)pi,ni=1− |/parenrightigg\nand forp=∞, deﬁne\ndx\n∞(f,g) = max |f(xi)−g(xi)\ni|.\nˆ Using the previous theorem, in order to bound Rx\nnwe need to bound the covering number\nwithdx\n1norm. We claim that it is suﬃcient to bound the covering number for the inﬁnity-\nnorm. In order to show this, we will compare the covering number of the norms dx\np(f,g) =\n1 /parenleftbig1\nn/summationtextn\ni=1|f(xpi)−g(xi)|/parenrightbig\npforp≥1 and conclude that a bound on N(F,dx\n∞,ε) implies a\nbound on N(F,dx\np,ε) for any p≥1.\nProposition: For any 1 ≤p≤qandε >0, we have that\nN(F,dx\np,ε)≤N(F,dx\nq,ε).\nProof.First note that if q=∞, then the inequality evidently holds. Because, we have\nn1(/summationdisplay 1\n|zi|p)p≤maxn ii=1|zi|,\nwhich leads to B(f,dx\n∞,ε)⊆B(f,dx\np,ε) andN(f,d∞,ε)≥N(f,dp,ε). Now suppose that\n1≤p≤q <∞. Using H¨ older’s inequality with r=q\np≥1 we obtain\n/parenleftigg /parenrightigg 1/parenleftigg /parenrightigg(11)1/parenleftigg /parenrightigg 1/parenleftigg /parenrightigg 1− n n n1p r p pr n1q\n1n/summationdisplay 1\n|z|pi≤−np/summationdisplay\ni1/summationdisplay\ni=1|zi|pr=ni=1 =/summationdisplay\n.\ni|zi|q\n=1\nThis inequality yeilds\nB(f,dx\nq,ε) ={g:dx\nq(f,g)≤ε} ⊆B(f,dx\np,ε),\nwhich leads to N(f,dq,ε)≥N(f,dp,ε).\nUsing this propositions we only need to bound N(F,dx\n∞,ε).\nLet the function class be F={f(x) =/a\\}bracketle{tf,x/a\\}bracketri}ht,f∈Bd,x∈Bd}, where1 1\np q + = 1. Thisp q\nleads to|f| ≤1.\nClaim: N(F,dx\n∞,ε)≤(2)d.ε\nThis leads to\nx/radicalbigg\n2dlog(4/ε)ˆRn(F)≤inf\n0{ε+ .\nε> n}\nTakingε=O(/radicalig\ndlogn), we obtainn\nˆRx d\nn(F)≤O(/radicalbigg\nlogn).n\n40\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 7\nScribe: Zach Izzo Sep. 30, 2015\nIn this lecture, we continue our discussion of covering numbers and compute upper\n^ bounds for speci\fc conditional Rademacher averages Rx\nn(F). We then discuss chaining and\nconclude by applying it to learning.\nRecall the following de\fnitions. We de\fne the risk function\nR(f) = I E[`(X;f (X))]; (X;Y )2X\u0002 [\u00001;1];\nfor some loss function `(\u0001;\u0001). The conditiona Rademacher average that we need to control\nis\nn1R(`l\u000eF) = sup I E sup \u001bi`(yi;f(xi)):\n(x1;y1);:::;(x n;yn)\"\nf\f\f\f\f\fn2FX\ni=1\f\f#\nFurthermore, we de\fned the conditional Rademacher average for a poin\f\f\ntx= (x1;:::;x n)\nto be\f\nR^x\nn(F) = I E\"\nsup\nf2F\f\f n1\u001bif(xi)ni=1\f\f#\n:\nLastly, we de\fne the \"-covering number N(X\nF;d;\f\f\n\") to be the m\f\f\ninimum number of balls (with\nrespect to the metric d) of radius\"needed to\f\ncover\f\nF. We proved the following theorem:\nTheorem: Assumejfj\u00141 for allf2F. Then\n2 log(2N(;dx;\"))R^x\nn(F) +r\nF\u0014inf1\n\">0(\n\"n)\n;\nwheredx\n1is given by\nn\ndx 1\n1(f;g) =nX\ni=1jf(xi)\u0000g(xi)j:\nWe make use of this theorem in the following example. De\fne Bd\np=fx2I Rd:jxjp\u00141g.\nThen takef(x) =ha;xi, setF=fha;\u0001i:a2Bdg, andX=Bd\n1. By H older's inequality,1\nwe have\njf(x)j\u0014jajx1j j1\u00141;\nso the theorem above holds. We need to compute the covering number N(F;dx\n1;\"). Note\nthat for all a2Bd, there exists v= (v1;:::;v n) such that vi=g(xi) and1\nn1\nnX\na;xivi\"\ni=1jh i\u0000 j\u0014\nfor some function g. For this case, we will take g(x) =hb;xi, sovi=hb;x ii. Now, note the\nfollowing. Given this de\fnition of g, we have\nn n\ndx 1 1\n1(f;g) = a;x 1b;xi=a b;x ia bnX\nni=1jh i\u0000h ijX\ni=1jh \u0000 ij\u0014j \u0000 j 1\n41\nby H older's inequality and the fact that jxj1= 1. So ifja\u0000bj1\u0014\", we can take vi=hb;x ii.\nWe just need to \fnd a set of fb1;:::;b Mg\u001aI Rdsuch that, for any athere exists bjsuch\nthatja\u0000bjj<1. We can do this by dividing Bdinto cubes with side length \"and 1 1\ntaking theb's to be the set of vertices of these cubes. Then any a2Bdj must land in one1\nof these cubes, so ja\u0000bjj \u0014\"as desired. There are c=\"dof suchb 1 j's for some constant\nc>0. Thus\nN(Bd;dx\n1 1;\")\u0014c=\"d:\nWe now plug this value into the theorem to obtain\nR^x 2 log(c=\"d)\nn(F)\u0014inf :\n\"(\n\"+\n\u00150r\nn)\nOptimizing over all choices of \"gives\nr\ndlog(n)\"\u0003=cn) R ^x\nn(F)\u0014cr\ndlog(n):n\nNote that in this \fnal inequality, the conditional empirical risk no longer depends on\nx, since we \\sup'd\" xout of the bound during our computations. In general, one should\nignorexunless it has properties which will guarantee a bound which is better than the sup.\nAnother important thing to note is that we are only considering one granularity of Fin our\n\fnal result, namely the one associated to \"\u0003. It is for this reason that we pick up an extra\nlog factor in our risk bound. In order to remove this term, we will need to use a technique\ncalled chaining.\n5.4 Chaining\nWe have the following theorem.\nTheorem: Assume thatjfj\u00141 for allf2F. Then\n\u001a12Z1\nR^x\nn\u0014inf 4\" +\n\">0p log(N (;dx))dt :n2;t\n\"q\nF\u001b\n(Note that the integrand decays with t.)\nProof. Fixx= (x1;:::;x n), and for all j= 1;:::;N , letVjbe a minimal 2\u0000j-net ofF\nunder thedx\n2metric. (The number Nwill be determined later.) For a \fxed f2F, this\nprocess will give us a \\chain\" of points fi\u000ewhich converges to f:dx\n2(fi\u000e;f)\u00142\u0000j.\nDe\fneF=f(f(x1);:::;f (xn))>; f2Fg\u001a [\u00001;1]n. Note that\nR^x 1\nn(F) = I E supn f2Fh\u001b;fi\nwhere\u001b= (\u001b1;:::;\u001b n). Observe that for all N, we can rewriteh\u001b;fias a telescoping sum:\nh\u001b;fi=h\u001b;f\u0000fN\u000ei+h\u001b;fN\u000e\u0000fN\u000e\n\u00001i+:::+h\u001b;f1\u000e\u0000f0\u000ei\n42\nwheref0\u000e:= 0. Thus\nN\nR^x 1 1\nn(F)\u0014I E supjh\u001b;f\u0000fN\u000eij+ I fn f2FX\nE sup\u001b;nj\u000efj\u000e\nf F\u00001:\nj=1jh \u0000 ij\n2\nWe can control the two terms in this inequality separately. Note \frst that by the Cauchy-\nSchwarz inequality,\n1 dx\n2(f;fN\u000e)I E supjh\u001b;f\u0000fN\u000eij\u0014j\u001bj2n fp:n 2F\nSincej\u001bj2=pnanddx\n2(f;fN\u000e)\u00142\u0000N, we have\n1I E supjh\u001b;f\u0000fN\u000e2n f2Fij\u0014\u0000N:\nNow we turn our attention to the second term in the inequality, that is\nN\nS=X1I E supjh\u001b;fj\u000e\u0000fj\u000e\nn fj=12F\u00001ij:\nNote that since fj\u000e2Vjandfj\u000e\n\u000012Vj V \u00001, there are at most jjjjVj\u00001jpossible di\u000berences\nfj\u000e\u0000fj\u000e.\u00001SincejV2j1j\u0014jV\u0000 jj=2,jVjjjVj1j\u0014jVjj=2 and we \fnd ourselves in the \fnite\u0000\ndictionary case. We employ a risk bound from earlier in the course to obtain the inequality\np\n2 log(2Rn(B)\u0014max\nb Bjbj2jBj):\n2 n\nIn the present case, B=ffj\u000e\u0000fj\u000e\n\u00001;f2Fgso thatjBj\u0014jV jj2=2. It yields\n2j22 log(jVj) logR2 Vj\nn(B)j\u0001q\nj\u0014r = 2rn\u0001p\n;n\nwherer= supf2Fjfj\u000e\u0000fj\u000e\n\u00001j2. Next, observe that\njfj\u000e\u0000fj\u000e\n1j2=pn d\u0000\u0001x\n2(fj\u000e;fj\u000e\n\u00001)p p\u0014n(dx\n2(fj\u000e;f) +dx\n2(f;fj\u000e)) 3 2\u0000jn:\u00001\u0014 \u0001\nby the triangle inequality and the fact that dx(f\u000e;f)\u00142\u0000j\n2j . Substituting this back into our\nbound forRn(B), we have\nlog(B)jVj6 2\u0000jnr\njj ;2j ( ))\u0014 \u0001 = 6n\u0001\u0000r\nlog(NFdx\n2;2\u0000j\nRn\nsinceVjjwas chosen to be a minimal 2\u0000-net.\nThe proof is almost complete. Note that 2\u0000j= 2(2\u0000j\u00002\u0000j\u00001) so that\nN6pXN122\u0000jq\nlog(N (F;dx\n2;2\u0000j)) =pX\n(2\u0000j\u00002\u0000j\u00001)q\nlog(N (F;dx\n2;2\u0000j)):n nj=1 j=1\nNext, by comparing sums and integrals (Figure 1), we see that\nXN\n(2\u0000j\nj=1\u00002\u0000j\u00001)q 1\nlog(N (F;dx\n2;2\u0000j))\u0014Z=2\nlog(N (;dx\n2;t))dt:\n2\u0000(N+1)q\nF\n43\nFigure 1: A comparison of the sum and integral in question.\nSo we choose Nsuch that 2\u0000(N +2)\u0014\"\u00142\u0000(N +1), and by combining our bounds we obtain\n121=2 1\nR^x)\u00142\u0000N\nn(F +pnZq\nlog(N (F;dx\n2;t))dt\n2\u0000( +1)\u00144\"+\nNZp\nlog(N;\n\"F;t)dt\nsince the integrand is non-negative. (Note: this integral is known as the \\Dudley Entropy\nIntegral.\")\nReturning to our earlier example, since N(F;dx\n2;\")\u0014c=\"d, we have\n1\nR^x\nn(F)\u0014inf\u001a124\"+pZq\nlog((c0=t)d)dt\n\">0 n\"\u001b\n:\nSinceR1p\nlog(c=t)dt =c\u0016 is \fnite, we then have0\nR^x\nn(F)\u001412c\u0016p\nd=n:\nUsing chaining, we've been able to remove the log factor!\n5.5 Back to Learning\nWe want to bound\nn1Rn(`\u000eF) = sup I E sup \u001bi`(yi;f(xi)):\n(x1;y1);:::;(x n;yn)\"\nf2F\f\f\f\nnX\ni\nx\f\n=1\f\f#\n\f\nR^ We considern(\b\u000eFn) = I E i\b\f\n\u0002\nsupf\f\f1P\ni=1\u001b\u000ef(x)2F ifor someLn\f\f\n-Lipschitz function\n\b, that isj\b(a)\u0000\b(b)j\u0014Lja\u0000bjfor alla;b2[\u00001;1]. We\f\f\u0003\nhave the following lemma.\n44\nTheorem: (Contraction Inequality) Let \b be L-Lipschitz and such that \b(0) = 0,\nthen\nR^x\nn(\b\u000eF)\u0014 R ^ 2L\u0001x\nn(F):\nThe proof is omitted and the interested reader should take a look at [LT91, Kol11] for\nexample.\nAs a \fnal remark, note that requiring the loss function to be Lipschitz prohibits the use\nofR-valued loss functions, for example `(Y;\u0001) = (Y\u0000\u0001)2:\n45\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 8\nScribe:Quan Li Oct. 5, 2015\nPart II\nConvexity\n1. CONVEX RELAXATION OF THE EMPIRICAL RISK MINIMIZATION\nˆ In the previous lectures, we have proved upper bounds on the excess risk R(herm)−R(h∗)\nof the Empirical Risk Minimizer\nˆherm 1= argmin\nh∈Hn\n1I(Yi=h(Xi)). (1.1)n/summationdisplay\ni=1/ne}ationslash\nHowever due to the nonconvexity of the objective function, the optimization problem\n(1.1) in general can not be solved eﬃciently. For some choices of Hand the classiﬁcation\nerror function (e.g. 1I( ·)), the optimization problem can be NP-hard. However, the problem\nwe deal with has some special features:\n1. Since the upper bound we obtained on the excess risk is O(/radicalBig\ndlogn), we only need ton\napproximate the optimization problem with error up to O(/radicalBig\ndlogn\nn).\n2. The optimization problem corresponds to the average case problem where the data\ni.i.d(Xi,Yi)∼PX,Y.\n3.Hcan be chosen to be some ’natural’ classiﬁers, e.g. H={half spaces }.\nThese special features might help us bypass the computational issue. Computational\nissueinmachinelearninghavebeenstudiedforquitesometime(see, e.g. [Kea90]), especially\nin the context of PAC learning. However, many of these problems are somewhat abstract\nand do not shed much light on the practical performance of machine learning algorithms.\nToavoid thecomputational problem, thebasicideais to minimizea convex upperbound\nof the classiﬁcation error function 1I( ·) in (1.1). For the purpose of computation, we shall\nalso require that the function class Hbe a convex set. Hence the resulting minimization\nbecomes a convex optimization problem which can be solved eﬃciently.\n1.1 Convexity\nDeﬁnition: A setCis convex if for all x,y∈Candλ∈[0,1],λx+(1−λ)y∈C.\n46\nDeﬁnition: A function f:D→IR on a convex domain Dis convex if it satisﬁes\nf(λx+(1−λ)y)≤λf(x)+(1−λ)f(y),∀x,y∈D,andλ∈[0,1].\n1.2 Convex relaxation\nThe convex relaxation takes three steps.\nStep 1: Spinning.\nUsing a mapping Y/ma√sto→2Y−1, the i.i.d. data ( X1,Y1),(X2,Y2),...,(Xn,Yn) is transformed\nto lie inX ×{−1,1}. These new labels are called spinnedlabels. Correspondingly, the task\nbecomes to ﬁnd a classiﬁer h:X /ma√sto→ {−1,1}. By the relation\nh(X)/ne}ationslash=Y⇔ −h(X)Y >0,\nwe can rewr i\nte the objective function in (1.1) by\nn n1/summationdisplay 11I(h(Xi) =Yi) =/summationdisplay\nϕ1 I(hn ni=1 i=1−(Xi)Yi) (1.2)\nwhereϕ1 I(z) = 1I(z >0).\nStep 2: Soft classiﬁers.\nThe setHof classiﬁers in (1.1) contains only functions taking values in {−1,1}. As a result,\nit is non convex if it contains at least two distinct classiﬁers. Soft classiﬁers provide a way\nto remedy this nuisance.\nDeﬁnition: Asoft classiﬁer is any measurable function f:X →[−1,1]. The hard\nclassiﬁer (or simply “classiﬁer”) associated to a soft classiﬁer fis given by h= sign(f).\nLetF ⊂IRXbe aconvexset soft classiﬁers. Several popular choices for Fare:\n•Linear functions:\nF:={/an}bracketle{ta,x/an}bracketri}ht:a∈ A}.\nfor some convex set A ∈IRd. The associated hard classiﬁer h= sign(f) splits IRdinto\ntwo half spaces.\n•Majority votes: given weak classiﬁers h1,...,h M,\nM M\nF:=/braceleftBig/summationdisplay\nλjhj(x) :λj\nj=≥0,/summationdisplay\nλj= 1/bracerightBig\n.\n1 j=1\n•Letϕj,j= 1,2,...a family of functions, e.g., Fourier basis or Wavelet basis. Deﬁne\n∞\nF:={/summationdisplay\nθjϕj(x) : (θ1,θ2,...)\nj=1∈Θ},\nwhere Θ is some convex set./ne}ationslash\n47\nStep 3: Convex surrogate.\nGiven a convex set Fof soft classiﬁers, using the rewriting in (1.2), we need to solve that\nminimizes the empirical classiﬁcation error\n1min\nf∈Fn\nϕ1 I(f(Xi)Yi),n/summationdisplay\ni=1−\nHowever, while we are now working with a convex constraint, our objective is still not\nconvex: we need a surrogate for the classiﬁcation error.\nDeﬁnition: A function ϕ: IR/ma√sto→IR+is called a convex surrogate if it is a convex\nnon-decreasing function such that ϕ(0) = 1 and ϕ(z)≥ϕ1 I(z) for allz∈IR.\nThe following is a list of convex surrogates of loss functions.\n•Hinge loss: ϕ(z) = max(1+ z,0).\n•Exponential loss: ϕ(z) = exp(z).\n•Logistic loss: ϕ(z) = log2(1+exp( z)).\nTo bypass the nonconvexity of ϕ1 I(·), we may use a convex surrogate ϕ(·) in place of\nˆ ϕ1 I(·) and consider the minimizing the empirical ϕ-riskRn,ϕdeﬁned by\n1ˆRn,ϕ(f) =nn/summationdisplay\ni=1ϕ(−Yif(Xi))\nIt is the empi\nrical counterpart of the ϕ-riskRϕdeﬁned by\nRϕ(f) = IE[ϕ(−Yf(X))].\n1.3ϕ-risk minimization\nIn this section, we will derive the relation between the ϕ-riskRϕ(f) of a soft classiﬁer fand\nthe classiﬁcation error R(h) = IP(h(X) =Y) of its associated hard classiﬁer h= sign(f)\nLet\nf∗\nϕ= argmin E[ϕ(Y\nf∈IRX−f(X))]\nwhere the inﬁmum is taken over all measurable functions f:X →IR.\nTo verify that minimizing the ϕserves our purpose, we will ﬁrst show that if the convex\nsurrogate ϕ(·) is diﬀerentiable, then sign( f∗\nϕ(X))≥0 is equivalent to η(X)≥1/2 where\nη(X) = IP(Y= 1|X). Conditional on {X=x}, we have\nIE[ϕ(−Yf(X))|X=x] =η(x)ϕ(−f(x))+(1−η(x))ϕ(f(x)).\nLet\nHη(α) =η(x)ϕ(−α)+(1−η(x))ϕ(α) (1.3)/ne}ationslash\n48\nso that\nf∗\nϕ(x) = argmin H∗η(α),andRϕ= minRϕ(f) = minHη)\nα f∈IRX(x)(α .\nα∈IR ∈IR\nSinceϕ(·) is diﬀerentiable, setting thederivative of H∗η(α) to zero gives fϕ(x) =α¯, where\nH′\nη(α¯) =−η(x)ϕ′(−α¯)+(1−η(x))ϕ′(α¯) = 0,\nwhich gives\nη(x)ϕ′(α¯)=1−η(x)ϕ′(−α¯)\nSinceϕ(·)isaconvex function, itsderivative ϕ′(·) isnon-decreasing. Thenfromtheequation\nabove, we have the following equivalence relation\n1η(x)≥ ⇔α¯≥0⇔sign(f∗\n2ϕ(x))≥0. (1.4)\nSince the equivalence relation holds for all x∈ X,\n1η(X)≥ ⇔sign(f∗\nϕ(X))2≥0.\nThe following lemma shows that if the excessϕ-riskR(f)−R∗ϕ ϕof a soft classiﬁer fis\nsmall, then the excess-risk of its associated hard classiﬁer sign( f) is also small.\nLemma (Zhang’s Lemma [Zha04]): Letϕ: IR/ma√sto→IR+be a convex non-decreasing\nfunction such that ϕ(0) = 1. Deﬁne for any η∈[0,1],\nτ(η) := inf Hη(α).\nα∈IR\nIf there exists c >0 andγ∈[0,1] such that\n1|η−c2| ≤(1−τ(η))γ,∀η∈[0,1], (1.5)\nthen\nR(sign(f))−R∗≤2c(Rϕ(f)−R∗\nϕ)γ\nProof.Note ﬁrst that τ(η)≤Hη(0) =ϕ(0) = 1 so that condition (2.5) is well deﬁned.\nNext, let h∗= argminh∈{−1,1}XIP[h(X) =Y] = sign(η−1/2) denote the Bayes classiﬁer,\nwhereη= IP[Y= 1|X=x], . Then it is easy to verify that\nR(sign(f))−R∗= IE[|2η(X)−1|1I(sign(f(X)) =h∗(X))]\n= IE[|2η(X)−1|1I(f(X)(η(X)−1/2)<0)]\n≤2cIE[((1−τ(η(X)))1I(f(X)(η(X)−1/2)<0))γ]\n≤2c(IE[(1−τ(η(X)))1I(f(X)(η(X)−1/2)<0)])γ,\nwhere the last inequality above follows from Jensen’s inequality./ne}ationslash\n/ne}ationslash\n49\nWe are going to show that for any x∈ X, it holds\n(1−τ(η))1I(f(x)(η(x)−1/2)<0)]≤IE[ϕ(−Yf(x))|X=x]−R∗\nϕ.(1.6)\nThis will clearly imply the result by integrating with respect to x.\nRecall ﬁrst that\nIE[ϕ(−Yf(x))|X=x] =Hη(x)(f(x)) and R∗\nϕ= minHη(x)(α) =τ(η(x)).\nα∈IR\nso that (2.6) is equivalent to\n(1−τ(η))1I(f(x)(η(x)−1/2)<0)]≤Hη(x)(α)−τ(η(x))\nSince the right-hand side above is nonnegative, the case where f(x)(η(x)−1/2)≥0 follows\ntrivially. If f(x)(η(x)−1/2)<0, (2.6) follows if we prove that Hη(x)(α)≥1. The convexity\nofϕ(·) gives\nHη(x)(α) =η(x)ϕ(−f(x))+(1−η(x))ϕ(f(x))\n≥ϕ(−η(x)f(x)+(1−η(x))f(x))\n=ϕ((1−2η(x))f(x))\n≥ϕ(0) = 1,\nwhere the last inequality follows from the fact that ϕis non decreasing and f(x)(η(x)−\n1/2)<0. This completes the proof of (2.6) and thus of the Lemma.\nIT is not hard to check the following values for the quantities τ(η),candγfor the three\nlosses introduced above:\n•Hinge loss: τ(η) = 1−|1−2η|withc= 1/2 andγ= 1.\n•Exponential loss: τ(η) = 2/radicalbig\nη(1−η) withc= 1/√\n2 andγ= 1/2.\n•Logistic loss: τ(η) =−ηlogη−(1−η)log(1−η) withc= 1/√\n2 andγ= 1/2.\n50\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 9\nScribe:Xuhong Zhang Oct. 7, 2015\nRecall that last lecture we talked about convex relaxation of the original problem\nn1ˆh= argmin 1I( h(Xi) =Yi)\nh∈Hn/summationdisplay\ni=1\nby considering soft classiﬁers (i.e. whose output is in [ −1,1] rather than in {0,1}) and\nconvex surrogates of the loss function (e.g. hinge loss, exponential loss, logistic loss):\nn1ˆ ˆ f= argminRϕ,n(f) = argmin ϕ(Yif(Xi))\nf∈F f∈Fn/summationdisplay\ni=1−\nˆ ˆAndh= sign(f) will be used as the ‘hard’ classiﬁer.\nˆ ¯ ¯ We want to bound the quantity Rϕ(f)−Rϕ(f), wheref= argminf∈FRϕ(f).\nˆ ˆ (1)f= argminf∈FRϕ,n(f), thus\nˆ ¯ ˆ¯ˆ¯ˆˆˆˆ ˆ ¯ Rϕ(f) =Rϕ(f)+Rϕ,n(f)−Rϕ,n(f)+Rϕ,n(f)−Rϕ,n(f)+Rϕ(f)−Rϕ(f)\n≤¯ˆ¯ˆˆ ˆ ¯ Rϕ(f)+Rϕ,n(f)−Rϕ,n(f)+Rϕ(f)−Rϕ(f)\n≤¯ ˆ Rϕ(f)+2sup |Rϕ,n(f)−Rϕ(f)\nf∈F|\nˆ (2) Let us ﬁrst focus on E[supf∈F|Rϕ,n(f)−Rϕ(f)|]. Using the symmetrization trick as\nbefore, weknowit isupper-boundedby2 Rn(ϕ◦F), wheretheRademacher complexity\nn1Rn(ϕ◦F) = sup E[sup|/summationdisplay\nσiϕ(−Yif(Xi)) ]\nX1,...,Xn,Y1,...,Ynf∈Fni=1|\nOne thing to notice is that ϕ(0) = 1 for the loss functions we consider (hinge loss,\nexponential loss and logistic loss), but in order to apply contraction inequality later,\nwe requireϕ(0) = 0. Let us deﬁne ψ(·) =ϕ(·)−1. Clearlyψ(0) = 0, and\nn1E[sup|/summationdisplay\n(ϕ(−Yif(Xi))−E[ϕ(−Yif(Xi))]) ]\nf∈Fni=1|\nn1=E[sup|/summationdisplay\n(ψ(−Yif(X)−Ei) [ψ( i\n1−Yif(X))])\nf∈Fni=|]\n≤2Rn(ψ◦F)\n(3) The Rademacher complexity of ψ◦ Fis still diﬃcult to deal with. Let us assume\nthatϕ(·) isL-Lipschitz, (as a result, ψ(·) is alsoL-Lipschitz), apply the contraction\ninequality, we have\nRn(ψ◦F)≤2LRn(F)/ne}ationslash\n51\n(4) LetZi= (Xi,Yi),i= 1,2,...,nand\nn1g(Z1,Z2ˆ ,...,Zn) = sup|Rϕ,n(f)−Rϕ(f) =\nf|sup\n∈F f∈F|n/summationdisplay\n(ϕ(\ni=1−Yif(Xi))−E[ϕ(−Yif(Xi))])|\nSinceϕ(·) ismonotonically increasing, itisnotdiﬃculttoverifythat ∀Z1,Z2,...,Zn,Z′\ni\n1 2L|g(Z1,...,Zi,...,Zn)−g(Z1,...,Z′\ni,...,Zn)| ≤(ϕ(1)−ϕ(n−1))≤n\nThe last inequality holds since gisL-Lipschitz. Apply Bounded Diﬀerence Inequality,\n2t2\nP(|s|ˆ ˆ upRϕ,n(f)−Rϕ(f)|−E[sup|Rϕ,n(f)−Rϕ(f)|]>\nF ∈F|t)≤2exp(\n∈− )\nf/summationtextnf i=(2L)21n\nSet the RHS of above equation to δ, we get:\nlog(2/δ)ˆ ˆ supR E ϕ,n(f)Rϕ(f) [sup Rϕ,n(f)\nf∈F| − | ≤\nf∈F| −Rϕ(f)|]+2L/radicalbigg\n2n\nwith probability 1 −δ.\n(5) Combining (1) - (4), we have\nˆ ¯Rϕ(f)≤Rϕ(f)+8LRn(F)+2L/radicalbigg\nlog(2/δ)\n2n\nwith probability 1 −δ.\n1.4 Boosting\nInthissection, wewillspecializetheaboveanalysistoaparticularlearningmodel: Boosting.\nThe basic idea of Boosting is to convert a set of weak learners (i.e. classiﬁers that do better\nthan random, but have high error probability) into a strong one by using the weighted\naverage of weak learners’ opinions. More precisely, we consider the following function class\nM\nF={/summationdisplay\nθjhj(·) :|θ|1≤1,hj:X /ma√sto→[−1,1],j∈ {1,2,...,Ma\nj=1}re classiﬁers }\nand we want to upper bound Rn(F) for this choice of F.\nn M n1 1Rn(F) = sup E[sup σiYif(Xi) ] = sup E[ supθjYiσihj(Xi) ]\nZ1,...,Znf∈F|n/summationdisplay\nnZ |θ|1≤11|\n1,...,Zi=n|/summationdisplay\nj=1/summationdisplay\ni=1|\nLetg(θ) =|/summationtextM\nj=1θj/summationtextn\ni=1Yiσihj(Xi)|. It is easy to see that g(θ) is a convex function, thus\nsup|θ|1≤1g(θ) is achieved at a vertex of the unit ℓ1ball{θ:/bardblθ/bardbl1≤1}. Deﬁne the ﬁnite set\nY1h1(X1) Y1h2(X1) Y1hM(X1)/braceleftiggY2h1(X2) Y2h2(X2)  Y2hM(X2)\nBX,Y/defines., ,...,.\n±\n.± ±/bracerightigg\nYnh1(Xn)\n \n. ..\nYnh2(Xn)\n\n.. .\nYnhM(Xn)\n52\nThen\nRn(F) = supRn(BX,Y).\nX,Y\nNotice max b∈BX,Yb√| |2≤nand|BX,Y|= 2M. Therefore, using a lemma from Lecture 5,\nwe get\n2log(2B 2 4RX,Y)log(M)\nn(BX,Y)≤max\nb∈BX,Y|b|2/radicalbig\n| |\nn≤/radicalbigg\nn\nThus for Boosting,/bracketleftbig /bracketrightbig\n2log(4M) log(2 /δ)ˆ ¯Rϕ(f)≤Rϕ(f)+8L/radicalbigg\n+2L/radicalbigg\nwith probability 1 - δn 2n\nTo get some ideas of what values Lusually takes, consider the following examples:\n(1) for hinge loss, i.e. ϕ(x) = (1+x)+,L= 1.\n(2) for exponential loss, i.e. ϕ(x) =ex,L=e.\n(3) for logistic loss, i.e. ϕ(x) = log2(1+ex),L=e\n1+elog2(e)≈2.43\nˆ ¯ Now we have bounded Rϕ(f)−Rϕ(f), but this is not yet the excess risk. Excess risk is\nˆ deﬁned asR(f)−R(f∗), wheref∗= argminfRϕ(f). The following theorem provides a\nbound for excess risk for Boosting.\nTheorem: LetF={/summationtextM\nj=1θjhj:/bardblθ/bardbl1≤1,hjsare weak classiﬁers }andϕis anL-\nˆ ˆ ˆ Lipschitz convex surrogate. Deﬁne f= argminf∈FRϕ,n(f) andh= sign(f). Then\nγ γ\n∗/parenleftbig∗/parenrightbigγ/parenleftigg\n2log(4M) log(2/δ)ˆR(h)−R≤2cinfRϕ(f)−Rϕ(f) +2c8L +\nf∈F/radicalbigg\nn/parenrightigg\n2c/parenleftigg\n2L/radicalbigg\n2n/parenrightigg\nwith probability 1 −δ\nProof.\nˆR(h)−R∗≤2c/parenleftbigγRϕ(f)−Rϕ(f∗)/parenrightbig\n/parenleftiggγ\n∗ 2log(4M) log(2 /δ)≤2cinfRϕ(f)Rϕ(f)+8L +2L\nf∈F−/radicalbigg\nn/radicalbigg\n2n/parenrightigg\nγ\n∗γ 2log(4M) log(2/δ)≤2cinfRϕ(f)−Rϕ(f) +2c\nf∈F/parenleftigg\n8L/radicalbigg\nn/parenrightigg\n+2c/parenleftigg\n2L/radicalbigg\n2n/parenrightiggγ/parenleftbig /parenrightbig\nHere the ﬁrst inequality uses Zhang’s lemma and the last one uses the fact that for ai≥0\nandγ∈[0,1], (a1+aγ2+a3)≤aγ\n1+aγ\n2+aγ\n3.\n1.5 Support Vector Machines\nIn this section, we will apply our analysis to another important learning model: Support\nVector Machines (SVMs). We will see that hinge loss ϕ(x) = (1 +x)+is used and the\nassociated function class is F={f:/bardblf/bardblW≤λ}whereWis a Hilbert space. Before\nanalyzing SVMs, let us ﬁrst introduce Reproducing Kernel Hilbert Spaces (RKHS).\n53\n1.5.1 Reproducing Kernel Hilbert Spaces (RKHS)\nDeﬁnition: A function K:X ×X /ma√sto→ IR is called a positive symmetric deﬁnite kernel\n(PSD kernel) if\n(1)∀x,x′∈ X,K(x,x′) =K(x′,x)\n(2)∀n∈Z+,∀x1,x2,...,xn, then\nth×nmatrix with K(xi,xj) as its element in ithrow\nandjcolumn is positive semi-deﬁnite. In other words, for any a1,a2,...,an∈IR,\n/summationdisplay\naiajK(xi,xj) 0\ni,j≥\nLet us look at a few examples of PSD kernels.\nExample 1 LetX= IR,K(x,x′) =/an}bracketle{tx,x′/an}bracketri}htIRdis a PSD kernel, since ∀a1,a2,...,an∈IR\n/summationdisplay\naiaj/an}bracketle{txi,xj/an}bracketri}htIRd=/summationdisplay\n/an}bracketle{taixi,ajxj/an}bracketri}htIRd=/an}bracketle{t/summationdisplay\naixi,/summationdisplay\najxj/an}bracketri}htIRd=/bardbl/summationdisplay\na2ixi/bardblIRd0\ni,j i,j i j i≥\nExample 2 The Gaussian kernel K(x,x′) = exp(−1 2\n2/bardbl′\n2x−x/bardblIRd) is also a PSD kernel.σ\nNote that here and in the sequel, /bardbl · /bardblWand/an}bracketle{t·,·/an}bracketri}htWdenote the norm and inner product\nof Hilbert space W.\nDeﬁnition: LetWbe a Hilbert space of functions X /ma√sto→IR. A symmetric kernel K(·,·)\nis called reproducing kernel ofWif\n(1)∀x∈ X, the function K(x,·)∈W.\n(2)∀x∈ X,f∈W,/an}bracketle{tf(·),K(x,·)/an}bracketri}htW=f(x).\nIf such aK(x,·) exists,Wis called a reproducing kernel Hilbert space (RKHS).\nClaim: IfK(·,·) is a reproducing kernel for some Hilbert space W, thenK(·,·) is a\nPSD kernel.\nProof.∀a1,a2,...,an∈IR, we have\n/summationdisplay\naiajK(xi,xj) =/summationdisplay\naiaj/an}bracketle{tK(xi,·),K(xj,·)/an}bracketri}ht(sinceK(,) is reproducing)\ni,j i,j· ·\n=/an}bracketle{t/summationdisplay\naiK(xi,), ajK(xj,)W\ni·/summationdisplay\nj· /an}bracketri}ht\n=/bardbl/summationdisplay\naiK(xi,\ni·)/bardbl2\nW≥0\n54\nIn fact, the above claim holds both directions, i.e. if a kernel K(·,·) is PSD, it is also a\nreproducing kernel.\nA natural question to ask is, given a PSD kernel K(·,·), how can we build the corresponding\nHilbert space (for which K(·,·) is a reproducing kernel)? Let us look at a few examples.\nExample 3 Letϕ1,ϕ2,...,ϕMbe a set of orthonormal functions in L2([0,1]), i.e. for any\nj,k∈ {1,2,...,M}/integraldisplay\nϕj(x)ϕk(x)dx=\nx/an}bracketle{tϕj,ϕk/an}bracketri}ht=δjk\nLetK(x,x′) =/summationtextM\nj=1ϕj(x)ϕj(x′). We claim that the Hilbert space\nM\nW={/summationdisplay\najϕj( :\n=1·)a1,a2,...,aM\nj∈IR}\nequipped with inner product /an}bracketle{t·,·/an}bracketri}htL2is a RKHS with reproducing kernel K(·,·).\nMProof. (1)K(x,·) =j=1ϕj(x)ϕj(·)∈W. (Chooseaj=ϕj(x)).\n(2) Iff(·) =/summationtextM\nj=1aj/summationtext\nϕj(·),\nM M M\n/an}bracketle{tf(·),K(x,·)/an}bracketri}htL2=/an}bracketle{t/summationdisplay\najϕj(·),/summationdisplay\nϕk(x)ϕk(·)/an}bracketri}htL2=/summationdisplay\najϕj(x) =f(x)\nj=1 k=1 j=1\n(3)K(x,x′) is a PSD kernel: ∀a1,a2,...,an∈IR,\n/summationdisplay\naiajK(x2i,xj) =/summationdisplay\naiajϕk(xi)ϕk(xj) =/summationdisplay\n(/summationdisplay\naiϕk(xi))\ni,j i,j,k ki≥0\nExample 4 IfX= IRd, andK(x,x′) =/an}bracketle{tx,x′/an}bracketri}htIRd, the corresponding Hilbert space is\nW={/an}bracketle{tw,·/an}bracketri}ht:w∈IRd}(i.e. all linear functions) equipped with the following inner product:\niff=/an}bracketle{tw,·/an}bracketri}ht,g=/an}bracketle{tv,·/an}bracketri}ht,/an}bracketle{tf,g/an}bracketri}ht/defines/an}bracketle{tw,v/an}bracketri}htIRd.\nProof. (1)∀x∈IRd,K(x,·) =/an}bracketle{tx,·/an}bracketri}htIRd∈W.\n(2)∀f=/an}bracketle{tw,·/an}bracketri}htIRd∈W,∀x∈IRd,/an}bracketle{tf,K(x,·)/an}bracketri}ht=/an}bracketle{tw,x/an}bracketri}htIRd=f(x)\n(3)K(x,x′) is a PSD kernel: ∀a1,a2,...,an∈IR,\n/summationdisplay\naiajK(xi,xj) =/summationdisplay\naiaj,\n,j i,j/an}bracketle{txixj\ni/an}bracketri}ht=/an}bracketle{t/summationdisplay\naixi,\ni/summationdisplay\najxj\nj/an}bracketri}htIRd=/bardbl/summationdisplay\naix2iIRd0\ni/bardbl ≥\n55\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 10\nScribe:Aden Forrow Oct. 13, 2015\nRecall the following deﬁnitions from last time:\nDeﬁnition: A function K:X ×X /ma√sto→ Ris called a positive symmetric deﬁnite kernel\n(PSD kernel) if\n1.∀x,x′∈ X,K(x,x′) =K(x′,x)\n2.∀n∈Z+,∀x1,x2,...,x n, then×nmatrix with entries K(xi,xj) is positive deﬁ-\nnite. Equivalently, ∀a R 1,a2,...,a n∈,\nn/summationdisplay\naiajK(xi,xj)\ni,j=1≥0\nDeﬁnition: LetWbe a Hilbert space of functions X /ma√sto→R. A symmetric kernel K(·,·)\nis called a reproducing kernel ofWif\n1.∀x∈ X, the function K(x,·)∈W.\n2.∀x∈ X,∀f∈W,/an}bracketle{tf(·),K(x,·)/an}bracketri}htW=f(x).\nIf such a K(x,·) exists, Wis called a reproducing kernel Hilbert space (RKHS).\nAs before, /an}bracketle{t·,·/an}bracketri}htWand/bardbl·/bardblWrespectively denote the inner product and norm of W. The\nsubscript Wwill occasionally be omitted. We can think of the elements of Was inﬁnite\nlinear combinations of functions of the form K(x,·). Also note that\n/an}bracketle{tK(x,·),K(y,·)/an}bracketri}htW=K(x,y)\nSince so many of our tools rely on functions being bounded, we’d like to be able to\nbound the functions in W. We can do this uniformly over x∈ Xif the diagonal K(x,x) is\nbounded.\nProposition: LetWbe a RKHS with PSD Ksuch that supx∈XK(x,x) =kmaxis\nﬁnite. Then ∀f∈W,\nsup|f(x)| ≤ /bardblf/bardblW/radicalbig\nkmax\nx∈X\n.\nProof.We rewrite f(x) as an inner product and apply Cauchy-Schwartz.\nf(x) =/an}bracketle{tf,K(x,·)/an}bracketri}htW≤ /bardblf/bardblW/bardblK(x,·)/bardblW\nNow/bardblK(x,·)/bardbl2\nW=/an}bracketle{tK(x,·),K(x,·)/an}bracketri}htW=K(x,x)≤kmax. The result follows immediately.\n56\n1.5.2 Risk Bounds for SVM\nWe now analyze support vector machines (SVM) the same way we analyzed boosting. The\ngeneral idea is to choose a linear classiﬁer that maximizes the margin (distance to classiﬁers)\nwhile minimizing empirical risk. Classes that are not linearly separable can be embedded\nin a higher dimensional space so that they are linearly separable. We won’t go into that,\nhowever; we’ll just consider the abstract optimization over a RKHS W.\nExplicitly, we minimize the empirical ϕ-risk over a ball in Wwith radius λ:\nˆ ˆ f= min Rn,ϕ(f)\nf∈W,/bardblf/bardblW≤λ\nˆ ˆ ˆ The soft classiﬁer fis then turned into a hard classiﬁer h= sign(f). Typically in SVM ϕ\nis the hinge loss, though all our convex surrogates behave similarly. To choose W(the only\nother free parameter), we choose a PSD K(x1,x2) that measures the similarity between two\npointsx1andx2.\nAs written, this is an intractable minimum over an inﬁnite dimensional ball {f,/bardblf/bardblW≤\nλ}. The minimizers, however, will all be contained in a ﬁnite dimensional subset.\nTheorem: Representer Theorem. LetWbe a RKHS with PSD Kand letG:\nnR/ma√sto→Rbe any function. Then\nminG(f(x1), ...,f(xn)) = min G(f(x1),...,f(xn))\nf∈W,/bardblf/bardbl≤λ ∈¯f Wn,/bardblf/bardbl≤λ\n= min G(gα(x1),...,gα(xn)),\nα∈Rn,α⊤IKα≤λ2\nwheren\n¯Wn={f∈W|f(·) =gα(·) =/summationdisplay\nαiK(xi,\ni=1·)}\nand IK ij=K(xi,xj).\nProof. ¯SinceWnis a linear subspace of W, we can decompose any f Wuniquely as\n¯⊥¯∈¯ ⊥∈\nf=f+fwithf W nandf∈¯W⊥\nn. The Pythagorean theorem then gives\n/bardblf/bardbl2\nW=/bardbl¯f/bardbl2\nW+/bardblf⊥/bardbl2\nW\n¯ Moreover, since K(xi,·)∈Wn,\nf⊥(xi) =/an}bracketle{tf⊥,K(xi,·)/an}bracketri}htW= 0\n¯ Sof(xi) =f(xi) and\nG(f(x1¯ ¯ ),...,f(xn)) =G(f(x1),...,f(xn)).\nBecause f⊥does not contribute to G, we can remove it from the constraint:\n¯ ¯ min G(f(x1), ...,f(xn)) = min G(f(x1),...,f(xn)).¯f∈W,/bardblf/bardbl2+/bardblf⊥/bardbl2≤λ2 f∈ /bardbl¯W, f/bardbl2≤λ2\n57\n¯ Restricting to f∈Wnnow does not change the minimum, which gives us the ﬁrst equality.\nFor the second, we need to show that /bardblgα/bardblW≤λis equivalent to α⊤IKα≤λ2.\n/bardblgα/bardbl2=/an}bracketle{tgα,gα\nn/an}bracketri}ht\nn\n=/an}bracketle{t/summationdisplay\nαiK(xi,\ni=1·),\nn/summationdisplay\nαjK(xj,\n=1·)\nj/an}bracketri}ht\n=/summationdisplay\nαiαj/an}bracketle{tK(xi,(\n,j=1·),K xj,\ni·)/an}bracketri}ht\nn\n=/summationdisplay\nαiαjK(xi,xj)\ni,j=1\n=α⊤IKα\nWe’ve reduced the inﬁnite dimensional problem to a minimization over α∈nR. This\nworks because we’re only interested in Gevaluated at a ﬁnite set of points. The matrix\nIK here is a Gram matrix, though we will not not use that. IK should be a measure of the\nsimilarity of the points xi. For example, we could have W={/an}bracketle{tx,·/an}bracketri}htRd,x∈dR}withK(x,y\nthe usual inner product K(x,y) =/an}bracketle{tx,y/an}bracketri}htRd.\nˆ ˆ We’ve shown that fonly depends on Kthrough IK, but does Rn,ϕdepend on K(x,y)\nforx,y∈/{xi}? It turns out not to:\nn n n1ˆRn,ϕ=/summationdisplay 1ϕ(−Yigα(xi)) =/summationdisplay\nϕ(−Yi/summationdisplay\nαjK(xj,xi)).n ni=1 i=1 j=1\nThe last expression only involves IK. This makes it easy to encode all the knowledge about\nour problem that we need. The hard classiﬁer is\nn\nˆ ˆ h(x) = sign( f(x)) = sign( gαˆ(x)) = sign(/summationdisplay\nαˆjK(xj,x))\nj=1\nIf we are given a new point xn+1, we need to compute a new column for IK. Note that\nxn+1must be in some way comparable or similar to the previous {xi}for the whole idea of\nextrapolating from data to make sense.\nThe expensive part of SVMs is calculating the n×nmatrix IK. In some applications,\nIK may be sparse; this is faster, but still not as fast as deep learning. The minimization\nover the ellipsoid α⊤IKαrequires quadratic programming, which is also relatively slow. In\npractice, it’s easier to solve the Lagrangian form of the problem\nn1αˆ = argmin/summationdisplay\nϕ(−Yigα(x′ ⊤i))+λ αIKα\nα∈Rnni=1\nThis formulation is equivalent to the constrained one. Note that λandλ′are diﬀerent.\nSVMs have few tuning parameters and so have less ﬂexibility than other methods.\nWe now turn to analyzing the performance of SVM.\n58\nTheorem: Excess Risk for SVM. Letϕbe anL-Lipschitz convex surrogate and\nˆ ˆ Wa RKHS with PSD Ksuch that max x|K(x,x)|=kmax<∞. Lethn,ϕ= signfn,ϕ,\nˆwherefn,ϕis the empirical ϕ-risk minimizer over F={f\nˆˆˆ∈W./bardblf/bardblW≤λ}(that is,\nRn,ϕ(fn,ϕ)≤Rn,ϕ(f)∀f∈ F). Suppose λ√kmax≤1. Then\nˆR(hn,ϕ)−R∗≤2c/parenleftbigg γ γγkmax 2log(2/δ)inf (Rϕ(f)−R∗\nϕ)/parenrightbigg\n+2c/parenleftBigg\n8Lλ +\nf∈/radicalbigg\n2L\nF n/parenrightBigg\n2c/parenleftBigg/radicalbigg\nn/parenrightBigg\nwith probability 1 −δ. The constants candγare those from Zhang’s lemma. For the\nhinge loss, c=1\n2andγ= 1.\nProof.The ﬁrst term comes from optimizing over a restricted set Finstead of all classiﬁers.\nThe third term comes from applying the bounded diﬀerence inequality. These arise in\nexactly the same way as they do for boosting, so we will omit the proof for those parts. For\nthe middle term, we need to show that Rn,ϕ(F)≤λkmax\nn.\nFirst,|f(x)| ≤ /bardblf/bardblW√kmax≤λ√kmax≤1 for all/radicalBig\nf∈ F, so we can use the contraction\ninequality to replace Rn,ϕ(F) withRn(F). Next we’ll expand f(xi) inside the Rademacher\ncomplexity and bound inner products using Cauchy-Schwartz.\nn1Rn(F) = sup Esup σif(xi)\nx1,...,xn/bracketleftBigg\nf∈F/vextendsingle/vextendsingle\n/vextendsingle\nni=1/vextendsingle/vextendsingle/bracketrightBigg/summationdisplay\n/vextendsingle/vextendsingle\n/vextendsingle\nn1= sup E/bracketleftBigg\nsup/vextendsingle/vextendsingle\n/vextendsingle/summationdisplay\nσ/vextendsingle\niK(x/vextendsingle\n/an}bracketle{ti,·),fnx1,...,xnf∈Fi=1/an}bracketri}ht/vextendsingle/vextendsingle/bracketrightBigg\nn1/vextendsingle\n= sup E/vextendsingle/vextendsingle\n/bracketleftBigg\nsup/vextendsingle/vextendsingle/vextendsingle/an}bracketle{t/summationdisplay\nσiK(xi,·),f/vextendsingle\nn/vextendsingle\nx1,...,xnf∈Fi=1/an}bracketri}ht/vextendsingle/vextendsingle/bracketrightBigg\n/vextendsingle\nλ/vextendsingle/vextendsingle /vextendsingle\n≤sup/radicaltp\n/radicalvertex/radicalvertex\n/radicalbtE/bracketleftBiggn\n/bardbl/summationdisplay\nσiK(x2i,/vextendsingle\nnx1,...,xni=1·)/bardblW/bracketrightBigg\nNow,\nn n n\n2E/bracketleftBigg\n/bardbl/summationdisplay\nσiK(xi,·)/bardblW/bracketrightBigg\n=E\n/an}bracketle{t/summationdisplay\nσiK(xi,·),/summationdisplay\nσjK(xj,\ni=1 i=1 j=1·)/an}bracketri}htW\nn\n=/summationdisplay\n/an}bracketle{tK(x E i,·),K(xj,·)/an}bracketri}ht[σiσj]\ni,j=1\nn\n=\ni/summationdisplay\nK(xi,xj)δij\n,j=1\n≤nkmax\nSoRn(F)≤λkmax\nnandwearedonewiththenewpartsoftheproof. Theremainderfollows\nas with boosti/radicalBig\nng, using symmetrization, contraction, the bounded diﬀerence inequality, and\nZhang’s lemma.\n59\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 11\nScribe: Kevin Li Oct. 14, 2015\n2. CONVEX OPTIMIZATION FOR MACHINE LEARNING\nIn this lecture, we will cover the basics of convex optimization as it applies to machine\nlearning. There is much more to this topic than will be covered in this class so you may beinterested in the following books.\nConvex Optimization by Boyd and Vandenberghe\nLecture notes on Convex Optimization by Nesterov\nConvex Optimization: Algorithms and Complexity by Bubeck\nOnline Convex Optimization by Hazan\nThe last two are drafts and can be obtained online.\n2.1 Convex Problems\nA convex problem is an optimization problem of the form min f(x) wherefand are\nx2CC\nconvex. First, we will debunk the idea that convex problems are easy by showing that\nvirtually all optimization problems can be written as a convex problem. We can rewrite anoptimization problem as follows.\nminf(x), mint, mint\nX2X t\u0015f(x);x2X (x;t)2epi(f )\nwhere the epigraph of a function is de\fned by\nepi(f)=f(x;t )2X\u0002 IR :t\u0015f(x)g\n, ,\nf 2 \u0002 \u0015 g\nFigure 1: An example of an epigraph.\nSource: https://en.wikipedia.org/wiki/Epigraph_(mathematics)\n60\nNow we observe that for linear functions,\nminc>x= min c>x\nx2D x2conv (D)\nwhere the convex hull is de\fned\nN N\nconv(D) =fy:9N2Z+;x1;:::;xN2D;\u000bi\u00150;X\n\u000bi= 1;y =\ni=1X\n\u000bixi\ni=1g\nTo prove this, we know that the left side is a least as big as the right side since D\u001aconv(D).\nFor the other direction, we have\nN\nminc>x= min min min c>\u000bixix2conv (D) N x 1;:::;x N2D\u000b1;:::;\u000b N\nNX\ni=1\n= min min min \u000bic>ximinc>x\nN x 1;:::;x N2D\u000b1;:::;\u000b NX\n=1\u0015\nx2Di\nN\n\u0015min min min \u000biminc>x\nN x 1;:::;x N2D\u000b1;:::;\u000b NX\nxi=12D\n= minc>x\nx2D\nTherefore we have\nminf(x) min\nx2X, t\n(x;t)2conv (epi(f ))\nwhich is a convex problem.\nWhy do we want convexity? As we will show, convexity allows us to infer global infor-\nmation from local information. First, we must de\fne the notion of subgradient .\nDe\fnition (Subgradient): LetC\u001a I Rd,f:C! I R. A vector g2I Rdis called a\nsubgradient offatx2Cif\nf(x)\u0000f(y)\u0014g>(x\u0000y)8y2C:\nThe set of such vectors gis denoted by @f(x).\nSubgradients essentially correspond to gradients but unlike gradients, they always ex-\nist for convex functions, even when they are not di\u000berentiable as illustrated by the next\ntheorem.\nTheorem: Iff:C ! I R is convex, then for all x,@f(x) =;. In addition, if fis\ndi\u000berentiable at x, then@f(x) =frf(x)g.\nProof. Omitted. Requires separating hyperplanes for convex sets.6f 9 2 2 \u0015 g\n\u001a\n\u0015\n\u0015\n,\n\u001a! 2\n2\n\u0000 \u0014 \u0000 2\n! 6=;\nfr g8\n6\n61\nTheorem: Letf;Cbe convex. If xis a local minimum of fonC, then it is also global\nminimum. Furthermore this happens if and only if 0 2@f(x).\nProof. 02@f(x) if and only if f(x)−f(y)\u00140 for ally2C. This is clearly equivalent to\nxbeing a global minimizer.\nNext assume\u0010xis a local minimum. Then for all y2Cthere exists \"small enough such\nthatf(x)\u0014f(1−\")x+\"y\u0011\n\u0014(1−\")f(x)+\"f(y)=)f(x)\u0014f(y) for ally2C.\nNot only do we know that local minimums are global minimums, looking at the subgra-\ndient also tells us where the minimum can be. If g>(x−y)<0 thenf(x)<f(y). This\nmeansf(y) cannot possibly be a minimum so we can narrow our search to ys such that\ng>(x−y). In one dimension, this corresponds to the half line fy2IR :y\u0014xgifg>0 and\nthe half linefy2IR :y\u0015xgifg<0 . This concept leads to the idea of gradient descent.\n2.2 Gradient Descent\ny\u0019xandfdi\u000berentiable the \frst order Taylor expansion of fatxyieldsf(y)\u0019f(x)+\ng>(y−x). This means that\nminf(x+\"\u0016^)\u0019minf(x)+g>(\"\u0016^)\nj\u0016^j2=1\ngwhich is minimized at \u0016^=− . Therefore to minimizes the linear approximation of fatjgj2x, one should move in direction opposite to the gradient.\nGradient descent is an algorithm that produces a sequence of points fxjgj\u00151such that\n(hopefully) f(xj+1)<f(xj).\n2\n2 \u0014 2\n2\n\u0014 \u0014 ) \u0014 2\nf 2 \u0014 g\nf 2 \u0015 g\n\u0019 \u0019\n\u0019\nf g\nFigure 2: Example where the subgradient of x1is a singleton and and the subgradient of\nx2contains multiple elements.\nSource: https://optimization.mccormick.northwestern.edu/index.php/\nSubgradient_optimization\n62\nAlgorithm 1 Gradient Descent algorithm\nInput:x12C, positive sequence f\u0011sgs\u00151\nfors= 1 tok\u00001do\nxs+1=xs\u0000\u0011sgs; gs2@f(xs)\nend for\nk1return Eitherx\u0016 =kX\nxsorx\u000e\ns=12argminf(x)\nx2fx 1;:::;x kg\nTheorem: Letfbe a convex L-Lipschitz function on I Rdsuch thatx\u00032argminI Rdf(x)\nexists. Assume that jx1\u0000x\u0003j2\u0014R. Then if\u0011Rs=\u0011=Lpfor allsk\u00151, then\nk1 LRf(kX\nxs)\ns=1\u0000f(x\u0003)\u0014p\nk\nandLRminf(xs)\n1s k\u0000f(x\u0003)\u0014p\n\u0014\u0014 k\nProof. Using the fact that gs=1(x2s+1 +\u0011\u0000xs) and the equality 2a>b=kak kbk2\u0000ka\u0000bk2,\n1f(xs)\u0000f(x\u0003)\u0014gs>(xs\u0000x\u0003) = (xs\u0011\u0000xs+1)>(xs\u0000x\u0003)\n1=x2sxs+1 +x x\u00032x2s s+1x\u0003\n2\u0011h\nk \u0000 k k \u0000 k \u0000k \u0000 k\n\u0011 1i\n=2kg2sk+ (\u000e2\n2\u0011s\u0000\u000e2\ns+1)\nwhere we have de\fned \u000es=kxs\u0000x\u0003k. Using the Lipschitz condition\n\u0011 1f(xs)\u0000f(x\u0003)\u0014L2+ (\u000e2\n2 2\u0011s\u0000\u000e2\ns+1)\nTaking the average from 1, to kwe get\nk1X \u0011 \u0011 1 R2\nf(xs)f(x\u0003)\u0014L2 1 \u0011\u0000 + (\u000e2\nk1\u0000\u000e2\nks)\u0011\u0014L2\n+1 +\u000e2\n2 2 2k\u00111\u0014L2+2 2 2k\u0011s=1\nTaking\u0011=R\nLpto minimize the expression, we obtaink\nk1\nkX LRf(xs)\ns=1\u0000f(x\u0003)\u0014p\nk\nk\nNoticing that the left-hand side of the inequality is larger than both f(Pxs)\u0000f(x\u0003) by\ns=1\nJensen's inequality and min f(xs)\n1\u0014s\u0014k\u0000f(x\u0003) respectively, completes the proof.2 f g\n\u0000\n\u0000 2\n2\n2\nj \u0000 j \u0014 \u0015\n\u0000 \u0014\n\u0000 \u0014p\n\u0000 k k kk k k\n\u0000 \u0014 \u0000 \u0000 \u0000\nk \u0000 k k \u0000 k \u0000k \u0000 k\nk k \u0000\nk \u0000 k\n\u0000 \u0014 \u0000\n\u0000 \u0014 \u0000 \u0014 \u0014\n\u0000 \u0014p\n\u0000\n\u0000p\n\u0000 \u0000\n63\nOne \naw with this theorem is that the step size depends on k. We would rather have\nstep sizes\u0011sthat does not depend on kso the inequalities hold for all k. With the new step\nsizes,\nXkXk k k\u00112\ns21[X R2\n\u0011sf(x)\u0000f x\u0003)]\u0014L \u000e2 L\ns ( + (\u000e2\ns\u0000s+1)\u00112+2 2s2 2s=1 s=1 s=1\u0014\u0010X\ns=1\u0011\nAfter dividing byPk\nPs=1\u0011s, wePwould like the right-hand side to approach 0. For this to\n\u00112happen we need Ps!0 and\u0011s!1. One candidate for the step size is \u0011s=G\n\u0011spsinces\nk\nthenP k\n\u00112\ns\u0014c1G2log(k ) and\u0011sc2Gp\nk. So we get\ns=1 sP\n=1\u0015\n\u0010Xkc1\u0011s\u0011k\u00001X GLlogk R2\n\u0011s[f(xs)f(x\u0003)]\n2c2p +\nk 2c2Gp\nks=1 s=1\u0000 \u0014\nChoosingGappropriately, the right-hand side approaches 0 at the rate of LRlogk. Noticepk\nthat we get an extra factor of log k. However, if we look at the sum from k=q\n2 tokinstead\nk\nof 1 tok,P k\n\u00112\ns\u0014c0\n1G2andP\u0011s\u0015c0\n2Gp\nk. Now we have\n=k s=1 s2\nk k\u00001 cLRminf(xs)f(x\u0003) minf(xs)f(x\u0003)\u0011s\u0011s[f(xs)f(x\u0003)]\n1\u0014s\u0014k\u0000 \u0014\nks k2\u0000 \u0014\u0010X\nk\u0011X\nk\u0000 \u0014p\n\u0014\u0014 ks= s=2 2\nwhich is the same rate as in the theorem and the step sizes are independent of k.\nImportant Remark: Note this rate only holds if we can ensure that jxk=2\u0000x\u0003j2\u0014R\nsince we have replaced x1byxk=2in the telescoping sum. In general, this is not true for\ngradient descent, but it will be true for projected gradient descent in the next lecture.\nOne \fnal remark is that the dimension ddoes not appear anywhere in the proof. How-\never, the dimension does have an e\u000bect because for larger dimensions, the conditions fis\nL-Lipschitz andjx1\u0000x\u0003j2\u0014Rare stronger conditions in higher dimensions.\u0000 \u0014 \u0000 \u0014\n!!1\n\u0014 \u0015p\n\u0000 \u0014pp\np\n\u0014 \u0015p\n\u0000 \u0014 \u0000 \u0014 \u0000 \u0014p\nj \u0000 j \u0014\nj j \u0014 \u0000\n64\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 12\nScribe:Michael Traub Oct. 19, 2015\n2.3 Projected Gradient Descent\nIn the original gradient descent formulation, we hope to optimize min xf(x) where nC Ca d ∈\nfare convex, but we did not constrain the intermediate xk. Projected gradient descent will\nincorporate this condition.\n2.3.1 Projection onto Closed Convex Set\nFirst we must establish that it is possible to always be able to keep xkin the convex set C.\nOne approach is to take the closest point π(xk)∈ C.\nDeﬁnition: LetCbe a closed convex subset of IRd. Then∀x∈IRd, letπ(x)∈ Cbe\nthe minimizer of\n/ba∇dblx−π(x)/ba∇dbl= minx z\nz∈C/ba∇dbl − /ba∇dbl\nwhere/ba∇dbl·/ba∇dbldenotes the Euclidean norm. Then π(x) is unique and,\n/an}b∇acketle{tπ(x)−x,π(x)−z/an}b∇acket∇i}ht ≤0∀z∈ C (2.1)\nProof.From the deﬁnition of π:=π(x), we have /ba∇dblx−π/ba∇dbl2≤ /ba∇dblx−v/ba∇dbl2for anyv∈ C. Fix\nw∈ Cand deﬁne v= (1−t)π+twfort∈(0,1]. Observe that since Cis convex we have\nv∈ Cso that\n/ba∇dbl − /ba∇dbl2≤ /ba∇dbl − /ba∇dbl2 2x π x v =/ba∇dblx−π−t(w−π)/ba∇dbl\nExpanding the right-hand side yields\n/ba∇dbl2 2 2x−π/ba∇dbl ≤ /ba∇dblx−π/ba∇dbl −2t/an}b∇acketle{tx−π,w−π/an}b∇acket∇i}ht+t2/ba∇dblw−π/ba∇dbl\nThis is equivalent to\n/an}b∇acketle{tx−π,w− /an}b∇acket∇i}ht ≤2π t/ba∇dblw−π/ba∇dbl\nSince this is valid for all t∈(0,1), letting t→0 yields (2.1).\nProof of Uniqueness. Assumeπ1,π2∈ Csatisfy\n/an}b∇acketle{tπ1−x,π1−z/an}b∇acket∇i}ht ≤0∀z∈C\n/an}b∇acketle{tπ2−x,π2−z/an}b∇acket∇i}ht ≤0∀z∈C\nTakingz=π2in the ﬁrst inequality and z=π1in the second, we get\n/an}b∇acketle{tπ1−x,π1−π2/an}b∇acket∇i}ht ≤0\n/an}b∇acketle{tx−π2,π1−π2/an}b∇acket∇i}ht ≤0\nAdding these two inequalities yields /ba∇dblπ1−π2/ba∇dbl2≤0 so that π1=π2.\n65\n2.3.2 Projected Gradient Descent\nAlgorithm 1 Projected Gradient Descent algorithm\nInput:x1∈ C, positive sequence {ηs}s≥1\nfors= 1 tok−1do\nys+1=xs−ηsgs, gs∈∂f(xs)\nxs+1=π(ys+1)\nend for\nk1return Eitherx¯ =/summationdisplay\nxsorx◦∈argmin f(x)kxs={x1,...,x1∈ k}\nTheorem: LetCbe a closed, nonempty convex subset of IRdsuch that diam( C)≤R.\nLetfbe a convex L-Lipschitz function on\nRCsuch that x∗∈argminxf(x) exists.∈C\nThen ifηs≡η=L√thenk\nLR LRf(x¯)−f(x∗)≤√andf(x¯◦)−f(x∗)\nk≤√\nk\nMoreover, if ηs=R√, then∃c >0 such thatL s\nLR LRf(x¯)−f(x∗)≤c√andf(x¯◦)f(x∗)\nk− ≤ c√\nk\nProof.Again we will use the identity that 2 a⊤b=/ba∇dbla/ba∇dbl2+/ba∇dblb/ba∇dbl2−/ba∇dbl2a−b/ba∇dbl.\nBy convexity, we have\nf(xs)−f(x∗)≤gs⊤(xs−x∗)\n1= (xs−ys+1)⊤(xsη−x∗)\n1=2η/bracketleftBig\n/ba∇dbl2xs−ys+1/ba∇dbl+/ba∇dblxs−x∗/ba∇dbl2−/ba∇dblys+1−x∗/ba∇dbl2/bracketrightBig\nNext,\n/ba∇dblys+1−x∗/ba∇dbl2=/ba∇dbl2ys+1−xs+1/ba∇dbl+/ba∇dblxs+1−x∗/ba∇dbl2+2/an}b∇acketle{tys+1\n2−xs+1,xs+1−x∗/an}b∇acket∇i}ht\n=/ba∇dblys+1−xs+1/ba∇dbl+/ba∇dbl2xs+1−x∗/ba∇dbl+2/an}b∇acketle{tys+1−π(ys+1),π(ys+1)−x∗/an}b∇acket∇i}ht\n≥ /ba∇dblxs+1−x∗/ba∇dbl2\nwhere we used that /an}b∇acketle{tx−π(x),π(x)−z/an}b∇acket∇i}ht ≥0∀z∈ C, andx∗\n2 2∈ C. Also notice that\n/ba∇dblxs−2y2 2s+1/ba∇dbl=η/ba∇dblgs/ba∇dbl ≤η LsincefisL-Lipschitz with respect to /ba∇dbl·/ba∇dbl. Using this\nwe ﬁnd\nk k1\nk/summationdisplay 1 1( )−(∗)≤/summationdisplay2 2+∗2 ∗2f xsf x η L x sx x s+1xk2ηs=1 s=1/bracketleftBig\n/ba∇dbl − /ba∇dbl −/ba∇dbl − /ba∇dbl/bracketrightBig\nηL212ηL2R2\n≤+x2k/ba∇dbl1−x∗\n2η/ba∇dbl ≤ +2 2ηk\n66\n2 2Minimizing over ηwe getL=R\n2=⇒η=R√, completing the proof22η k L k\nRLf(x¯)−f(x∗)≤√\nk\n2\nMoreover, the proof of the bound for f(/summationtextk\nkxs)−f(x∗) is identical because xkxs=/vextenddouble/vextenddouble\n2−∗\n2/vextenddouble/vextenddouble≤\nR2as well./vextenddouble /vextenddouble\n2.3.3 Examples\nSupport Vector Machines\nThe SVM minimization as we have shown before is\nn1minn/summationdisplay\nmax(0,1Yifα(Xi))\nαR\n⊤∈In\n≤2i=1−\nαIKα C\nwherefα(Xi) =α⊤IKei=/summationtextn\n=1αjK(X ,j jXi). Forconvenience, call gi(α) = max(0 ,1−Yifα(Xi)).\nIn this case executing the projection onto the ellipsoid {α:α⊤IKα≤C2}is not too hard,\nbut we do not know about C,R, orL. We must determine these we can know that our\nbound is not exponential with respect to n. First we ﬁnd Land start with the gradient of\ngi(α):\n∇gi(α) = 1I(1−Yifα(Xi)≥0)YiIKei\nˆ With this we bound the gradient of the ϕ-riskRn,ϕ(fα) =1\nn\nn n/summationtextn\n=1gi(αi).\n/vextenddouble/vextenddouble∂ 1 1/vextenddoubleRˆn,ϕ(fα)/vextenddouble/vextenddouble/summationdisplay/vextenddouble=/vextenddouble/vextenddouble/vextenddouble∇gi(α)∂α/vextenddouble/vextenddoublen/vextenddouble\n/vextenddoublei=1/vextenddouble/vextenddouble≤/summationdisplay\nIKe/vextenddouble /vextenddouble in2\ni=1/ba∇dbl /ba∇dbl\nby the triangle inequality and the fact that that 1I(1/vextenddouble\n−Yifα(Xi)≥0)Yi≤1. We can now\nuse the properties of our kernel K. Notice that /ba∇dblIKei\n1\n2/ba∇dblis theℓ2norm of the ithcolumn so\n/ba∇dblIKei/ba∇dbln\n2=/parenleftBig/summationtext\nj=1K(Xj,Xi)2/parenrightBig\n. We also know that\nK(Xj,Xi)2=/an}b∇acketle{tK(X2j,·),K(Xi,·)/an}b∇acket∇i}ht ≤ /ba∇dblK(Xj,·)/ba∇dblKH/ba∇dbl(Xi,·)/ba∇dblH≤kmax\nCombining all of these we get\n1/vextenddouble n n2/vextenddouble∂ 1/vextenddoubleRˆn,ϕ(fα)/vextenddouble\n≤max\n/summationdisplay /summationdisplay/vextenddouble/vextenddoublek2=kmax√n=L/vextenddouble∂α/vextenddoubleni=1j=1\nTo ﬁndRwe try to evaluate diam {α⊤IKα≤C2}= 2 max√\nα\nα⊤⊤α. We can use the\nIKα≤C2\ncondition to put bounds on the diameter\nC2 2C≥α⊤IKα≥λmin(IK)α⊤α=⇒diam{α⊤IKα≤C2} ≤/radicalbig\nλmin(IK)\nWe need to understand how small λmincan get. While it is true that these exist random\nsamples selected by an adversary that make λmin= 0, we will consider a random sample of\n67\ni.i.dX1,...,X n∼ N(0,Id). This we can write these d-dimensional samples as a d×nmatrix\nX. We can rewrite the matrix IK with entries IK ij=K(Xi,Xj) =/an}b∇acketle{tXi,Xj/an}b∇acket∇i}htIRdas a Wishart\nmatrix IK = X⊤X(in particular,1Xd⊤Xis Wishart). Using results from random matrix\ntheory, if we take n,d→ ∞but holdnas a constant γ, thenλ(IK 2min) (d→1√−γ) . Takingd\nan approximation since we cannot take n,dto inﬁnity, we get\nλmin(IK)≃d/parenleftbiggn d1−2/radicalbigg\nd/parenrightbigg\n≥2\nusing the fact that d≫n. This means that λminbecoming too small is not a problem when\nwe model our samples as coming from multivariate Gaussians.\nNow we turn our focus to the number of iterations k. Looking at our bound on the\nexcess risk\nnRˆn,ϕ(f ˆα◦\nR)≤minRn,ϕ(fα)+C/radicalbigg\nkmax\nα⊤IKα≤C2 kλmin(IK)\nwe notice that our all of the constants in our stochastic term can be computed given the\nnumber of points and the kernel. Since statistical error is often √1, to be generous we wantn\nto have precision up to1\nnto allow for fast rates in special cases. This gives us\nn3k2C2\nk≥max\nλmin(IK)\nwhich is not bad since nis often not very big.\nIn [Bub15], the rates for many a wide rage of problems with various assumptions are\navailable. For example, if we assume strong convexity and Lipschitz we can get an exponen-\ntial rate so k∼logn. If gradient is Lipschitz, then we get get1\nkinstead of √1in the bound.k\nHowever, often times we are not optimizing over functions with these nice properties.\nBoosting\nWe already know that ϕisL-Lipschitz for boosting because we required it before.\nRemember that our optimization problem is\nn1min/summationdisplay\nϕ(−Yifα(Xi))\nαRNn\n|α∈I\n|1≤i=11\nwherefα=N\nj=1αjfjandfjis thejthweak classiﬁer. Remember before we had some rate\nlike/radicalBig\nlogNc/summationtext\nnand we would hope to get some other rate that grows with log NsinceNcan\nbe very large. Taking the gradient of the ϕ-loss in this case we ﬁnd\nN1∇Rˆn,ϕ(fα) =/summationdisplay\nϕ′(−Yifα(Xi))(−Yi)F(Xi)ni=1\nwhereF(x) is the column vector [ f1(x),...,fN(x)]⊤. Since|Yi| ≤1 andϕ′≤L, we can\nbound the ℓ2norm of the gradient as\n/vextenddouble nL /vextenddouble∇Rˆ/vextenddoublen,ϕ(fα)/vextenddouble/vextenddouble/vextenddouble\n2≤n/vextenddouble /vextenddouble/vextenddouble/vextenddouble/summationdisplay\nF(X/vextenddouble i)/vextenddoublei=1/vextenddouble\nn/vextenddouble\nL/vextenddouble/vextenddouble\n≤n/summationdisplay\n)\ni=/ba∇dblF(Xi\n1/ba∇dbl ≤L√\nN\n68\nusing triangle inequality and the fact that F(Xi) is aN-dimensional vector with each\ncomponent bounded in absolute value by 1.\nUsing the fact th√at the diameter of the ℓ1ball is 2, R= 2 and the Lipschitz associated\nwith our ϕ-risk isL NwhereLis the Lipschitz constant for ϕ. Our stochastic termR√L\nk\nbecomes 2 L/radicalBig\nN\nk. Imposing the same1\nnerror as before we ﬁnd that k∼N2n, which is very\nbad especially since we want log N.\n2.4 Mirror Descent\nBoosting is an example of when we want to do gradient descent on a non-Euclidean space,\nin particular a ℓ1space. While the dual of the ℓ2-norm is itself, the dual of the ℓ1norm is\ntheℓor sup norm. We want this appear if we have an ℓ1constraint. The reason for this ∞\nis not intuitive because we are taking about measures on the same space IRd, but when we\nconsider optimizations on other spaces we want a procedure that does is not indiﬀerent to\nthe measure we use. Mirror descent accomplishes this.\n2.4.1 Bregman Projections\nDeﬁnition: If/ba∇dbl·/ba∇dblis some norm on IRd, then/ba∇dbl·/ba∇dblis its dual norm.∗\nExample: If dual norm of the ℓpnorm/ba∇dbl·/ba∇dblpis theℓqnorm/ba∇dbl·/ba∇dblq, then1\np+1\nq= 1. This is the\nlimiting case of H¨ older’s inequality.\nIn general we can also reﬁne our bounds on inner products in IRdtox⊤y≤ /ba∇dblx/ba∇dbl/ba∇dbly/ba∇dblif∗\nwe consider xto be the primal and yto be the dual. Thinking like this, gradients live in\nthe dual space, e.g. in gs⊤(x−x∗),x−x∗is in the primal space, so gsis in the dual. The\ntranspose of the vectors suggest that these vectors come from spaces with diﬀerent measure,\neven though all the vectors are in IRd.\nDeﬁnition: Convex function Φ on a convex set Dis said to be\n(i) L-Lipschitz with respect to /ba∇dbl·/ba∇dblif/ba∇dblg/ba∇dbl∗≤L∀g∈∂Φ(x)∀x∈D\n(ii)α-strongly convex with respect to /ba∇dbl·/ba∇dblif\nαΦ(y)≥Φ(x)+g⊤(y−x)+2/ba∇dbly−x/ba∇dbl2\nfor allx,y∈Dand forg∈∂f(x)\nExample: If Φ is twice diﬀerentiable with Hessian Hand/ba∇dbl·/ba∇dblis theℓ2norm, then all\neig(H)≥α.\nDeﬁnition (Bregman divergence): For a given convex function Φ on a convex set\nDwithx,y∈ D, the Bregman divergence of yfromxis deﬁned as\nDΦ(y,x) = Φ(y)−Φ(x)−∇Φ(x)⊤(y−x)\n69\nThis divergence is the error of the function Φ( y) from the linear approximation at x.\nAlso note that this quantity is not symmetric with respect to xandy. If Φ is convex then\nDΦ(y,x)≥0 because the Hessian is positive semi-deﬁnite. If Φ is α-strongly convex then\nDΦ(y,x)≥α\n2/ba∇dbly−x/ba∇dbl2and if the quadratic approximation is good then this approximately\nholds in equality and this divergence behaves like Euclidean norm.\nProposition: Given convex function Φ on Dwithx,y,z∈ D\n(∇Φ(x)−∇Φ(y))⊤(x−z) =DΦ(x,y)+DΦ(z,x)−DΦ(z,y)\nProof.Looking at the right hand side\n= Φ(x)−Φ(y)−∇Φ(y)⊤(x−y)+Φ(z)−Φ(x)−∇Φ(x)⊤(z−x)\n−/bracketleftBig\nΦ(z)−Φ(y)−∇Φ(y)⊤(z−y)/bracketrightBig\n=∇Φ(y)⊤(y−x+z−y)−∇Φ(x)⊤(z−x)\n= (∇Φ(x)−∇Φ(y))⊤(x−z)\nDeﬁnition (Bregman projection): Givenx∈IRd, Φaconvex diﬀerentiablefunction\nonD ⊂ D¯ IRdand convex C⊂, the Bregman projection of xwith respect to Φ is\nπΦ(x)∈argminDφ(x,z)\nz∈C\n70\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 13\nScribe:Mina Karzand Oct. 21, 2015\nPreviously, we analyzed the convergence of the projected gradient descent algorithm.\nWe proved that optimizing the convex L-Lipschitz function fon a closed, convex set Cwith\ndiam(C)≤Rwith step sizes ηs=R√would give us accuracy of f(x)≤f(x∗)+LR\nL k√afterk\nkiterations.\nAlthough it might seem that projected gradient descent algorithm provides dimension-\nfree convergence rate, it is not always true. Reviewing the proof of convergence rate, we\nrealize that dimension-free convergence is possible when the objective function fand the\nconstraint set Care well-behaved in Euclidean norm (i.e., for all x∈ Candg∈∂f(x), we\nhave that |x|2and|g|2are independent of the ambient dimension). We provide an examples\nof the cases that these assumptions are not satisﬁed.\n•Consider the diﬀerentiable, convex function fon the Euclidean ball B2,nsuch that\n/ba∇dbl∇f(x)/ba∇dbl ≤1,∀x∈B2,n. This implies that f(x)√nand the projected ∞ |∇ | 2≤\ngradient descent converges to the minimum of finBn2,/radicalignat ratek. Using the\nlog(n)method of mirror descent we can get convergence rate of/radicalbig\nk\nTo get better rates of convergence in the optimization problem, we can use the Mirror\nDescent algorithm. The idea is to change the Euclidean geometry to a more pertinent\ngeometry to a problem at hand. We will deﬁne a new geometry by using a function which\nis sometimes called potential function Φ( x). We will use Bregman projection based on\nBregman divergence to deﬁne this geometry.\nThe geometric intuition behind the mirror Descent algorithm is the following: The\nprojected gradient described in previous lecture works in any arbitrary Hilbert space Hso\nthat thenormof vectors isassociated withaninnerproduct. Now, supposeweareinterested\nin optimization in a Banach space D. In other words, the norm (or the measure of distance)\nthat we use does not derive from an inner product. In this case, the gradient descent does\nnot even make sense since the gradient ∇f(x) are elements of dual space. Thus, the term\nx−η∇f(x) cannot be performed. (Note that in Hilbert space used in projected gradient\ndescent, the dual space of His isometric to H. Thus, we didn’t have any such problems.)\nThe geometric insight of the Mirror Descent algorithm is that to perform the optimiza-\ntion in the primal space D, one can ﬁrst map the point x∈ Din primal space to the dual\nspaceD∗, then perform the gradient update in the dual space and ﬁnally map the optimal\npoint back to the primal space. Note that at each update step, the new point in the primal\nspaceDmight be outside of the constraint set C ⊂ D, in which case it should be projected\ninto the constraint set C. The projection associate with the Mirror Descent algorithm is\nBergman Projection deﬁned based on the notion of Bergman divergence.\nDeﬁnition (Bregman Divergence): Forgivendiﬀerentiable, α-stronglyconvexfunc-\ntion Φ(x) :D →R, we deﬁne the Bregman divergence associated with Φ to be:\nDΦ(y,x) = Φ(y)−Φ(x)−∇Φ(x)T(y−x)\n71\nWe will usetheconvex openset D ⊂nRwhoseclosure contains theconstraint set C ⊂ D.\nBregman divergence is the error term of the ﬁrst order Taylor expansion of the function Φ\ninD.\nAlso, note that the function Φ( x) is said to be α-strongly convex w.r.t. a norm /ba∇dbl./ba∇dblif\nΦ(y)−Φ(x)−∇Φ(x)T α(y−x)≥2/ba∇dbly−x/ba∇dbl2.\nWe used the following property of the Euclidean norm:\n2a⊤b=/ba∇dbla/ba∇dbl2+/ba∇dblb/ba∇dbl2−/ba∇dbla−b/ba∇dbl2\nin the proof of convergence of projected gradient descent, where we chose a=xs−ys+1and\nb=xs−x∗.\nTo prove the convergence of the Mirror descent algorithm, we use the following property\noftheBregmandivergence inasimilarfashion. Thispropositionshowsthat theBregmandi-\nvergenceessentially behaves astheEuclideannormsquaredintermsof projections:\nProposition: Givenα-strongly diﬀerentiable convex function Φ : D →R, for all\nx,y,z∈ D,\n[∇Φ(x)−∇Φ(y)]⊤(x−z) =DΦ(x,y)+DΦ(z,x)−DΦ(z,y).\nAs described previously, the Bregman divergence is used in each step of the Mirror descent\nalgorithm to project the updated value into the constraint set.\nDeﬁnition (Bregman Projection): Givenα-strongly diﬀerentiable convex function\nΦ :D →Rand for all x∈ Dand closed convex set C ⊂ D\nΠΦ(x) = argmin DC Φ(z,x)\nz∈C∩D\n2.4.2 Mirror Descent Algorithm\nAlgorithm 1 Mirror Descent algorithm\nInput:x1∈argmin Φ( x),ζ:d dR R such that ζ(x) = Φ(x)C∩D → ∇\nfors= 1,···,kdo\nζ(ys+1) =ζ(xs)−ηgsforgs∈∂f(xs)\nxs+1= ΠΦ(yCs+1)\nend for\nreturn Eitherx=1\nk/summationtextk\ns=1xsorx◦∈argminx x1, ,xkf(x)∈{ ··· }\nProposition: Letz∈ C ∩D, then∀y∈ D,\n(∇Φ(π(y)−∇Φ(y))⊤(π(y)−z)≤0\n72\nMoreover, DΦ(z,π(y))≤DΦ(z,y).\nProof.Deﬁneπ= ΠΦ(y) andh(t) =DΦ(π+t(z−π),y).Sinceh(t) is minimized at t= 0C\n(due to the deﬁnition of projection), we have\nh′(0) =∇xDΦ(x,y)|x=π(z−π)≥0\nwhere suing the deﬁnition of Bregman divergence,\n∇xDΦ(x,y) =∇Φ(x)−∇Φ(y)\nThus,\n(∇Φ(π)−∇Φ(y))⊤(π−z)≤0.\nUsing proposition 1, we know that\n(∇Φ(π)−∇Φ(y))⊤(π−z) =DΦ(π,y)+DΦ(z,π)−DΦ(z,y)≤0,\nand since DΦ(π,y)≥0, we would have DΦ(z,π)≤DΦ(z,y).\nTheorem: Assume that fis convex and L-Lipschitz w.r.t. /ba∇dbl./ba∇dbl. Assume that Φ is\nα-strongly convex on C ∩ Dw.r.t./ba∇dbl./ba∇dbland\nR2= sup Φ( x) m\nx−in Φ(x)\n∈C∩D x∈C∩D\nta/radicaligkex1= argminxΦ(x) (assume that it exists). Then, Mirror Descent with η=∈C∩D\nR2α\nL Rgives,\n2 2f(x)−f(x∗)≤RL/radicalbigg\nandf(x◦)αk−f(x∗)≤RL/radicalbigg\n,αk\nProof.Takex♯∈ C ∩D. Similar to the proof of the projected gradient descent, we have:\n(i)\nf(xs)−f(x♯)≤gs⊤(xs−x♯)\n(ii)1= (ζ(x♯s)−ζ(ys+1))⊤(xs)η−x\n(iii)1= ( Φ( xs) Φ(ys+1))⊤(xsx♯)η∇ −∇ −\n(iv)1=/bracketleftig\nD♯Φ(xs,ys+1)+D♯Φ(x ,xs)η−DΦ(x ,ys+1)/bracketrightig\n(v)1≤/bracketleftig\nDΦ(xs,ys+1)+DΦ(x♯,xs)−DΦ(x♯,xs+1)η/bracketrightig\n(vi)ηL21≤+/bracketleftig\nDΦ(x♯,xs)2α2η−DΦ(x♯,xs+1)/bracketrightig\nWhere (i) is due to convexity of the function f.\n73\nEquations (ii) and (iii) are direct results of Mirror descent algorithm.\nEquation (iv) is the result of applying proposition 1.\nInequality (v) is a result of the fact that x= ΠΦ ♯s+1 (yCs+1), thus for x\n♯ ♯∈ C ∩ D, we have\nDΦ(x ,ys+1)≥DΦ(x ,xs+1).\nWe will justify the following derivations to prove inequality (vi):\n(a)DΦ(xs,ys+1) = Φ(xs)−Φ(ys+1)−∇Φ(ys+1)⊤(xs−ys+1)\n(b) α≤[∇Φ(x2s)−∇Φ(ys+1)]⊤(xs−ys+1)−2/ba∇dblys+1−xs/ba∇dbl\n(c) α≤η/ba∇dblgs/ba∇dbl∗/ba∇dblxs−ys+1/ba∇dbl−2/ba∇dblys+1−xs/ba∇dbl2\n(d)η2L2\n≤.2α\nEquation (a) is the deﬁnition of Bregman divergence.\nTo show inequality (b), we used the fact that Φ is α-strongly convex which implies that\nΦ(ys+1)−Φ(xs)≥ ∇Φ(xs)T(ys+1−xs)α\n2/ba∇dbly2s+1−xs/ba∇dbl.\nAccording to the Mirror descent algorithm, ∇Φ(xs)− ∇Φ(ys+1) =ηgs. We use H¨ older’s\ninequality to show that gs⊤(xs−ys+1)≤ /ba∇dblgs/ba∇dbl∗/ba∇dblxs−ys+1/ba∇dbland derive inequality (c).\nLookingatthequadraticterm ax−bx2fora,b >0,itisnothardtoshowthatmax ax\na−bx2=\n2. We use this statement with x=/ba∇dblys+1−xs/ba∇dbl,a=η gb/ba∇dbls/ba∇dblL4 ∗≤andb=αto derive2\ninequality (d).\nAgain, we use telescopic sum to get\nk1/summationdisplay ηL2D(x♯Φ,x1)[f(xs)f(x♯)] + . (2.1)k 2α kηs=1− ≤\nWe use the deﬁnition of Bregman divergence to get\nDΦ(x♯,x1) = Φ(x♯)−Φ(x1)−∇Φ(x1)(x♯−x1)\n≤Φ(x♯)−Φ(x1)\n≤sup Φ(x) min Φ( x)\nx x−\n∈C∩D ∈C∩D\n≤R2.\nWhere we used the fact x1∈argmin Φ( x) in the description of the Mirror Descent\n♯C∩D\nalgorithm to prove ∇Φ(x1)(x−x1)≥0. We optimize the right hand side of equation (2.1)\nforηto get\nk1/summationdisplay\n(x♯ 2[f(xs)−f)]ks=≤RL\n1/radicalbigg\n.αk\nTo conclude the proof, let x♯→x∗∈ C.\nNote that with the right geometry, we can get projected gradient descent as an instance\nthe Mirror descent algorithm.\n74\n2.4.3 Remarks\nThe Mirror Descent is sometimes called Mirror Prox. We can write xs+1as\nxs+1= argmin DΦ(x,ys+1)\nx∈C∩D\n= argminΦ( x)\nx−∇Φ⊤(ys+1)x\n∈C∩D\n= argminΦ( x) xs\nx−[∇Φ( )−ηgs]⊤x\n∈C∩D\n= argmin η(gs⊤x)+Φ(x)\nx−∇Φ⊤(xs)x\n∈C∩D\n= argmin η(gs⊤x)+DΦ(x,xs)\nx∈C∩D\nThus, we have\nxs+1= argmin η(gs⊤x)+DΦ(x,xs).\nx∈C∩D\nTo getxs+1, in the ﬁrst term on the right hand side we look at linear approximations\nclose toxsin the direction determined by the subgradient gs. If the function is linear, we\nwould just look at the linear approximation term. But if the function is not linear, the\nlinear approximation is only valid in a small neighborhood around xs. Thus, we penalized\nby adding the term DΦ(x,xs). We can penalized by the square norm when we choose\nDΦ(x,xs) =/ba∇dblx−xs/ba∇dbl2. In this case we get back the projected gradient descent algorithm\nas an instance of Mirror descent algorithm.\nBut if we choose a diﬀerent divergence DΦ(x,xs), we are changing the geometry and we\ncan penalize diﬀerently in diﬀerent directions depending on the geometry.\nThus, using the Mirror descent algorithm, we could replace the 2-norm in projected\ngradient descent algorithm by another norm, hoping to get less constraining Lipschitz con-\nstant. On the other hand, the norm is a lower bound on the strong convexity parameter.\nThus, there is trade oﬀ in improvement of rate of convergence.\n2.4.4 Examples\nEuclidean Setup:\nΦ(x) =1x2, =dR, Φ(x) =ζ(x) =x. Thus, the updates will be similar to2/ba∇dbl /ba∇dbl D ∇\nthe gradient descent.\n1DΦ(y,x) =/ba∇dbly/ba∇dbl21− /ba∇dblx2\n2/ba∇dbl2x2−⊤y+/ba∇dblx/ba∇dbl\n1=/ba∇dblx−y/ba∇dbl2.2\nThus, Bregman projection with this potential function Φ( x) is the same as the usual Eu-\nclidean projection and the Mirror descent algorithm is exactly the same as the projected\ndescent algorithm since it has the same update and same projection operator.\nNote that α= 1 since D1Φ(y,x)≥2/ba∇dblx−y/ba∇dbl2.\nℓ1Setup:\nWe look at D=dR+\\{0}.\n75\nDeﬁne Φ( x) to be the negative entropy so that:\nd\nΦ(x) =/summationdisplay\nxilog(xi), ζ(x) =∇Φ(x) ={1+log(xdi)\ni=1}i=1\n(s+1)∇(s)−(s+1)Thus, looking at the update function y= Φ(x)ηgs, we get log( yi) =\n(s)log(xi)−(s) (s+1) ( s) (s)ηgiand for all i= 1,···,d, we have yi=xiexp(−ηgi). Thus,\ny(s)=x(s)exp(−ηg(s)).\nWe call this setup exponential Gradient Descent or Mirror Descent with multiplicative\nweights.\nThe Bregman divergence of this mirror map is given by\nDΦ(y,x) = Φ(y)−Φ(x)−∇Φ⊤(x)(y−x)\n/summationdisplayd /summationdisplayd d\n=yilog(yi)−xilog(xi) (1+log( xi))(yixi)\ni 1/summationdisplay\ni−\n=1 i−\n= =1\n/summationdisplaydyi=yilog( )+xii=1/summationdisplayd\n(yi\ni=1−xi)\nNote that/summationtextd\ni=1yilog(yi\ni) is call the Kullback-Leibler divergence (KL-div) between yx\nandx.\nWe show that the projection with respect to this Bregman divergence on the simplex\n∆d={x∈dR:d\ni=1xi= 1,xi≥0}amounts to a simple renormalization y/ma√sto→y/|y|1. To\nprove so, we prov/summationtext\nide the Lagrangian:\n/summationdisplayd /summationdisplayd dyLi=yilog( )+ ( xixii=1 i=−yi)+λ(\n1/summationdisplay\nxi\ni=1−1).\nTo ﬁnd the Bregman projection, for all i= 1,···,dwe write\n∂ yL −i= +1+ λ= 0∂xixi\nThus, for all i, we have xi=γyi. We know that/summationtextd\ni=1xi= 1. Thus, γ=1/summationtextyi.\nThus, we have ΠΦ y\n∆d(y) =\n1. The Mirror Descent algorithm with this update and|y|\nprojection would be:\nys+1=xsexp(−ηgs)\nyxs+1=.|y|1\nTo analyze the rate of convergence, we want to study the ℓ1norm on ∆ d. Thus, we have\nto show that for some α, Φ isα-strongly convex w.r.t |·|1on ∆d.\n76\nDΦ(y,x) =KL(y,x)+/summationdisplay\n(xi\ni−yi)\n=KL(y,x)\n1≥2|x−y|2\n1\nWhere we used the fact that x,y∈∆dto showi(xi−yi) = 0 and used Pinsker\ninequality show the result. Thu/summationtexts, Φ is 1-strongly conve/summationtext\nx w.r.t.|·|1on ∆d.\nRemembering that Φ( x) =d\ni=1xilog(xi) was deﬁned to be negative entropy, we know\nthat−log(d)≤Φ(x)≤0 forx∈∆d. Thus,\nR2= maxΦ( x)\nx∈∆d−min Φ(x) = log(d).\nx∈∆d\nCorollary: Letfbe a convex function on ∆ dsuch that\n/ba∇dblg/ba∇dbl∞≤L,∀g∈∂f(x),∀x∈∆d.\n2log(d)Then, Mirror descent with η=1/radicalig\ngivesL k\n2log(d) 2log(d)f(xk)−f(x∗)≤L/radicalbigg\n, f(x◦\nk)−f(x∗)k≤L/radicalbigg\nk\nBoosting: For weak classiﬁers f1(x),···,fN(x) andα∈∆n, we deﬁne\nN f1(x)\n.fα=\n/summationdisplay\nαjfjandF(x) =\nj=1..\nfN(x)\nso thatfα(x) is the weighted majority vote classiﬁer. Note that |F|∞≤1.\nAs shown before, in boosting, we have:\nn\ng=∇R/hatwide1\nn,φ(fα) =/summationdisplay\nφ′(−yifα(xi))(−yi)F(xi),ni=1\nSince|F| ≤1 and|y| ≤1, then|g| ≤LwhereLis the Lipschitz constant of φ ∞ ∞ ∞\n(e.g., a constant like eor 2).\n/radicalbigg\n/hatwide /hatwide2log(N)Rn,φ(fα◦\nk)−minRn,φ(fα)L\nα∈∆n≤k\nWe need the number of iterations k≈n2log(N).\nThe functions fj’s could hit all the vertices. Thus, if we want to ﬁt them in a ball, the\n/radicaligball has to be radius√\nN. This is why the projected gradient descent would give the rate of\nN\nk. But by looking at the gradient we can determine the right geometry. In this case, the\ngradient is bounded by sup-norm which is usually the most constraining norm in projected\n77\ngradient descent. Thus, using Mirror descent would be most beneﬁcial.\nOther Potential Functions:\nThere are other potential functions which are strongly convex w.r.t ℓ1norm. In partic-\nular, for\n1Φ(x) =|x|p 1\np, p= 1+p log(d)\nthen Φ is c/radicalbig\nlog(d)-strongly convex w.r.t ℓ1norm.\n78\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 14\nScribe:Sylvain Carpentier Oct. 26, 2015\nIn this lecture we will wrap up the study of optimization techniques with stochastic\noptimization. The tools that we are going to develop will turn out to be very eﬃcient in\nminimizing the ϕ-risk when we can bound the noise on the gradient.\n3. STOCHASTIC OPTIMIZATION\n3.1 Stochastic convex optimization\nWeareconsideringrandomfunctions x/ma√sto→ℓ(x,Z)wherexistheoptimization parameterand\nZa random variable. Let PZbe the distribution of Zand let us assume that x/ma√sto→ℓ(x,Z) is\nconvexPZa.s. In particular, IE[ ℓ(x,Z)] will also be convex. The goal of stochastic convex\noptimization is to approach min xIE[ℓ(x,Z)] when is convex. For our purposes, will ∈C C C\nbe a deterministic convex set. However, stochastic convex optimization can be deﬁned more\nbroadly. The constraint can be itself stochastic :\nC={x,IE[g(x,Z)]≤0}, gconvexPZa.s.\nC={x,IP[g(x,Z)≤0]≥1−ε},“chance constraint”\nThe second constraint is not convex a priori but remedies are possible (see [NS06, Nem12]).\nIn the following, we will stick to the case where Xis deterministic. A few optimization\nproblems we tackled can be interpreted in this new framework.\n3.1.1 Examples\nBoosting. Recall that the goal in Boosting is to minimize the ϕ-risk:\nminIE[ϕ(f\n∈Λ−Yα(X))],\nα\nwhere Λ is the simplex of IRd. Deﬁne Z= (X,Y) and the random function ℓ(α,Z) =\nϕ(−Yfα(X)), convex PZa.s.\nLinear regression. Here the goal is the minimize the ℓ2risk:\nmin IE[(Y−f2α(X)) ].\nα∈IRd\nDeﬁneZ= (X,Y) and the random function ℓ(α,Z) = (Y−fα(X))2, convex PZa.s.\nMaximum likelihood. We consider samples Z1,...,Z niid with density pθ,θ∈Θ. For\ninstance, ZN(θ,1). The likelihood functions associated to this set of samples is θ ∼ /ma√sto→/producttextn\n=1pθ(Zi). Letp∗(Z) denote the true density of Zi (it does not have to be of the form pθ\nforsomeθ∈Θ. Then\nn1/productdisplay/integraldisplayp∗(z)IE[log pθ(Zi)] =−log( ) p∗(z)dz+C=n pθ(z)i=1−KL(p∗,pθ)+C\n79\nwhereCis a constant in θ. Hence maximizing the expected log-likelihood is equivalent to\nminimizing the expected Kullback-Leibler divergence:\nn\nmaxIE[log/productdisplay\npθ(Zi)]\nθi=1⇐⇒KL(p∗,pθ)\nExternal randomization. Assume that we want to minimize a function of the form\n1f(x) =n/summationdisplay\nfi(x),ni=1\nwhere the functions f1,...,fnare convex. As we have seen, this arises a lot in empirical\nrisk minimization. In this case, we treat this problem as deterministic problem but inject\nartiﬁcial randomness as follows. Let Ibe a random variable uniformly distributed on\n[n] =:{1,...,n}. We have the representation f(x) = IE[fI(x)], which falls into the context\nof stochastic convex optimization with Z=Iandℓ(x,I) =fI(x).\nImportant Remark :There is a key diﬀerence between the case where we assume that\nwearegivenindependentrandomvariablesandthecasewherewegenerateartiﬁcialran-\ndomness. LetusillustratethisdiﬀerenceforBoosting. Wearegiven( X1,Y1),...,(Xn,Yn)\ni.i.d from some unknown distribution. In the ﬁrst example, our aim is to minimize\nIE[ϕ(−Yfα(X))] based on these nobservations and we will that the stochastic gradient\nallows to do that by take one pair ( Xi,Yi) in each iteration. In particular, we can use\neach pair at most once. We say that we do one pass on the data.\nWe could also leverage our statistical analysis of the empirical risk minimizer from\nprevious lectures and try to minimize the empirical ϕ-risk\n1Rˆn,ϕ(fα) =n/summationdisplay\nϕ(α\ni=1−Yif(Xi))n\nby generating kindependent random variables I1,...,Ikuniform over [ n] and run the\nstochasticgradientdescenttousonerandomvariable Ijineachiteration. Thediﬀerence\nhereisthat kcanbearbitrarylarge, regardlessofthenumber nofobservations(wemake\nmultiple passes onthedata). However, minimizingIE I[ϕ(−YIfα(XI))|X1,Y1,...,X n,Yn]\nwill perform no better than the empirical risk minimizer whose statistical performance\nis limited by the number nof observations.\n3.2 Stochastic gradient descent\nIf the distribution of Zwas known, then the function x/ma√sto→IE[ℓ(x,Z)] would be known and\nwe could apply gradient descent, projected gradient descent or any other optimization tool\nseen before in the deterministic setup. However this is not the case in reality where the\ntrue distribution PZis unknown and we are only given the samples Z1,...,Z nand the\nrandom function ℓ(x,Z). In what follows, we denote by ∂ℓ(x,Z) the set of subgradients of\nthe function y/ma√sto→ℓ(y,Z) at point x.\n80\nAlgorithm 1 Stochastic Gradient Descent algorithm\nInput:x1∈ C, positive sequence {ηs}s1, independent random variables Z ,...,Z ≥ 1 k\nwith distribution PZ.\nfors= 1 tok−1do\nys+1=xs−ηsg˜s, g˜s∈∂ℓ(xs,Zs)\nxs+1=π(yCs+1)\nend for\n1return x¯k=k\nk/summationdisplay\nxs\ns=1\nNote the diﬀerence here with the deterministic gradient descent which returns either\nx¯korx◦\nk= argmin f(x). In the stochastic framework, the function f(x) = IE[ℓ(x,ξ)] is\nx1,...,xn\ntypically unknown and x˚kcannot be computed.\nTheorem: LetCbea closed convex subset of IRdsuch that diam( C)≤R. Assume that\nhe convex function f(x) = IE[ℓ(x,Z)] attains its minimum on Catx∗∈IRd. Assume\nthatℓ(x,Z) is convex PZa.s. and that IE /ba∇dblg˜/ba∇dbl2≤L2for allg˜∈∂ℓ(x,Z) for allx. Then\nifηs≡η=R\nL√,k\nLRIE[f(x¯k)]−f(x∗)≤√\nk\nProof.\nf xsf x g sxsx\n= IE[g˜s⊤(xs−x∗)|xs]\n1= IE[(ys+1xs)⊤(xsx∗)xs]η− − |\n1= IE[/ba∇dblx2 2s−y2s+1/ba∇dbl+η/ba∇dblxs−x∗\n2/ba∇dbl −/ba∇dblys+1−x∗/ba∇dbl |xs]\n1≤(η2IE[/ba∇dblg˜s/ba∇dbl2|xs]+IE[/ba∇dblx2s−x∗/ba∇dbl |xs]−IE[/ba∇dblxs+1−x∗xη/ba∇dbl2\n2|s]\nTaking expectations and summing over swe get\nk1/summationdisplay ηL2R2\nf(xs) (\ns=1−f x∗)k≤+.2 2ηk\nUsing Jensen’s inequality and chosing η=R\nL√, we getk\nLRIE[f(x¯k)]−f(x∗)≤√\nk( )−(∗)≤⊤(−∗)\n81\n3.3 Stochastic Mirror Descent\nWe can also extend the Mirror Descent to a stochastic version as follows.\nAlgorithm 2 Mirror Descent algorithm\nInput: x1∈argmin Φ( x),ζ:dR→dRsuch that ζ(x) =∇Φ(x), independentC∩D\nrandom variables Z1,...,Z kwith distribution PZ.\nfors= 1,···,kdo\nζ(ys+1) =ζ(xs)−ηg˜sforg˜s∈∂ℓ(xs,Zs)\nxΦs+1= Π (yCs+1)\nend for\nreturn x=1k\nk/summationtext\ns=1xs\nTheorem: Assume that Φ is α-strongly convex on C ∩ Dw.r.t./ba∇dbl·/ba∇dbland\nR2= sup Φ( x) Φ x)\nx−min (\n∈C∩D x∈C∩D\ntakex1= argminxΦ(x) (assume that it exists). Then, Stochastic Mirror Descent∈C∩D\nwithη=R\nL/radicalBig\n2αxRoutputs ¯ k, such that\nIE[f(x¯k)]−f(x∗)≤RL/radicalbigg\n2.αk\nProof.We essentially reproduce the proof for the Mirror Descent algorithm.\nTakex♯∈ C ∩D. We have\nf(xs ss\nIE[g˜s⊤(xs−x∗)|xs]\n1= IE[(ζ(xs)η−ζ(ys+1))⊤(xs−x♯)|xs]\n1= IE[(∇Φ(xs)−∇Φ(ys+1))⊤(xsη−x♯)|xs]\n1= IE/bracketleftBig\nD♯Φ(xs,ys+1)+DΦ(x♯,xs)−DΦ(x ,ys+1)η/vextendsingle/vextendsinglexs/bracketrightBig\n1≤IE/bracketleftBig\nDΦ(x♯s,ys+1)+DΦ(x ,xs)−DΦ(x♯,xs+1)xη/vextendsingle/vextendsingles/bracketrightBig\nη≤2IE[/ba∇dblg˜s/ba∇dbl21|xs]+ IE2α∗η/bracketleftBig\nDΦ(x♯,xs)−DΦ(x♯,xs+1)/vextendsingle/vextendsinglexs/bracketrightBig)−f(x♯)≤g⊤(x−x♯)\n82\nwhere the last inequality comes from\nDΦ(xs,ys+1) = Φ(xs)−Φ(ys+1)−∇Φ(ys+1)⊤(xs−ys+1)\nα≤[∇Φ(x2s)−∇Φ(ys+1)]⊤(xs−ys+1)−2/ba∇dblys+1−xs/ba∇dbl\nα≤η/ba∇dblg˜s/ba∇dblx y y x2∗/ba∇dbls−s+1/ba∇dbl−2/ba∇dbls+1−s/ba∇dbl\nη2/ba∇dblg˜≤s/ba∇dbl2\n∗.2α\nSumming and taking expectations, we get\nk1/summationdisplay ηL2DΦ(x♯,x1)[f(x♯s) ]\ns=1≤+k−f(x) . (3.1)2α kη\nWe conclude as in the previous lecture.\n3.4 Stochastic coordinate descent\nLetfbe a convex L-Lipschitz and diﬀerentiable function on IRd. Let us denote by ∇ifthe\npartial derivative of fin the direction ei. One drawback of the Gradient Descent Algorithm\nis that at each step one has to update every coordinate ∇ifof the gradient. The idea of\nthe stochastic coordinate descent is to pick at each step a direction ejuniformly and to\nchoose that ejto be the direction of the descent at that step. More precisely, of Iis drawn\nuniformly on [ d], then IE[ d∇If(x)eI] =∇f(x). Therefore, the vector d∇If(x)eIthat has\nonly one nonzero coordinate is an unbiased estimate of the gradient ∇f(x). We can use\nthis estimate to perform stochastic gradient descent.\nAlgorithm 3 Stochastic Coordinate Descent algorithm\nInput: x1∈ C, positive sequence {ηs}s1, independent random variables I ,...,I ≥ 1 k\nuniform over [ d].\nfors= 1 tok−1do\nys+1=xs−ηsd∇If(x)eI, g˜s∈∂ℓ(xs,Zs)\nxs+1=π(ys+1) C\nend for\nk1return x¯k=k/summationdisplay\nxs\ns=1\nIf we apply Stochastic Gradient Descent to this problem for η=R/radicalBig\n2, we directlyL dk\nobtain\n2dIE[f(x¯k)]−f(x∗)≤RL/radicalbigg\nk\nWe are in a trade-oﬀ situation where the updates are much easier to implement but where\nwe need more steps to reach the same precision as the gradient descent alogrithm.\n83\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 15\nScribe:Zach Izzo Oct. 27, 2015\nPart III\nOnline Learning\nIt is often the case that we will be asked to make a sequence of predictions, rather\nthan just one prediction given a large number of data points. In particular, this situa-\ntion will arise whenever we need to perform online classiﬁcation: at time t, we have\n(X1,Y1),...,(Xt−1,Yt−1) iid random variables, and given Xt, we are asked to predict\nYt∈ {0,1}. Consider the following examples.\nOnline Shortest Path: We have a graph G= (V,E) with two distinguished vertices\nsandt, and we wish to ﬁnd the shortest path from stot. However, the edge weights\nE1,...,E tchange with time t. Our observations after time tmay be all of the edge weights\nE1,...,E t; or our observations may only be the weights of edges through which our path\ntraverses; orourobservationmayonlybethesumoftheweightsoftheedgeswe’vetraversed.\nDynamic Pricing: We have a sequence of customers, each of which places a value vt\non some product. Our goal is to set a price ptfor thetth customer, and our reward for\ndoing so is ptifpt≤vt(in which case the customer buys the product at our price) or 0\notherwise (in which case the customer chooses not to buy the product). Our observations\nafter time tmay bev1,...,vt; or, perhaps more realistically, our observations may only be\n1I(p1< v1),...,1I(pt< vt). (In this case, we only know whether or not the customer bought\nthe product.)\nSequential Investment: GivenNassets, a portfolio is ω∈∆N={x∈IRn:xi≥\n0,/summationtextNxi=1i= 1}. (ωtells what percentage of our funds to invest in each stock. We could\nalso allow for negative weights, which would correspond to shorting a stock.) At each time\nt, we wish to create a portfolio ωt∈∆Nto maximize ωT\ntzt, wherezt∈IRNis a random\nvariable which speciﬁes the return of each asset at time t.\nThere are two general modelling approaches we can take: statistical or adversarial.\nStatistical methods typically require that the observations are iid, and that we can learn\nsomethingaboutfuturepointsfrompastdata. Forexample, inthedynamicpricingexample,\nwe could assume vt∼N(v,1). Another example is the Markowitz model for the sequential\ninvestment example, in which we assume that log( zt)∼ N(µ,Σ).\nIn this lecture, we will focus on adversarial models. We assume that ztcan be any\nbounded sequence of numbers, and we will compare our predictions to the performance of\nsome benchmark. In these types of models, one can imagine that we are playing a game\nagainst an opponent, and we are trying to minimize our losses regardless of the moves he\nplays. In this setting, we will frequently use optimization techniques such as mirror descent,\nas well as approaches from game theory and information theory.\n84\n1. PREDICTION WITH EXPERT ADVICE\n1.1 Cumulative Regret\nLetAbe a convex set of actions we can take. For example, in the sequential investment\nexample, A= ∆N. If our options are discrete–for instance, choosing edges in a graph–then\nthink ofAas the convex hull of these options, and we can play one of the choices randomly\naccording to some distribution. We will denote our adversary’s moves by Z. At time t,\nwe simultaneously reveal at∈ Aandzt∈ Z. Denote by ℓ(at,zt) the loss associated to the\nplayer/decision maker taking action atand his adversary playing zt.\nInthegeneral case,/summationtextn\ntℓ=1(at,zt)canbearbitrarilylarge. Therefore, ratherthanlooking\nat theabsoluteloss foraseriesof nsteps, wewill compareourloss totheloss ofabenchmark\ncalled an expert. An expert is simply some vector b∈ An,b= (b1,...,bt,...,bTn) . If we\nchooseKexpertsb(1),...,b(K), then our benchmark value will be the minimum cumulative\nloss amongst of all the experts:\nn\n(j)benchmark = min\n≤j≤K/summationdisplay\nℓ(bt,zt).\n1t=1\nThecumulative regret is then deﬁned as\nn n\nRn=/summationdisplay(j)ℓ(at,zt)−min ℓ(bt,zt).\n1≤j≤Kt=1/summationdisplay\nt=1\nAt timet, we have access to the following information:\n1. All of our previous moves, i.e. a1,...,at−1,\n2. all of our adversary’s previous moves, i.e. z1,...,zt−1, and\n3. All of the experts’ strategies, i.e. b(1),...,b(K).\nNaively, one might try a strategy which chooses a=b∗ ∗tt, where bis the expert which\nhas incurred minimal total loss for times 1 ,...,t−1. Unfortunately, this strategy is easily\nexploitable by the adversary: he can simply choose an action which maximizes the loss for\nthat move at each step. To modify our approach, we will instead take a convex combination\nof the experts’ suggested moves, weighting each according to the performanceof that expert\nthus far. To that end, we will replace ℓ(at,zt) byℓ(p,(bt,zt)), where p∈∆Kdenotes a\n(1) ( K)convex combination, bt= (bt,...,bt)T∈ AKis the vector of the experts’ moves at time\nt, andzt∈ Zis our adversary’s move. Then\nn n\nRn=/summationdisplay\nℓ(pt,zt)−min ℓ(ej,zt)\n1≤j≤Kt=1/summationdisplay\nt=1\nwhereejis the vector whose jth entry is 1 and the rest of the entries are 0. Since we are\nrestricting ourselves to convex combinations of the experts’ moves, we can write A= ∆K.\nWe can now reduce our goal to an optimization problem:\nK n\nmin\n∈∆K/summationdisplay\nθj\nθj=1/summationdisplay\nℓ(ej,zt).\nt=1\n85\nFrom here, one option would be to use a projected gradient descent type algorithm: we\ndeﬁne\nqt+1=pt−η(ℓ(eT1,zt),...,ℓ(eK,zT))\nKand then p∆t+1=π(pt) to be the projection of qt+1onto the simplex.\n1.2 Exponential Weights\nSuppose we instead use stochastic mirror descent with Φ = negative entropy. Then\nqtqt+1,j=pt+1,jexp(−ηℓ(ej,zt)), pt+1,j= ,/summationtextK\nlq=1t+1,l\nwhere we have deﬁned\nK/parenleftBigg\nwt,jpt= ej, expKwj=1 l=1,l/parenrightBigg\nwt,j=\nt/parenleftBiggt−1 /summationdisplay\n−η/summationdisplay\nℓ(ej,zs)\ns=1/parenrightBigg\n.\nThis process looks at the los/summationtext\ns from each expert and downweights it exponentially according\nto the fraction of total loss incurred. For this reason, this method is called an exponential\nweighting (EW) strategy .\nRecall the deﬁnition of the cumulative regret Rn:\nn n\nRn=/summationdisplay\nℓ(pt,zt)−min\n1≤j≤Kt=1/summationdisplay\nℓ(ej,zt).\nt=1\nThen we have the following theorem.\nTheorem: Assume ℓ(·,z) is convex for all z∈ Zand that ℓ(p,z)∈[0,1] for all p∈\n∆K,z∈ Z. Then the EW strategy has regret\nlogK ηnRn≤ +.η2\nIn particular, for η=/radicalBig\n2logK,n\nRn≤/radicalbig\n2nlogK.\nProof.We will recycle much of the mirror descent proof. Deﬁne\nK\nft(p) =/summationdisplay\npjℓ(ej,zt).\nj=1\nDenote/bardbl·/bardbl:=|·|1. Then\nn n1/summationdisplay η1/bardblg/bardbl2tlogKft( (∗pt)−f∗t)≤n/summationtext\nt=1p +,n 2 ηnt=1\n86\nwheregt∈∂ft(pt) and/bardbl · /bardbl∗is the dual norm (in this case /bardbl · /bardbl∗=| · |∞). The 2 in the\ndenominator of the ﬁrst term of this sum comes from setting α= 1 in the mirror descent\nproof. Now,\ngt∈∂ft(pt)⇒gt= (ℓ(e1,zt),...,ℓ(eTK,zt)).\nFurthermore, since ℓ(p,z)∈[0,1], we have /bardblgt/bardbl∗=|gt|∞≤1 for all t. Thus\nnη1\nn/summationtext\nt=1/bardblgt/bardbl2\n∗logK η logK+ ≤+.2 nη2ηn\nSubstituting for ftyields\nn K K n/summationdisplay/summationdisplay ηnlogKpt,jℓ(ej,zt)−min/summationdisplay\npjℓ(ej,zt)≤+.\np∈∆K 2ηt=1j=1 j=1/summationdisplay\nt=1\nNote that the boxed term is actually min 1≤j≤K/summationtextnℓt=1(ej,zt). Furthermore, applying\nJensen’s to the unboxed term gives\nn K n /summationdisplay/summationdisplay\npt,jℓ(ej,zt)≥/summationdisplay\nℓ(pt,zt).\nt=1j=1 t=1\nSubstituting these expressions then yields\nηnlogKRn≤+.2η\nWe optimize over ηto reach the desired conclusion.\nWe now oﬀer a diﬀerent proof of the same theorem which will give us the optimal\nconstant in the error bound. Deﬁne\n/parenleftBiggt−1/parenrightBiggK K/summationdisplay /summationdisplay/summationtextwt,jej=1 jwt,j= exp−η ℓ(ej,zs), Wt=wt,j, pt= .Wts=1 j=1\nFort= 1, we initialize w1,j= 1, soW1=K. It should be noted that the starting values for\nw1,jare uniform, so we’re starting at the correct point (i.e. maximal entropy) for mirrored\ndescent. Now we have\n/parenleftbigg Kexp−t−1η ℓ ,zW j=1 s=1(ej s) exp(−ηℓ(ej,zt))t+1log gW/summationtext\n\nK tt/parenrightbigg\n= lo/parenleftBig\n· /summationtext−1\nl ℓ e=e/summationtext\nxp/parenleftBig\n−η1/summationtext\nj,z=1/parenrightBig\n(ls)\n= log(IE J∼pt[exp(−ηℓ(eJ,zt))])/parenrightBig\n12Hoeﬀding’s lemma ⇒ ≤log/parenleftBig\nηIEe e−ηJ J8ℓ(e ,zt)\nη2/parenrightBig\n=−ηIEJℓ(eJ,zt)8\nη2η2\nJensen’s ⇒ ≤ − ηℓ(IEJeJ,zt) =−ηℓ(pt,zt)8 8\n87\nsince IE Jej=/summationtextKpj=1t,jej. If we sum over t, the sum telescopes. Since W1=K, we are left\nwithnnη2\nlog(Wn+1)−log(K)≤ − η/summationdisplay\nℓ(pt,zt).8t=1\nWe have\nK n\nlog(Wn+1) = log\n/summationdisplay\nexp/parenleftBigg\n−η/summationdisplay\nℓ(ej,zs)\nj=1 s=1/parenrightBigg\n,\nso settingnj∗= argmin1≤j≤K/summationtextℓ(e \nt=1j,zt), we obtain\nn n\nlog(Wn+1)≥log/parenleftBigg\nexp/parenleftBigg\n−η/parenrightBigg/parenrightBigg/summationdisplay\nℓ(ej∗,zs) =−η/summationdisplay\nℓ(ej∗,zt).\ns=1 t=1\nRearranging, we have\nn n /summationdisplay ηnlogKℓ(pt,zt)−ℓ\nt=/summationdisplay\n(ej∗,zt)≤+.8η1 t=1\nFinally, we optimize over ηto arrive at\nη=/radicalbigg\n8logK oRn≤n/radicalbigg\nnl gK⇒ .2\nThe improved constant comes from the assumption that our loss lies in an interval of size\n1 (namely [0 ,1]) rather than in an interval of size 2 (namely [ −1,1]).\n88\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 16\nScribe:Haihao (Sean) Lu Nov. 2, 2015\nRecall that in last lecture, we talked about prediction with expert advice. Remember\nthatl(ej,zt) means the loss of expert jat timet, whereztis one adversary’s move. In this\nlecture, for simplexity we replace the notation ztand denote by ztthe loss associated to all\nexperts at time t:\nzt=\nℓ(e1,zt)\n... ,\nℓ(eK,zt)\nwhereby for p∈∆K,p \n⊤zt=Kp ej=1jℓ(j,zt). This gives an alternative deﬁnition of ft(p)\nin last lecture. Actually it is easy to check ft(p) =p⊤zt, thus we can rewrite the theorem\nfor exponential weighting(EW/summationtext\n) strategy as\nn n\nRn≤/summationdisplay\np⊤\ntzt−minp\npt=1∈∆k/summationdisplay⊤zt2nlogK,\nt=1≤/radicalbig\nwhere the ﬁrst inequality is Jensen inequality:\nn n/summationdisplay\np⊤\nzzt\nt=1≥/summationdisplay\nℓ(pz,zt).\nt=1\nWe consider EW strategy for bounded convex losses. Without loss of generality, we\nassumeℓ(p,z)∈[0,1], for all ( p,z)∈∆K×Z, thus in notation here, we expect pt∈∆K\nandzK ¯t∈[0,1] . Indeed if ℓ(p,z)∈[m,M] then one can work with a rescaled loss ℓ(a,z) =\nℓ(a,z)−m. Note that now we have bounded gradient on pt, sinceztis bounded.M−m\n2. FOLLOW THE PERTURBED LEADER (FPL)\nIn this section, we consider a diﬀerent strategy, called Follow the Perturbed Leader.\nAt ﬁrst, we introduce Follow the Leader strategy, and give an example to show that\nFollow the Leader can be hazardous sometimes. At time t, assume that choose\nt−1\npt= argmin\np∈∆K/summationdisplay\np⊤zs.\ns=1\nNotethat thefunctiontobeoptimized islinear in p, wherebytheoptimal solutionshould\nbe a vertex of the simplex. This method can be viewed as a greedy algorithm, however, it\nmight not be a good strategy.\nConsider the following example. Let K= 2,z1= (0,ε)⊤,z2= (0,1)⊤,z3= (1,0)⊤,\nz4= (0,1)⊤and so on (alternatively having (0 ,1)⊤and (1,0)⊤whent≥2), where εis small\nenough. Then with Following the Leader Strategy, we have that p1is arbitrary and in the\nbest case p1= (1,0)⊤, andp2= (1,0)⊤,p3= (0,1)⊤,p4= (1,0)⊤and so on (alternatively\nhaving (0 ,1)⊤and (1,0)⊤whent≥2).\n89\nIn the above example, we have\nn n/summationdisplay n np⊤\ntztminp⊤ztn1 1,\np∆k 2 2t=1− ≤ − − ≤\n∈/summationdisplay\nt=1−\nwhich gives raise to linear regret.\nNow let’s consider FPL. FPL regularizes FL by adding a small amount of noise, which\ncan guarantee square root regret under oblivious adversary situation.\nAlgorithm 1 Follow the Perturbed Leader (FPL)\nInput:Letξbe a random variables uniformly drawn on [0 ,1]K.η\nfort= 1 tondo\nt−1\npt= argmin\np∈∆K/summationdisplay\ns=1/parenleftbig\np⊤zs+ξ/parenrightbig\n.\nend for\nWe analyze this strategy in oblivious adversaries, which means the sequence ztis chosen\nahead of time, rather than adaptively given. The following theorem gives a bound for regret\nof FPL:\nTheorem: FPL with η=√1yields expected regret:kn\nIEξ[Rn]≤2√\n2nK .\nBefore proving the theorem, we introduce the so-called Be-The-Leader Lemma at ﬁrst.\nLemma: (Be-The-Leader)\nFor all loss function ℓ(p,z), let\nt\np∗\nt= arg min ℓ(p,zs),\np∈∆K/summationdisplay\ns=1\nthen we haven n /summationdisplay\nℓ(p∗\nt,zt)\nt=1≤/summationdisplay\nℓ(pn∗,zt)\nt=1\nProof.The proof goes by induction on n. Forn= 1, it is clearly true. From nton+1, it\n90\nfollows from:\nn+1 n /summationdisplay\nℓ(pt∗,zt) =/summationdisplay\nℓ(p∗\nt,zt)+ℓ(pn∗\n+1,zn+1)\nt=1 i=1\nn\n≤/summationdisplay\nℓ(p∗\nn,zt)+ℓ(p∗\nn+1,zn+1)\ni=1\nn\n≤/summationdisplay\nℓ(p∗\nn+1,zt)+ℓ(p∗\nn+1,zn+1),\ni=1\nwheretheﬁrstinequality usesinductionandthesecondinequality followsfromthedeﬁnition\nofp∗\nn.\nProof of Theorem . Deﬁne\nt\nqt= argmin p⊤(ξ+\np∈∆K/summationdisplay\nzs).\ns=1\nUsing the Be-The-Leader Lemma with\n/braceleftbiggpT(ξ+z1)if t= 1ℓ(p,zt) =pTzt if t >1,\nwe haven n\nq1⊤ξ+/summationdisplay\nqt⊤zt≤minq⊤(ξ+\nqt=∈∆K\n1/summationdisplay\nzt),\nt=1\nwhereby for any q∈∆K,\nn/summationdisplay/parenleftig2qt⊤zt−q⊤zt/parenrightig\n≤/parenleftig\nq⊤−q1⊤/parenrightig\nξ≤ /bardblq 1\ni−q1\n=1/bardbl /bardblξ/bardbl∞≤,η\nwhere the second inequality uses H¨ older’s inequality and the third inequality is from the\nfact that qandq1are on the simplex and ξis in the box.\nNow let\nt\nqt= arg min p⊤/parenleftigg\nξ+zt+/summationdisplay\nzs\np∈∆K\ns=1/parenrightigg\nand\nt\npt= arg min p⊤+\n∈∆/parenleftigg\nξ+0\npK/summationdisplay\nzs\ns=1/parenrightigg\n.\nTherefore,\nn n\nIE[Rn]≤/summationdisplay\np⊤\ntzt\ni−minp⊤zt\np∈∆k\n=1/summationdisplay\ni=1\nn/parenleftig /parenrightign\n≤/summationdisplay\nqt⊤zt−p∗Tzt+/summationdisplay\nIE[(pt t\ni=1 =1−q)⊤zt]\ni\nn2≤+/summationdisplay\nIE[(pt−qt)⊤zt], (2.1)ηi=1\n91\nwherep∗= argminp∈∆K/summationtextnp zt=1⊤t.\nNow let\nt−1\nh(ξ) =zt⊤/parenleftigg\narg min p⊤[ξ+\np∈∆K/summationdisplay\nzs],\ns=1/parenrightigg\nthen we have a easy observation that\nIE[zt⊤(pt−qt)] = IE[h(ξ)]−IE[h(ξ+zt)].\nHence,\nIE[zt⊤(pt−qK Kt)] =η/integraldisplay\nh(ξ)dξ−η/integraldisplay\nh(ξ)dξ\nξ∈[0,1]K ξ∈z+[0,1]Ktη η\n≤ηK/integraldisplay\nh(ξ)dξ\nξ∈[0,1]K\nη\\/braceleftBig\nz ,1]Kt+[0η/bracerightBig\n≤ηK/integraldisplay\n1dξ\nξ∈[0,1]K\\/braceleftBig\nzt+[0,1]K\nη η/bracerightBig\n= IP(∃i∈[K],ξ(i)≤zt(i))\nK\n≤/summationdisplay\nIP/parenleftbigg\nUnif/parenleftbigg1[0,]/parenrightbigg\n≤zt(i)ηi=1/parenrightbigg\n≤ηKzt(i)≤ηK , (2.2)\nwhere the ﬁrst inequality is from the fact that h(ξ)≥0, the second inequality uses\nh(ξ)≤1, the second equation is just geometry and the last inequality is due to zt(i)≤1.\nCombining (2.1) and (2.2) together, we have\n2IE[Rn]≤+ηKn .η\nIn particular, with η=/radicalig\n2, we haveKn\nIE[Rn]≤2√\n2Kn ,\nwhich completes the proof.\n92\nonline learning with structured experts–a biased\nsurvey\nG´ abor Lugosi\nICREA and Pompeu Fabra University, Barcelona\n93\non-line prediction\nA repeated game between forecaster and environment.\nAt each round t,\nthe forecaster chooses an action t2{1,..., };\n(actions are often called experts )\nthe environment chooses losses `t(1),...,` t(N)2[0,1];\nthe forecaster su ↵ers loss`t(It).\nThe goal is to minimize the regret\nRn= Xn n\n`t(It)\u0000min\nitN=1X\n`t(i)\nt=1!\n.\n7I N\n94\nsimplest example\nIs it possible to make (1/n)Rn!0forall loss assignments?\nLetN=2 and deﬁne, for all t=1,..., n,\n`t(1) =⇢0ifIt=2\n1ifIt=1\nand`t(2) = 1 \u0000`t(1).\nThen\nXn nn`t(It)=n and min`\ni=12t=1X\nt(i)\n,\nt=12\nso1 1Rnn\u0000.2\n95\nrandomized prediction\nKey to solution: randomization .\nAt time t, the forecaster chooses a probability distribution\npt\u00001=(p1,t\u00001,..., pN,t\u00001)\nand chooses action iwith probability pi,t\u00001.\nSimplest model: all losses `s(i),i=1,..., N,s<t,a r e\nobserved: full information .\n12\n96\nHannan and Blackwell\nHannan (1957) andBlackwell (1956) showed that the forecaster\nhas a strategy such that\n1 Xn n\n`t(It)n\u0000min\nitN=1X\n`t(i)\nt=1!\n!0\nalmost surely for all strategies of the environment.\n13\n97\nbasic ideas\nexpected loss of the forecaster:\nN\n`t(pt\u00001)=X\npi,t (\u00001`ti)=E t`t(It)\ni=1\nBy martingale convergence,\n Xn n1``1 2t(It)\u0000X/t(pnt\u00001) = OP(n\u0000)\nt=1 t=1\nso it su\u0000 ces to study\n1\nn Xn n\n`t(pt)min`(i)\u00001\u0000\nN=1X\ntit t=1!!\n98\nweighted average prediction\nIdea: assign a higher probability to better-performing actions.\nVovk (1990), Littlestone and Warmuth (1989) .\nA popular choice is\nexp⇣\n\u0000⌘Pt\u00001\n=1`(s si)\npi,t =\u00001⌘\nP =1N\n=1exp⇣\n\u0000⌘P i ,...,N .t\u00001\n=1`s( )k sk\nwhere⌘> 0.Then⌘\n1Xn\nn n\n`t(pt\u00001)\u0000min\nit=1NX\n`t(i)\nt=1!\n=r\nlnN\n2n\nwith⌘=p\n8l nN/n.\n99\nproof\ntLetLi,t=P\ns=1`s(i)and\nN\nWt=X XN\nwi,t= e\u0000⌘Li,t\ni=1 i=1\nfort\u00001,a n dW 0=N. First observe that\nWnln eWXN\n=ln,\n0 \n\u0000⌘Li n\ni=1!\n\u0000lnN\n\u0000ln✓\nmax e\u0000⌘Li,n ln\ni=1,...,N◆\n\u0000N\n=\u0000⌘ min Li,ni=1,...,N\u0000lnN.\n100\nproof\nOn the other hand, for each t=1,...,n\nWtPN\ni=1wi,t1e\u0000⌘`t(i)\nln =ln\u0000\nWt\u00001PN\nPj=1wj,t\u00001\nN\n=1w\nPi,t\u00001`(\u0000i ti)⌘2\n⌘ +N8j=1wj,t\u00001\n⌘2\n=\u0000⌘` t(pt\u00001)+8\nby Hoe↵ding’s inequality.\nHoe↵ding (1963) :i fX2[0,1],\nlnEe\u0000⌘⌘2\nX\u0000⌘EX+8\n101\nproof\nfor each t=1,..., n\nW2t ⌘ln \u0000⌘`t(p )+Wt\nt\u00001\u000018\nSumming over t=1,..., n,\nWn\nn ⌘ln\u0000⌘X 2\n`t(pt1)+ n.W0\u00008t=1\nCombining these, we get\nXnlnN⌘`t(pt1) min Li,n+ + n\u0000i=1,...,N ⌘ 8t=1\n102\nlower bound\nThe upper bound is optimal: for all predictors,\nPn\n=1`t(Itp\u0000n)mint iN`tsup=1 t(i)1.\nn,N,`t(i) (n/2) ln NP\n\u0000\nIdea: choose`t(i)to be i.i.d. symmetric Bernoulli coin ﬂips.\nn\nsup Xn\n`t(It)\nt(i)\u0000min\n` it=1NX\n`t(i)\n\"t=1!\nn\n\u0000EX\n`t(It)\u0000min\nit=1N\nnXn\n`t(i)\nt=1#\n=2\u0000minBiiN\nWhere B1,..., BNare independent Binomial (n,1/2) .\nUse the central limit theorem.\n103\nfollow the perturbed leader\nt\u00001\nIt=arg minX\n`s(i)+Zi,t\ni=1,...,Ns=1\nwhere the Zi,tare random noise variables.\nThe original forecaster of Hannan (1957) is based on this idea.\n104\nfollow the perturbed leader\nIf the Zi,tare i.i.d. uniform [0,p\nnN],t h e n\n1 NRnn2r\n+Op(n\u00001/2).n\nIf the Zzi,tare i.i.d. with density (⌘/2)e\u0000⌘| |,t h e nf o r\n⌘⇡p\nlogN/n,\n1 log NR1/2ncr\n+Op(n\u0000).n n\nKalai and Vempala (2003) .\n105\ncombinatorial experts\nOften the class of experts is very large but has some combinatorial\nstructure. Can the structure be exploited?\npath planning. At each time\ninstance, the forecaster chooses apath in a graph between twoﬁxed nodes. Each edge has anassociated loss. Loss of a path isthe sum of the losses over theedges in the path.\nNis huge!!!\n© Google. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, seehttp://ocw.mit.edu/fairuse.\n106\nassignments: learning permutations\nGiven a complete\nbipartite graphK\nm,m,t h e\nforecaster chooses aperfect matching.The loss is the sumof the losses overthe edges.\nHelmbold and Warmuth (2007): full information case.\nThis image has been removed due to copyright restrictions.\nPlease see the image athttp://38.media.tumblr.com/tumblr_m0ol5tggjZ1qir7tc.gif\n107\nspanning trees\nThe forecaster chooses a\nspanning tree in the completegraph K\nm. The cost is the sum\nof the losses over the edges.\n108\ncombinatorial experts\ndFormally, the class of experts is a set S⇢{ 0,1} of cardinality\n|S|=N.\nt 2RdAt each time , a loss is assigned to each component: `t .\nLoss of expert v2S is`t(v)=`>\ntv.\nForecaster chooses It2S.\nThe goal is to control the regret\nXnXn\n`t(It)\u0000 min`t(k).\nk=1,...,Nt=1 t=1\n109\ncomputing the exponentially weighted average forecaster\nOne needs to draw a random element of Swith distribution\nproportional to\nwt(v) = exp\u0000t\n\u0000⌘Lt(v)\u0000\n= exp \u00001\n\u0000⌘X\n`>\ntv.\ns=1!\nt\nexp\njYd\n=\n=1 \u00001\n\u0000⌘X\n`t,jvj\ns=1!\n.\n110\ncomputing the exponentially weighted average forecaster\npath planning: Sampling may be done by dynamic programming.\nassignments: Sum of weights (partition function) is the permanent\nof a non-negative matrix. Sampling may be done by a FPAS of\nJerrum, Sinclair, and Vigoda (2004) .\nspanning trees: Propp and Wilson (1998) deﬁne an exact sampling\nalgorithm. Expected running time is the average hitting time ofthe Markov chain deﬁned by the edge weights w\nt(v).\n111\ncomputing the follow-the-perturbed leader forecaster\nIn general, much easier. One only needs to solve a linear\noptimization problem over S.T h i sm a yb eh a r db u ti ti sw e l l\nunderstood.\nIn our examples it becomes either a shortest path problem, or an\nassignment problem, or a minimum spanning tree problem.\n112\nfollow the leader: random walk perturbation\nSuppose Nexperts, no structure. Deﬁne\nt\nIt=arg min (`i,s1+Xs)\ni=1,...,NX\n\u0000\ns=1\nwhere the Xsare either i.i.d. normal or ±1coinﬂips.\nThis is like follow-the-perturbed-leader but with random walk\ntperturbation:s=1Xt.\nAdvantage: foP\nrecaster rarely changes actions!\n113\nfollow the leader: random walk perturbation\nIfRnis the regret and Cnis the number of times It6=It\u00001,t h e n\nERn2ECn8p\n2nlogN+ 16 log n+ 16 .\nDevroye, Lugosi, and Neu (2015).\nKey tool: number of leader changes in Nindependent random\nwalks with drift.\n114\nfollow the leader: random walk perturbation\nThis also works in the “combinatorial” setting: just add an\nindependent N(0,d)at each time to every component.\nERn=Oe(B3/2p\nnlogd)\nand\nECn=O(Bpnlogd),\nwhere B= max v2Skvk1.\n115\nwhy exponentially weighted averages?\nMay be adapted to many di ↵erent variants of the problem,\nincluding bandits, tracking, etc.\n116\nmulti-armed bandits\nThe forecaster only observes `t(It)but not`t(i)fori6=It.\nHerbert Robbins (1952).\nThis image has been removed due to copyright restrictions. Please see the image at\nhttps://en.wikipedia.org/wiki/Herbert_Robbins#/media/File:1966-HerbertRobbins.jpg\n117\nmulti-armed bandits\nTrick: estimate`t(i)by\n`e`t(It)\nt(iI=i)={t}\npIt,t\u00001\nThis is an unbiased estimate:\neXN`t(j)Et`t(i)= pj,t1{j=i}=`t(i) \u0000\njj,=1pt\u00001\nUse the estimated losses to deﬁne exponential weights and mix\nwith uniform ( Auer, Cesa-Bianchi, Freund, and Schapire, 2002):\n⇣\n\u0000Ptexp⌘\u00001`( )\nP⇣s=1pi,t 1\u0000\u0000esi\n1=( ) \u0000N\nk=1exp⌘\n\u0000\n\u0000⌘P +t\u00001\ns=1`s(k)⌘\nN\nexploration\ne\n| {z }\nexploitation|{z}\n118\nmulti-armed bandits\nXn1E \n`t(pt1)\u0000minXn\n`t(i)!\n=O r\nNlnN!\n,n\u0000it=1Nt=1n\n119\nmulti-armed bandits\nLower bound:\nn1 NsupE Xn\n`t(pt1)\u0000minX\n`t(i) ,\n`t(i)n\u0000i N=1=1!\n\u0000Cr\nnt t\nDependence on Nis not logarithmic anymore!\nAudibert and Bubeck (2009) constructed a forecaster with\nn n1 NmaxE`t(pti\u00001)`t(i) = O ,\nNn \n\u0000\nt=1 t=1! r\nn!X X\n120\ncalibration\nSequential probability assignment.\nAb i n a r ys e q u e n c ex 1,x2,... is revealed one by one.\nAfter observing x1,...,x t\u00001, the forecaster issues prediction\nIt2{0,1,..., N}.\nMeaning: “chance of rain is It/N”.\nForecast is calibrated if\n\u0000\u0000P\n\u0000n\n\u0000t=1xt\n\u0000\n{It=i}Pn\nt=1\n {It=i}\u0000i\nN\u0000\u0000\u0000\u0000\u00001\n2N+o(1)\nwhenever lim supn(1/n)Pn\nt=1\n0 {It=i>. }\nIs there a forecaster that is calibrated for all possible sequences?\nNO. (Dawid, 1985).\n121\nrandomized calibration\nHowever, if the forecaster is allowed to randomize then it is\npossible! (Foster and Vohra, 1997).\nThis can be achieved by a simple modiﬁcation of any regret\nminimization procedure.\nSet of actions (experts): {0,1,...,N }.\nAt time t, assign loss`t(i)=( xt\u0000i/N)2to action i.\nOne can now deﬁne a forecaster. Minimizing regret is not\nsu\u0000cient.\n122\ninternal regret\nRecall that the (expected) regret is\nXn\n`t(ptXn n\n1)\u0000min`t(i) = max\u0000i it=1 t=1X\npj,t(`t(j)\u0000`t(i))\nt=1X\nj\nThe internal regret is deﬁned by\nn\nmaxX\npj,t(`t(j)\u0000`t(i))\ni,jt=1\npj,t(`t(j)\u0000`t(i)) =Et\n` {It=j(}t(j)\u0000`t(i))\nis the expected regret of having taken action jinstead of action i.\n123\ninternal regret and calibration\nBy guaranteeing small internal regret, one obtains a calibrated\nforecaster.\nThis can be achieved by an exponentially weighted average\nforecaster deﬁned over N2actions.\nCan be extended even for calibration with checking rules.\n124\nprediction with partial monitoring\nFor each round t=1,..., n,\nthe environment chooses the next outcome Jt2{1,...,M }\nwithout revealing it;\nthe forecaster chooses a probability distribution ptand\u00001\ndraws an action It2{1,...,N }according to pt\u00001;\nthe forecaster incurs loss `(It,Jt)and each action iincurs loss\n`(i,Jt). None of these values is revealed to the forecaster;\nthe feedback h(It,Jt)is revealed to the forecaster.\nH=[h(i,j)]N⇥Mis the feedback matrix.\nL=[`(i,j)]N⇥Mis the loss matrix.\n125\nexamples\nDynamic pricing. HereM=N,a n dL =[`(i,j)]N⇥Nwhere\n(j )i j\u0000i{ij+c(,)=} {i >j`}.N\nandh(i,j)= {i>jor}\nh(i,j)=a i j +b i>j,i,j=1,..., N. {} { }\nMulti-armed bandit problem. The only information the forecaster\nreceives is his own loss: H=L.\n126\nexamples\nApple tasting. = =2.\nL=01\n10\u0000\nH=aabc\u0000\n.\nThe predictor only receives feedback when he chooses the secondaction.Label e\u0000 cient prediction. N=3,M=2.\nL=211\n401103\n5\nH=2ab\ncc3\n.N M\n4\ncc5\n127\nag e n e r a lp r e d i c t o r\nA forecaster ﬁrst proposed by Piccolboni and Schindelhauer (2001).\nCrucial assumption: Hcan be encoded such that there exists an\nN⇥Nmatrix K=[k(i,j)]N⇥Nsuch that\nL=K·H.\nThus,\n`(i,j)=XN\nk(i,l)h(l,j).\nl=1\nThen we may estimate the losses by\n`ek(i,It)h(It,Jt)(i,Jt)=pIt,t.\n128\nag e n e r a lp r e d i c t o r\nObserve\neXNk(i,k)h(k,Jt)Et`(i,Jt) = pk,t\u00001\nk=1pk,t\u00001\nk\nkXN\n= ( i,k)h(k,Jt)=` (i,Jt),\n=1\n`e(i,Jt)is an unbiased estimate of`(i,Jt).\nLet\ne\u0000⌘Li,t\u00001\u0000pi,t\u00001=( 1\u0000\u0000)e\nP +N\n=1e\u0000⌘Lek,t\u00001 Nk\nwhere Li,t=Pt\n=1`(i,Jt). ese\n129\nperformance bound\nWith probability at least 1\u0000\u0000,\n1Xn1`(It,Jt)n\u0000 min\ni=1,...,Nt=1\n\u0000Xn\n`(i,Jt)nt=1\nCn1/3N2/3p\nln(N/\u0000).\nwhere Cdepends on K.(Cesa-Bianchi, Lugosi, Stoltz (2006) )\nHannan consistency is achieved with rate O(n\u00001/3)whenever\nL=K·H.\nThis solves the dynamic pricing problem.\nBart´ ok, P´ al, and Szepesv´ ari (2010) :i fM=2, only possible rates\naren\u00001/2,n\u00001/3,1\n130\nimperfect monitoring: a general framework\nSis a ﬁnite set of signals.\nFeedback matrix: H:{1,...,N }⇥{ 1,...,M }!P (S).\nFor each round t=1,2...,n ,\nthe environment chooses the next outcome Jt2{1,...,M }\nwithout revealing it;\nthe forecaster chooses pt\u00001and draws an action\nIt2{1,...,N }according to it;\nthe forecaster receives loss `(It,Jt)and each action isu↵ers\nloss`(i,Jt), none of these values is revealed to the forecaster;\naf e e d b a c ks tdrawn at random according to H(It,Jt)is\nrevealed to the forecaster.\n131\ntarget\nDeﬁne\n`(p,q)=X\npiqj`(i,j)\ni,j\nH(·,q)=( H(1,q),...,H (N,q))\nwhere H(i,q)=jqjH(i,j).\nDenote by Fthe setP\nof those \u0000that can be written as H(·,q)for\nsome q.\nFis the set of “observable” vectors of signal distributions \u0000.\nThe key quantity is\n⇢(p,\u0000) = max `(p,q)\nq:H(·,q)=\u0000\n⇢is convex in pand concave in \u0000.\n132\nrustichini’s theorem\nThe value of the base one-shot game is\nminmax`(p,q)=m i n max⇢(p,\u0000)\np q p\u00002F\nIfqnis the empirical distribution of J1,...,J n,e v e nw i t ht h e\nknowledge of H(·,qn)we cannot hope to do better than\nmin p⇢(p,H(·,qn)).\nRustichini (1999) proved that there exists a strategy such that for\nall strategies of the opponent, almost surely,\nlim sup0\n@1X\n`(It,Jt)\u0000min⇢(p,H(\nn n !1 p·,qn))\nt=1,...,n1A0\n133\nrustichini’s theorem\nRustichini’s proof relies on an approachability theorem for a\ncontinuum of types ( Mertens, Sorin, and Zamir, 1994).\nIt is non-constructive.\nIt does not imply any convergence rate.Lugosi, Mannor, and Stoltz (2008) construct e\u0000 ciently computable\nstrategies that guarantee fast rates of convergence.\n134\ncombinatorial bandits\nThe class of actions is a set S⇢{ 0d,1} of cardinality |S|=N.\nAt each time td, a loss is assigned to each component: `t2R.\nLoss of expert v2S is`t(v)=`>\ntv.\nForecaster chooses It2S.\nThe goal is to control the regret\nXn n\n`t(It)\u0000 min\nk=1,...,Nt=1X\n`t(k).\nt=1\n135\ncombinatorial bandits\nThree models.\n(Full information.) Alldcomponents of the loss vector are\nobserved.(Semi-bandit.) Only the components corresponding to the chosen\nobject are observed.(Bandit.) Only the total loss of the chosen object is observed.\nChallenge: IsO(n\n\u00001/2poly(d ))regret achievable for the\nsemi-bandit and bandit problems?\n136\ncombinatorial prediction game\nAdversary\nPlayer\n137\ncombinatorial prediction game\nAdversary\nPlayer\n© Google. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, seehttp://ocw.mit.edu/fairuse.\n138\ncombinatorial prediction game\nAdversary\nPlayer\n© Google. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, seehttp://ocw.mit.edu/fairuse.\n139\ncombinatorial prediction game\nAdversary\nPlayer`2`6 `d\u00001`1`4\n`5\n`9`d\u00002\n`d `3\n`8`7\n© Google. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, seehttp://ocw.mit.edu/fairuse.\n140\ncombinatorial prediction game\nAdversary\nPlayer`2`6 `d\u00001`1`4\n`5\n`9`d\u00002\n`d `3\n`8`7\nloss su ↵ered:`2+`7+...+`d\n© Google. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, seehttp://ocw.mit.edu/fairuse.\n141\ncombinatorial prediction game\nAdversary\nPlayer`2`6 `d\u00001`1`4\n`5\n`9`d\u00002\n`d `3\n`8`7\n<8\nFull Info:`1,`2,...,` d\nFeedback::\nloss su ↵ered:`2+`7+...+`d\n© Google. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, seehttp://ocw.mit.edu/fairuse.\n142\ncombinatorial prediction game\nAdversary\nPlayer`2`6 `d\u00001`14\n`5\n`9`d\u00002\n`d `3\n`8`7`\n< n d\nb ck:8\nFull I fo: `1,`2,...,`\nFeed a:Semi-Bandit: `2,`7,...,` d\nBandit: `2+`7+...+`d\nloss su ↵ered:`2+`7+...+`d\n© Google. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, seehttp://ocw.mit.edu/fairuse.\n143\nnotation\n`2`6 `d\u00001`1`4\n`5\n`9`d\u00002\n`d `3\n`8`7\n`2`6 `d\u00001`1`4\n`5\n`9`d\u00002\n`d `3\n`8`7S⇢{ 0,1}d\n`t2Rd\n+\nVt2ST, loss su ↵ered:`tVt\nregret:\nn\nR EXn\nTV\u0000 EXTn=`ttmin`tu\nut=12St=1\nTloss assumption: `tv 1for all v andt=1,...,n . || 2S\n144\nweighted average forecaster\nAt time tassign a weight wt,ito each i=1,...,d .\nThe weight of each vk2S is\nwt(k)= wt,i.\ni:vkY\n(i)=1\nLetqt\u00001(k)=w t\u00001(kN)/k=1wt\u00001(k).\nAt each time t,d r a wK tfromP\nthe distribution\npt\u00001(k)=( 1\u0000\u0000)qtk\u00001()+\u0000µ(k)\nwhere µis a ﬁxed distribution on Sand\u0000> 0.H e r e\nwt,i= exp\u0000⌘Lt,i\nwhere Lt,i=`1,i+ +`t,iand\u0000\n`t,iis a\u0000\nn estimated loss.e\nee ···e e\n145\nloss estimates\nDani, Hayes, and Kakade (2008) .\nDeﬁne the scaled incidence vector\nXt=`t(Kt)VKt\nwhere Ktis distributed according to pt\u00001.\nLetPt\u00001=E⇥\nVKtV>be theKtd⇥dcorrelation matrix.\nHence\nPt1⇤\n\u0000(i,j)=\nk:vk(iX\npt\u00001(k).\n)=v k(j)=1\nSimilarly, let Qt\u00001andMbe the correlation matrices of EVV>\nwhen Vhas law, qt\u00001andµ.T h e n⇥ ⇤\nPt\u00001(i,j)=( 1\u0000\u0000)Qt\u00001(i,j)+\u0000M(i,j).\nThe vector of loss estimates is deﬁned by\ne`t=P+\nt\u00001Xt\nwhere P+\nt\u00001is the pseudo-inverse of Pt\u00001.\n146\nkey properties\nMM+v=vfor all v2S.\nQt\u00001is positive semideﬁnite for every t.\nPt1P+r at1v=vfo ll t v \u0000 and\u00002S.\nBy deﬁnition,\nEtXt=Pt\u00001`t\nand therefore\nEte`t=P+\nt\u00001EtXt=`t\nAn unbiased estimate!\n147\nperformance bound\nThe regret of the forecaster satisﬁes\n1✓lnELb2B2 d N\nn\u0000 min Ln(k) 2 +1 .n k=1,...,N◆\ns✓\nd\u0000min(M)◆\nn\nwhere\nT\u0000min(M)= m i n xMx>0\nx2span (S):kxk=1\nis the smallest “relevant” eigenvalue of M.(Cesa-Bianchi and\nLugosi, 2009.)\nLarge\u0000min(M)is needed to make sure no `t,iis too large.|e|\n148\nperformance bound\nOther bounds:\nBp\ndlnN/n(Dani, Hayes, and Kakade ). No condition on S.\nSampling is over a barycentric spanner .\ndp\n(✓lnn)/n(Abernethy, Hazan, and Rakhlin). Computationally\ne\u0000cient.\n149\neigenvalue bounds\n\u0000min(M)= m i n E(V,x)2.\nx2span (S):kxk=1\nwhere Vhas distribution µoverS.\nIn many cases it su\u0000ces to take µuniform.\n150\nmultitask bandit problem\nThe decision maker acts in mgames in parallel.\nIn each game, the decision maker selects one of Rpossible actions.\nAfter selecting the mactions, the sum of the losses is observed.\n1\u0000min=R\nmaxELn\u0000Ln(k) lnR.\nk2mp\n3nR\nThe price of only obsh\nb\nerving the sui\nm of losses is a factor of m.\nGenerating a random joint action can be done in polynomial time.\n151\nassignments\nPerfect matchings of Km,m.\nAt each time one of the N=m!perfect matchings of Km,mis\nselected.\n1\u0000min(M)=m\u00001\nmaxELn\u0000Ln(k)2m 3nln(m !).\nk\nOnly a factor of mh\nwb\norse than ni\naive full-ip\nnformation bound.\n152\nspanning trees\nIn a network of mnodes, the cost of communication between two\nnodes joined by edge eis`t(e)at time t.A te a c ht i m eam i n i m a l\nconnected subnetwork (a spanning tree) is selected. The goal is to\nminimize the total cost. N=mm\u00002.\n1\u0000min(M)=1Om\u0000✓\nm2◆\n.\nThe entries of Mare\n2P{V i=1}=m\nP\u0000 3Vi=1,Vj=1 \n= ifm2i⇠j\n4PVi=1,Vj=1 = if .2i6⇠jm\u0000 \n153\nstars\nAt each time a central node of a network of mnodes is selected.\nCost is the total cost of the edges adjacent to the node.\n1\u0000min\u00001\u0000O✓\nm◆\n.\n154\ncut sets\nAb a l a n c e dc u ti nK 2mis the collection of all edges between a set\nofmvertices and its comp\u0000lement. Each balanced cut has m2\n2medges and there are N=m\u0000\nbalanced cuts.\n1 1\u0000min(M)=\u0000O .4✓\nm2◆\nChoosing from the exponentially weighted average distribution is\nequivalent to sampling from ferromagnetic Ising model. FPAS byRandall and Wilson (1999) .\n155\nhamiltonian cycles\nA Hamiltonian cycle in Kmis a cycle that visits each vertex exactly\nonce and returns to the starting vertex. N=(m\u00001)!\n2\u0000min\u0000m\nE\u0000cient computation is hopeless.\n156\nsampling paths\nIn all these examples µis uniform over S.\nForpath planning it does not always work.\nWhat is the optimal choice of µ?\nWhat is the optimal way of exploration?\n157\nminimax regret\nRn=i n f max sup Rnstrategy S⇢{ 0,1}dadversary\nTheorem\nLetn\u0000d2.I nt h e full information and semi-bandit games, we\nhave\n0.008 dpnRndp\n2n,\nand in the bandit game,\n0.01d3/2pnRn2d5/2p\n2n.\n158\nproof\nupper bounds:\nD=[ 0,+1)d,F(x)=1 dog⌘ i=1xilxiworks for full\ninformation but it is only optiP\nmal up to a logarithmic factor in the\nsemi-bandit case.\nin the bandit case it does not work at all! Exponentially weightedaverage forecaster is used.\nlower bounds:careful construction of randomly chosen set Sin each case.\n159\nag e n e r a ls t r a t e g y\nLetDdbe a convex subset of Rwith nonempty interior int(D).\nA function F:D! RisLegendre if\n•Fisstrictly convex and admits continuous ﬁrst partial\nderivatives on int(D),\n•Foru2@D,a n dv2int(D),w eh a v e\nlim (u\u0000vT)rF\u0000\n(1\u0000s)u+sv=+\ns!0,s >01.\nThe Bregman divergence DF:D⇥ int(\u0000\nD)associated to a\nLegendre function Fis\nDF(uT,v)=F (u)\u0000F(v)\u0000(u\u0000v)rF(v).\n160\nCLEB (Combinatorial LEarning with Bregman divergences)\nParameter: FLegendre on D\u0000 Conv (S)\nConv (S)D\n\u0000(S )ptwtw0\nt+1\nwt+1\npt+1(1)wt0\n+12D :\nrF(w0\n+1)=rF ( )twt\u0000`˜t\n(2)wt+12arg min DF(w,wt0\n+1)\nw2Conv (S)\n(3)pt+12\u0000(S ):w t+1=EV⇠pt+1V\n161\nGeneral regret bound for CLEB\nTheorem\nIfFadmits a Hessian r2Falways invertible then,\nn\nRn/diam ˜T 2\u00001˜DF(S)+EX\n`t\n=1⇣\nrF(wt)\nt⌘\n`t.\n162\nDi↵erent instances of CLEB: LinExp (Entropy Function)\nD 1Pd=[ 0,+ )d,F(x)=1\n⌘ i=1xilogxi\n8\n>><Full Info :Exponentially weighted average\n>>:Semi-Bandit=Bandit :Exp3\nAuer et al. [2002]\n8\n>>>Full Info :Component Hedge\n>>>>Koolen, Warmuth and Kivinen [2010]\n><\n>>Semi-Bandit: MW\n>>Kale, Reyzin and Schapire [2010]\nBandit: new algorithm >>>>:\n163\nDi↵erent instances of CLEB: LinINF (Exchangeable\nHessian)\nD=[ 0,+1)d,F(x)=Pd\ni=1Rxi 1( )0\u0000s ds\nINF, Audibert and Bubeck [2009]\n⇢ (x) = exp(⌘ x):LinExp\n (x)=(\u0000⌘x)\u0000q,q>1:LinPoly\n164\nDi↵erent instances of CLEB: Follow the regularized leader\nD=Conv (S),t h e n\nwt+12arg min Xt\n˜T`sw+F(w)\nw2Ds=1!\nParticularly interesting choice: Fself-concordant barrier function ,\nAbernethy, Hazan and Rakhlin [2008]\n165\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 1\nScribe:Haihao (Sean) Lu Nov. 2, 2015\n3. STOCHASTIC BANDITS\n3.1 Setup\nThe stochastic multi-armed bandit is a classical model for decision making and is deﬁned\nas follows:\nThere are Karms(diﬀerent actions). Iteratively, a decision maker chooses an arm k∈\n{1,...,K}, yielding a sequence XK,1,...,X K,t,..., which are i.i.d random variables with\nmeanµk. Deﬁne µ∗= max jµjor∗ ∈argmax. A policy πis a sequence {πt}t≥1, which\nindicates which arm to be pulled at time t.πt∈ {1,...,K}and it depends only on the\nobservations strictly interior to t. The regret is then deﬁned as:\nn n\nRn= maxIE[ XK,t]−IE[Xπt,t]\nk/summationdisplay\nt=1/summationdisplay\nt=1\nn\n=nµ∗−IE[/summationdisplay\nXπt,t]\nt=1\nn\n=nµ∗−IE[IE[\nK/summationdisplay\nXπt,t|πt]]\nt=1\n=/summationdisplay\n∆kIE[Tk(n)],\nk=1\nnwhere ∆ k=µ∗−µkandTk(n) =/summationtext\nt=11I(πt=k) is the number of time when arm kwas\npulled.\n3.2 Warm Up: Full Info Case\nX1,t\n.Assume in this subsection that K= 2 and we observe the full information\n..\nat\nXK,t\ntimetafter choosing πt. So in each iteration, a normal idea is to choose\nthe arm\nwith\nhighest average return so far. That is\n¯ πt= argmax Xk,t\nk=1,2\nwhere\n1¯Xk,t=tt/summationdisplay\ns=1\nAssume from now on that all random variable Xk,tare subGaussian with variance proxy\n2 2\nσ2, which means IE[ euxu σ]≤e2for allu∈IR. For example, N(0,σ2) is subGaussian withXk,s8\n166\nvariance proxy σ2and any boundedrandomvariable X∈[a,b] is subGaussianwith variance\nproxy (b−a)2/4 by Hoeﬀding’s Lemma.\nTherefore,\nRn= ∆IE[T2(n)], (3.1)\nwhere ∆ = µ1−µ2. Besides,\nn\n¯ ¯ T2(n) = 1+/summationdisplay\n1I(X2,t> X1,t)\nt=2\nn\n= 1+/summationdisplay¯ ¯1I(X2,t−X1,t−(µ2−µ1)≥∆).\nt=2\n¯ ¯ It is easy to check that ( X2,t−X1,t)−(µ2−µ1) is centered subGaussian with variance proxy\n2σ2, whereby\n2¯ ¯−t∆IE[1I(X22,t> X1,t)]≤e4σ\nby a simple Chernoﬀ Bound. Therefore,\n∞2\nRn≤∆(1+/summationdisplay 4σ2−t∆e24σ)≤∆+ , (3.2)∆t=0\nwhereby the benchmark is\n4σ2\nRn≤∆+ .∆\n3.3 Upper Conﬁdence Bound (UCB)\nWithout loss of generality, from now on we assume σ= 1. A trivial idea is that after s\npulls on arm k, we use µˆk,s=1/summationtext\nj∈{pulls ofk}XK,jand choose the one with largest µˆk,s.s\nThe problem of this trivial policy is that for some arm, we might try it for only limited\ntimes, which give a bad average and then we never try it again. In order to overcome this\nlimitation, a good idea is to choose the arm with highest upperbound estimate on the mean\nof each arm at some probability lever. Note that the arm with less tries would have a large\ndeviations from its mean. This is called Upper Conﬁdence Bound policy.\n167\nAlgorithm 1 Upper Conﬁdence Bound (UCB)\nfort= 1 toKdo\nπt=t\nend for\nfort=K+1 tondo\nt−1\nTk(t) =/summationdisplay\n1I(πt=k)\ns=1\n(number of time we have pull arm kbefore time t)\n1µˆk,t=/summationdisplay\nXK,t∧sTk(t)s=1\nlogt)πt∈argmax/braceleftigg\n2 (µˆk,t+2\nk∈[K]/radicaligg\nTk(t)/bracerightigg\n,\nend for\nTheorem: The UCB policy has regret\nK /summationdisplaylogn π2\nRn≤8 +(1+ )∆k 3k,∆k>0/summationdisplay\n∆k\nk=1\nProof.From now on we ﬁx ksuch that ∆ k>0. Then\nn\nIE[Tk(n)] = 1+\nt=/summationdisplay\nIP(πt=k).\nK+1\nNote that for t > K,\n2logt 2logt{πt=k} ⊆ {µˆk,t+2/radicaligg\n≤µˆ∗,t+2 }Tk(t)/radicaligg\nT∗(t)\n/braceleftigg /radicaligg\n2logt/radicaligg\n/uniondisplay 2logt 2logt⊆ {µk≥µˆk,t+2 } {µ∗≥µˆ∗,t+2 }/uniondisplay\n{µ∗≤µk+2Tk(t) T∗(t)/radicaligg\n,πt=k}Tk(t)/bracerightigg\nAnd from a union bound, we have\n/radicaligg\n2logt 2logtIP(µˆk,t−µk<−2 ) = IP( µˆk,t−µk<2Tk(t)/radicaligg\n)Tk(t)\nt−s8logt\n≤/summationdisplay\nexp(s)2s=1\n1=t3t−1\n168\n2logt 2logtThus IP( µk> µˆk t+ 2\nk)≤1, 3and similarly we have IP( µ∗> µˆ∗,t+ 2 ) ≤1,T(t)t T3∗(t)t\nwhereby/radicalig /radicalig\nn n n /summationdisplay 1 2logtIP(πt=k)≤2 µk+2/radicaligg\n/summationdisplay\n+/summationdisplay\nIP(µ∗≤ ,πt=k)t3 Tk(t)t=K+1 t=1 t=1\n∞ n1 8logt≤2/summationdisplay\n+/summationdisplay\nIP(Tk(t)≤ ,πt=k)t3∆2\nt=1 t=1 k\n∞ n\n≤2/summationdisplay1 gn+/summationdisplay 8loIP(Tk(t)≤ ,πt=k)t3∆2\nt=1 t=1 k\n∞ ∞/summationdisplay1 8 o≤2 + (3\nt=1/summationdisplay l gnIPs≤ )t ∆2\ns=1 k\n∞\n≤2/summationdisplay1 8log n+t2∆2\nt=1 k\nπ28logn= + ,3∆2\nk\nwheresis the counter of pulling arm k. Therefore we have\nK\nRn=/summationdisplay /summationdisplay π28logn∆kIE[Tk(n)]≤ ∆k(1+ + ) ,3∆2\nk k,∆kk =1 >0\nwhich furnishes the proof.\nConsider the case K= 2 at ﬁrst, then from the theorem above we know Rn∼logn,∆\nwhich is consistent with intuition that when the diﬀerence of two arm is small, it is hard to\ndistinguish which to choose. On the other hand, it always hold that Rn≤n∆. Combining\nlogn log(n∆2)these two results, we have Rn≤ ∧ n∆, whereby Rn≤ up to a constant.∆ ∆\nActually it turns out to be the optimal bound. When K≥3, we can similarly get the\nlog(n∆2)result that Rn≤/summationtext\nk\nkk. This, however, is not the optimal bound. The optimal bound∆\nshould be/summationtextlog(n/H)\nkk, which includes the harmonic sum and H=1. See [Lat15].∆/summationtext\nk∆2\nk\n3.4 Bounded Regret\nFrom above we know UCB policy can give regret that increases with at most rate log nwith\nn. In this section we would consider whether it is possible to have bounded regret. Actually\nit turns out that if there is a known separator between the expected reward of optimal arm\nand other arms, there is a bounded regret policy.\nWe would only consider the case when K= 2 here. Without loss of generality, we\nassumeµ1=∆andµ∆2=−, then there is a natural separator 0.2 2\n169\nAlgorithm 2 Bounded Regret Policy (BRP)\nπ1= 1 and π2= 2\nfort= 3 tondo\nifmaxkµˆk,t>0then\nthenπt= argmaxkµˆk,t\nelse\nπt= 1,πt+1= 2\nend if\nend for\nTheorem: BRP has regret\n16Rn≤∆+.∆\nProof.\nIP(πt= 2) = IP( µˆ2,t>0,πt= 2)+IP( µˆ2,t≤0,πt= 2)\nNote that\nn n /summationdisplay\nIP(µˆ2,t>0,πt= 2)≤IE 1I(µˆ2,t>0,πt= 2)\nt=3/summationdisplay\nt=3\nn\n≤IE/summationdisplay\n1I(µˆ2,t−µ2>0,πt= 2)\nt=3\n∞\n≤/summationdisplay 2\ne−s∆\n8\ns=1\n8=,∆2\nwheresis the counter of pulling arm 2 and the third inequality is a Chernoﬀ bound.\nSimilarly,\nn n/summationdisplay\nIP(µˆ2,t≤0,πt= 2) =/summationdisplay\nIP(µˆ1,t≤0,πt−1= 1)\nt=3 t=3\n8≤,∆2\nCombining these two inequality, we have\n16Rn≤∆(1+ ) ,∆2\n170\n18.657: Mathematics of Machine Learning\nLecturer: Alexander Rakhlin Lecture 19\nScribe:Kevin Li Nov. 16, 2015\n4. PREDICTION OF INDIVIDUAL SEQUENCES\nIn this lecture, we will try to predict the next bit given the previous bits in the sequence.\nGiven completely random bits, it would be impossible to correctly predict more than half\nof the bits. However, certain cases including predicting bits generated by a human can\nbe correct greater than half the time due to the inability of humans to produce truly\nrandom bits. We will show that the existence of a prediction algorithm that can predict\nbetter than a given threshold exists if and only if the threshold satiﬁes certain probabilistic\ninequalities. For more information on this topic, you can look at the lecture notes at\nhttp://stat.wharton.upenn.edu/ ~rakhlin/courses/stat928/stat928_notes.pdf\n4.1 The Problem\nTo state the problem formally, given a sequence y1,...,yn,...∈ {−1,+1}, we want to ﬁnd\na prediction algorithm yˆt=yˆt(y1,...,yt1) that correctly predicts ytas much as possible. −\niidIn order to get a grasp of the problem, we will consider the case where y1,...,yn∼Ber(p).\nIt is easy to see that we can get\nn1IE/bracketleftBigg\nn/summationdisplay\n=\n=1{yˆtyt\nt}/bracketrightBigg\n→min{p,1−p}\nby letting yˆtequal majority vote of the ﬁrst t−1 bits. Eventually, the bit that occurs\nwith higher probability will alway/BDs have occurred more times. So the central limit theorem\nshows that our loss will approach min {1p,1−p}at the rate of O(√).n\nKnowing that the distribution of the bits are iid Bernoulli random variables made the\nprediction problem fairly easy. More surprisingly is the fact that we can achieve the same\nfor any individual sequence.\nClaim: There is an algorithm such that the following holds for any sequence y1,...,yn,....\nn1limsup/summationdisplay\n{yˆt=yt}−min{y¯n,1n\nnnt=1−y¯} ≤0 a.s.\n→∞\nIt is clear that no deterministic strategy can achieve this bound. For any deterministic\nstrategy, we can just choose yt=−yˆtand the predictions would be wrong every time. So\nwe need a non-deterministic algorithm that chooses qˆt= IE[yˆt]∈[−1,1].\nTo prove this claim, we will look at a more general problem. Take a ﬁxed horizon n≥1,\nand function φ:{±1}n→ /CA. Does/BDthere exist a randomized prediction strategy such that\nfor anyy1,...,ynn1IE[/summationdisplay\n{yˆt=ytnt=1}]≤φ(y1,...,yn) ?\n/BD/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n171\nFor certain φsuch asφ≡0, it is clear that no randomized strategy exists. However for\n1φ≡, the strategy of randomly predicting the next bit ( qˆt= 0) satisﬁes the inequality.2\nLemma: For a stable φ, the following are equivalent\nn1a)∃(qˆt)t=1,...,n∀y1,...,ynIE[/summationdisplay\n{yˆt=yt}]≤φ(y1,...,yn)nt=1\n1b) IE[φ(ǫ1,...,ǫn)]≥whereǫ1,...,ǫnare Rademacher random variables2\nwhere stable is deﬁned as follows\nDeﬁnition (Stable Function): A function φ:{±1}n→is stable if\n1|φ(...,yi,...)−φ(...,−yi,...)| ≤n\nProof.(a=\n1⇒1b)SupposeIE φ <. Take(y1,...,yn) = (ǫ1,...\n/CA\n,ǫn). ThenIE[1nynt=1{ˆt=2\nǫt}] =>IE[φ]sotheremustexistasequence(nǫ1,...,ǫ1n)suchthatIE[/summationtext\n/summationtext\nt{yˆt=ǫt}]>2 n=1\nφ(ǫ1,...,ǫn).\n(b=⇒a) Recu/BDrsively deﬁne V(y1,...,yt) such that ∀y1,...,yn\n/BD\n1V(y1,...,yt1) = min max/parenleftBig\nIE[{yˆt=yt}]+V(y1,...y\n/BD\n− n)\nqt∈[−1,1]yt∈±1n/parenrightBig\nLooking at the deﬁnition, we can see that IE[1n\nt=1{yˆt=yt}] =V( )n∅ −V(y1,...,yn).\nNow we note that V(y1,...,yt) =−t−IE[φ(y1,/summationtext\n.\n/BD\n..,yt,ǫt n +1,...,ǫn)] satisﬁes the recursive2\ndeﬁnition since\n1 tminmax IE[ yˆt=yt] IE[φ(y1,...,yt,ǫt+1,...,ǫn)]\nqˆtytn{ } −\n/BD\n−2n\nqˆtyt t1=minmax−−IE[φ(y1,...,yt,ǫt+1,...,ǫn)]−\nqˆtyt2n−2n\nqˆ t1q 1=minm x {−t −ˆ ta −tIE[φ(y1,...,yt1,1,ǫt+1,...,ǫn)]−,−IE[φ(y . −1, − 1, ..,yt−1,ǫt+1,...,ǫn)]−\nqˆt 2n 2n2n−2n}\nt1=−IE[φ(y1,...,yt1,ǫt,ǫt+1,...,ǫ − n)]−−2n\n=V(y1,...,yt1)−\nThe ﬁrst equality uses the fact that for a,b∈ {±1},{1a=b}=−ab, the second uses the2\nfact that yt∈ {±1}, the third minimizes the entire expression by choosing qˆtso that the\ntwo expressions in the max are equal. Here the fact that φis stable means qˆt∈[−1,1] and\nis the only place where we need φto be stable.\nTherefore we have\n/BD\nn1IE[/summationdisplay 1{yˆt=yt}] =V(∅)−V(y1,...,yn) =−IE[φ(ǫ1,...,ǫn)]+ +φ(y1,...,yn)n 2t=1≤φ(y1,...,yn)\n/BD/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash/BD/ne}ationslash\n172\nby b).\nBy choosing φ= min{y¯,1−y¯}+c√, this shows there is an algorithm that satisﬁes ourn\noriginal claim.\n4.2 Extensions\n4.2.1 Supervised Learning\nWe can extend the problem to a regression type problem by observing xtand trying to\npredictyt. In this case, the objective we are trying to minimize would be\n1 1ln/summationdisplay\n(yˆt,yt)−inf\nf∈F,n/summationdisplay\nl(f(xt)yt)\nIt turns out that the best achievable performance in such problems is governed by martin-\ngale (or, sequential) analogues of Rademacher averages, covering numbers, combinatorial\ndimensions, and so on. Much of Statistical Learning techniques extend to this setting of\nonline learning. In addition, the minimax/relaxation framework gives a systematic way of\ndeveloping new prediction algorithms (in a way similar to the bit prediction problem).\n4.2.2 Equivalence to Tail Bounds\nWe can also obtain probabilistic tail bound on functions φon hypercube by using part a) of\nthe earlier lemma. Rearranging part a) of the lemma we get 1 −2φ(1y1,...,yn)≤qˆtyt.n\nThis implies/summationtext\n2\nIP/parenleftbig 1µ 1 µφ(ǫ1,...,ǫn)<−/parenrightbig\n= IP/parenleftbig\n1−2φ(ǫ1,...,ǫn)> µ≤IP2 n/summationdisplay\nqˆtǫt> µ≤e−2n\nSo IEφ≥1=⇒existence of a strategy = ⇒tail boun/parenrightbig\nd forφ/parenleftbig\n1<./parenrightbig\n2 2\nWe can extend the results to higher dimensions. Consider z1,...,zn∈B2whereB2is\na ball in a Hilbert space. We can deﬁne recursively yˆ0= 0 and yˆt+1= ProjB2(yˆt−1√zt).n\nBased on the properties of projections, for every∗∈, we have1/summationtext/an}bracketle{tˆ−∗/an}bracketri}ht ≤1y B 2 yty ,zt n√.n\nzTakingy∗ t= ,/bardbl/summationtext\n/summationtextzt/bardbl\nn n\n∀z1,...,zn,/summationdisplay\nzt√/bardbl\nt=1/bardbl−n≤/summationdisplay\nyˆt, zt\nt=1/an}bracketle{t − /an}bracketri}ht\nTake a martingale diﬀerence sequence Z1,...,Z nwith values in B2. Then\nn n\nIP/parenleftbig\n/bardbl/summationdisplay 2nµZt√\nt=1/bardbl−n > µ/parenrightbig\n≤IP(/summationdisplay\nt=1/an}bracketle{tyˆt,−Zt/an}bracketri}ht> µ)≤e−2\nIntegrating out the tail,\nn\nIE/bardbl/summationdisplay\nZt\nt=1/bardbl ≤c√n\n173\nIt can be shown using Von Neumann minimax theorem that\nn n\n∃(yˆt)∀z1,...,zn,y∗∈B2/summationdisplay\nWt√/an}bracketle{tyˆt−y∗,zt/an}bracketri}ht ≤ supE c n\nMDSWt1,...,W=1n/bardbl/summationdisplay\nt=1/bardbl ≤\nwhere the supremum is over all martingale diﬀerence sequences (MDS) with values in B2.\nBy the previous part, this upper bound is c√n. We conclude an interesting equivalence of\n(a) deterministic statements that hold for all sequences, (b) tail bounds on the size of a\nmartingale, and (c) in-expectation bound on this size.\nIn fact, this connection between probabilistic bounds and existence of prediction strate-\ngies for individual sequences is more general and requires further investigation.\n174\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 20\nScribe:Vira Semenova Nov. 23, 2015\nIn this lecture, we talk about the adversarial bandits under limited feedback. Adver-\nsarial bandit is a setup in which the loss function l(a,z) :AxZis determinitic. Lim-\nited feedback means that the information available to the DM after the step tisIt=\n{l(a1,z1),...,l(at−1,zt)}, namely consits of the realised losses of the past steps only.\n5. ADVERSARIAL BANDITS\nConsider the problem of prediction with expert advice. Let the set of adversary moves be Z\nand the set of actions of a decision maker A={e1,...,eK}. At time t,at∈ Aandzt∈ Zare\nsimultaneously revealed.Denote the loss associated to the decision at∈ Aand his adversary\nplayingztbyl(at,zt). We compare the total loss after nsteps to the minimum expert loss,\nnamely:\nn\nmin\n≤ ≤/summationdisplay\nlt(ej,zt),\n1j Kt=1\nwhereejis the choice of expert j∈ {1,2,..,K}.\nThe cumulative regret is then deﬁned as\nn n\nRn=/summationdisplay\nlt(at,zt)−min/summationdisplay\nlt(ej,zt)\n1≤j≤Kt=1 t=1\nThe feedback at step tcan be either full or limited. The full feedback setup means that\nthevector f= (l(e1,z⊤t),...,l(eK,zt)) oflossesincurredatapairofadversary’schoice ztand\neach bandit ej∈ {e1,...,eK}is observed after each step t. Hence, the information available\nto the DM after the step tisI=∪t ′t ′{l(a1,zt),...,l(aK,zt=1 t′)}. The limited feedback means\nthat the time −tfeedback consists of the realised loss l(at,zt) only. Namely, the information\navailable to the DM is It={l(a1,z1),...,l(at,zt)}. An example of the ﬁrst setup is portfolio\noptimization problems, where the loss of all possible portfolios is observed at time t. An\nexample of the second setup is a path planning problem and dynamic pricing, where the\nloss of the chosen decision only is observed. This lecture has limited feedback setup.\nThe two strategies, deﬁned in the past lectures, were exponential weights, which yield\nthe regret of order Rn≤c√nlogKand Follow the Perturbed Leader. We would like to\nplay exponential weights, deﬁned as:\n1exp(tηpt,j/summationtext−l=s=1(ej,zs))/summationtextk−\nl=1exp(−η/summationtextt−1ls=1(ej,zs))\nThis decision rule is not feasible, since the loss l(ej,zt) are not part of the feedback if\nej=at. We will estimate it by\nl(ej,zt)1I(aˆt=ej)l(ej,zt) =P(at=ej)/ne}ationslash\n175\nˆ Lemma: l(ej,zt) is an unbiased estimator of l(ej,zt)\nK I(P ˆl(e e =roof.Ek,zt)1ket)\natl(ej,zt) =/summationtextPk=1 (a=e) =l(e ,z)P(at=ej) tk j t\nDeﬁnition (Exp 3 Algorithm): Letη >0 be ﬁxed. Deﬁne the exponential weights\nas\n−/summationtextt−1ηˆ(j) exp( l(ej,zs))ps=1\nt+1,j=/summationtextk\nl=1exp(−η/summationtextt−1ˆls=1(ej,zs))\n(Exp3 stands for Exponential weights for Exploration and Exploitation.)\nWe will show that the regret of Exp3 is bounded by√2nKlogK. This bound is√\nK\ntimes bigger than the bound on the regret under the full feedback. The√\nKmultiplier is\nthe price of have smaller information set at the time t. The are methods that allow to get\nrid of log Kterm in this expression. On the other hand, it can be shown that√\n2nKis the\noptimal regret.\n−/summationtextt/summationtextK−1ˆW eProof.LetWt,j= exp(kη l(e ,=jzs)),W1 t=/summationtextWs j=1t,j, andp=j=1t,j j\nt W.t\nW/summationtextK(−η/summationtext−1exptlˆe ,z ηl ˆe ,zt+1 j=1 s=1(j s))exp( ( j t))\nlog( ) = log( ) (5.1)W/summationtextKet−\nxp(−/summationtext−1t ηˆls=1(ej,zj=1 s))\nt−1\nˆ = log(IE J∼ptexp(−η/summationdisplay\nl(eJ,zs))) (5.2)\ns=1\n≤∗ η2\nlog(1−2ηˆIEJ∼ptl(eJ,z ˆs)+ IE J∼ptl(eJ,zs) (5.3)2\n∗ ˆ where inequality is obtained by plugging in IE J∼ptl(eJ,zt) into the inequality\nη2x2\nexpx≥1−ηx+2\n.\nK Kl(ej,zt)1I(a e)ˆIEJ∼ptl(eJzt) =/summationdisplayˆt=j, p t,jl(eJ,zt) =/summationdisplay\npt,j =l(at,zt) (5.4)P(at=ej)j=1 j=1\nK Kl2\n2 (ej,zt)1I(a)ˆ ˆ t=ejIEJ∼ptl(eJ,zt) =/summationdisplay\npt,jl2(eJ,zt) =/summationdisplay\npt,j (5.5)P2(at=ej)j=1 j=1\nl2(ej,zt) 1=Pat,t≤ (5.6)Pat,t\nSumming from 1 through n, we get\nlog(Wt+1)≤2log(n\n1)−η/summationtextlt=1(at,zt)+ηW2/summationtext\n1.\nPa ,tt\n176\nFort= 1, we initialize w1,j= 1, soW1=K.\nSince IE1 p\nJ=Pa ,t/summationtextK j,tKj=1= , the expression above becomespt j,t\nIElog(n η2KnWn+1)−logK≤ −η/summationtext\ntl a=1(t,zt)+2ˆ Noting that log( Wn+1) = log(/summationtextK\nj=1exp(−η/summationtextt−1l e ,zs=1(j s))\nand deﬁning j∗= argmin1≤j≤K/summationtextnlt=1(ej,zt), we obtain:\nK t−1 t−1\nlog(Wn+1)≥log(/summationdisplay\nexp(−η/summationdisplay\nl(ej,zs))) =−η/summationdisplay\nl(ej,zs)\nj=1 s=1 s=1\nTogether:\nn nlogKηKn /summationdisplay\nl(at,zt)−min t≤j≤/summationdisplay\nl(ej,z)\nKt t=1≤ +\n1 η2=1\nThe choice of η:=√2logKnKyields the bound Rn√≤2KlogKn.\n177\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 21\nScribe:Ali Makhdoumi Nov. 25, 2015\n6. LINEAR BANDITS\nRecall form last lectures that in prediction with expert advise, at each time t, the player\nplaysat∈ {e1,...,ek}and the adversary plays ztsuch that l(at,zt)≤1 for some loss\nfunction. One example of such loss function is linear function l(at,zt) =aT\ntztwhere|zt|∞≤\n1. Linear bandits are a more general setting where the player selects an action at∈ A ⊂Rk,\nwhereAis a convex set and the adversary selects zTt∈ Zsuch that |ztat| ≤1. Similar to\nthe prediction with expert advise, the regret is deﬁned as\nRn=E/bracketleftBiggn n/summationdisplayTAtzt/bracketrightBigg\n−minTa zt,\na∈Kt=1/summationdisplay\nt=1\nwhereAtis a random variable in A. Note that in the prediction with expert advise, the set\nAwas essentially apolyhedronandwehadminn Ta∈K aTzt=1 t= min1≤j≤ke zjt. However, in\nthe linear bandit setting the minimizer of aTztcan be any point of the set Aand essentially\nthe umber experts that the player tries to ”comp/summationtext\nete” with are inﬁnity. Similar, to the\nprediction with expert advise we have two settings:\n1Full feedback: after time t, the player observes zt.\n2Bandit feedback: after time t, the player observes AT\ntzt, whereAtis the action that\nplayer has chosen at time t.\nWe next, see if we can use the bounds we have developed in the prediction with expert\nadvise in this setting. In particular, we have shown the following bounds for prediction\nwith expert advise:\n1Prediction with kexpert advise, full feedback: Rn√≤2nlogk.\n2Prediction with kexpert advise, bandit feedback: Rn√≤2nklogk.\nThe idea to deal with linear bandits is to discretize the set A. Suppose that Ais bounded\n(e.g.,A ⊂B2, whereB2is thel2ball inRk). We can use a1-covering ofnAwhich we\nhave shown to be of size (smaller than) O(nk). This means there exist y1,...,y|N|such that\nfor anya∈ A, there exist yisuch that ||yi−a|| ≤1. We now can bound the regret forn\ngeneral case, where the experts can be any point in A, based on the regret on the discrete\nset,N={y1,...,y|N|},as follows.\nRn=E/bracketleftBiggn/bracketrightBiggn /summationdisplayTAtzt\nt1−minTa zta∈A= t=1/bracketleftBiggn/summationdisplay\nn\n=E/summationdisplayTAtzt/bracketrightBigg\n−minTa zt+o(1).\na∈Nt=1/summationdisplay\nt=1\nTherefore, werestrictactions Attoacombinationoftheactionsthatbelongto {y1,...,y|N|}\n(we can always do this), then using the bounds for the prediction with expert advise, we\nobtain the following bounds:\n178\n1Linear bandit, full feedback: Rn≤/radicalbig\n2nlog(nk) =O(√knlogn), which in terms\nof dependency to nis of order O(√n) that is what we expect to have.\n2Linear bandit, bandit feedback: Rn≤/radicalbig\n2nnklog(nk) = Ω(n), which is useless in\nterms of dependency of nas we expect to obtain O(√n) behavior.\nThe topic of this lecture is to provide bounds for the linear bandit in the bandit feedback.\nProblem Setup: Let us recap the problem formulation:\n•at timet, player chooses action at∈ A ⊂[−1,1]k.\n•at timet, adversary chooses zt∈ Z ⊂Rk, whereaT\ntzt=/an}bracketle{tat,zt/an}bracketri}ht ∈[0,1].\n•Bandit feedback: player observes /an}bracketle{tat,zt/an}bracketri}ht( rather than ztin the full feedback setup).\nLiterature: O(n3/4) regret bound has been shown in [BB04]. Later on this bound has\nbeen improved to O(n2/3) in [BK04] and [VH06] with ”Geometric Hedge algorithm”, which\nwe will describe and analyze below. We need the following assumption to show the results:\nAssumption: There exist δsuch that δe1,...,δe k∈ A. This assumption guarantees that\nAhas full-dimension around zero.\nWe also discretize Awith a1-net of size Cnkand only consider the resulting discrete setn\nand denote it by A, where|A| ≤(3n)k. All we need to do is to bound\nRn=E/bracketleftBiggn/summationdisplayTAtzt/bracketrightBiggn\n−min/summationdisplayTa zt.\na∈At=1 t=1\nFor anytanda, we deﬁne\nt−1expη\nt(/parenleftBig\n− zs=1ˆT\nsa\np a) = ,t−1\na∈Aexp/summationtext/parenrightBig\n/parenleftBig\n−η zs=1ˆTsa/parenrightBig\nwhereηis a parameter (that we will/summationtext\nchoose later) an/summationtext\ndzˆtis deﬁnedto incorporate the idea of\nexploration versus exploitation. The algorithm which is termed Geometric Hedge Algorithm\nis as follows:\nAt timetwe have\n•Exploitation: with probability 1 −γdrawataccording to ptand letzˆt= 0.\n•Exploration: with probabilityγletat=δejfor some 1k≤j≤kandzˆt=\nk j)\n2/an}bracketle{t(akt,ztaγ/an}bracketri}htt=z eδ γtj.\nNote that δis the the parameter that we have by assumption on the set A, andηandγ\nare the parameters of the algorithm that we shall choose later.\nTheorem: Using Geometric Hedge algorithm for linear bandit with bandit feedback,\nwithγ=1=g\n1/3andη/radicalBig\nlon\n4/3, we haven kn\nE[Rn]≤2/3Cn/radicalbig\nlog3/2n k .\n179\nProof.Let the overall distribution of atbeqtdeﬁned as qt= (1−γ)pt+γU, whereUis a\nuniform distribution over the set {δe1,...,δe k}. Under this distribution, zˆtis an unbiased\nestimator of zt, i.e.,\nkγ k(Eat∼qt[zˆt] = 0(1−j)γ)+/summationdisplay\nztej=zt.k γj=1\nfollowing thesamelines of theproofthat wehadforanalyzingexponential weight algorithm,\nwe will deﬁne\n1\nwt= e p z\na∈A/parenleftBiggt−\nx−Tη aˆs\ns=1/parenrightBigg/summationdisplay /summationdisplay\n.\nWe then have\nlog/parenleftbiggwt+1/parenrightbigg\n= log/parenleftBigg/summationdisplayTpt(a)exp ηa zˆtwta∈/parenleftbig\n−\nA/parenrightBigg\n/parenrightbig\ne−2x≤1−x+x\n≤2log/parenleftBigg/summationdisplayT1( )/parenleftbigg\n1−ˆ +2(T2pta ηa z tη a zˆs)2a∈A/parenrightbigg/parenrightBigg\n(−T1= log/parenleftBigg\n1+/summationdisplay\n)/parenleftbigg\nˆ +2(T2pta ηa z tη a zˆt)2a∈A/parenrightbigg/parenrightBigg\nlog(1+x)≤x\n≤/summationdisplayT12pt(T2a)/parenleftbigg\n−ηa zˆt+η(a zˆt)2a∈A/parenrightbigg\n.\nTaking expectation from both sides leads to\nEat∼qt/bracketleftbiggwt+1log/parenleftbigg\nwt/parenrightbigg/bracketrightbigg\n≤ −ηEat∼qt/bracketleftBigg\npt(a)Ta zˆt\na∈A/bracketrightBigg/bracketleftBigg/summationdisplay η2\n+TEat∼qt/summationdisplay2pt(a)(a zˆt)2a∈A/bracketrightBigg\n=−/bracketleftbig2\nTηηEt∼ptatzˆt/bracketrightbig\n+2E a a∼qpt(a)(Ta zˆt)2t t/bracketleftBigg\na/summationdisplay\n∈A/bracketrightBigg\n2qt=(1−γ)pt+γUη=− γEat∼qt/bracketleftbigT ηatzˆT 2E t a−γ/bracketrightbig\n+η1 −∼U1tzˆTa t+Eγt a2t∼qt/bracketleftBigg\npt(a)(a zˆt)\na∈A/bracketrightBigg\naTzt≤1−η/bracketleftbig /bracketrightbig ηγ η2/bracketleftbig /bracketrightbig /summationdisplay\nt≤TEat∼qtatzˆt+ + ∼qt/bracketleftBigg/summationdisplayT2Eat pt(a)(a zˆ1−γ 1−t)γ2a∈A/bracketrightBigg\n.\nWe next, take summation of the previous relation for t= 1 up to nand use a telescopic\ncancellation to obtain\nn nηT ηγ η2\nT2E[logw E+]≤[logw1]−E n1/bracketleftBigg/summationdisplay\natzˆt/bracketrightBigg\n+n+E pt(a)(a zˆ1−t)γ 1γ2t=1−/bracketleftBigg/summationdisplay\nt=1a/summationdisplay\n∈A/bracketrightBigg\nn nηγ η2\n≤E[log ]−ηE/bracketleftBigg/summationdisplayT Tatzˆ2w1 t/bracketrightBigg\n+n+E pt(a)(a zˆt).(6.1)1γ2t=1−/bracketleftBigg/summationdisplay\nt=1a∈A/bracketrightBigg/summationdisplay\n180\nNote that for all a∗∈ Awe have\nn\nlog(wn+1) = log/parenleftBigg /parenleftBiggn\na/parenrightBigg/parenrightBigg/summationdisplay\nexp−η\n∈A/summationdisplayTa zˆs\ns=1≥ −η/summationdisplay\n.\ns=/an}bracketle{t∗a ,zˆs\n1/an}bracketri}ht\nUsingE[zˆs] =zs, leads to\nn\nE[log(wn+1)]≥ −η/summationdisplay∗a ,zs. (6.2)\ns=1/an}bracketle{t /an}bracketri}ht\nWe also have that\nlog(w1) = log|A| ≤2klogn. (6.3)\nPlugging (6.2) and (6.3) into (6.1), leads to\nγ ηE[Rn]≤n+E/bracketleftBiggn/bracketrightBigg/summationdisplay/summationdisplay\npta)(T 2n(at)2klogzˆ + . (6.4)1−γ2 ηt=1a∈A\nnIt remains to control the quadratic term E/bracketleftbig/summationtextp a aTz2\nt=1a∈At( )( ˆ t) . We use the fact that\n|(j)zt|,|(j)at| ≤1 to obtain/summationtext /bracketrightbig\nE/bracketleftBiggn n /summationdisplay/summationdisplayT2 T2pt(a)(a zˆt) = p a)Et(qt[(a zˆt) ]\nt=1a∈A/bracketrightBigg/summationdisplay\nt=1a/summationdisplay\n∈A\nn k\n=/summationdisplay/summationdisplay2γ k (j)pt(a)/parenleftbigg\n(1−γ)0+/summationdisplay/parenrightbigg\n[j2a zt]k γt=1a∈A j=1\n(j)n|ajzt|≤1\n≤/summationdisplay/summationdisplay k2\n(a)/parenleftbigg2\nt/parenrightbiggkp =n .γ γt=1a∈A\nPlugging this bound into (6.4), we have\nη k22klognE[Rn]≤γn+n+ .2γ η\nLettinglognγ=1\n/3andη=n1/radicalBig\nkn4/3leads to\nE[3/2 2/3Rn]≤Ck n/radicalbig\nlogn.\nLiterature: The bound we just proved has been improved in [VKH07] where they show\nO(d3/2√nlogn) bound with a better exploration in the algorithm. The exploration that we\nused in the algorithm was coordinate-wise. The key is that we have a linear problem and we\ncan use better tools from linear regression such as least square estimation. However, we will\ndescribe a slightly diﬀerent approach in which we never explore and the exploration is com-\npletely done with the exponential weighting. This approach also gives a better performance\nin terms of the dependency on k. In particular, we obtain the bound O(d√nlogn) which\ncoincides with the bound recently shown in [BCK 12] using a John’s ellipsoid.\n181\n−1Theorem: LetCt=Eat∼qt[a aT\nt],zˆt= (aTt tzt)Ctat, andγ= 0(sothat pt=qt). Using\nGeometric Hedge algorithm with η= 2/radicalBig\nlognfor linear bandit with bandit feedbackn\nleads to\nE[Rn]≤CK/radicalbig\nnlogn.\nProof.We follow the same lines of the proof as the previous theorem to obtain (6.4). Note\nthat the only fact that we used in order to obtain (6.4) is unbiasedness, i.e., E[zˆt] =zt,\nwhich holds here as well since\nE[zˆ−Et] = [1TCtatatzt] =−1CEt[Tatat]zt=zt.\nNote that we can use pseudo-inverse instead of inverse so that invertibility is not an issue.\nTherefore, rewriting (6.4) with γ= 0, we obtain\n/bracketleftBiggnη/summationdisplay/summationdisplayT22klognE[Rn]≤Eat∼pt pt(a)(a zˆt)\n∈/bracketrightBigg\n+ .2 ηt=1aA\nWe now bound the quadratic term as follows\nn n\nT2 T2Eat∼pt/bracketleftBigg/summationdisplay/summationdisplay\npt(a)(a zˆt)/bracketrightBigg\n=/summationdisplay/summationdisplay\np a)Et(at∼pt(a zˆt)\nt=1a∈A t=1a∈A/bracketleftbig /bracketrightbig\nCT\nt=C−1n nTt, zˆt=(atzt)Ctat=/summationdisplay\na)Tp aE t(zˆT 2tˆa= pt(T\nt a)Tz aE(−1T−1atzt)CtatatCta\nt=1a/summationdisplay\n∈A/bracketleftbig /bracketrightbig/summationdisplay\nt=1a\nT/summationdisplay\n∈A\nn n|a/bracketleftbig\ntzt|≤1 E/bracketrightbig\n/summationdisplay/summationdisplayT−1 T−1[a≤taT\np(a)t]=CtT−1a CE t tatatCta= pt(a)a Cta\nt=1a∈A/bracketleftbig /bracketrightbig /summationdisplay\nt=1a/summationdisplay\n∈A\nn n /summationdisplay/summationdisplaytr(AB)=tr(BA)= ( )tr(T−1 1pt r(−ta a C a) =/summationdisplay/summationdisplay\npt(a)tTCtaa)\nt=1a∈A t=1a∈A\nn n n\n=/summationdisplay\ntr(−1CEta∼pt[Taa]) =/summationdisplay\ntr(−1CtCt) = tr( Ik) =kn.\nt=1 t=1/summationdisplay\nt=1\nPlugging this bound into previous bound yields\nη2klognE[Rn]≤nk+ .2 η\nlognLettingη= 2/radicalBig\n, leads to E[Rn]n≤Ck√nlogn.\n182\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 22\nScribe:Aden Forrow Nov. 30, 2015\n7. BLACKWELL’S APPROACHABILITY\n7.1 Vector Losses\nDavid Blackwell introduced approachability in 1956 as a generalization of zero sum game\ntheory to vector payoﬀs. Born in 1919, Blackwell was the ﬁrst black tenured professor at\nUC Berkeley and the seventh black PhD in math in the US.\nRecall our setup for online linear optimization. At time t, we choose an action at∈∆K\nand the adversary chooses zt∈B∞(1). We then get a loss ℓ(at,zt) =/an}bracketle{tat,zt/an}bracketri}ht. In the full\ninformation case, where we observe ztand not just ℓ(at,zt), this is the same as prediction\nwith expert advice. Exponential weights leads to a regret bound\nRn≤/radicalbiggn\n2log(K).\nThe setup of a z\nero sum game is nearly identical:\n•Player 1 plays a mixed strategy p∈∆n.\n•Player 2 plays q∈∆m.\n•Player 1’s payoﬀ is p⊤Mq.\nHereMis the game’s payoﬀ matrix.\nTheorem: Von Neumann Minimax Theorem\nmax min⊤p Mq= min max⊤p Mq.\np∈∆nq∈∆m q∈∆mp∈∆n\nThe minimax is called the value of the game. Each player can prevent the other from doing\nany better than this. The minimax theorem implies that if there is a good response pqto\nany individual q, then there is a silver bullet strategy pthat works for any q.\nCorollary: If∀q∈∆n,∃psuch that p⊤Mq≥c, then∃psuch that ∀q,p⊤Mq≥c.\nVon Neumann’s minimax theorem can be extended to more general sets. The following\ntheorem is due to Sion (1958).\nTheorem: Sion’s Minimax Theorem LetAandZbe convex, compact spaces, and\nf:A×Z→R. Iff(a,·) is upper semicontinuous and quasiconcave on Z∀a∈Aand\n183\nf(·,z) is lower semicontinuous and quasiconvex on A∀z∈Z, then\ninf supf(a,z) = sup inf f(a,z).\na∈Az∈Z z∈aZ∈A\n(Note - this wasn’t given explicitly in lecture, but we do use it later.) Quasiconvex and\nquasiconcave are weaker conditions than convex and concave respectively.\nBlackwell looked at the case with vector losses. We have the following setup:\n•Player 1 plays a∈A\n•Player 2 plays z∈Z\n•Player 1’s payoﬀ is ℓ(a,z)∈dR\nWe suppose AandZare both compact and convex, that ℓ(a,z) is bilinear, and that\n/bardblℓ(a,z)/bardbl ≤R∀a∈A,z∈Z. All norms in this section are Euclidean norms. Can we\ntranslate the minimax theorem directly to this new setting? That is, if we ﬁx a set S⊂dR,\nand if∀z∃asuch that ℓ(a,z)∈S, does there exist an asuch that ∀z ℓ(a,z)∈S?\nNo. We’ll construct a counterexample. Let A=Z= [0,1],ℓ(a,z) = (a,z), and\nS={(a,z)∈[0,1]2:a=z}. Clearly, for any z∈Zthere is an a∈Asuch that a=zand\nℓ(a,z)∈S, but there is no a∈Asuch that ∀z,a=z.\nInstead of looking for a single best strategy, we’ll play a repeated game. At time t,\nplayer 1 plays at=at(a1,z1,...,at−1,zt−1) and player 2 plays zt=zt(a1,z1,...,a t−1,zt−1).\nPlayer 1’s average loss after niterations is\n1ℓ¯n=n/summationdisplay\nℓ(at,zt)nt=1\nLetd(x,S) be the distance between a point x∈dRand the set S, i.e.\nd(x,S) = inf\ns∈S/bardblx−s/bardbl.\nIfSis convex, the inﬁmum is a minimum attained only at the projection of xinS.\nDeﬁnition: AsetSisapproachable ifthereexistsastrategy at=at(a1,z1,...,a t−1,zt−1)\n¯ such that lim n→∞d(ℓn,S) = 0.\nWhether a set is approachable depends on the loss function ℓ(a,z). In our example, we can\nchoosea0= 0 and at=zt−1to get\nn1ℓ¯limn= lim/summationdisplay\n(zt−1,zt) = (z¯,z¯)∈S.\nn→∞ n→∞nt=1\nSo thisSis approachable.\n7.2 Blackwell’s Theorem\nWe have the same conditions on A,Z, andℓ(a,z) as before.\n184\nTheorem: Blackwell’s Theorem LetSbe a closed convex set of2Rwith/bardblx/bardbl ≤R\n∀x∈S. If∀z,∃asuch that ℓ(a,z)∈S, thenSis approachable.\nMoreover, there exists a strategy such that\n2Rd ℓ¯(n,S)≤√n\nProof.We’ll prove the rate; approachability of Sfollows immediately. The idea here is to\ntransform the problem to a scalar one where Sion’s theorem applies by using half spaces.\nSuppose we have a half space H={x∈dR:/an}bracketle{tw,x/an}bracketri}ht ≤c}withS⊂H. By assumption,\n∀z∃asuch that ℓ(a,z)∈H. That is, ∀z∃asuch that /an}bracketle{tw,ℓ(a,z)/an}bracketri}ht ≤c, or\nmaxmin/an}bracketle{tw,ℓ(a,z)/an}bracketri}ht ≤c.\nz∈Z a∈A\nBy Sion’s theorem,\nminmax/an}bracketle{tw,ℓ(a,z)\na∈A z∈Z/an}bracketri}ht ≤c.\nSo∃a∗\nHsuch that ∀z ℓ(a,z)∈H.\nThis works for any Hcontaining S. We want to choose Htso thatℓ(at,zt) brings the\n¯averageℓ ¯tcloser to Sthanℓt−1. An intuitive choice is to have the hyperplane Wbounding\nH ¯tbe the separating hyperplane between Sandℓt−1closest to S. This is Blackwell’s\n¯ strategy: let Wbe the hyperplane through πt∈argminµ∈S/bardblℓt−1−µ/bardblwith normal vector\nℓ¯t−1−πt. Then\nH={x∈dR:/an}bracketle{tx−πt,ℓ¯t−1−πt/an}bracketri}ht ≤0}.\nFinda∗\nHand play it.\nWe need one more equality before proving convergence. The average loss can be ex-\npanded:\nt1ℓ¯t t−1 tt t\nt=−1 t1 1¯(ℓt−1−πt)+−πt+ℓtt t t\nNow we look at the distance of the average from S, using the above equation and the\ndeﬁnition of πt+1:\nd¯(ℓ ,S)2ℓ¯t=/bardblt−πt+1/bardbl2\n≤ /bardblℓ¯t−πt/bardbl2\n=/vextenddouble/vextenddouble/vextenddouble2t−1 1¯(ℓt−1−πt)+ (ℓt−πt)t/vextenddouble\n/vextenddoublet\nℓt/vextenddouble\n=/parenleftbiggt−1/parenrightbigg2\nd¯(ℓ2 π2tt1¯t−1,S) +/bardbl −/vextenddouble/vextenddouble\n/bardbl+2−/an}bracketle{tℓt−πt,ℓt t2 t2t−1−πt/an}bracketri}ht\nSinceℓt∈H, the last term is negative; since ℓtandπtare both bounded by R, the middle\n2term is bounded by4R\n2. Letting µ2\nt=t2d(ℓ¯2t,S) , we have a recurrence relationt\nµ2\nt≤µ2\nt−1+4R2,=−¯ℓ+1ℓ\n185\nimplying\nµ2\nn≤4nR2.\nRewriting in terms of the distance gives the desired bound,\n2Rd ℓ¯(t,S)≤√n\nNote that this proof fails for nonconvex S.\n7.3 Regret Minimization via Approachability\nConsider the case A= ∆KK,Z=B∞(1). As we showed before, exponential weights Rn≤\nc/radicalbig\nnlog(K). We can get the same dependence on nwith an approachability-based strategy.\nFirst recall that\nn n1 1Rn=/summationdisplay 1ℓ(at,zt)−min ℓ(ej,zt)n n jnt=1/summationdisplay\nt=1\nn n\n= m x/bracketleftBigg\n1a/summationdisplay 1ℓ(at,zt) ℓ(ej,zt)\njn nt=1−/summationdisplay\nt=1/bracketrightBigg\nIf we deﬁne a vector average loss\nn1ℓ¯n=/summationdisplay\n(ℓ(at,zt)−ℓ(e1,zt),...,ℓ(aKR t,zt)e\nt=1−ℓ(K,zt))n∈,\nRn ¯ ¯\nn→0 if and only if all components of ℓnare nonpositive. That is, we need d(−ℓn,OK)→0,\nwhere−O={x∈KR:−1≤xi≤0, iK∀ }is the nonpositive orthant. Using Blackwell’s\napproachability strategy, we get\nRn≤d(ℓ¯−n,O/radicalbigg\nK)≤c .nKn\nTheKdependence is worse than exponential weights,√\nKinstead of log( K).\nHow do we ﬁnd a∗\nH? As a concrete example, let K= 2. We need a/radicalbig\n∗\nHtp satisfy\n/an}bracketle{t∗w,ℓ(∗aH,z)/an}bracketri}ht=/an}bracketle{tw,/an}bracketle{taH,z/an}bracketri}hty−z/an}bracketri}ht ≤c\nfor allz. Hereyis the vector of all ones. Note that c≥0 since 0 is in Sand therefore in\nH. Rearranging,\n/an}bracketle{t∗aH,z/an}bracketri}ht/an}bracketle{tw,y/an}bracketri}ht ≤ /an}bracketle{tw,z/an}bracketri}ht+c,\nChoosing a∗\nH=wwill work; the inequality reduces to/angbracketleftw,y/angbracketright\n/an}bracketle{tw,z/an}bracketri}ht ≤ /an}bracketle{tw,z/an}bracketri}ht+c.\nApproachability in the banditsetting with only partial feedback is still an openproblem.\n186\n18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 23\nScribe: Jonathan Weed Dec. 2, 2015\n1. POTENTIAL BASED APPROACHABILITY\nLast lecture, we saw Blackwell's celebrated Approachability Theorem, which establishes a\nprocedure by which a player can ensure that the average (vector) payo\u000b in a repeated game\napproaches a convex set. The central idea was to construct a hyperplane separating the\n\u0016 convex set from the point `t1, the average loss so far. By projecting perpendicular to\u0000\nthis hyperplane, we obtained a scalar-valued problem to which von Neumann's minimax\ntheorem could be applied. The set Sis approachable as long as we can always \fnd a \\silver\nbullet,\" a choice of action atfor which the loss vector `tlies on the side of the hyperplane\ncontainingS. (See Figure 1.)\nFigure 1: Blackwell approachability\nConcretely, Blackwell's Theorem also implied the existence of a regret-minimizing algo-\nrithm for expert advice. Indeed, if we de\fne the vector loss `tby (`t)i=`(at;zt)\u0000`(ei;zt),\nthen the average regret at time tis equivalent to the sup-norm distance between the average\n\u0016loss`tand the negative orthant. Approaching the negative orthant therefore corresponds\nto achieving sublinear regret.\nHowever, this reduction yielded suboptimal rates. To bound average regret, w\nthe sup-norm distance by the Euclidean distance, which led to an extra factor ofpe replaced\nkappear-\ning in our bound. In the sequel, we develop a more sophisticated version of approachability\nthat allows us to adapt to the geometry of our problem. (Much of what follows resem-\nbles out development of the mirror descent algorithm, though the two approaches di\u000ber in\ncrucial details.)\n1.1 Potential functions\nWe recall the setup of mirror descent, \frst described in Lecture 13. Mirror descent achieved\naccelerated rates by employing a potential function which was strongly convex with respect\n187\nto the given norm. In this case, we seek what is in some sense the opposite: a function\nwhose gradient does not change too quickly. In particular, we make the following de\fnition.\nDe\fnition: A function \b : I Rd!I R is a potential forS2I R if it satis\fes the following\nproperties:\n\u000f\b is convex.\n\u000f\b(x)\u00140 forx2S.\n\u000f\b(y) = 0 fory2@S.\n\u000f\b(y)\u0000\b(x)\u0000hr\b(x);y\u0000xi\u0014hx2k \u0000yk2, where by abuse of notation we use\nr\b(x) to denote a subgradient of \b at x.\nGiven such a function, we recall two associated notions from the mirror descent algo-\nrithm. The Bregman divergence associated to \b is given by\nD\b(y;x) = \b(y)\u0000\b(x)\u0000hr\b(x);y\u0000xi:\nLikewise, the associated Bregman projection is\n\u0019(x) = argmin D\b(y;x):\ny2S\nWe aim to use the function \b as a stand-in for the Euclidean distance that we employed\nin our proof of Blackwell's theorem. To that end, the following lemma establishes several\nproperties that will allow us to generalize the notion of a separating hyperplane.\nLemma: For any convex, closed set Sandz2S,x2SC, the following properties\nhold.\n\u000f hz\u0000\u0019(x);r\b(x)i\u0014 0;\n\u000f hx\u0000\u0019(x);r\b(x)i\u0015 \b(x):\nIn particular, if \b is positive on SC, thenH:=fyjhy\u0000\b(x);r\b(x)i = 0g is a\nseparating hyperplane.\nOur proof requires the following proposition, whose proof appears in our analysis of the\nmirror descent algorithm and is omitted here.\nProposition: For allz2S, it holds\nhr\b(\u0019 (x))\u0000r\b(x);\u0019 (x)\u0000zi\u00140:\nProof of Lemma. Denote by\u0019the projection \u0019(x). The \frst claim follows upon expanding\nthe expression on the left-hand side as follows\nhz\u0000\u0019;r\b(x)i =hz\u0000\u0019;r\b(x)\u0000r\b(\u0019 )i+hz\u0000\u0019;r\b(\u0019 )i:\n188\nThe above Proposition implies that the \frst term is nonpositive. Since the function \b is\nconvex, we obtain\n0\u0015\b(z)\u0015\b(\u0019) +hz\u0000\u0019;r\b(\u0019 )i:\nSince\u0019lies on the boundary of S, by assumption \b( \u0019) = 0 and the claim follows.\nFor the second claim, we again use convexity:\n\b(\u0019)\u0015\b(x) +h\u0019\u0000x;r\b(x)i:\nSince \b(\u0019 ) = 0, the claim follows.\n1.2 Potential based approachability\nWith the de\fnitions in place, the algorithm for approachability is essentially the same as it\nbefore we introduced the potential function. As before, we will use a projection de\fned by\nthe hyperplane H=fyjhy\u0000\u0016 \u0016\u0019(`t\u00001);r\b(`t= 0 and von Neumann's minmax theorem\u00001i g\nto \fnd a \\silver bullet\" a\u0003\ntsuch that`t=`(a\u0003\nt;zt) satis\fes\nh`t\u0000 \u0016\u0019t;r\b(`t\u00001)i\u0014 0:\nAll that remains to do is to analyze this procedure's performance. We have the following\ntheorem.\nTheorem: Ifk`(a;z )k\u0014Rholds for all z2A;z2Z and all assumptions above are\nsatis\fed, then\n4R2hlogn\u0016\b(`n)\u0014 :n\nProof. The de\fnition of the potential \b required that \b be upper bounded by a quadratic\nfunction. The proof below is a simple application of that bound.\nAs before, we note the identity\n`\u0016 \u0016t t\u00001`t=`t\u00001+ :t\nThis expression and the de\fnition of \b imply.\n1 h\u0016\u0014 \u0016 )h \u0016 \b(`t\b(`t\u00001) +`t\u0000\u0016 \u0016`2t1; )\u0000r\b(`t1i+k`\u0000 t`t 22\u0000t\u00001tk:\n\u0016 The last term is the easiest to control. By assumption, `tand`t1are contained in a ball\u0000\nof radiusR, sok`t\u0000\u0016`t\u00001k2\u00144R2.\nTo bound the second term, write\n1 1 1h \u0016 `t\u0000\u0016 \u0016 \u0016 \u0016 `t1;r\b(`t1)i=h`t\u0000\u0019t;r\b(`t1) +\u0019 ` ; \b(` ):t\u0000 \u0000 \u0000ithtt\u0000t\u00001rt\u00001i\nThe \frst term is nonpositive by assumption, since this is how the algorithm constructs\nthe silver bullet. By the above Lemma, the inner product in the second term is at most\n\u0000\u0016\b(`t\u00001).\nWe obtain \u0012t\u00001 2\u0016\b(`t)\u0014\u0013hR2\u0016\b(`t)\u00001+:t t2\u0000\u0016`\n189\n\u0016 De\fningut=t\b(`t) and rearranging, we obtain the recurrence\n2hR2\nut\u0014ut\u00001+;t\nSon\nun=Xn\nut\u0000ut\u00001\u00142hR2X1\ntt=1 t=1\nApplying the de\fnition of unproves the claim.\n1.3 Application to regret minimization\nWe now show that potential based approachability providespan improved bound on regret\nminimization. Our ultimate goal is to replace the bound nk(which we proved last lecture)\nbypnlogk(which we know to be the optimal bound for prediction with expert advice).\nWe will be able to achieve this goal up to logarithmic terms in n. (A more careful analysis\nof the potential de\fned below does actually yields an optimal rate.)\nRecall thatRn \u0016 =d(`n;On K\u0000), whereRnis the cumulative regret after nrounds and O\u00001 K\nis the negative orthant. It is not hard to see that d=kx+k, wherex+is the positive 11\npart of the vector x.\nWe de\fne the following potential function:\nK1 1\b(x) = log\u00110\nKX\ne\u0011(xj)+\nj=11\n:\nThe function \b is a kind of \\soft max\" of the@\npositive entriesA\nofx. (Note that this de\fnition\ndoes not agree with the use of the term soft max in the literature|the di\u000berence is the\npresence of the factor1.) The terminology soft max is justi\fed by noting thatK\n1 1kx+k= max(x log+logK logK\nj)+\u0014max e\u0011(xj)+ :\u0011\u0014\b(x) + 1j j\u0011 K \u0011\nThe potential function therefore serves as an upper bound on the sup distance, up to an\nadditive logarithmic factor.\nThe function \b de\fned in this way is clearly convex and zero on the negative orthant.\nTo verify that it is a potential, it remains to show that \b can be bounded by a quadratic.\nAway from the negative orthant, \b is twice di\u000berentiable and we can compute the\nHessian explicitly:\nr2\b(x) =\u0011diag(r\b(x))\u0000\u0011r\br\b>:\nFor any vector usuch thatkuk2= 1, we therefore have\nK K\nu>r2\b(x)u =\u0011X\nu2\nj(r\b(x))j\u0000\u0011(u>\njr\b(x))2\u0014\u0011\n=1X\nu2\nj(\nj=1r\b(x))j\u0014\u0011;\nsincekuk2= 1 andkr\b(x)\n2k1\u00141.\nWe conclude that r\b(x)\u0016\u0011I, which for nonnegative xandyimplies the bound\n\u0011\b(y)\u0000\b(x)\u0000hr\b(x);y\u0000xi\u00142ky\u0000xk2:\u00144hR2logn:\n190\nIn fact, this bound holds everywhere. Therefore \b is a valid potential function for the\nnegative orthant, with h=\u0011.\nThe above theorem then implies that we can ensure\nRn logK 4R2\u0011lognlogK\u0014 \u0016\b(`n) +\u0014 +:n \u0011 n \u0011\nTo optimize this bound, we pick \u0011=1q\nnlogKand obtain the bound2R logn\nRn\u00144Rp\nnlognlogK:\nAs alluded to earlier, a more careful analysis can remove the log nterm. Indeed, for this\nparticular choice of \b, we can modify the above Lemma to obtain the sharper bound\nhx\u0000\u0019(x);r\b(x)i\u0015 2\b(x):\nWhen we substitute this expression into the above proof, we obtain the recurrence\nrelationt\u0016\b(`t)\u00002 c\u0014 \u0016\b(`t\u00001) +:t t2\nThis small change is enough to prevent the appearance of log nin the \fnal bound.\n191\nReferences\n[Nem12] Arkadi Nemirovski, On safe tractable approximations of chance constraints , Euro-\npean J. Oper. Res. 219(2012), no. 3, 707–718. MR 2898951 (2012m:90133)\n[NS06] Arkadi Nemirovski and Alexander Shapiro, Convex approximations of chance\nconstrained programs , SIAM J. Optim. 17(2006), no. 4, 969–996. MR 2274500\n(2007k:90077)[BB04] McMahan, H. Brendan, and Avrim Blum. Online geometric optimization in the\nbandit setting against an adaptive adversary . Conference on Learning theory\n(COLT) 2004.\n[BCK 12] Bubeck, Sbastien, Nicolo Cesa-Bianchi, and Sham M. Kakade. Towards mini-\nmax policies for online linear optimization with bandit feedback . arXiv preprint\narXiv:1202.3079 (2012). APA\n[BK04] Awerbuch, Baruch, and Robert D. Kleinberg. Adaptive routing with end-to-end\nfeedback: Distributed learning and geometric approaches .Proceedings of thethirty-\nsixth annual ACM symposium on Theory of computing. ACM, 2004.\n[Bla56] D. Blackwell, An analog of the minimax theorem for vector payoﬀs , Paciﬁc J.\nMath. 6 (1956), no. 1, 1–8\n[Sio58] M. Sion, On general minimax theorems . Paciﬁc J. Math. 8 (1958), no. 1, 171–176.\n[VH06] Dani, Varsha, and ThomasP. Hayes. Robbing the bandit: Less regret in online geo-\nmetric optimization against an adaptive adversary . Proceedings of the seventeenth\nannual ACM-SIAM symposium on Discrete algorithm. Society for Industrial and\nApplied Mathematics, 2006.\n[VKH07] Dani, Varsha, Sham M. Kakade, and Thomas P. Hayes, The price of bandit in-\nformation for online optimization , Advances in Neural Information Processing\nSystems. 2007./ne}ationslash\n[Bub15] S´ ebastien Bubeck, Convex optimization: algorithms and complexity , Now Publish-\ners Inc., 2015./ne}ationslash\n[DGL96] L. Devroye, L. Gyo ¨rﬁ, and G. Lugosi, A probabilistic theory of pattern recognition ,\nApplications of Mathematics (New York), vol. 31, Springer-Verlag, New York,\n1996. MR MR1383093 (97d:68196)\n[HTF09] Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The elements of statis-\ntical learning , second ed., Springer Series in Statistics, Springer, New York, 2009,\nData mining, inference, and prediction. MR 2722294 (2012d:62081)/ne}ationslash\n[Kol11] Vladimir Koltchinskii. Oracle inequalities in empirical risk minimization and sparse\n\u0013 \u0013 recovery problems. Ecole d'Et\u0013 e de Probabilit\u0013 es de Saint-Flour XXXVIII-2008. Lec-\nture Notes in Mathematics 2033. Berlin: Springer. ix, 254 p. EUR 48.10 , 2011.\n[LT91] Michel Ledoux and Michel Talagrand. Probability in Banach spaces, volume 23 of\nErgebnisse der Mathematik und ihrer Grenzgebiete (3) [Results in Mathematics and\nRelated Areas (3)]. Springer-Verlag, Berlin, 1991. Isoperimetry and processes./ne}ationslash\n[Kea90] Michael J Kearns. The computational complexity of machine learning . PhD thesis,\nHarvard University, 1990.\n192", "topic": "Math for ML", "subtopic": "gradient descent", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "81406c87dccb9e873cfafa876a4d69c3 MIT18 657F15 LecNote", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "c5493a24-341a-4654-ba8e-a93d4f191bda", "text": "2033. Berlin: Springer. ix, 254 p. EUR 48.10 , 2011. [LT91] Michel Ledoux and Michel Talagrand. Probability in Banach spaces, volume 23 of Ergebnisse der Mathematik und ihrer Grenzgebiete (3) [Results in Mathematics and Related Areas (3)]. Springer-Verlag, Berlin, 1991. Isoperimetry and processes./ne}ationslash [Kea90] Michael J Kearns. The computational complexity of machine learning . PhD thesis, Harvard University, 1990. 192\n\n/ne}ationslash/ne}ationslash\n[Zha04] Tong Zhang. Statistical behavior and consistency of classiﬁcation methods based\non convex risk minimization. Ann. Statist. , 32(1):56–85, 2004.\n193\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "convex", "section_heading": "2033. Berlin: Springer. ix, 254 p. EUR 48.10 , 2011. [LT91] Michel Ledoux and Michel Talagrand. Probability in Banach sp", "source_title": "81406c87dccb9e873cfafa876a4d69c3 MIT18 657F15 LecNote", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "92e3fb5b-b99a-423e-9093-eb6ca6dea1cd", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 21\nScribe:Ali Makhdoumi Nov. 25, 2015\n6. LINEAR BANDITS\nRecall form last lectures that in prediction with expert advise, at each time t, the player\nplaysat∈ {e1,...,ek}and the adversary plays ztsuch that l(at,zt)≤1 for some loss\nfunction. One example of such loss function is linear function l(at,zt) =aT\ntztwhere|zt|∞≤\n1. Linear bandits are a more general setting where the player selects an action at∈ A ⊂Rk,\nwhereAis a convex set and the adversary selects zTt∈ Zsuch that |ztat| ≤1. Similar to\nthe prediction with expert advise, the regret is deﬁned as\nRn=E/bracketleftBiggn n/summationdisplayTAtzt/bracketrightBigg\n−minTa zt,\na∈Kt=1/summationdisplay\nt=1\nwhereAtis a random variable in A. Note that in the prediction with expert advise, the set\nAwas essentially apolyhedronandwehadminn Ta∈K aTzt=1 t= min1≤j≤ke zjt. However, in\nthe linear bandit setting the minimizer of aTztcan be any point of the set Aand essentially\nthe umber experts that the player tries to ”comp/summationtext\nete” with are inﬁnity. Similar, to the\nprediction with expert advise we have two settings:\n1Full feedback: after time t, the player observes zt.\n2Bandit feedback: after time t, the player observes AT\ntzt, whereAtis the action that\nplayer has chosen at time t.\nWe next, see if we can use the bounds we have developed in the prediction with expert\nadvise in this setting. In particular, we have shown the following bounds for prediction\nwith expert advise:\n1Prediction with kexpert advise, full feedback: Rn√≤2nlogk.\n2Prediction with kexpert advise, bandit feedback: Rn√≤2nklogk.\nThe idea to deal with linear bandits is to discretize the set A. Suppose that Ais bounded\n(e.g.,A ⊂B2, whereB2is thel2ball inRk). We can use a1-covering ofnAwhich we\nhave shown to be of size (smaller than) O(nk). This means there exist y1,...,y|N|such that\nfor anya∈ A, there exist yisuch that ||yi−a|| ≤1. We now can bound the regret forn\ngeneral case, where the experts can be any point in A, based on the regret on the discrete\nset,N={y1,...,y|N|},as follows.\nRn=E/bracketleftBiggn/bracketrightBiggn /summationdisplayTAtzt\nt1−minTa zta∈A= t=1/bracketleftBiggn/summationdisplay\nn\n=E/summationdisplayTAtzt/bracketrightBigg\n−minTa zt+o(1).\na∈Nt=1/summationdisplay\nt=1\nTherefore, werestrictactions Attoacombinationoftheactionsthatbelongto {y1,...,y|N|}\n(we can always do this), then using the bounds for the prediction with expert advise, we\nobtain the following bounds:\n1\n1Linear bandit, full feedback: Rn≤/radicalbig\n2nlog(nk) =O(√knlogn), which in terms\nof dependency to nis of order O(√n) that is what we expect to have.\n2Linear bandit, bandit feedback: Rn≤/radicalbig\n2nnklog(nk) = Ω(n), which is useless in\nterms of dependency of nas we expect to obtain O(√n) behavior.\nThe topic of this lecture is to provide bounds for the linear bandit in the bandit feedback.\nProblem Setup: Let us recap the problem formulation:\n•at timet, player chooses action at∈ A ⊂[−1,1]k.\n•at timet, adversary chooses zt∈ Z ⊂Rk, whereaT\ntzt=/an}bracketle{tat,zt/an}bracketri}ht ∈[0,1].\n•Bandit feedback: player observes /an}bracketle{tat,zt/an}bracketri}ht( rather than ztin the full feedback setup).\nLiterature: O(n3/4) regret bound has been shown in [BB04]. Later on this bound has\nbeen improved to O(n2/3) in [BK04] and [VH06] with ”Geometric Hedge algorithm”, which\nwe will describe and analyze below. We need the following assumption to show the results:\nAssumption: There exist δsuch that δe1,...,δe k∈ A. This assumption guarantees that\nAhas full-dimension around zero.\nWe also discretize Awith a1-net of size Cnkand only consider the resulting discrete setn\nand denote it by A, where|A| ≤(3n)k. All we need to do is to bound\nRn=E/bracketleftBiggn/summationdisplayTAtzt/bracketrightBiggn\n−min/summationdisplayTa zt.\na∈At=1 t=1\nFor anytanda, we deﬁne\nt−1expη\nt(/parenleftBig\n− zs=1ˆT\nsa\np a) = ,t−1\na∈Aexp/summationtext/parenrightBig\n/parenleftBig\n−η zs=1ˆTsa/parenrightBig\nwhereηis a parameter (that we will/summationtext\nchoose later) an/summationtext\ndzˆtis deﬁnedto incorporate the idea of\nexploration versus exploitation. The algorithm which is termed Geometric Hedge Algorithm\nis as follows:\nAt timetwe have\n•Exploitation: with probability 1 −γdrawataccording to ptand letzˆt= 0.\n•Exploration: with probabilityγletat=δejfor some 1k≤j≤kandzˆt=\nk j)\n2/an}bracketle{t(akt,ztaγ/an}bracketri}htt=z eδ γtj.\nNote that δis the the parameter that we have by assumption on the set A, andηandγ\nare the parameters of the algorithm that we shall choose later.\nTheorem: Using Geometric Hedge algorithm for linear bandit with bandit feedback,\nwithγ=1=g\n1/3andη/radicalBig\nlon\n4/3, we haven kn\nE[Rn]≤2/3Cn/radicalbig\nlog3/2n k .\n2\nProof.Let the overall distribution of atbeqtdeﬁned as qt= (1−γ)pt+γU, whereUis a\nuniform distribution over the set {δe1,...,δe k}. Under this distribution, zˆtis an unbiased\nestimator of zt, i.e.,\nkγk(Eat∼qt[zˆt] = 0(1−j)γ)+/summationdisplay\nztej=zt.k γj=1\nfollowing thesamelines of theproofthat wehadforanalyzingexponential weight algorithm,\nwe will deﬁne\n1\nwt= e p z\na∈A/parenleftBiggt−\nx−Tηaˆs\ns=1/parenrightBigg/summationdisplay/summationdisplay\n.\nWe then have\nlog/parenleftbiggwt+1/parenrightbigg\n= log/parenleftBigg/summationdisplayTpt(a)exp ηa zˆtwta∈/parenleftbig\n−\nA/parenrightBigg\n/parenrightbig\ne−2x≤1−x+x\n≤2log/parenleftBigg/summationdisplayT1( )/parenleftbigg\n1−ˆ +2(T2pta ηa z tη a zˆs)2a∈A/parenrightbigg/parenrightBigg\n(−T1= log/parenleftBigg\n1+/summationdisplay\n)/parenleftbigg\nˆ +2(T2pta ηa z tη a zˆt)2a∈A/parenrightbigg/parenrightBigg\nlog(1+x)≤x\n≤/summationdisplayT12pt(T2a)/parenleftbigg\n−ηa zˆt+η(a zˆt)2a∈A/parenrightbigg\n.\nTaking expectation from both sides leads to\nEat∼qt/bracketleftbiggwt+1log/parenleftbigg\nwt/parenrightbigg/bracketrightbigg\n≤ −ηEat∼qt/bracketleftBigg\npt(a)Ta zˆt\na∈A/bracketrightBigg/bracketleftBigg/summationdisplay η2\n+TEat∼qt/summationdisplay2pt(a)(a zˆt)2a∈A/bracketrightBigg\n=−/bracketleftbig2\nTηηEt∼ptatzˆt/bracketrightbig\n+2E a a∼qpt(a)(Ta zˆt)2t t/bracketleftBigg\na/summationdisplay\n∈A/bracketrightBigg\n2qt=(1−γ)pt+γUη=− γEat∼qt/bracketleftbigT ηatzˆT 2E t a−γ/bracketrightbig\n+η1 −∼U1tzˆTa t+Eγt a2t∼qt/bracketleftBigg\npt(a)(a zˆt)\na∈A/bracketrightBigg\naTzt≤1−η/bracketleftbig /bracketrightbig ηγ η2/bracketleftbig /bracketrightbig /summationdisplay\nt≤TEat∼qtatzˆt+ + ∼qt/bracketleftBigg/summationdisplayT2Eat pt(a)(a zˆ1−γ 1−t)γ2a∈A/bracketrightBigg\n.\nWe next, take summation of the previous relation for t= 1 up to nand use a telescopic\ncancellation to obtain\nn nηT ηγη2\nT2E[logw E+]≤[logw1]−E n1/bracketleftBigg/summationdisplay\natzˆt/bracketrightBigg\n+n+E pt(a)(a zˆ1−t)γ 1γ2t=1−/bracketleftBigg/summationdisplay\nt=1a/summationdisplay\n∈A/bracketrightBigg\nn nηγ η2\n≤E[log ]−ηE/bracketleftBigg/summationdisplayT Tatzˆ2w1 t/bracketrightBigg\n+n+E pt(a)(a zˆt).(6.1)1γ2t=1−/bracketleftBigg/summationdisplay\nt=1a∈A/bracketrightBigg/summationdisplay\n3\nNote that for all a∗∈ Awe have\nn\nlog(wn+1) = log/parenleftBigg /parenleftBiggn\na/parenrightBigg/parenrightBigg/summationdisplay\nexp−η\n∈A/summationdisplayTa zˆs\ns=1≥ −η/summationdisplay\n.\ns=/an}bracketle{t∗a ,zˆs\n1/an}bracketri}ht\nUsingE[zˆs] =zs, leads to\nn\nE[log(wn+1)]≥ −η/summationdisplay∗a ,zs. (6.2)\ns=1/an}bracketle{t /an}bracketri}ht\nWe also have that\nlog(w1) = log|A| ≤2klogn. (6.3)\nPlugging (6.2) and (6.3) into (6.1), leads to\nγηE[Rn]≤n+E/bracketleftBiggn/bracketrightBigg/summationdisplay/summationdisplay\npta)(T 2n(at)2klogzˆ+ . (6.4)1−γ2 ηt=1a∈A\nnIt remains to control the quadratic term E/bracketleftbig/summationtextp a aTz2\nt=1a∈At( )( ˆ t) . We use the fact that\n|(j)zt|,|(j)at| ≤1 to obtain/summationtext /bracketrightbig\nE/bracketleftBiggn n /summationdisplay/summationdisplayT2 T2pt(a)(a zˆt) = p a)Et(qt[(a zˆt) ]\nt=1a∈A/bracketrightBigg/summationdisplay\nt=1a/summationdisplay\n∈A\nn k\n=/summationdisplay/summationdisplay2γ k (j)pt(a)/parenleftbigg\n(1−γ)0+/summationdisplay/parenrightbigg\n[j2a zt]k γt=1a∈A j=1\n(j)n|ajzt|≤1\n≤/summationdisplay/summationdisplay k2\n(a)/parenleftbigg2\nt/parenrightbiggkp =n.γ γt=1a∈A\nPlugging this bound into (6.4), we have\nηk22klognE[Rn]≤γn+n+ .2γ η\nLettinglognγ=1\n/3andη=n1/radicalBig\nkn4/3leads to\nE[3/2 2/3Rn]≤Ck n/radicalbig\nlogn.\nLiterature: The bound we just proved has been improved in [VKH07] where they show\nO(d3/2√nlogn) bound with a better exploration in the algorithm. The exploration that we\nused in the algorithm was coordinate-wise. The key is that we have a linear problem and we\ncan use better tools from linear regression such as least square estimation. However, we will\ndescribe a slightly diﬀerent approach in which we never explore and the exploration is com-\npletely done with the exponential weighting. This approach also gives a better performance\nin terms of the dependency on k. In particular, we obtain the bound O(d√nlogn) which\ncoincides with the bound recently shown in [BCK 12] using a John’s ellipsoid.\n4\n−1Theorem: LetCt=Eat∼qt[a aT\nt],zˆt= (aTt tzt)Ctat, andγ= 0(sothat pt=qt). Using\nGeometric Hedge algorithm with η= 2/radicalBig\nlognfor linear bandit with bandit feedbackn\nleads to\nE[Rn]≤CK/radicalbig\nnlogn.\nProof.We follow the same lines of the proof as the previous theorem to obtain (6.4). Note\nthat the only fact that we used in order to obtain (6.4) is unbiasedness, i.e., E[zˆt] =zt,\nwhich holds here as well since\nE[zˆ−Et] = [1TCtatatzt] =−1CEt[Tatat]zt=zt.\nNote that we can use pseudo-inverse instead of inverse so that invertibility is not an issue.\nTherefore, rewriting (6.4) with γ= 0, we obtain\n/bracketleftBiggnη/summationdisplay/summationdisplayT22klognE[Rn]≤Eat∼pt pt(a)(a zˆt)\n∈/bracketrightBigg\n+ .2 ηt=1aA\nWe now bound the quadratic term as follows\nn n\nT2 T2Eat∼pt/bracketleftBigg/summationdisplay/summationdisplay\npt(a)(a zˆt)/bracketrightBigg\n=/summationdisplay/summationdisplay\np a)Et(at∼pt(a zˆt)\nt=1a∈A t=1a∈A/bracketleftbig /bracketrightbig\nCT\nt=C−1n nTt, zˆt=(atzt)Ctat=/summationdisplay\na)Tp aE t(zˆT 2tˆa= pt(T\nt a)Tz aE(−1T−1atzt)CtatatCta\nt=1a/summationdisplay\n∈A/bracketleftbig /bracketrightbig/summationdisplay\nt=1a\nT/summationdisplay\n∈A\nn n|a/bracketleftbig\ntzt|≤1 E/bracketrightbig\n/summationdisplay/summationdisplayT−1 T−1[a≤taT\np(a)t]=CtT−1a CE t tatatCta= pt(a)a Cta\nt=1a∈A/bracketleftbig /bracketrightbig /summationdisplay\nt=1a/summationdisplay\n∈A\nn n /summationdisplay/summationdisplaytr(AB)=tr(BA)= ( )tr(T−1 1pt r(−ta a C a) =/summationdisplay/summationdisplay\npt(a)tTCtaa)\nt=1a∈A t=1a∈A\nn n n\n=/summationdisplay\ntr(−1CEta∼pt[Taa]) =/summationdisplay\ntr(−1CtCt) =tr(Ik) =kn.\nt=1 t=1/summationdisplay\nt=1\nPlugging this bound into previous bound yields\nη2klognE[Rn]≤nk+ .2 η\nlognLettingη= 2/radicalBig\n, leads to E[Rn]n≤Ck√nlogn.\nReferences\n[BCK 12] Bubeck, Sbastien, Nicolo Cesa-Bianchi, and Sham M. Kakade. Towards mini-\nmax policies for online linear optimization with bandit feedback . arXiv preprint\narXiv:1202.3079 (2012). APA\n5\n[BB04] McMahan, H. Brendan, and Avrim Blum. Online geometric optimization in the\nbandit setting against an adaptive adversary . Conference on Learning theory\n(COLT) 2004.\n[VH06] Dani, Varsha, and ThomasP. Hayes. Robbing the bandit: Less regret in online geo-\nmetric optimization against an adaptive adversary . Proceedings of the seventeenth\nannual ACM-SIAM symposium on Discrete algorithm. Society for Industrial and\nApplied Mathematics, 2006.\n[BK04] Awerbuch, Baruch, and Robert D. Kleinberg. Adaptive routing with end-to-end\nfeedback: Distributed learning and geometric approaches .Proceedings of thethirty-\nsixth annual ACM symposium on Theory of computing. ACM, 2004.\n[VKH07] Dani, Varsha, Sham M. Kakade, and Thomas P. Hayes, The price of bandit in-\nformation for online optimization , Advances in Neural Information Processing\nSystems. 2007.\n6\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "convex", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "c70563158c4cc2ddbed4987d8f458d4e MIT18 657F15 L21", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "e7b700a4-2032-4dff-abdd-38ba6bebcfbd", "text": "18.657: Mathematics of Machine Learning\nLecturer: Alexander Rakhlin Lecture 19\nScribe:Kevin Li Nov. 16, 2015\n4. PREDICTION OF INDIVIDUAL SEQUENCES\nIn this lecture, we will try to predict the next bit given the previous bits in the sequence.\nGiven completely random bits, it would be impossible to correctly predict more than half\nof the bits. However, certain cases including predicting bits generated by a human can\nbe correct greater than half the time due to the inability of humans to produce truly\nrandom bits. We will show that the existence of a prediction algorithm that can predict\nbetter than a given threshold exists if and only if the threshold satiﬁes certain probabilistic\ninequalities. For more information on this topic, you can look at the lecture notes at\nhttp://stat.wharton.upenn.edu/ ~rakhlin/courses/stat928/stat928_notes.pdf\n4.1 The Problem\nTo state the problem formally, given a sequence y1,...,yn,...∈ {−1,+1}, we want to ﬁnd\na prediction algorithm yˆt=yˆt(y1,...,yt1) that correctly predicts ytas much as possible. −\niidIn order to get a grasp of the problem, we will consider the case where y1,...,yn∼Ber(p).\nIt is easy to see that we can get\nn1IE/bracketleftBigg\nn/summationdisplay\n=\n=1{yˆtyt\nt}/bracketrightBigg\n→min{p,1−p}\nby letting yˆtequal majority vote of the ﬁrst t−1 bits. Eventually, the bit that occurs\nwith higher probability will alway/BDs have occurred more times. So the central limit theorem\nshows that our loss will approach min {1p,1−p}at the rate of O(√).n\nKnowing that the distribution of the bits are iid Bernoulli random variables made the\nprediction problem fairly easy. More surprisingly is the fact that we can achieve the same\nfor any individual sequence.\nClaim: There is an algorithm such that the following holds for any sequence y1,...,yn,....\nn1limsup/summationdisplay\n{yˆt=yt}−min{y¯n,1n\nnnt=1−y¯} ≤0 a.s.\n→∞\nIt is clear that no deterministic strategy can achieve this bound. For any deterministic\nstrategy, we can just choose yt=−yˆtand the predictions would be wrong every time. So\nwe need a non-deterministic algorithm that chooses qˆt= IE[yˆt]∈[−1,1].\nTo prove this claim, we will look at a more general problem. Take a ﬁxed horizon n≥1,\nand function φ:{±1}n→ /CA. Does/BDthere exist a randomized prediction strategy such that\nfor anyy1,...,ynn1IE[/summationdisplay\n{yˆt=ytnt=1}]≤φ(y1,...,yn) ?\n1\n/BD/ne}ationslash\n/ne}ationslash\n/ne}ationslash\nFor certain φsuch asφ≡0, it is clear that no randomized strategy exists. However for\n1φ≡, the strategy of randomly predicting the next bit ( qˆt= 0) satisﬁes the inequality.2\nLemma: For a stable φ, the following are equivalent\nn1a)∃(qˆt)t=1,...,n∀y1,...,ynIE[/summationdisplay\n{yˆt=yt}]≤φ(y1,...,yn)nt=1\n1b) IE[φ(ǫ1,...,ǫn)]≥whereǫ1,...,ǫnare Rademacher random variables2\nwhere stable is deﬁned as follows\nDeﬁnition (Stable Function): A function φ:{±1}n→is stable if\n1|φ(...,yi,...)−φ(...,−yi,...)| ≤n\nProof.(a=\n1⇒1b)SupposeIE φ <. Take(y1,...,yn) = (ǫ1,...\n/CA\n,ǫn). ThenIE[1nynt=1{ˆt=2\nǫt}] =>IE[φ]sotheremustexistasequence(nǫ1,...,ǫ1n)suchthatIE[/summationtext\n/summationtext\nt{yˆt=ǫt}]>2 n=1\nφ(ǫ1,...,ǫn).\n(b=⇒a) Recu/BDrsively deﬁne V(y1,...,yt) such that ∀y1,...,yn\n/BD\n1V(y1,...,yt1) = min max/parenleftBig\nIE[{yˆt=yt}]+V(y1,...y\n/BD\n− n)\nqt∈[−1,1]yt∈±1n/parenrightBig\nLooking at the deﬁnition, we can see that IE[1n\nt=1{yˆt=yt}] =V( )n∅ −V(y1,...,yn).\nNow we note that V(y1,...,yt) =−t−IE[φ(y1,/summationtext\n.\n/BD\n..,yt,ǫt n +1,...,ǫn)] satisﬁes the recursive2\ndeﬁnition since\n1 tminmax IE[yˆt=yt] IE[φ(y1,...,yt,ǫt+1,...,ǫn)]\nqˆtytn{ } −\n/BD\n−2n\nqˆtyt t1=minmax−−IE[φ(y1,...,yt,ǫt+1,...,ǫn)]−\nqˆtyt2n−2n\nqˆ t1q 1=minm x {−t −ˆ ta −tIE[φ(y1,...,yt1,1,ǫt+1,...,ǫn)]−,−IE[φ(y . −1, − 1, ..,yt−1,ǫt+1,...,ǫn)]−\nqˆt 2n 2n2n−2n}\nt1=−IE[φ(y1,...,yt1,ǫt,ǫt+1,...,ǫ − n)]−−2n\n=V(y1,...,yt1)−\nThe ﬁrst equality uses the fact that for a,b∈ {±1},{1a=b}=−ab, the second uses the2\nfact that yt∈ {±1}, the third minimizes the entire expression by choosing qˆtso that the\ntwo expressions in the max are equal. Here the fact that φis stable means qˆt∈[−1,1] and\nis the only place where we need φto be stable.\nTherefore we have\n/BD\nn1IE[/summationdisplay 1{yˆt=yt}] =V(∅)−V(y1,...,yn) =−IE[φ(ǫ1,...,ǫn)]++φ(y1,...,yn)n 2t=1≤φ(y1,...,yn)\n2\n/BD/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash/BD/ne}ationslash\nby b).\nBy choosing φ= min{y¯,1−y¯}+c√, this shows there is an algorithm that satisﬁes ourn\noriginal claim.\n4.2 Extensions\n4.2.1 Supervised Learning\nWe can extend the problem to a regression type problem by observing xtand trying to\npredictyt. In this case, the objective we are trying to minimize would be\n1 1ln/summationdisplay\n(yˆt,yt)−inf\nf∈F,n/summationdisplay\nl(f(xt)yt)\nIt turns out that the best achievable performance in such problems is governed by martin-\ngale (or, sequential) analogues of Rademacher averages, covering numbers, combinatorial\ndimensions, and so on. Much of Statistical Learning techniques extend to this setting of\nonline learning. In addition, the minimax/relaxation framework gives a systematic way of\ndeveloping new prediction algorithms (in a way similar to the bit prediction problem).\n4.2.2 Equivalence to Tail Bounds\nWe can also obtain probabilistic tail bound on functions φon hypercube by using part a) of\nthe earlier lemma. Rearranging part a) of the lemma we get 1 −2φ(1y1,...,yn)≤qˆtyt.n\nThis implies/summationtext\n2\nIP/parenleftbig 1µ 1 µφ(ǫ1,...,ǫn)<−/parenrightbig\n= IP/parenleftbig\n1−2φ(ǫ1,...,ǫn)> µ≤IP2 n/summationdisplay\nqˆtǫt> µ≤e−2n\nSo IEφ≥1=⇒existence of a strategy = ⇒tail boun/parenrightbig\nd forφ/parenleftbig\n1<./parenrightbig\n2 2\nWe can extend the results to higher dimensions. Consider z1,...,zn∈B2whereB2is\na ball in a Hilbert space. We can deﬁne recursively yˆ0= 0 and yˆt+1= ProjB2(yˆt−1√zt).n\nBased on the properties of projections, for every∗∈, we have1/summationtext/an}bracketle{tˆ−∗/an}bracketri}ht ≤1y B 2 yty ,zt n√.n\nzTakingy∗ t= ,/bardbl/summationtext\n/summationtextzt/bardbl\nn n\n∀z1,...,zn,/summationdisplay\nzt√/bardbl\nt=1/bardbl−n≤/summationdisplay\nyˆt, zt\nt=1/an}bracketle{t − /an}bracketri}ht\nTake a martingale diﬀerence sequence Z1,...,Z nwith values in B2. Then\nn n\nIP/parenleftbig\n/bardbl/summationdisplay 2nµZt√\nt=1/bardbl−n > µ/parenrightbig\n≤IP(/summationdisplay\nt=1/an}bracketle{tyˆt,−Zt/an}bracketri}ht> µ)≤e−2\nIntegrating out the tail,\nn\nIE/bardbl/summationdisplay\nZt\nt=1/bardbl ≤c√n\n3\nIt can be shown using Von Neumann minimax theorem that\nn n\n∃(yˆt)∀z1,...,zn,y∗∈B2/summationdisplay\nWt√/an}bracketle{tyˆt−y∗,zt/an}bracketri}ht ≤ supE c n\nMDSWt1,...,W=1n/bardbl/summationdisplay\nt=1/bardbl ≤\nwhere the supremum is over all martingale diﬀerence sequences (MDS) with values in B2.\nBy the previous part, this upper bound is c√n. We conclude an interesting equivalence of\n(a) deterministic statements that hold for all sequences, (b) tail bounds on the size of a\nmartingale, and (c) in-expectation bound on this size.\nIn fact, this connection between probabilistic bounds and existence of prediction strate-\ngies for individual sequences is more general and requires further investigation.\n4\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "erm", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "f278788cce095391fa4deebf01fd1357 MIT18 657F15 L19", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "3d86c382-f7ef-48ad-9a83-d9b057592562", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 1\nScribe:Haihao (Sean) Lu Nov. 2, 2015\n3. STOCHASTIC BANDITS\n3.1 Setup\nThe stochastic multi-armed bandit is a classical model for decision making and is deﬁned\nas follows:\nThere are Karms(diﬀerent actions). Iteratively, a decision maker chooses an arm k∈\n{1,...,K}, yielding a sequence XK,1,...,X K,t,..., which are i.i.d random variables with\nmeanµk. Deﬁne µ∗= max jµjor∗ ∈argmax. A policy πis a sequence {πt}t≥1, which\nindicates which arm to be pulled at time t.πt∈ {1,...,K}and it depends only on the\nobservations strictly interior to t. The regret is then deﬁned as:\nn n\nRn= maxIE[ XK,t]−IE[Xπt,t]\nk/summationdisplay\nt=1/summationdisplay\nt=1\nn\n=nµ∗−IE[/summationdisplay\nXπt,t]\nt=1\nn\n=nµ∗−IE[IE[\nK/summationdisplay\nXπt,t|πt]]\nt=1\n=/summationdisplay\n∆kIE[Tk(n)],\nk=1\nnwhere ∆ k=µ∗−µkandTk(n) =/summationtext\nt=11I(πt=k) is the number of time when arm kwas\npulled.\n3.2 Warm Up: Full Info Case\nX1,t\n.Assume in this subsection that K= 2 and we observe the full information\n..\nat\nXK,t\ntimetafter choosing πt. So in each iteration, a normal idea is to choose\nthe arm\nwith\nhighest average return so far. That is\n¯ πt= argmax Xk,t\nk=1,2\nwhere\n1¯Xk,t=tt/summationdisplay\ns=1\nAssume from now on that all random variable Xk,tare subGaussian with variance proxy\n2 2\nσ2, which means IE[ euxu σ]≤e2for allu∈IR. For example, N(0,σ2) is subGaussian withXk,s\n18\nvariance proxy σ2and any boundedrandomvariable X∈[a,b] is subGaussianwith variance\nproxy (b−a)2/4 by Hoeﬀding’s Lemma.\nTherefore,\nRn= ∆IE[T2(n)], (3.1)\nwhere ∆ = µ1−µ2. Besides,\nn\n¯ ¯ T2(n) = 1+/summationdisplay\n1I(X2,t> X1,t)\nt=2\nn\n= 1+/summationdisplay¯ ¯1I(X2,t−X1,t−(µ2−µ1)≥∆).\nt=2\n¯ ¯ It is easy to check that ( X2,t−X1,t)−(µ2−µ1) is centered subGaussian with variance proxy\n2σ2, whereby\n2¯ ¯−t∆IE[1I(X22,t> X1,t)]≤e4σ\nby a simple Chernoﬀ Bound. Therefore,\n∞2\nRn≤∆(1+/summationdisplay 4σ2−t∆e24σ)≤∆+ , (3.2)∆t=0\nwhereby the benchmark is\n4σ2\nRn≤∆+ .∆\n3.3 Upper Conﬁdence Bound (UCB)\nWithout loss of generality, from now on we assume σ= 1. A trivial idea is that after s\npulls on arm k, we use µˆk,s=1/summationtext\nj∈{pulls ofk}XK,jand choose the one with largest µˆk,s.s\nThe problem of this trivial policy is that for some arm, we might try it for only limited\ntimes, which give a bad average and then we never try it again. In order to overcome this\nlimitation, a good idea is to choose the arm with highest upperbound estimate on the mean\nof each arm at some probability lever. Note that the arm with less tries would have a large\ndeviations from its mean. This is called Upper Conﬁdence Bound policy.\n2\nAlgorithm 1 Upper Conﬁdence Bound (UCB)\nfort= 1 toKdo\nπt=t\nend for\nfort=K+1 tondo\nt−1\nTk(t) =/summationdisplay\n1I(πt=k)\ns=1\n(number of time we have pull arm kbefore time t)\n1µˆk,t=/summationdisplay\nXK,t∧sTk(t)s=1\nlogt)πt∈argmax/braceleftigg\n2 (µˆk,t+2\nk∈[K]/radicaligg\nTk(t)/bracerightigg\n,\nend for\nTheorem: The UCB policy has regret\nK /summationdisplaylogn π2\nRn≤8 +(1+ )∆k 3k,∆k>0/summationdisplay\n∆k\nk=1\nProof.From now on we ﬁx ksuch that ∆ k>0. Then\nn\nIE[Tk(n)] = 1+\nt=/summationdisplay\nIP(πt=k).\nK+1\nNote that for t > K,\n2logt 2logt{πt=k} ⊆ {µˆk,t+2/radicaligg\n≤µˆ∗,t+2 }Tk(t)/radicaligg\nT∗(t)\n/braceleftigg /radicaligg\n2logt/radicaligg\n/uniondisplay 2logt 2logt⊆ {µk≥µˆk,t+2 } {µ∗≥µˆ∗,t+2 }/uniondisplay\n{µ∗≤µk+2Tk(t) T∗(t)/radicaligg\n,πt=k}Tk(t)/bracerightigg\nAnd from a union bound, we have\n/radicaligg\n2logt 2logtIP(µˆk,t−µk<−2 ) = IP( µˆk,t−µk<2Tk(t)/radicaligg\n)Tk(t)\nt−s8logt\n≤/summationdisplay\nexp(s)2s=1\n1=t3\n3t−1\n2logt 2logtThus IP( µk> µˆk t+ 2\nk)≤1, 3and similarly we have IP( µ∗> µˆ∗,t+ 2 ) ≤1,T(t)t T3∗(t)t\nwhereby/radicalig /radicalig\nn n n /summationdisplay 1 2logtIP(πt=k)≤2 µk+2/radicaligg\n/summationdisplay\n+/summationdisplay\nIP(µ∗≤ ,πt=k)t3 Tk(t)t=K+1 t=1 t=1\n∞ n1 8logt≤2/summationdisplay\n+/summationdisplay\nIP(Tk(t)≤ ,πt=k)t3∆2\nt=1 t=1 k\n∞ n\n≤2/summationdisplay1 gn+/summationdisplay 8loIP(Tk(t)≤ ,πt=k)t3∆2\nt=1 t=1 k\n∞ ∞/summationdisplay1 8 o≤2 + (3\nt=1/summationdisplay l gnIPs≤ )t ∆2\ns=1 k\n∞\n≤2/summationdisplay1 8log n+t2∆2\nt=1 k\nπ28logn= + ,3∆2\nk\nwheresis the counter of pulling arm k. Therefore we have\nK\nRn=/summationdisplay /summationdisplay π28logn∆kIE[Tk(n)]≤ ∆k(1+ + ) ,3∆2\nk k,∆kk =1 >0\nwhich furnishes the proof.\nConsider the case K= 2 at ﬁrst, then from the theorem above we know Rn∼logn,∆\nwhich is consistent with intuition that when the diﬀerence of two arm is small, it is hard to\ndistinguish which to choose. On the other hand, it always hold that Rn≤n∆. Combining\nlogn log(n∆2)these two results, we have Rn≤ ∧ n∆, whereby Rn≤ up to a constant.∆ ∆\nActually it turns out to be the optimal bound. When K≥3, we can similarly get the\nlog(n∆2)result that Rn≤/summationtext\nk\nkk. This, however, is not the optimal bound. The optimal bound∆\nshould be/summationtextlog(n/H)\nkk, which includes the harmonic sum and H=1. See [Lat15].∆/summationtext\nk∆2\nk\n3.4 Bounded Regret\nFrom above we know UCB policy can give regret that increases with at most rate log nwith\nn. In this section we would consider whether it is possible to have bounded regret. Actually\nit turns out that if there is a known separator between the expected reward of optimal arm\nand other arms, there is a bounded regret policy.\nWe would only consider the case when K= 2 here. Without loss of generality, we\nassumeµ1=∆andµ∆2=−, then there is a natural separator 0.2 2\n4\nAlgorithm 2 Bounded Regret Policy (BRP)\nπ1= 1 and π2= 2\nfort= 3 tondo\nifmaxkµˆk,t>0then\nthenπt= argmaxkµˆk,t\nelse\nπt= 1,πt+1= 2\nend if\nend for\nTheorem: BRP has regret\n16Rn≤∆+.∆\nProof.\nIP(πt= 2) = IP( µˆ2,t>0,πt= 2)+IP( µˆ2,t≤0,πt= 2)\nNote that\nn n /summationdisplay\nIP(µˆ2,t>0,πt= 2)≤IE 1I(µˆ2,t>0,πt= 2)\nt=3/summationdisplay\nt=3\nn\n≤IE/summationdisplay\n1I(µˆ2,t−µ2>0,πt= 2)\nt=3\n∞\n≤/summationdisplay 2\ne−s∆\n8\ns=1\n8=,∆2\nwheresis the counter of pulling arm 2 and the third inequality is a Chernoﬀ bound.\nSimilarly,\nn n/summationdisplay\nIP(µˆ2,t≤0,πt= 2) =/summationdisplay\nIP(µˆ1,t≤0,πt−1= 1)\nt=3 t=3\n8≤,∆2\nCombining these two inequality, we have\n16Rn≤∆(1+ ) ,∆2\nReferences\n[Lat15] Tor Lattimore, Optimally conﬁdent UCB : Improved regret for ﬁnite-armed bandits ,\nArxiv:1507.07880, 2015.\n5\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "norm", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "7b9dd3e7208aa3a217bfa9d8ffa48447 MIT18 657F15 L18", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "d2a8d09b-b44d-4601-8f24-a54004829674", "text": "18.657. Fall 2105\nRigollet November 6, 2015\nProblem set #3 (due Wed., November 11)\nShould be typed in LAT XE\nProblem 1. Kernels\nLetk1andk2be two PSD kernels on a space X.\n1. Show that the kernel kdeﬁned by k(u,v) =k1(u,v)k2(u,v) for any u,v∈ Xis PSD.\n[Hint: consider the Hadamard product between eigenvalue decompositions\nof the Gram matrices associated to k1andk2].\n2. Letg:C →IR be a given function. Show that the kernel kdeﬁned by k(u,v) =\ng(u)g(v) is PSD.\n3. LetQbe a polynomial with nonnegative coeﬃcients. Show that the kernel kdeﬁned\nbyk(u,v) =Q(k1(u,v)) for any u,v∈ Xis PSD.\n4. Show that the kernel kdeﬁned by k(u,v) = exp(k1(u,v)) for any u,v∈ Xis PSD.\n[Hint: use series expansion] .\n5. LetX= IRdand/ba∇dbl ·/ba∇dbldenote the Euclidean norm on IRd. Show that the kernel k\ndeﬁned by k(u,v) = exp(−/ba∇dblu−v/ba∇dbl2) is PSD.\nProblem 2. Convexity and Projections\n1. Give an algorithm that computes projections on the set\nC=/braceleftbig\nx∈IRd: max|xi| ≤1\n1≤i≤d/bracerightbig\nand prove a rate of convergence.\n2. Give an algorithm that computes projections on the set\nd\n∆ =/braceleftbig\nx∈IRd:/summationdisplay\nxi= 1,xi≥0/bracerightbig\ni=1\nand prove a rate of convergence.\npage 1 of 3\n3. Recall that theEuclidean normon n×nreal matrices isalso known as theFrobenius\nnorm and is deﬁned by /ba∇dblM/ba∇dbl2=Trace(M⊤M). LetSnbe the set of n×nsymmetric\nmatrices with real entries. Let S+\nndenote theset of n×nsymmetric positive deﬁnite\nmatrices with real entries, that is M∈ Snif and only if M∈ Snand\nx⊤Mx≥0,∀x∈IRn.\n(a) Show that S+\nnis convex and closed.\n(b) Giveanexplicit formulafortheprojection(withrespecttotheFrobeniusnorm)\nof a matrix M∈ SnontoS+\nn\n4. LetC⊂IRdbe a closed convex set and for any x∈IRddenote by π(x) its projection\nontoC. Show that for any x,y∈IRd, it holds\n/ba∇dblπ(x)−π(y)/ba∇dbl ≤ /ba∇dblx−y/ba∇dbl\nwhere/ba∇dbl·/ba∇dbldenotes the Euclidean norm.\nShow that for any y∈ C,\n/ba∇dblπ(x)−y/ba∇dbl ≤ /ba∇dblx−y/ba∇dbl,\nProblem 3. Convex conjugate\nFor any function f:D⊂IRd→IR, deﬁne its convex conjugate f∗by\nf∗(y) = sup/parenleftbig\ny⊤x−f(x)\nx∈/parenrightbig\n. (1)\nC\nThe domain of the function f∗is taken to be the set D={y∈IRd:f∗(y)<∞}.\n1. Findf∗andDif\n(a)f(x) = 1/x,C= (0,∞),\n(b)f(x) =1\n2\n(c)f(x) = log/summationtextd\nj=1exp(xj),x= (x1,...,x d),C= IRd.\nLetfbe strictly convex and diﬀerentiable and that C= IRd.\n2. Show that f(x)≥f∗∗(x) for allx∈C.\n3. Show that the supremum in (1) is attained at x∗such that ∇f(x∗) =y.\n4. Recall that Df(·,·) denotes the Bregman divergence associated to f. Show that\nDf(x,y) =Df∗/parenleftbig\n∇f(y),∇f(x)/parenrightbig\npage 2 of 3|x|2\n2,C=IRd,\nProblem 4. Around gradient descent\nIn what follows, we want to solve the constrained problem:\nminf(x).\nx∈C\nwherefis aL-Lipschitz convex function and C⊂IRdis a compact convex set with\ndiameter at most R(in Euclidean norm). Denote by x∗a minimum of fonC.\n1. Assume that we replace the updates in the projected gradient descent algorithm by\ngsys+1=xs−η, g s∈∂f(xs)./ba∇dblgs/ba∇dbl\nxs+1=πC(ys+1),\nwhereπC(·) is the projection onto C.\nWhat guarantees can you prove for this algorithm under the same assumptions?\n2. Consider the following updates:\nys∈argmin∇f(xs)⊤y\ny∈C\nxs+1= (1−γs)xs+γsys,\nwhereγs= 2/(s+1).\nIn what follows, we assume that fis diﬀerentiable and β-smooth:\nβf(y)−f(x)≤ ∇f(x)⊤(y−x)+|y−x|2\n22.\n(a) Show that\nβf(x∗\ns+1)−f(xs)≤γs(f(x)−f(xs))+γ2 2\nsR2\n(b) Conclude that for any k≥2,\n2\nf(xk)−f(x∗2βR)≤.k+1\npage 3 of 3\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "gradient descent", "section_heading": "18.657. Fall 2105", "source_title": "b7fec9284ff39b01661e87e0153b1cdc MIT18 657F15 PS3", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "9c734915-de4f-4c4f-9420-850b21b975e6", "text": "18.657 PS 2 SOLUTIONS\n1.Problem 1\n(1) LetY1;:::;Ynbe ghost copies of X1;:::;Xn. Then we have\nE\"\n1sup\f\f\fXn n\n\f1\n\f\u001bi[f(Xi)\u0000E[f(X)]]\f\f#\n\f\f\f\n\f\f=E\"\nsup\fX\n\f\f\u001bi[f(Xi)\u0000E[f(Yi)]]\nfn n 2Fi=1\f\f#\n\f\n\u0014E\f\f\n\"f2Fi=1\nn1sup\u001bi[f(Xi)f(Yi)];\nf\f\nn2F\f\fX\ni=1\u0000\f\f#\n\f\nby Jensen's inequality,\f\f\f\f\n=E\"\nsup ;\nf\f\f\fn\n\f1\n\f[f(Xi)f(Yi)]n2FX\ni=1\u0000\f\f#\n\f\nas\u001bi[f(Xi)\u0000f(Yi)] andf(Xi)\f\n\u0000f(Yi) have the same distribution,\f\n=E\"\nsup\nf2F\f\f\f\f\fXn1[(f(Xi)\u0000E[f(Xi)])\u0000(f(Yi)Eni=1\u0000[f(Yi)])]\f\f#\n1\f\n\u00142E\"\nsup\f\f\fXn\n\f\f [f(Xi) (\f\nfn2Fi=1\u0000E[f X\f\ni)]]#\nby the triangle inequality.\f\f\f\f\f;\n(2) The distribution of giis the same as that of jgij\u001bi, so we can write\nE\"n n\nsupX\ngif(Xi)i g\ni#\n=EXi;\u001b\nf\"\nEgi\"\nsup i\nf=1X\ni=1j j\u001bif(Xi)jXi;\u001bi\n2F 2F##\n\u0015EXi;\u001bi\"n\nsup [\nfX\nE\ni=1jgij]\u001bif(Xi)\n2F#\n;\nby Jensen's inequality,\nrn2= E\"\nsupX\n\u001bif(Xi)#\n;\u0019f2Fi=1\nusing the \frst absolute moment of the standard Gaussian.\nDate : Fall 2015.\n1\n2 18.657 PS 2 SOLUTIONS\n(3) We compute:\nRn(F+h) =E\"\n1sup\f\f\fXn\n\f\f\u001bi(f(Xi) +h(Xi))\nfn2Fi=1\f\f#\nn1\f\nn1\u0014Esup\u001b\f\n\"\f\fX\n\f\f\n\fif(Xi)n2Fi=1\f\f#\n\f\f\f+E\nf\"\nn\f\n\f\f\fX\n\f\fh(Xi);\ni=1\f\f#\n\f\f\f\nby the triangle inequality,\nn1=Rn(F) +E\"\nn\f\f#\n\fX\n\fh(Xi)\ni=1\f\f\f\f;\n2n1\u0014Rn(F) +\f \f\nvuuutEn2 !3\n4X\n\u001bih(Xi)\ni=15;\nby Jensen's inequality,\n1=Rn(F) +nvuuXn\nt E[h(Xi)2] + 2 E[\u001bi\u001bjh(Xi)h(Xj)]\nvi=1X\ni<j\n1=Rn(F) +uun\ntX\nE[h(X2i) ];ni=1\nby symmetry,\n1\u0014Rn(F) +np\nnkhk21\n=Rn(F) +khk1:n\n(4) By the triangle inequality, we have\nn k1Rn(F1+:::+Fk) =E2\n4sup\f\nX\nj(Xi\nfj2Fjn\f\f\n\u001biX\nf )\f3\nXk\f\f \f\f\nE\f\f \f5\n\u0014\f\nj=1\"i=1j=1\n1sup\nfj2Fjn\f\f\fXn\n\f\f\u001bifj(Xi)\ni=1\f\f#\nk\f\n=\f\f\nX\nRn(\nj=1Fj):\n18.657 PS 2 SOLUTIONS 3\n(5) The supremum over di\u000berent choices of fjis at least the supremum over a single repeated\nchoice:\nn k1Rn(F|+:{z::+F}) =E2\n4sup\u001bifj(Xi)\nfjn2F\f\f\f\ni=1X\nj=1\f\nkX\f3\n\f\f\f\nn k1\f\f5\n\u0015E2\n4sup\f \f\nfn2F\f\f\f\fX\n\f\f\u001bi\ni=1X\nf(Xi)\n\"\fj=1\f\f3\n\f\f\n\u001b\f5\n=kEsupXn1\f\f\n\f\fif(Xi)\nfn\f\n2Fi=1\f#\n=kRn(F);\f\f\f\nwhich provides the reverse bound to that of (4), establishing\f\nequality in this case.\n4 18.657 PS 2 SOLUTIONS\n2.Problem 2\n(1) LetAbe a maximum 2\"-packing of (T;d); let Bbe a minimum \"-covering. As Bis an\"-\ncovering, for each a2Athere exists some choice of b(a)2Bsuch thatd(a;b(a))\u0014\". This\nmapb:A!Bis an injection: by the triangle inequality, no point in Bcan be within \"of two\npoints ofA, as these are at least 2\" apart. ThusjAj\u0014jBj, so thatD(T;d; 2\")\u0014N(T;d;\").\nLetCbe maximum \"-packing. Note that Cis in fact an \"-covering: if some point of T\nhad distance greater than \"to each point of C, then we could add it to Cto produce a larger\n\"-packing, contradicting maximality. It follows that N(T;d;\")\u0014D(T;d;\").\n(2) LetBndenote the Euclidean ball in Rn. Letu2Bnandv2Bm. Then we can write\nu=u+\"\u0003u,v=v+\"1\u0003v, whereu\u00032Nn,v\u00032Nm,\"u2Bn, and\"v 421Bm. We compute:4\nkMk= sup u>Mv\nu2Bn;v2Bm\n= sup u>Mv +\"\u0003>\nuMv +u>M\"\nu ;v ;\" ;\" v\u0003\u0003 v\n\u0003 \u0003 u\n\u0014\u0012\nsupu>Mv\u0013\n+ \nsup\">u>\nu2Nn;v\u00032N \u0003 m\u0003\u0003 uMv\n1n\u0003!\n+ \nsup M\"v\nn\n\u0012 \u0013\"u2B ;v2Nm u2B ;\"21m\n4 \u0003 vB4!\n1 1\u0014 maxu>Mv +kMk+kMk:\nu\u00032Nn;v\u00032Nm\u00034 4 \u0003\nRearranging, we have\nkMk\u00142 max u>Mv\nu\u00032Nn;v\u00032Nm\u0003\u0003\nas desired.\n(3) By independence and Hoe\u000bding's lemma, we have, for all s>0,\nE[exp(su>Mv)] =Y\nE[exp(suiMijvj)]\ni;j\n\u0014Y 1Es2i;j\u0014\nexp\u0012\n2u2\niv2\nj\u0013\u0015\n=E2\n4exp0\n@1s2 2\n2X\nu2\nivj\ni;j13\nA5\n\u0014exp(s2=2);\nwheneveru2Bnandv2Bm.\ndRecall from the notes that Bdhas covering number at most3, so that we are max-\"\nimizing overjNn\u0002Nmj\u001412n+mpoints. It follows from the standard maximal inequality\nfor subgaussian random variables, or else by explicitly using Jensen\u0000 \u0001\nwith log and exp and\nreplacing a maximum by a sum, that\nEpkMk\u00142p\n2 log(12m+n)\u0014C(m+pn):\n18.657 PS 2 SOLUTIONS 5\n3.Problem 3\n(1) LetAbe an\"-net for [0; 1]; we can construct one with size at most1+ 1. LetXpredenote2\"\nthe set of non-decreasing functions from fx1;:::;xn;gtoA, and letXbe the set of functions\n[0;1]![0;1] de\fned by piecewise linear extension of functions in Xpre(and constant extension\non [0;x1] and [xn;1]). It is straightforward to see that Xis an\"-net for (F;dx): givenf ,12F\nwe de\fneg2Xpreby takingg(xi) to be the least point of Alying within \"off(xi), and then\nthe function in Xde\fned by extension of gis within\"off.\nIt remains to count X. Leta1<:::<a kbe the elements of A, wherek\u00141+1. Functions2\"\nf2Xare uniquely de\fned by the count for each 1 \u0014i\u0014kof how many xjsatisfyf(xj) =ai.\nA naive count of the possibilities for these values yields\nN(F;dx;\")\u0014jXj\u0014(n+ 1)1+1=2\"n1 \u00142\";\nvalid forn\u00153.\n(2) AsN(F;dx\n2;\")\u0014N(F;dx;\"), andf 1 for allf , the chaining bound yields1j j\u0014 2F\n12Rn\u0014inf 4\" +pZ1q\nlogN(F;dxd\n0n2;t)t\n\">\"\n121\n\u0014inf 4\" +\n\">0ptnZ\n\"p\n\u00001logndt\nlogn= inf 4\" + 24\n\">0r\n(1p\u0000\")\n\u0014lim 4\" + 24\n\"!0rn\nlogn(1p\nn\u0000\")\n= 24r\nlogn:n\n(3) We bound N(F;dx\n1;\")\u0014N(F;dx;\")1\u0014n2=\"\nr. The theorem in Section 5.2.1 yields\n2 log(2N(Rn\u0014inf\"+F;dx\n1;\"))\n\" n\n2\"\u00001logn+ 2 log 2\u0014inf\"+\n\"r\n:n\nSetting the two terms equal, to optimize over \"(in asymptotics), we have\n\"3=2 2 logn+ 2\" log 2 2 log n=n\u0019 ;n\nfor a bound ofr r\nRn.\u0012logn\nn\u00131=3\n;\nwhich is strictly weaker than the chaining bound.\n6 18.657 PS 2 SOLUTIONS\n4.Problem 4\n(1) We adapt the proof from class to the case of a penalized, rather than constrained, norm.\n\u0016 LetWnbe the span of the functions k(xi;\u0000). We can decompose any function guniquely\n\u0016 \u0016 asg=gn+g?, wheregn2Wnandg??Wn. As in class, g?(xi) =hg?;k(xi;\u0000)i= 0, so\nthatg(xi) =gn(xi).\nPlugging in to the objective function:\n (Y\u0000g) +\u0016kgk2\nW= (Y\u0000gn) +\u0016kg2 2\nnkW+\u0016kg?kW:\nFor any \fxed gn, this is a constant plus \u0016kg?k2\nW, which is minimized uniquely at g?= 0.\n^ \u0016 Thus (un\fxing gn) any minimizerPfmust lie in Wn, as desired.\n^ (2) Asf2\u0016 ^n ^ Wn, writef=i=1\u0012ik(xi;\u0000), so that f^=K\u0012andkfk2\nW=\u0012>K\u0012. Then the\n\frst-order optimalit\n\u0010y conditions read\n0 =r\u001e(Y\u0000^f) +\u0016k^fk2\nW\n=r\u0010\nY>\u0006\u00001=2Y\u0011\n\u00002Y>\u0006\u00001=2K\u0012+\u0012>K\u0006\u00001=2K\u0012+\u0016\u0012>K\u0012\n= 2\u0011\n\u0000K\u0006\u00001=2Y+ 2K \u0006\u00001=2K\u0012+ 2\u0016K\u0012:\nRearranging, and recalling that K\u0012=^f, we have\n(K\u0006\u00001=2+\u0016In)^f=K\u0006\u00001=2Y;\nas desired.\n(3) As above, it su\u000eces to consider f,g2\u0016Wn, so that we can write f=K\u0012f,g=K\u0012g. From\n^ the de\fnition of f, we have\n (f+\u0018\u0000^f) +\u0016k^fk2\nW\u0014 (f+\u0018\u0000g) +\u0016kgk2\nW:\nSeparating out the contribution of \u0018, we obtain\n (f\u0000^f) +\u001e(\u0018) +\u0018>\u0006\u00001=2(f\u0000^f ^ ) +\u0016kfk2\nW\u0014 (f\u0000g) + (\u0018) +\u0018>\u0006\u00001=2(f\u00002g) +\u0016kgkW:\nRearranging,\n (f\u0000^f)\u0014\u0000\u0018>\u0006\u00001=2(f\u0000^f ^ )\u0000\u0016kfk2\nW+ (f\u0000) +\u0018>\u0006\u00001=2g (f\u0000g) +\u0016kgk2\nW\n= (f\u0000g) + 2\u0016kgk2+\u0018>\u0006\u00001=2(f^ ^W\u0000g)\u0000\u0016kfk2\nW\u0000\u0016kgk2\nW\n\u0016\u0014 (f\u0000g) + 2\u0016k ^ gk2\nW+\u0018>\u0006\u00001=2(^f\u0000g)\u00002kf\u0000gk2\nW;\nby the inequality ka\u0000bk2\u00142kak2+ 2kbk2. LetZ= \u0006\u00001=2\nW W W \u0018, which is distributed as\nN(0;I). Continuing the line of algebra,\n\u0016 (f\u0000^f)\u0014 ^ (\u0000g) + 2\u0016kgk2fW+Z>(^f\u0000g)\u00002kf\u0000gk2\nW\n= (f\u0000g) + 2\u0016kgk2 \u0016\nW+Z>K(\u0012^\u0012g) (\u0012 ^\u0012g)K(\u0012^\u0012g)f\u0000 \u00002f\u0000>\nf\u0000\n1\u0014 (f\u0000g) + 2\u0016kgk2\nW+Z>KZ;\u0016\nby the inequality\n\u0016 1a>Pb\u0014a>Pa+b>Pb2\u0016\nfor any PSD matrix P. Now since\nn\nZ>KZ=Xn\nZihk(xi;\u0000);k (xj;\ni;j\u0000)iW=\n=1kX\nZik(xi;\ni=1\u0000)k2\nW;\n\u0016 we conclude by taking the in\fmum over g2Wn, which equals the in\fmum over g2W.\n18.657 PS 2 SOLUTIONS 7\n(4) This follows from the previous part together with the observation\nE", "topic": "Math for ML", "subtopic": "norm", "section_heading": "18.657 PS 2 SOLUTIONS", "source_title": "091a852cd0d398a3ca4a41446ab2d30f MIT18 657F15 PS2 Sol", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "271ef015-7ef3-4ae0-9c78-a789970d684b", "text": "1\u0014 (f\u0000g) + 2\u0016kgk2 W+Z>KZ;\u0016 by the inequality \u0016 1a>Pb\u0014a>Pa+b>Pb2\u0016 for any PSD matrix P. Now since n Z>KZ=Xn Zihk(xi;\u0000);k (xj; i;j\u0000)iW= =1kX Zik(xi; i=1\u0000)k2 W; \u0016 we conclude by taking the in mum over g2Wn, which equals the in mum over g2W. 18.657 PS 2 SOLUTIONS 7 (4) This follows from the previous part together with the observation E\n\n2\nX2n n\nZik(xi;\u0000)\n\n=E4X\nZ2\niZjKij\ni=1 W i;j3\n\u0016 (5) It is su\u000ecient to prove the bound for f Wn, since5=X\nE[Zi]Kii= Tr(K ):\ni=1\n2 only its evaluations at the design points\nmatter. For the Gaussian kernel we have k(x;x) = 1, so that Tr(K ) =n. Applying the\nprevious part, and taking the case g=fas an upper bound on the minimizer, we have\nn nE[ (f\u0000^f)]\u0014 (f\u0000f) + 2\u0016kfk2\nW+ = 2\u0016 :\u0016kfk2\nW+\u0016\nTaking\u0016=kfkWp\nn=2 gives the result.\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "1\u0014 (f\u0000g) + 2\u0016kgk2 W+Z>KZ;\u0016 by the inequality \u0016 1a>Pb\u0014a>Pa+b>Pb2\u0016 for any PSD matrix P. Now since n Z>KZ=Xn Zihk(xi;\u0000);k ", "source_title": "091a852cd0d398a3ca4a41446ab2d30f MIT18 657F15 PS2 Sol", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "589184d3-c7c5-4d86-a1e8-0b3b7c154d9e", "text": "online learning with structured experts–a biased\nsurvey\nG´ abor Lugosi\nICREA and Pompeu Fabra University, Barcelona\n1\non-line prediction\nA repeated game between forecaster and environment.\nAt each round t,\nthe forecaster chooses an action t2{1,..., };\n(actions are often called experts )\nthe environment chooses losses `t(1),...,` t(N)2[0,1];\nthe forecaster su ↵ers loss`t(It).\nThe goal is to minimize the regret\nRn= Xn n\n`t(It)\u0000min\nitN=1X\n`t(i)\nt=1!\n.\n7I N\n7\nsimplest example\nIs it possible to make (1/n)Rn!0forall loss assignments?\nLetN=2 and deﬁne, for all t=1,..., n,\n`t(1) =⇢0ifIt=2\n1ifIt=1\nand`t(2) = 1 \u0000`t(1).\nThen\nXn nn`t(It)=n and min`\ni=12t=1X\nt(i)\n,\nt=12\nso1 1Rnn\u0000.2\n11\nrandomized prediction\nKey to solution: randomization .\nAt time t, the forecaster chooses a probability distribution\npt\u00001=(p1,t\u00001,..., pN,t\u00001)\nand chooses action iwith probability pi,t\u00001.\nSimplest model: all losses `s(i),i=1,..., N,s<t,a r e\nobserved: full information .\n1212\nHannan and Blackwell\nHannan (1957) andBlackwell (1956) showed that the forecaster\nhas a strategy such that\n1 Xn n\n`t(It)n\u0000min\nitN=1X\n`t(i)\nt=1!\n!0\nalmost surely for all strategies of the environment.\n1313\nbasic ideas\nexpected loss of the forecaster:\nN\n`t(pt\u00001)=X\npi,t (\u00001`ti)=E t`t(It)\ni=1\nBy martingale convergence,\n Xn n1``1 2t(It)\u0000X/t(pnt\u00001) = OP(n\u0000)\nt=1 t=1\nso it su\u0000 ces to study\n1\nn Xn n\n`t(pt)min`(i)\u00001\u0000\nN=1X\ntit t=1!!\n16\nweighted average prediction\nIdea: assign a higher probability to better-performing actions.\nVovk (1990), Littlestone and Warmuth (1989) .\nA popular choice is\nexp⇣\n\u0000⌘Pt\u00001\n=1`(s si)\npi,t =\u00001⌘\nP =1N\n=1exp⇣\n\u0000⌘P i ,...,N .t\u00001\n=1`s( )k sk\nwhere⌘> 0.Then⌘\n1Xn\nn n\n`t(pt\u00001)\u0000min\nit=1NX\n`t(i)\nt=1!\n=r\nlnN\n2n\nwith⌘=p\n8l nN/n.\n19\nproof\ntLetLi,t=P\ns=1`s(i)and\nN\nWt=X XN\nwi,t= e\u0000⌘Li,t\ni=1 i=1\nfort\u00001,a n dW 0=N. First observe that\nWnln eWXN\n=ln,\n0 \n\u0000⌘Li n\ni=1!\n\u0000lnN\n\u0000ln✓\nmax e\u0000⌘Li,n ln\ni=1,...,N◆\n\u0000N\n=\u0000⌘ min Li,ni=1,...,N\u0000lnN.\n21\nproof\nOn the other hand, for each t=1,...,n\nWtPN\ni=1wi,t1e\u0000⌘`t(i)\nln =ln\u0000\nWt\u00001PN\nPj=1wj,t\u00001\nN\n=1w\nPi,t\u00001`(\u0000i ti)⌘2\n⌘ +N8j=1wj,t\u00001\n⌘2\n=\u0000⌘` t(pt\u00001)+8\nby Hoe↵ding’s inequality.\nHoe↵ding (1963) :i fX2[0,1],\nlnEe\u0000⌘⌘2\nX\u0000⌘EX+8\n23\nproof\nfor each t=1,..., n\nW2t ⌘ln \u0000⌘`t(p )+Wt\nt\u00001\u000018\nSumming over t=1,..., n,\nWn\nn ⌘ln\u0000⌘X 2\n`t(pt1)+ n.W0\u00008t=1\nCombining these, we get\nXnlnN⌘`t(pt1) min Li,n+ + n\u0000i=1,...,N ⌘ 8t=1\n24\nlower bound\nThe upper bound is optimal: for all predictors,\nPn\n=1`t(Itp\u0000n)mint iN`tsup=1 t(i)1.\nn,N,`t(i) (n/2) ln NP\n\u0000\nIdea: choose`t(i)to be i.i.d. symmetric Bernoulli coin ﬂips.\nn\nsup Xn\n`t(It)\nt(i)\u0000min\n` it=1NX\n`t(i)\n\"t=1!\nn\n\u0000EX\n`t(It)\u0000min\nit=1N\nnXn\n`t(i)\nt=1#\n=2\u0000minBiiN\nWhere B1,..., BNare independent Binomial (n,1/2) .\nUse the central limit theorem.\n27\nfollow the perturbed leader\nt\u00001\nIt=arg minX\n`s(i)+Zi,t\ni=1,...,Ns=1\nwhere the Zi,tare random noise variables.\nThe original forecaster of Hannan (1957) is based on this idea.\n28\nfollow the perturbed leader\nIf the Zi,tare i.i.d. uniform [0,p\nnN],t h e n\n1 NRnn2r\n+Op(n\u00001/2).n\nIf the Zzi,tare i.i.d. with density (⌘/2)e\u0000⌘| |,t h e nf o r\n⌘⇡p\nlogN/n,\n1 log NR1/2ncr\n+Op(n\u0000).n n\nKalai and Vempala (2003) .\n30\ncombinatorial experts\nOften the class of experts is very large but has some combinatorial\nstructure. Can the structure be exploited?\npath planning. At each time\ninstance, the forecaster chooses apath in a graph between twoﬁxed nodes. Each edge has anassociated loss. Loss of a path isthe sum of the losses over theedges in the path.\nNis huge!!!\n32© Google. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, seehttp://ocw.mit.edu/fairuse.\nassignments: learning permutations\nGiven a complete\nbipartite graphK\nm,m,t h e\nforecaster chooses aperfect matching.The loss is the sumof the losses overthe edges.\nHelmbold and Warmuth (2007): full information case.\n33This image has been removed due to copyright restrictions.\nPlease see the image athttp://38.media.tumblr.com/tumblr_m0ol5tggjZ1qir7tc.gif\nspanning trees\nThe forecaster chooses a\nspanning tree in the completegraph K\nm. The cost is the sum\nof the losses over the edges.\n34\ncombinatorial experts\ndFormally, the class of experts is a set S⇢{ 0,1} of cardinality\n|S|=N.\nt 2RdAt each time , a loss is assigned to each component: `t .\nLoss of expert v2S is`t(v)=`>\ntv.\nForecaster chooses It2S.\nThe goal is to control the regret\nXnXn\n`t(It)\u0000 min`t(k).\nk=1,...,Nt=1 t=1\n35\ncomputing the exponentially weighted average forecaster\nOne needs to draw a random element of Swith distribution\nproportional to\nwt(v) = exp\u0000t\n\u0000⌘Lt(v)\u0000\n= exp \u00001\n\u0000⌘X\n`>\ntv.\ns=1!\nt\nexp\njYd\n=\n=1 \u00001\n\u0000⌘X\n`t,jvj\ns=1!\n.\n36\ncomputing the exponentially weighted average forecaster\npath planning: Sampling may be done by dynamic programming.\nassignments: Sum of weights (partition function) is the permanent\nof a non-negative matrix. Sampling may be done by a FPAS of\nJerrum, Sinclair, and Vigoda (2004) .\nspanning trees: Propp and Wilson (1998) deﬁne an exact sampling\nalgorithm. Expected running time is the average hitting time ofthe Markov chain deﬁned by the edge weights w\nt(v).\n39\ncomputing the follow-the-perturbed leader forecaster\nIn general, much easier. One only needs to solve a linear\noptimization problem over S.T h i sm a yb eh a r db u ti ti sw e l l\nunderstood.\nIn our examples it becomes either a shortest path problem, or an\nassignment problem, or a minimum spanning tree problem.\n40\nfollow the leader: random walk perturbation\nSuppose Nexperts, no structure. Deﬁne\nt\nIt=arg min (`i,s1+Xs)\ni=1,...,NX\n\u0000\ns=1\nwhere the Xsare either i.i.d. normal or ±1coinﬂips.\nThis is like follow-the-perturbed-leader but with random walk\ntperturbation:s=1Xt.\nAdvantage: foP\nrecaster rarely changes actions!\n42\nfollow the leader: random walk perturbation\nIfRnis the regret and Cnis the number of times It6=It\u00001,t h e n\nERn2ECn8p\n2nlogN+ 16 log n+ 16 .\nDevroye, Lugosi, and Neu (2015).\nKey tool: number of leader changes in Nindependent random\nwalks with drift.\n43\nfollow the leader: random walk perturbation\nThis also works in the “combinatorial” setting: just add an\nindependent N(0,d)at each time to every component.\nERn=Oe(B3/2p\nnlogd)\nand\nECn=O(Bpnlogd),\nwhere B= max v2Skvk1.\n44\nwhy exponentially weighted averages?\nMay be adapted to many di ↵erent variants of the problem,\nincluding bandits, tracking, etc.\n45\nmulti-armed bandits\nThe forecaster only observes `t(It)but not`t(i)fori6=It.\nHerbert Robbins (1952).\n46\nThis image has been removed due to copyright restrictions. Please see the image at\nhttps://en.wikipedia.org/wiki/Herbert_Robbins#/media/File:1966-HerbertRobbins.jpg\nmulti-armed bandits\nTrick: estimate`t(i)by\n`e`t(It)\nt(iI=i)={t}\npIt,t\u00001\nThis is an unbiased estimate:\neXN`t(j)Et`t(i)= pj,t1{j=i}=`t(i) \u0000\njj,=1pt\u00001\nUse the estimated losses to deﬁne exponential weights and mix\nwith uniform ( Auer, Cesa-Bianchi, Freund, and Schapire, 2002):\n⇣\n\u0000Ptexp⌘\u00001`( )\nP⇣s=1pi,t 1\u0000\u0000esi\n1=( ) \u0000N\nk=1exp⌘\n\u0000\n\u0000⌘P +t\u00001\ns=1`s(k)⌘\nN\nexploration\ne\n| {z }\nexploitation|{z}\n49\nmulti-armed bandits\nXn1E \n`t(pt1)\u0000minXn\n`t(i)!\n=O r\nNlnN!\n,n\u0000it=1Nt=1n\n50\nmulti-armed bandits\nLower bound:\nn1 NsupE Xn\n`t(pt1)\u0000minX\n`t(i) ,\n`t(i)n\u0000i N=1=1!\n\u0000Cr\nnt t\nDependence on Nis not logarithmic anymore!\nAudibert and Bubeck (2009) constructed a forecaster with\nn n1 NmaxE`t(pti\u00001)`t(i) = O ,\nNn \n\u0000\nt=1 t=1! r\nn!X X\n52\ncalibration\nSequential probability assignment.\nAb i n a r ys e q u e n c ex 1,x2,... is revealed one by one.\nAfter observing x1,...,x t\u00001, the forecaster issues prediction\nIt2{0,1,..., N}.\nMeaning: “chance of rain is It/N”.\nForecast is calibrated if\n\u0000\u0000P\n\u0000n\n\u0000t=1xt\n\u0000\n{It=i}Pn\nt=1\n {It=i}\u0000i\nN\u0000\u0000\u0000\u0000\u00001\n2N+o(1)\nwhenever lim supn(1/n)Pn\nt=1\n0 {It=i>. }\nIs there a forecaster that is calibrated for all possible sequences?\nNO. (Dawid, 1985).\n55\nrandomized calibration\nHowever, if the forecaster is allowed to randomize then it is\npossible! (Foster and Vohra, 1997).\nThis can be achieved by a simple modiﬁcation of any regret\nminimization procedure.\nSet of actions (experts): {0,1,...,N }.\nAt time t, assign loss`t(i)=( xt\u0000i/N)2to action i.\nOne can now deﬁne a forecaster. Minimizing regret is not\nsu\u0000cient.\n56\ninternal regret\nRecall that the (expected) regret is\nXn\n`t(ptXn n\n1)\u0000min`t(i) = max\u0000i it=1 t=1X\npj,t(`t(j)\u0000`t(i))\nt=1X\nj\nThe internal regret is deﬁned by\nn\nmaxX\npj,t(`t(j)\u0000`t(i))\ni,jt=1\npj,t(`t(j)\u0000`t(i)) =Et\n` {It=j(}t(j)\u0000`t(i))\nis the expected regret of having taken action jinstead of action i.\n59\ninternal regret and calibration\nBy guaranteeing small internal regret, one obtains a calibrated\nforecaster.\nThis can be achieved by an exponentially weighted average\nforecaster deﬁned over N2actions.\nCan be extended even for calibration with checking rules.\n60\nprediction with partial monitoring\nFor each round t=1,..., n,\nthe environment chooses the next outcome Jt2{1,...,M }\nwithout revealing it;\nthe forecaster chooses a probability distribution ptand\u00001\ndraws an action It2{1,...,N }according to pt\u00001;\nthe forecaster incurs loss `(It,Jt)and each action iincurs loss\n`(i,Jt). None of these values is revealed to the forecaster;\nthe feedback h(It,Jt)is revealed to the forecaster.\nH=[h(i,j)]N⇥Mis the feedback matrix.\nL=[`(i,j)]N⇥Mis the loss matrix.\n61\nexamples\nDynamic pricing. HereM=N,a n dL =[`(i,j)]N⇥Nwhere\n(j )i j\u0000i{ij+c(,)=} {i >j`}.N\nandh(i,j)= {i>jor}\nh(i,j)=a i j +b i>j,i,j=1,..., N. {} { }\nMulti-armed bandit problem. The only information the forecaster\nreceives is his own loss: H=L.\n63\nexamples\nApple tasting. = =2.\nL=01\n10\u0000\nH=aabc\u0000\n.\nThe predictor only receives feedback when he chooses the secondaction.Label e\u0000 cient prediction. N=3,M=2.\nL=211\n401103\n5\nH=2ab\ncc3\n.N M\n4\ncc5\n65\nag e n e r a lp r e d i c t o r\nA forecaster ﬁrst proposed by Piccolboni and Schindelhauer (2001).\nCrucial assumption: Hcan be encoded such that there exists an\nN⇥Nmatrix K=[k(i,j)]N⇥Nsuch that\nL=K·H.\nThus,\n`(i,j)=XN\nk(i,l)h(l,j).\nl=1\nThen we may estimate the losses by\n`ek(i,It)h(It,Jt)(i,Jt)=pIt,t.\n66\nag e n e r a lp r e d i c t o r\nObserve\neXNk(i,k)h(k,Jt)Et`(i,Jt) = pk,t\u00001\nk=1pk,t\u00001\nk\nkXN\n= ( i,k)h(k,Jt)=` (i,Jt),\n=1\n`e(i,Jt)is an unbiased estimate of`(i,Jt).\nLet\ne\u0000⌘Li,t\u00001\u0000pi,t\u00001=( 1\u0000\u0000)e\nP +N\n=1e\u0000⌘Lek,t\u00001 Nk\nwhere Li,t=Pt\n=1`(i,Jt). ese\n67\nperformance bound\nWith probability at least 1\u0000\u0000,\n1Xn1`(It,Jt)n\u0000 min\ni=1,...,Nt=1\n\u0000Xn\n`(i,Jt)nt=1\nCn1/3N2/3p\nln(N/\u0000).\nwhere Cdepends on K.(Cesa-Bianchi, Lugosi, Stoltz (2006) )\nHannan consistency is achieved with rate O(n\u00001/3)whenever\nL=K·H.\nThis solves the dynamic pricing problem.\nBart´ ok, P´ al, and Szepesv´ ari (2010) :i fM=2, only possible rates\naren\u00001/2,n\u00001/3,1\n70\nimperfect monitoring: a general framework\nSis a ﬁnite set of signals.\nFeedback matrix: H:{1,...,N }⇥{ 1,...,M }!P (S).\nFor each round t=1,2...,n ,\nthe environment chooses the next outcome Jt2{1,...,M }\nwithout revealing it;\nthe forecaster chooses pt\u00001and draws an action\nIt2{1,...,N }according to it;\nthe forecaster receives loss `(It,Jt)and each action isu↵ers\nloss`(i,Jt), none of these values is revealed to the forecaster;\naf e e d b a c ks tdrawn at random according to H(It,Jt)is\nrevealed to the forecaster.\n71\ntarget\nDeﬁne\n`(p,q)=X\npiqj`(i,j)\ni,j\nH(·,q)=( H(1,q),...,H (N,q))\nwhere H(i,q)=jqjH(i,j).\nDenote by Fthe setP\nof those \u0000that can be written as H(·,q)for\nsome q.\nFis the set of “observable” vectors of signal distributions \u0000.\nThe key quantity is\n⇢(p,\u0000) = max `(p,q)\nq:H(·,q)=\u0000\n⇢is convex in pand concave in \u0000.\n72\nrustichini’s theorem\nThe value of the base one-shot game is\nminmax`(p,q)=m i n max⇢(p,\u0000)\np q p\u00002F\nIfqnis the empirical distribution of J1,...,J n,e v e nw i t ht h e\nknowledge of H(·,qn)we cannot hope to do better than\nmin p⇢(p,H(·,qn)).\nRustichini (1999) proved that there exists a strategy such that for\nall strategies of the opponent, almost surely,\nlim sup0\n@1X\n`(It,Jt)\u0000min⇢(p,H(\nn n !1 p·,qn))\nt=1,...,n1A0\n73\nrustichini’s theorem\nRustichini’s proof relies on an approachability theorem for a\ncontinuum of types ( Mertens, Sorin, and Zamir, 1994).\nIt is non-constructive.\nIt does not imply any convergence rate.Lugosi, Mannor, and Stoltz (2008) construct e\u0000 ciently computable\nstrategies that guarantee fast rates of convergence.\n74\ncombinatorial bandits\nThe class of actions is a set S⇢{ 0d,1} of cardinality |S|=N.\nAt each time td, a loss is assigned to each component: `t2R.\nLoss of expert v2S is`t(v)=`>\ntv.\nForecaster chooses It2S.\nThe goal is to control the regret\nXn n\n`t(It)\u0000 min\nk=1,...,Nt=1X\n`t(k).\nt=1\n75\ncombinatorial bandits\nThree models.\n(Full information.) Alldcomponents of the loss vector are\nobserved.(Semi-bandit.) Only the components corresponding to the chosen\nobject are observed.(Bandit.) Only the total loss of the chosen object is observed.\nChallenge: IsO(n\n\u00001/2poly(d ))regret achievable for the\nsemi-bandit and bandit problems?\n77\ncombinatorial prediction game\nAdversary\nPlayer\n78\ncombinatorial prediction game\nAdversary\nPlayer\n79© Google. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, seehttp://ocw.mit.edu/fairuse.\ncombinatorial prediction game\nAdversary\nPlayer\n80© Google. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, seehttp://ocw.mit.edu/fairuse.\ncombinatorial prediction game\nAdversary\nPlayer`2`6 `d\u00001`1`4\n`5\n`9`d\u00002\n`d `3\n`8`7\n81\n© Google. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, seehttp://ocw.mit.edu/fairuse.\ncombinatorial prediction game\nAdversary\nPlayer`2`6 `d\u00001`1`4\n`5\n`9`d\u00002\n`d `3\n`8`7\nloss su ↵ered:`2+`7+...+`d\n82© Google. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, seehttp://ocw.mit.edu/fairuse.\ncombinatorial prediction game\nAdversary\nPlayer`2`6 `d\u00001`1`4\n`5\n`9`d\u00002\n`d `3\n`8`7\n<8\nFull Info:`1,`2,...,` d\nFeedback::\nloss su ↵ered:`2+`7+...+`d\n83© Google. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, seehttp://ocw.mit.edu/fairuse.\ncombinatorial prediction game\nAdversary\nPlayer`2`6 `d\u00001`14\n`5\n`9`d\u00002\n`d `3\n`8`7`\n< n d\nb ck:8\nFull I fo: `1,`2,...,`\nFeed a:Semi-Bandit: `2,`7,...,` d\nBandit: `2+`7+...+`d\nloss su ↵ered:`2+`7+...+`d\n85© Google. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, seehttp://ocw.mit.edu/fairuse.\nnotation\n`2`6 `d\u00001`1`4\n`5\n`9`d\u00002\n`d `3\n`8`7\n`2`6 `d\u00001`1`4\n`5\n`9`d\u00002\n`d `3\n`8`7S⇢{ 0,1}d\n`t2Rd\n+\nVt2ST, loss su ↵ered:`tVt\nregret:\nn\nR EXn\nTV\u0000 EXTn=`ttmin`tu\nut=12St=1\nTloss assumption: `tv 1for all v andt=1,...,n . || 2S\n86\nweighted average forecaster\nAt time tassign a weight wt,ito each i=1,...,d .\nThe weight of each vk2S is\nwt(k)= wt,i.\ni:vkY\n(i)=1\nLetqt\u00001(k)=w t\u00001(kN)/k=1wt\u00001(k).\nAt each time t,d r a wK tfromP\nthe distribution\npt\u00001(k)=( 1\u0000\u0000)qtk\u00001()+\u0000µ(k)\nwhere µis a ﬁxed distribution on Sand\u0000> 0.H e r e\nwt,i= exp\u0000⌘Lt,i\nwhere Lt,i=`1,i+ +`t,iand\u0000\n`t,iis a\u0000\nn estimated loss.e\nee ···e e\n89\nloss estimates\nDani, Hayes, and Kakade (2008) .\nDeﬁne the scaled incidence vector\nXt=`t(Kt)VKt\nwhere Ktis distributed according to pt\u00001.\nLetPt\u00001=E⇥\nVKtV>be theKtd⇥dcorrelation matrix.\nHence\nPt1⇤\n\u0000(i,j)=\nk:vk(iX\npt\u00001(k).\n)=v k(j)=1\nSimilarly, let Qt\u00001andMbe the correlation matrices of EVV>\nwhen Vhas law, qt\u00001andµ.T h e n⇥ ⇤\nPt\u00001(i,j)=( 1\u0000\u0000)Qt\u00001(i,j)+\u0000M(i,j).\nThe vector of loss estimates is deﬁned by\ne`t=P+\nt\u00001Xt\nwhere P+\nt\u00001is the pseudo-inverse of Pt\u00001.\n92\nkey properties\nMM+v=vfor all v2S.\nQt\u00001is positive semideﬁnite for every t.\nPt1P+r at1v=vfo ll t v \u0000 and\u00002S.\nBy deﬁnition,\nEtXt=Pt\u00001`t\nand therefore\nEte`t=P+\nt\u00001EtXt=`t\nAn unbiased estimate!\n93\nperformance bound\nThe regret of the forecaster satisﬁes\n1✓lnELb2B2 d N\nn\u0000 min Ln(k) 2 +1 .n k=1,...,N◆\ns✓\nd\u0000min(M)◆\nn\nwhere\nT\u0000min(M)= m i n xMx>0\nx2span (S):kxk=1\nis the smallest “relevant” eigenvalue of M.(Cesa-Bianchi and\nLugosi, 2009.)\nLarge\u0000min(M)is needed to make sure no `t,iis too large.|e|\n94\nperformance bound\nOther bounds:\nBp\ndlnN/n(Dani, Hayes, and Kakade ). No condition on S.\nSampling is over a barycentric spanner .\ndp\n(✓lnn)/n(Abernethy, Hazan, and Rakhlin). Computationally\ne\u0000cient.\n95\neigenvalue bounds\n\u0000min(M)= m i n E(V,x)2.\nx2span (S):kxk=1\nwhere Vhas distribution µoverS.\nIn many cases it su\u0000ces to take µuniform.\n96\nmultitask bandit problem\nThe decision maker acts in mgames in parallel.\nIn each game, the decision maker selects one of Rpossible actions.\nAfter selecting the mactions, the sum of the losses is observed.\n1\u0000min=R\nmaxELn\u0000Ln(k) lnR.\nk2mp\n3nR\nThe price of only obsh\nb\nerving the sui\nm of losses is a factor of m.\nGenerating a random joint action can be done in polynomial time.\n97\nassignments\nPerfect matchings of Km,m.\nAt each time one of the N=m!perfect matchings of Km,mis\nselected.\n1\u0000min(M)=m\u00001\nmaxELn\u0000Ln(k)2m 3nln(m !).\nk\nOnly a factor of mh\nwb\norse than ni\naive full-ip\nnformation bound.\n98\nspanning trees\nIn a network of mnodes, the cost of communication between two\nnodes joined by edge eis`t(e)at time t.A te a c ht i m eam i n i m a l\nconnected subnetwork (a spanning tree) is selected. The goal is to\nminimize the total cost. N=mm\u00002.\n1\u0000min(M)=1Om\u0000✓\nm2◆\n.\nThe entries of Mare\n2P{V i=1}=m\nP\u0000 3Vi=1,Vj=1 \n= ifm2i⇠j\n4PVi=1,Vj=1 = if .2i6⇠jm\u0000 \n99\nstars\nAt each time a central node of a network of mnodes is selected.\nCost is the total cost of the edges adjacent to the node.\n1\u0000min\u00001\u0000O✓\nm◆\n.\n100\ncut sets\nAb a l a n c e dc u ti nK 2mis the collection of all edges between a set\nofmvertices and its comp\u0000lement. Each balanced cut has m2\n2medges and there are N=m\u0000\nbalanced cuts.\n1 1\u0000min(M)=\u0000O .4✓\nm2◆\nChoosing from the exponentially weighted average distribution is\nequivalent to sampling from ferromagnetic Ising model. FPAS byRandall and Wilson (1999) .\n101\nhamiltonian cycles\nA Hamiltonian cycle in Kmis a cycle that visits each vertex exactly\nonce and returns to the starting vertex. N=(m\u00001)!\n2\u0000min\u0000m\nE\u0000cient computation is hopeless.\n102\nsampling paths\nIn all these examples µis uniform over S.\nForpath planning it does not always work.\nWhat is the optimal choice of µ?\nWhat is the optimal way of exploration?\n104\nminimax regret\nRn=i n f max sup Rnstrategy S⇢{ 0,1}dadversary\nTheorem\nLetn\u0000d2.I nt h e full information and semi-bandit games, we\nhave\n0.008 dpnRndp\n2n,\nand in the bandit game,\n0.01d3/2pnRn2d5/2p\n2n.\n106\nproof\nupper bounds:\nD=[ 0,+1)d,F(x)=1 dog⌘ i=1xilxiworks for full\ninformation but it is only optiP\nmal up to a logarithmic factor in the\nsemi-bandit case.\nin the bandit case it does not work at all! Exponentially weightedaverage forecaster is used.\nlower bounds:careful construction of randomly chosen set Sin each case.\n108\nag e n e r a ls t r a t e g y\nLetDdbe a convex subset of Rwith nonempty interior int(D).\nA function F:D! RisLegendre if\n•Fisstrictly convex and admits continuous ﬁrst partial\nderivatives on int(D),\n•Foru2@D,a n dv2int(D),w eh a v e\nlim (u\u0000vT)rF\u0000\n(1\u0000s)u+sv=+\ns!0,s >01.\nThe Bregman divergence DF:D⇥ int(\u0000\nD)associated to a\nLegendre function Fis\nDF(uT,v)=F (u)\u0000F(v)\u0000(u\u0000v)rF(v).\n111\nCLEB (Combinatorial LEarning with Bregman divergences)\nParameter: FLegendre on D\u0000 Conv (S)\nConv (S)D\n\u0000(S )ptwtw0\nt+1\nwt+1\npt+1(1)wt0\n+12D :\nrF(w0\n+1)=rF ( )twt\u0000`˜t\n(2)wt+12arg min DF(w,wt0\n+1)\nw2Conv (S)\n(3)pt+12\u0000(S ):w t+1=EV⇠pt+1V\n117\nGeneral regret bound for CLEB\nTheorem\nIfFadmits a Hessian r2Falways invertible then,\nn\nRn/diam ˜T 2\u00001˜DF(S)+EX\n`t\n=1⇣\nrF(wt)\nt⌘\n`t.\n118\nDi↵erent instances of CLEB: LinExp (Entropy Function)\nD 1Pd=[ 0,+ )d,F(x)=1\n⌘ i=1xilogxi\n8\n>><Full Info :Exponentially weighted average\n>>:Semi-Bandit=Bandit :Exp3\nAuer et al. [2002]\n8\n>>>Full Info :Component Hedge\n>>>>Koolen, Warmuth and Kivinen [2010]\n><\n>>Semi-Bandit: MW\n>>Kale, Reyzin and Schapire [2010]\nBandit: new algorithm >>>>:\n124\nDi↵erent instances of CLEB: LinINF (Exchangeable\nHessian)\nD=[ 0,+1)d,F(x)=Pd\ni=1Rxi 1( )0\u0000s ds\nINF, Audibert and Bubeck [2009]\n⇢ (x) = exp(⌘ x):LinExp\n (x)=(\u0000⌘x)\u0000q,q>1:LinPoly\n128\nDi↵erent instances of CLEB: Follow the regularized leader\nD=Conv (S),t h e n\nwt+12arg min Xt\n˜T`sw+F(w)\nw2Ds=1!\nParticularly interesting choice: Fself-concordant barrier function ,\nAbernethy, Hazan and Rakhlin [2008]\n130\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "convex", "section_heading": "online learning with structured experts–a biased", "source_title": "20907a506d03148f37ce90f1af2659b1 MIT18 657F15 L17", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "f0b6cfaf-f1d1-4f73-af9e-92c03c93e1a4", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 15\nScribe:Zach Izzo Oct. 27, 2015\nPart III\nOnline Learning\nIt is often the case that we will be asked to make a sequence of predictions, rather\nthan just one prediction given a large number of data points. In particular, this situa-\ntion will arise whenever we need to perform online classiﬁcation: at time t, we have\n(X1,Y1),...,(Xt−1,Yt−1) iid random variables, and given Xt, we are asked to predict\nYt∈ {0,1}. Consider the following examples.\nOnline Shortest Path: We have a graph G= (V,E) with two distinguished vertices\nsandt, and we wish to ﬁnd the shortest path from stot. However, the edge weights\nE1,...,E tchange with time t. Our observations after time tmay be all of the edge weights\nE1,...,E t; or our observations may only be the weights of edges through which our path\ntraverses; orourobservationmayonlybethesumoftheweightsoftheedgeswe’vetraversed.\nDynamic Pricing: We have a sequence of customers, each of which places a value vt\non some product. Our goal is to set a price ptfor thetth customer, and our reward for\ndoing so is ptifpt≤vt(in which case the customer buys the product at our price) or 0\notherwise (in which case the customer chooses not to buy the product). Our observations\nafter time tmay bev1,...,vt; or, perhaps more realistically, our observations may only be\n1I(p1< v1),...,1I(pt< vt). (In this case, we only know whether or not the customer bought\nthe product.)\nSequential Investment: GivenNassets, a portfolio is ω∈∆N={x∈IRn:xi≥\n0,/summationtextNxi=1i= 1}. (ωtells what percentage of our funds to invest in each stock. We could\nalso allow for negative weights, which would correspond to shorting a stock.) At each time\nt, we wish to create a portfolio ωt∈∆Nto maximize ωT\ntzt, wherezt∈IRNis a random\nvariable which speciﬁes the return of each asset at time t.\nThere are two general modelling approaches we can take: statistical or adversarial.\nStatistical methods typically require that the observations are iid, and that we can learn\nsomethingaboutfuturepointsfrompastdata. Forexample, inthedynamicpricingexample,\nwe could assume vt∼N(v,1). Another example is the Markowitz model for the sequential\ninvestment example, in which we assume that log( zt)∼ N(µ,Σ).\nIn this lecture, we will focus on adversarial models. We assume that ztcan be any\nbounded sequence of numbers, and we will compare our predictions to the performance of\nsome benchmark. In these types of models, one can imagine that we are playing a game\nagainst an opponent, and we are trying to minimize our losses regardless of the moves he\nplays. In this setting, we will frequently use optimization techniques such as mirror descent,\nas well as approaches from game theory and information theory.\n1\n1. PREDICTION WITH EXPERT ADVICE\n1.1 Cumulative Regret\nLetAbe a convex set of actions we can take. For example, in the sequential investment\nexample, A= ∆N. If our options are discrete–for instance, choosing edges in a graph–then\nthink ofAas the convex hull of these options, and we can play one of the choices randomly\naccording to some distribution. We will denote our adversary’s moves by Z. At time t,\nwe simultaneously reveal at∈ Aandzt∈ Z. Denote by ℓ(at,zt) the loss associated to the\nplayer/decision maker taking action atand his adversary playing zt.\nInthegeneral case,/summationtextn\ntℓ=1(at,zt)canbearbitrarilylarge. Therefore, ratherthanlooking\nat theabsoluteloss foraseriesof nsteps, wewill compareourloss totheloss ofabenchmark\ncalled an expert. An expert is simply some vector b∈ An,b= (b1,...,bt,...,bTn) . If we\nchooseKexpertsb(1),...,b(K), then our benchmark value will be the minimum cumulative\nloss amongst of all the experts:\nn\n(j)benchmark = min\n≤j≤K/summationdisplay\nℓ(bt,zt).\n1t=1\nThecumulative regret is then deﬁned as\nn n\nRn=/summationdisplay(j)ℓ(at,zt)−min ℓ(bt,zt).\n1≤j≤Kt=1/summationdisplay\nt=1\nAt timet, we have access to the following information:\n1. All of our previous moves, i.e. a1,...,at−1,\n2. all of our adversary’s previous moves, i.e. z1,...,zt−1, and\n3. All of the experts’ strategies, i.e. b(1),...,b(K).\nNaively, one might try a strategy which chooses a=b∗ ∗tt, where bis the expert which\nhas incurred minimal total loss for times 1 ,...,t−1. Unfortunately, this strategy is easily\nexploitable by the adversary: he can simply choose an action which maximizes the loss for\nthat move at each step. To modify our approach, we will instead take a convex combination\nof the experts’ suggested moves, weighting each according to the performanceof that expert\nthus far. To that end, we will replace ℓ(at,zt) byℓ(p,(bt,zt)), where p∈∆Kdenotes a\n(1) ( K)convex combination, bt= (bt,...,bt)T∈ AKis the vector of the experts’ moves at time\nt, andzt∈ Zis our adversary’s move. Then\nn n\nRn=/summationdisplay\nℓ(pt,zt)−min ℓ(ej,zt)\n1≤j≤Kt=1/summationdisplay\nt=1\nwhereejis the vector whose jth entry is 1 and the rest of the entries are 0. Since we are\nrestricting ourselves to convex combinations of the experts’ moves, we can write A= ∆K.\nWe can now reduce our goal to an optimization problem:\nK n\nmin\n∈∆K/summationdisplay\nθj\nθj=1/summationdisplay\nℓ(ej,zt).\nt=1\n2\nFrom here, one option would be to use a projected gradient descent type algorithm: we\ndeﬁne\nqt+1=pt−η(ℓ(eT1,zt),...,ℓ(eK,zT))\nKand then p∆t+1=π(pt) to be the projection of qt+1onto the simplex.\n1.2 Exponential Weights\nSuppose we instead use stochastic mirror descent with Φ = negative entropy. Then\nqtqt+1,j=pt+1,jexp(−ηℓ(ej,zt)), pt+1,j= ,/summationtextK\nlq=1t+1,l\nwhere we have deﬁned\nK/parenleftBigg\nwt,jpt= ej, expKwj=1 l=1,l/parenrightBigg\nwt,j=\nt/parenleftBiggt−1 /summationdisplay\n−η/summationdisplay\nℓ(ej,zs)\ns=1/parenrightBigg\n.\nThis process looks at the los/summationtext\ns from each expert and downweights it exponentially according\nto the fraction of total loss incurred. For this reason, this method is called an exponential\nweighting (EW) strategy .\nRecall the deﬁnition of the cumulative regret Rn:\nn n\nRn=/summationdisplay\nℓ(pt,zt)−min\n1≤j≤Kt=1/summationdisplay\nℓ(ej,zt).\nt=1\nThen we have the following theorem.\nTheorem: Assume ℓ(·,z) is convex for all z∈ Zand that ℓ(p,z)∈[0,1] for all p∈\n∆K,z∈ Z. Then the EW strategy has regret\nlogKηnRn≤ +.η2\nIn particular, for η=/radicalBig\n2logK,n\nRn≤/radicalbig\n2nlogK.\nProof.We will recycle much of the mirror descent proof. Deﬁne\nK\nft(p) =/summationdisplay\npjℓ(ej,zt).\nj=1\nDenote/bardbl·/bardbl:=|·|1. Then\nn n1/summationdisplay η1/bardblg/bardbl2tlogKft( (∗pt)−f∗t)≤n/summationtext\nt=1p +,n 2 ηnt=1\n3\nwheregt∈∂ft(pt) and/bardbl · /bardbl∗is the dual norm (in this case /bardbl · /bardbl∗=| · |∞). The 2 in the\ndenominator of the ﬁrst term of this sum comes from setting α= 1 in the mirror descent\nproof. Now,\ngt∈∂ft(pt)⇒gt= (ℓ(e1,zt),...,ℓ(eTK,zt)).\nFurthermore, since ℓ(p,z)∈[0,1], we have /bardblgt/bardbl∗=|gt|∞≤1 for all t. Thus\nnη1\nn/summationtext\nt=1/bardblgt/bardbl2\n∗logK η logK+ ≤+.2 nη2ηn\nSubstituting for ftyields\nnK K n/summationdisplay/summationdisplay ηnlogKpt,jℓ(ej,zt)−min/summationdisplay\npjℓ(ej,zt)≤+.\np∈∆K 2ηt=1j=1 j=1/summationdisplay\nt=1\nNote that the boxed term is actually min 1≤j≤K/summationtextnℓt=1(ej,zt). Furthermore, applying\nJensen’s to the unboxed term gives\nn K n /summationdisplay/summationdisplay\npt,jℓ(ej,zt)≥/summationdisplay\nℓ(pt,zt).\nt=1j=1 t=1\nSubstituting these expressions then yields\nηnlogKRn≤+.2η\nWe optimize over ηto reach the desired conclusion.\nWe now oﬀer a diﬀerent proof of the same theorem which will give us the optimal\nconstant in the error bound. Deﬁne\n/parenleftBiggt−1/parenrightBiggK K/summationdisplay /summationdisplay/summationtextwt,jej=1 jwt,j= exp−ηℓ(ej,zs), Wt=wt,j, pt= .Wts=1 j=1\nFort= 1, we initialize w1,j= 1, soW1=K. It should be noted that the starting values for\nw1,jare uniform, so we’re starting at the correct point (i.e. maximal entropy) for mirrored\ndescent. Now we have\n/parenleftbigg Kexp−t−1η ℓ ,zW j=1 s=1(ej s) exp(−ηℓ(ej,zt))t+1log gW/summationtext\n\nK tt/parenrightbigg\n= lo/parenleftBig\n· /summationtext−1\nl ℓ e=e/summationtext\nxp/parenleftBig\n−η1/summationtext\nj,z=1/parenrightBig\n(ls)\n= log(IE J∼pt[exp(−ηℓ(eJ,zt))])/parenrightBig\n12Hoeﬀding’s lemma ⇒ ≤log/parenleftBig\nηIEe e−ηJ J8ℓ(e ,zt)\nη2/parenrightBig\n=−ηIEJℓ(eJ,zt)8\nη2η2\nJensen’s ⇒ ≤ − ηℓ(IEJeJ,zt) =−ηℓ(pt,zt)8 8\n4\nsince IE Jej=/summationtextKpj=1t,jej. If we sum over t, the sum telescopes. Since W1=K, we are left\nwithnnη2\nlog(Wn+1)−log(K)≤−η/summationdisplay\nℓ(pt,zt).8t=1\nWe have\nK n\nlog(Wn+1) = log\n/summationdisplay\nexp/parenleftBigg\n−η/summationdisplay\nℓ(ej,zs)\nj=1 s=1/parenrightBigg\n,\nso settingnj∗= argmin1≤j≤K/summationtextℓ(e \nt=1j,zt), we obtain\nn n\nlog(Wn+1)≥log/parenleftBigg\nexp/parenleftBigg\n−η/parenrightBigg/parenrightBigg/summationdisplay\nℓ(ej∗,zs) =−η/summationdisplay\nℓ(ej∗,zt).\ns=1 t=1\nRearranging, we have\nn n /summationdisplay ηnlogKℓ(pt,zt)−ℓ\nt=/summationdisplay\n(ej∗,zt)≤+.8η1 t=1\nFinally, we optimize over ηto arrive at\nη=/radicalbigg\n8logK oRn≤n/radicalbigg\nnl gK⇒ .2\nThe improved constant comes from the assumption that our loss lies in an interval of size\n1 (namely [0 ,1]) rather than in an interval of size 2 (namely [ −1,1]).\n5\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "gradient descent", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "2b91e5658877966194c2571ab9146fba MIT18 657F15 L15", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "67294008-61b3-4b56-846b-f823b00b75c3", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 11\nScribe: Kevin Li Oct. 14, 2015\n2. CONVEX OPTIMIZATION FOR MACHINE LEARNING\nIn this lecture, we will cover the basics of convex optimization as it applies to machine\nlearning. There is much more to this topic than will be covered in this class so you may beinterested in the following books.\nConvex Optimization by Boyd and Vandenberghe\nLecture notes on Convex Optimization by Nesterov\nConvex Optimization: Algorithms and Complexity by Bubeck\nOnline Convex Optimization by Hazan\nThe last two are drafts and can be obtained online.\n2.1 Convex Problems\nA convex problem is an optimization problem of the form min f(x) wherefand are\nx2CC\nconvex. First, we will debunk the idea that convex problems are easy by showing that\nvirtually all optimization problems can be written as a convex problem. We can rewrite anoptimization problem as follows.\nminf(x), mint, mint\nX2X t\u0015f(x);x2X (x;t)2epi(f )\nwhere the epigraph of a function is de\fned by\nepi(f)=f(x;t )2X\u0002 IR :t\u0015f(x)g\n, ,\nf 2 \u0002 \u0015 g\nFigure 1: An example of an epigraph.\nSource: https://en.wikipedia.org/wiki/Epigraph_(mathematics)\n1\nNow we observe that for linear functions,\nminc>x= min c>x\nx2D x2conv (D)\nwhere the convex hull is de\fned\nN N\nconv(D) =fy:9N2Z+;x1;:::;xN2D;\u000bi\u00150;X\n\u000bi= 1;y =\ni=1X\n\u000bixi\ni=1g\nTo prove this, we know that the left side is a least as big as the right side since D\u001aconv(D).\nFor the other direction, we have\nN\nminc>x= min min min c>\u000bixix2conv (D) N x 1;:::;x N2D\u000b1;:::;\u000b N\nNX\ni=1\n= min min min \u000bic>ximinc>x\nN x 1;:::;x N2D\u000b1;:::;\u000b NX\n=1\u0015\nx2Di\nN\n\u0015min min min \u000biminc>x\nN x 1;:::;x N2D\u000b1;:::;\u000b NX\nxi=12D\n= minc>x\nx2D\nTherefore we have\nminf(x) min\nx2X, t\n(x;t)2conv (epi(f ))\nwhich is a convex problem.\nWhy do we want convexity? As we will show, convexity allows us to infer global infor-\nmation from local information. First, we must de\fne the notion of subgradient .\nDe\fnition (Subgradient): LetC\u001a I Rd,f:C! I R. A vector g2I Rdis called a\nsubgradient offatx2Cif\nf(x)\u0000f(y)\u0014g>(x\u0000y)8y2C:\nThe set of such vectors gis denoted by @f(x).\nSubgradients essentially correspond to gradients but unlike gradients, they always ex-\nist for convex functions, even when they are not di\u000berentiable as illustrated by the next\ntheorem.\nTheorem: Iff:C ! I R is convex, then for all x,@f(x) =;. In addition, if fis\ndi\u000berentiable at x, then@f(x) =frf(x)g.\nProof. Omitted. Requires separating hyperplanes for convex sets.\n26f 9 2 2 \u0015 g\n\u001a\n\u0015\n\u0015\n,\n\u001a! 2\n2\n\u0000 \u0014 \u0000 2\n! 6=;\nfr g8\n6\nTheorem: Letf;Cbe convex. If xis a local minimum of fonC, then it is also global\nminimum. Furthermore this happens if and only if 0 2@f(x).\nProof. 02@f(x) if and only if f(x)−f(y)\u00140 for ally2C. This is clearly equivalent to\nxbeing a global minimizer.\nNext assume\u0010xis a local minimum. Then for all y2Cthere exists \"small enough such\nthatf(x)\u0014f(1−\")x+\"y\u0011\n\u0014(1−\")f(x)+\"f(y)=)f(x)\u0014f(y) for ally2C.\nNot only do we know that local minimums are global minimums, looking at the subgra-\ndient also tells us where the minimum can be. If g>(x−y)<0 thenf(x)<f(y). This\nmeansf(y) cannot possibly be a minimum so we can narrow our search to ys such that\ng>(x−y). In one dimension, this corresponds to the half line fy2IR :y\u0014xgifg>0 and\nthe half linefy2IR :y\u0015xgifg<0 . This concept leads to the idea of gradient descent.\n2.2 Gradient Descent\ny\u0019xandfdi\u000berentiable the \frst order Taylor expansion of fatxyieldsf(y)\u0019f(x)+\ng>(y−x). This means that\nminf(x+\"\u0016^)\u0019minf(x)+g>(\"\u0016^)\nj\u0016^j2=1\ngwhich is minimized at \u0016^=− . Therefore to minimizes the linear approximation of fatjgj2x, one should move in direction opposite to the gradient.\nGradient descent is an algorithm that produces a sequence of points fxjgj\u00151such that\n(hopefully) f(xj+1)<f(xj).\n2\n2 \u0014 2\n2\n\u0014 \u0014 ) \u0014 2\nf 2 \u0014 g\nf 2 \u0015 g\n\u0019 \u0019\n\u0019\nf g\nFigure 2: Example where the subgradient of x1is a singleton and and the subgradient of\nx2contains multiple elements.\nSource: https://optimization.mccormick.northwestern.edu/index.php/\nSubgradient_optimization\n3\nAlgorithm 1 Gradient Descent algorithm\nInput:x12C, positive sequence f\u0011sgs\u00151\nfors= 1 tok\u00001do\nxs+1=xs\u0000\u0011sgs; gs2@f(xs)\nend for\nk1return Eitherx\u0016 =kX\nxsorx\u000e\ns=12argminf(x)\nx2fx 1;:::;x kg\nTheorem: Letfbe a convex L-Lipschitz function on I Rdsuch thatx\u00032argminI Rdf(x)\nexists. Assume that jx1\u0000x\u0003j2\u0014R. Then if\u0011Rs=\u0011=Lpfor allsk\u00151, then\nk1 LRf(kX\nxs)\ns=1\u0000f(x\u0003)\u0014p\nk\nandLRminf(xs)\n1s k\u0000f(x\u0003)\u0014p\n\u0014\u0014 k\nProof. Using the fact that gs=1(x2s+1 +\u0011\u0000xs) and the equality 2a>b=kak kbk2\u0000ka\u0000bk2,\n1f(xs)\u0000f(x\u0003)\u0014gs>(xs\u0000x\u0003) = (xs\u0011\u0000xs+1)>(xs\u0000x\u0003)\n1=x2sxs+1 +x x\u00032x2s s+1x\u0003\n2\u0011h\nk \u0000 k k \u0000 k \u0000k \u0000 k\n\u0011 1i\n=2kg2sk+ (\u000e2\n2\u0011s\u0000\u000e2\ns+1)\nwhere we have de\fned \u000es=kxs\u0000x\u0003k. Using the Lipschitz condition\n\u0011 1f(xs)\u0000f(x\u0003)\u0014L2+ (\u000e2\n2 2\u0011s\u0000\u000e2\ns+1)\nTaking the average from 1, to kwe get\nk1X \u0011 \u0011 1 R2\nf(xs)f(x\u0003)\u0014L2 1 \u0011\u0000 + (\u000e2\nk1\u0000\u000e2\nks)\u0011\u0014L2\n+1 +\u000e2\n2 2 2k\u00111\u0014L2+2 2 2k\u0011s=1\nTaking\u0011=R\nLpto minimize the expression, we obtaink\nk1\nkX LRf(xs)\ns=1\u0000f(x\u0003)\u0014p\nk\nk\nNoticing that the left-hand side of the inequality is larger than both f(Pxs)\u0000f(x\u0003) by\ns=1\nJensen's inequality and min f(xs)\n1\u0014s\u0014k\u0000f(x\u0003) respectively, completes the proof.\n42 f g\n\u0000\n\u0000 2\n2\n2\nj \u0000 j \u0014 \u0015\n\u0000 \u0014\n\u0000 \u0014p\n\u0000 k k kk k k\n\u0000 \u0014 \u0000 \u0000 \u0000\nk \u0000 k k \u0000 k \u0000k \u0000 k\nk k \u0000\nk \u0000 k\n\u0000 \u0014 \u0000\n\u0000 \u0014 \u0000 \u0014 \u0014\n\u0000 \u0014p\n\u0000\n\u0000p\n\u0000 \u0000\nOne \naw with this theorem is that the step size depends on k. We would rather have\nstep sizes\u0011sthat does not depend on kso the inequalities hold for all k. With the new step\nsizes,\nXkXk k k\u00112\ns21[X R2\n\u0011sf(x)\u0000f x\u0003)]\u0014L \u000e2 L\ns ( + (\u000e2\ns\u0000s+1)\u00112+2 2s2 2s=1 s=1 s=1\u0014\u0010X\ns=1\u0011\nAfter dividing byPk\nPs=1\u0011s, wePwould like the right-hand side to approach 0. For this to\n\u00112happen we need Ps!0 and\u0011s!1. One candidate for the step size is \u0011s=G\n\u0011spsinces\nk\nthenP k\n\u00112\ns\u0014c1G2log(k ) and\u0011sc2Gp\nk. So we get\ns=1 sP\n=1\u0015\n\u0010Xkc1\u0011s\u0011k\u00001X GLlogk R2\n\u0011s[f(xs)f(x\u0003)]\n2c2p +\nk 2c2Gp\nks=1 s=1\u0000 \u0014\nChoosingGappropriately, the right-hand side approaches 0 at the rate of LRlogk. Noticepk\nthat we get an extra factor of log k. However, if we look at the sum from k=q\n2 tokinstead\nk\nof 1 tok,P k\n\u00112\ns\u0014c0\n1G2andP\u0011s\u0015c0\n2Gp\nk. Now we have\n=k s=1 s2\nk k\u00001 cLRminf(xs)f(x\u0003) minf(xs)f(x\u0003)\u0011s\u0011s[f(xs)f(x\u0003)]\n1\u0014s\u0014k\u0000 \u0014\nks k2\u0000 \u0014\u0010X\nk\u0011X\nk\u0000 \u0014p\n\u0014\u0014 ks= s=2 2\nwhich is the same rate as in the theorem and the step sizes are independent of k.\nImportant Remark: Note this rate only holds if we can ensure that jxk=2\u0000x\u0003j2\u0014R\nsince we have replaced x1byxk=2in the telescoping sum. In general, this is not true for\ngradient descent, but it will be true for projected gradient descent in the next lecture.\nOne \fnal remark is that the dimension ddoes not appear anywhere in the proof. How-\never, the dimension does have an e\u000bect because for larger dimensions, the conditions fis\nL-Lipschitz andjx1\u0000x\u0003j2\u0014Rare stronger conditions in higher dimensions.\n5\u0000 \u0014 \u0000 \u0014\n!!1\n\u0014 \u0015p\n\u0000 \u0014pp\np\n\u0014 \u0015p\n\u0000 \u0014 \u0000 \u0014 \u0000 \u0014p\nj \u0000 j \u0014\nj j \u0014 \u0000\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "gradient descent", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "35139a38b88d438099e767f0b1a6dfe8 MIT18 657F15 L11", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "59678c22-1500-4823-846f-b9ebdcbbd972", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 16\nScribe:Haihao (Sean) Lu Nov. 2, 2015\nRecall that in last lecture, we talked about prediction with expert advice. Remember\nthatl(ej,zt) means the loss of expert jat timet, whereztis one adversary’s move. In this\nlecture, for simplexity we replace the notation ztand denote by ztthe loss associated to all\nexperts at time t:\nzt=\nℓ(e1,zt)\n... ,\nℓ(eK,zt)\nwhereby for p∈∆K,p\n⊤zt=Kp ej=1jℓ(j,zt). This gives an alternative deﬁnition of ft(p)\nin last lecture. Actually it is easy to check ft(p) =p⊤zt, thus we can rewrite the theorem\nfor exponential weighting(EW/summationtext\n) strategy as\nn n\nRn≤/summationdisplay\np⊤\ntzt−minp\npt=1∈∆k/summationdisplay⊤zt2nlogK,\nt=1≤/radicalbig\nwhere the ﬁrst inequality is Jensen inequality:\nn n/summationdisplay\np⊤\nzzt\nt=1≥/summationdisplay\nℓ(pz,zt).\nt=1\nWe consider EW strategy for bounded convex losses. Without loss of generality, we\nassumeℓ(p,z)∈[0,1], for all ( p,z)∈∆K×Z, thus in notation here, we expect pt∈∆K\nandzK ¯t∈[0,1] . Indeed if ℓ(p,z)∈[m,M] then one can work with a rescaled loss ℓ(a,z) =\nℓ(a,z)−m. Note that now we have bounded gradient on pt, sinceztis bounded.M−m\n2. FOLLOW THE PERTURBED LEADER (FPL)\nIn this section, we consider a diﬀerent strategy, called Follow the Perturbed Leader.\nAt ﬁrst, we introduce Follow the Leader strategy, and give an example to show that\nFollow the Leader can be hazardous sometimes. At time t, assume that choose\nt−1\npt= argmin\np∈∆K/summationdisplay\np⊤zs.\ns=1\nNotethat thefunctiontobeoptimized islinear in p, wherebytheoptimal solutionshould\nbe a vertex of the simplex. This method can be viewed as a greedy algorithm, however, it\nmight not be a good strategy.\nConsider the following example. Let K= 2,z1= (0,ε)⊤,z2= (0,1)⊤,z3= (1,0)⊤,\nz4= (0,1)⊤and so on (alternatively having (0 ,1)⊤and (1,0)⊤whent≥2), where εis small\nenough. Then with Following the Leader Strategy, we have that p1is arbitrary and in the\nbest case p1= (1,0)⊤, andp2= (1,0)⊤,p3= (0,1)⊤,p4= (1,0)⊤and so on (alternatively\nhaving (0 ,1)⊤and (1,0)⊤whent≥2).\n1\nIn the above example, we have\nn n/summationdisplay nnp⊤\ntztminp⊤ztn1 1,\np∆k 2 2t=1− ≤ − − ≤\n∈/summationdisplay\nt=1−\nwhich gives raise to linear regret.\nNow let’s consider FPL. FPL regularizes FL by adding a small amount of noise, which\ncan guarantee square root regret under oblivious adversary situation.\nAlgorithm 1 Follow the Perturbed Leader (FPL)\nInput:Letξbe a random variables uniformly drawn on [0 ,1]K.η\nfort= 1 tondo\nt−1\npt= argmin\np∈∆K/summationdisplay\ns=1/parenleftbig\np⊤zs+ξ/parenrightbig\n.\nend for\nWe analyze this strategy in oblivious adversaries, which means the sequence ztis chosen\nahead of time, rather than adaptively given. The following theorem gives a bound for regret\nof FPL:\nTheorem: FPL with η=√1yields expected regret:kn\nIEξ[Rn]≤2√\n2nK .\nBefore proving the theorem, we introduce the so-called Be-The-Leader Lemma at ﬁrst.\nLemma: (Be-The-Leader)\nFor all loss function ℓ(p,z), let\nt\np∗\nt= arg min ℓ(p,zs),\np∈∆K/summationdisplay\ns=1\nthen we haven n /summationdisplay\nℓ(p∗\nt,zt)\nt=1≤/summationdisplay\nℓ(pn∗,zt)\nt=1\nProof.The proof goes by induction on n. Forn= 1, it is clearly true. From nton+1, it\n2\nfollows from:\nn+1 n /summationdisplay\nℓ(pt∗,zt) =/summationdisplay\nℓ(p∗\nt,zt)+ℓ(pn∗\n+1,zn+1)\nt=1 i=1\nn\n≤/summationdisplay\nℓ(p∗\nn,zt)+ℓ(p∗\nn+1,zn+1)\ni=1\nn\n≤/summationdisplay\nℓ(p∗\nn+1,zt)+ℓ(p∗\nn+1,zn+1),\ni=1\nwheretheﬁrstinequality usesinductionandthesecondinequality followsfromthedeﬁnition\nofp∗\nn.\nProof of Theorem . Deﬁne\nt\nqt= argmin p⊤(ξ+\np∈∆K/summationdisplay\nzs).\ns=1\nUsing the Be-The-Leader Lemma with\n/braceleftbiggpT(ξ+z1)if t= 1ℓ(p,zt) =pTzt if t >1,\nwe haven n\nq1⊤ξ+/summationdisplay\nqt⊤zt≤minq⊤(ξ+\nqt=∈∆K\n1/summationdisplay\nzt),\nt=1\nwhereby for any q∈∆K,\nn/summationdisplay/parenleftig2qt⊤zt−q⊤zt/parenrightig\n≤/parenleftig\nq⊤−q1⊤/parenrightig\nξ≤ /bardblq 1\ni−q1\n=1/bardbl /bardblξ/bardbl∞≤,η\nwhere the second inequality uses H¨ older’s inequality and the third inequality is from the\nfact that qandq1are on the simplex and ξis in the box.\nNow let\nt\nqt= arg min p⊤/parenleftigg\nξ+zt+/summationdisplay\nzs\np∈∆K\ns=1/parenrightigg\nand\nt\npt= arg min p⊤+\n∈∆/parenleftigg\nξ+0\npK/summationdisplay\nzs\ns=1/parenrightigg\n.\nTherefore,\nn n\nIE[Rn]≤/summationdisplay\np⊤\ntzt\ni−minp⊤zt\np∈∆k\n=1/summationdisplay\ni=1\nn/parenleftig /parenrightign\n≤/summationdisplay\nqt⊤zt−p∗Tzt+/summationdisplay\nIE[(pt t\ni=1 =1−q)⊤zt]\ni\nn2≤+/summationdisplay\nIE[(pt−qt)⊤zt], (2.1)ηi=1\n3\nwherep∗= argminp∈∆K/summationtextnp zt=1⊤t.\nNow let\nt−1\nh(ξ) =zt⊤/parenleftigg\narg min p⊤[ξ+\np∈∆K/summationdisplay\nzs],\ns=1/parenrightigg\nthen we have a easy observation that\nIE[zt⊤(pt−qt)] = IE[h(ξ)]−IE[h(ξ+zt)].\nHence,\nIE[zt⊤(pt−qK Kt)] =η/integraldisplay\nh(ξ)dξ−η/integraldisplay\nh(ξ)dξ\nξ∈[0,1]K ξ∈z+[0,1]Ktη η\n≤ηK/integraldisplay\nh(ξ)dξ\nξ∈[0,1]K\nη\\/braceleftBig\nz ,1]Kt+[0η/bracerightBig\n≤ηK/integraldisplay\n1dξ\nξ∈[0,1]K\\/braceleftBig\nzt+[0,1]K\nη η/bracerightBig\n= IP(∃i∈[K],ξ(i)≤zt(i))\nK\n≤/summationdisplay\nIP/parenleftbigg\nUnif/parenleftbigg1[0,]/parenrightbigg\n≤zt(i)ηi=1/parenrightbigg\n≤ηKzt(i)≤ηK , (2.2)\nwhere the ﬁrst inequality is from the fact that h(ξ)≥0, the second inequality uses\nh(ξ)≤1, the second equation is just geometry and the last inequality is due to zt(i)≤1.\nCombining (2.1) and (2.2) together, we have\n2IE[Rn]≤+ηKn .η\nIn particular, with η=/radicalig\n2, we haveKn\nIE[Rn]≤2√\n2Kn ,\nwhich completes the proof.\n4\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "convex", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "30d28d30e752970a9ddaed9a5332543f MIT18 657F15 L16", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "920832c6-ef9b-447d-a68d-42d5a2d5267f", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 10\nScribe:Aden Forrow Oct. 13, 2015\nRecall the following deﬁnitions from last time:\nDeﬁnition: A function K:X ×X /ma√sto→ Ris called a positive symmetric deﬁnite kernel\n(PSD kernel) if\n1.∀x,x′∈ X,K(x,x′) =K(x′,x)\n2.∀n∈Z+,∀x1,x2,...,x n, then×nmatrix with entries K(xi,xj) is positive deﬁ-\nnite. Equivalently, ∀a R 1,a2,...,a n∈,\nn/summationdisplay\naiajK(xi,xj)\ni,j=1≥0\nDeﬁnition: LetWbe a Hilbert space of functions X /ma√sto→R. A symmetric kernel K(·,·)\nis called a reproducing kernel ofWif\n1.∀x∈ X, the function K(x,·)∈W.\n2.∀x∈ X,∀f∈W,/an}bracketle{tf(·),K(x,·)/an}bracketri}htW=f(x).\nIf such a K(x,·) exists, Wis called a reproducing kernel Hilbert space (RKHS).\nAs before, /an}bracketle{t·,·/an}bracketri}htWand/bardbl·/bardblWrespectively denote the inner product and norm of W. The\nsubscript Wwill occasionally be omitted. We can think of the elements of Was inﬁnite\nlinear combinations of functions of the form K(x,·). Also note that\n/an}bracketle{tK(x,·),K(y,·)/an}bracketri}htW=K(x,y)\nSince so many of our tools rely on functions being bounded, we’d like to be able to\nbound the functions in W. We can do this uniformly over x∈ Xif the diagonal K(x,x) is\nbounded.\nProposition: LetWbe a RKHS with PSD Ksuch that supx∈XK(x,x) =kmaxis\nﬁnite. Then ∀f∈W,\nsup|f(x)| ≤ /bardblf/bardblW/radicalbig\nkmax\nx∈X\n.\nProof.We rewrite f(x) as an inner product and apply Cauchy-Schwartz.\nf(x) =/an}bracketle{tf,K(x,·)/an}bracketri}htW≤ /bardblf/bardblW/bardblK(x,·)/bardblW\nNow/bardblK(x,·)/bardbl2\nW=/an}bracketle{tK(x,·),K(x,·)/an}bracketri}htW=K(x,x)≤kmax. The result follows immediately.\n1\n1.5.2 Risk Bounds for SVM\nWe now analyze support vector machines (SVM) the same way we analyzed boosting. The\ngeneral idea is to choose a linear classiﬁer that maximizes the margin (distance to classiﬁers)\nwhile minimizing empirical risk. Classes that are not linearly separable can be embedded\nin a higher dimensional space so that they are linearly separable. We won’t go into that,\nhowever; we’ll just consider the abstract optimization over a RKHS W.\nExplicitly, we minimize the empirical ϕ-risk over a ball in Wwith radius λ:\nˆ ˆ f= min Rn,ϕ(f)\nf∈W,/bardblf/bardblW≤λ\nˆ ˆ ˆ The soft classiﬁer fis then turned into a hard classiﬁer h= sign(f). Typically in SVM ϕ\nis the hinge loss, though all our convex surrogates behave similarly. To choose W(the only\nother free parameter), we choose a PSD K(x1,x2) that measures the similarity between two\npointsx1andx2.\nAs written, this is an intractable minimum over an inﬁnite dimensional ball {f,/bardblf/bardblW≤\nλ}. The minimizers, however, will all be contained in a ﬁnite dimensional subset.\nTheorem: Representer Theorem. LetWbe a RKHS with PSD Kand letG:\nnR/ma√sto→Rbe any function. Then\nminG(f(x1), ...,f(xn)) = min G(f(x1),...,f(xn))\nf∈W,/bardblf/bardbl≤λ ∈¯f Wn,/bardblf/bardbl≤λ\n= min G(gα(x1),...,gα(xn)),\nα∈Rn,α⊤IKα≤λ2\nwheren\n¯Wn={f∈W|f(·) =gα(·) =/summationdisplay\nαiK(xi,\ni=1·)}\nand IK ij=K(xi,xj).\nProof. ¯SinceWnis a linear subspace of W, we can decompose any f Wuniquely as\n¯⊥¯∈¯ ⊥∈\nf=f+fwithf W nandf∈¯W⊥\nn. The Pythagorean theorem then gives\n/bardblf/bardbl2\nW=/bardbl¯f/bardbl2\nW+/bardblf⊥/bardbl2\nW\n¯ Moreover, since K(xi,·)∈Wn,\nf⊥(xi) =/an}bracketle{tf⊥,K(xi,·)/an}bracketri}htW= 0\n¯ Sof(xi) =f(xi) and\nG(f(x1¯ ¯ ),...,f(xn)) =G(f(x1),...,f(xn)).\nBecause f⊥does not contribute to G, we can remove it from the constraint:\n¯ ¯ min G(f(x1), ...,f(xn)) = min G(f(x1),...,f(xn)).¯f∈W,/bardblf/bardbl2+/bardblf⊥/bardbl2≤λ2 f∈ /bardbl¯W, f/bardbl2≤λ2\n2\n¯ Restricting to f∈Wnnow does not change the minimum, which gives us the ﬁrst equality.\nFor the second, we need to show that /bardblgα/bardblW≤λis equivalent to α⊤IKα≤λ2.\n/bardblgα/bardbl2=/an}bracketle{tgα,gα\nn/an}bracketri}ht\nn\n=/an}bracketle{t/summationdisplay\nαiK(xi,\ni=1·),\nn/summationdisplay\nαjK(xj,\n=1·)\nj/an}bracketri}ht\n=/summationdisplay\nαiαj/an}bracketle{tK(xi,(\n,j=1·),K xj,\ni·)/an}bracketri}ht\nn\n=/summationdisplay\nαiαjK(xi,xj)\ni,j=1\n=α⊤IKα\nWe’ve reduced the inﬁnite dimensional problem to a minimization over α∈nR. This\nworks because we’re only interested in Gevaluated at a ﬁnite set of points. The matrix\nIK here is a Gram matrix, though we will not not use that. IK should be a measure of the\nsimilarity of the points xi. For example, we could have W={/an}bracketle{tx,·/an}bracketri}htRd,x∈dR}withK(x,y\nthe usual inner product K(x,y) =/an}bracketle{tx,y/an}bracketri}htRd.\nˆ ˆ We’ve shown that fonly depends on Kthrough IK, but does Rn,ϕdepend on K(x,y)\nforx,y∈/{xi}? It turns out not to:\nn n n1ˆRn,ϕ=/summationdisplay 1ϕ(−Yigα(xi)) =/summationdisplay\nϕ(−Yi/summationdisplay\nαjK(xj,xi)).n ni=1 i=1 j=1\nThe last expression only involves IK. This makes it easy to encode all the knowledge about\nour problem that we need. The hard classiﬁer is\nn\nˆ ˆ h(x) = sign( f(x)) = sign( gαˆ(x)) = sign(/summationdisplay\nαˆjK(xj,x))\nj=1\nIf we are given a new point xn+1, we need to compute a new column for IK. Note that\nxn+1must be in some way comparable or similar to the previous {xi}for the whole idea of\nextrapolating from data to make sense.\nThe expensive part of SVMs is calculating the n×nmatrix IK. In some applications,\nIK may be sparse; this is faster, but still not as fast as deep learning. The minimization\nover the ellipsoid α⊤IKαrequires quadratic programming, which is also relatively slow. In\npractice, it’s easier to solve the Lagrangian form of the problem\nn1αˆ = argmin/summationdisplay\nϕ(−Yigα(x′ ⊤i))+λ αIKα\nα∈Rnni=1\nThis formulation is equivalent to the constrained one. Note that λandλ′are diﬀerent.\nSVMs have few tuning parameters and so have less ﬂexibility than other methods.\nWe now turn to analyzing the performance of SVM.\n3\nTheorem: Excess Risk for SVM. Letϕbe anL-Lipschitz convex surrogate and\nˆ ˆ Wa RKHS with PSD Ksuch that max x|K(x,x)|=kmax<∞. Lethn,ϕ= signfn,ϕ,\nˆwherefn,ϕis the empirical ϕ-risk minimizer over F={f\nˆˆˆ∈W./bardblf/bardblW≤λ}(that is,\nRn,ϕ(fn,ϕ)≤Rn,ϕ(f)∀f∈ F). Suppose λ√kmax≤1. Then\nˆR(hn,ϕ)−R∗≤2c/parenleftbigg γ γγkmax 2log(2/δ)inf (Rϕ(f)−R∗\nϕ)/parenrightbigg\n+2c/parenleftBigg\n8Lλ +\nf∈/radicalbigg\n2L\nF n/parenrightBigg\n2c/parenleftBigg/radicalbigg\nn/parenrightBigg\nwith probability 1 −δ. The constants candγare those from Zhang’s lemma. For the\nhinge loss, c=1\n2andγ= 1.\nProof.The ﬁrst term comes from optimizing over a restricted set Finstead of all classiﬁers.\nThe third term comes from applying the bounded diﬀerence inequality. These arise in\nexactly the same way as they do for boosting, so we will omit the proof for those parts. For\nthe middle term, we need to show that Rn,ϕ(F)≤λkmax\nn.\nFirst,|f(x)| ≤ /bardblf/bardblW√kmax≤λ√kmax≤1 for all/radicalBig\nf∈ F, so we can use the contraction\ninequality to replace Rn,ϕ(F) withRn(F). Next we’ll expand f(xi) inside the Rademacher\ncomplexity and bound inner products using Cauchy-Schwartz.\nn1Rn(F) = sup Esup σif(xi)\nx1,...,xn/bracketleftBigg\nf∈F/vextendsingle/vextendsingle\n/vextendsingle\nni=1/vextendsingle/vextendsingle/bracketrightBigg/summationdisplay\n/vextendsingle/vextendsingle\n/vextendsingle\nn1=supE/bracketleftBigg\nsup/vextendsingle/vextendsingle\n/vextendsingle/summationdisplay\nσ/vextendsingle\niK(x/vextendsingle\n/an}bracketle{ti,·),fnx1,...,xnf∈Fi=1/an}bracketri}ht/vextendsingle/vextendsingle/bracketrightBigg\nn1/vextendsingle\n=supE/vextendsingle/vextendsingle\n/bracketleftBigg\nsup/vextendsingle/vextendsingle/vextendsingle/an}bracketle{t/summationdisplay\nσiK(xi,·),f/vextendsingle\nn/vextendsingle\nx1,...,xnf∈Fi=1/an}bracketri}ht/vextendsingle/vextendsingle/bracketrightBigg\n/vextendsingle\nλ/vextendsingle/vextendsingle /vextendsingle\n≤sup/radicaltp\n/radicalvertex/radicalvertex\n/radicalbtE/bracketleftBiggn\n/bardbl/summationdisplay\nσiK(x2i,/vextendsingle\nnx1,...,xni=1·)/bardblW/bracketrightBigg\nNow,\nn n n\n2E/bracketleftBigg\n/bardbl/summationdisplay\nσiK(xi,·)/bardblW/bracketrightBigg\n=E\n/an}bracketle{t/summationdisplay\nσiK(xi,·),/summationdisplay\nσjK(xj,\ni=1 i=1 j=1·)/an}bracketri}htW\nn\n=/summationdisplay\n/an}bracketle{tK(x E i,·),K(xj,·)/an}bracketri}ht[σiσj]\ni,j=1\nn\n=\ni/summationdisplay\nK(xi,xj)δij\n,j=1\n≤nkmax\nSoRn(F)≤λkmax\nnandwearedonewiththenewpartsoftheproof. Theremainderfollows\nas with boosti/radicalBig\nng, using symmetrization, contraction, the bounded diﬀerence inequality, and\nZhang’s lemma.\n4\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "convex", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "566289985bff05b214ccf11cbec4e94a MIT18 657F15 L10", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "1c7d787c-e223-40ca-9330-c527d52c9d0d", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 20\nScribe:Vira Semenova Nov. 23, 2015\nIn this lecture, we talk about the adversarial bandits under limited feedback. Adver-\nsarial bandit is a setup in which the loss function l(a,z) :AxZis determinitic. Lim-\nited feedback means that the information available to the DM after the step tisIt=\n{l(a1,z1),...,l(at−1,zt)}, namely consits of the realised losses of the past steps only.\n5. ADVERSARIAL BANDITS\nConsider the problem of prediction with expert advice. Let the set of adversary moves be Z\nand the set of actions of a decision maker A={e1,...,eK}. At time t,at∈ Aandzt∈ Zare\nsimultaneously revealed.Denote the loss associated to the decision at∈ Aand his adversary\nplayingztbyl(at,zt). We compare the total loss after nsteps to the minimum expert loss,\nnamely:\nn\nmin\n≤ ≤/summationdisplay\nlt(ej,zt),\n1j Kt=1\nwhereejis the choice of expert j∈ {1,2,..,K}.\nThe cumulative regret is then deﬁned as\nn n\nRn=/summationdisplay\nlt(at,zt)−min/summationdisplay\nlt(ej,zt)\n1≤j≤Kt=1 t=1\nThe feedback at step tcan be either full or limited. The full feedback setup means that\nthevector f= (l(e1,z⊤t),...,l(eK,zt)) oflossesincurredatapairofadversary’schoice ztand\neach bandit ej∈ {e1,...,eK}is observed after each step t. Hence, the information available\nto the DM after the step tisI=∪t ′t ′{l(a1,zt),...,l(aK,zt=1 t′)}. The limited feedback means\nthat the time −tfeedback consists of the realised loss l(at,zt) only. Namely, the information\navailable to the DM is It={l(a1,z1),...,l(at,zt)}. An example of the ﬁrst setup is portfolio\noptimization problems, where the loss of all possible portfolios is observed at time t. An\nexample of the second setup is a path planning problem and dynamic pricing, where the\nloss of the chosen decision only is observed. This lecture has limited feedback setup.\nThe two strategies, deﬁned in the past lectures, were exponential weights, which yield\nthe regret of order Rn≤c√nlogKand Follow the Perturbed Leader. We would like to\nplay exponential weights, deﬁned as:\n1exp(tηpt,j/summationtext−l=s=1(ej,zs))/summationtextk−\nl=1exp(−η/summationtextt−1ls=1(ej,zs))\nThis decision rule is not feasible, since the loss l(ej,zt) are not part of the feedback if\nej=at. We will estimate it by\nl(ej,zt)1I(aˆt=ej)l(ej,zt) =P(at=ej)\n1/ne}ationslash\nˆ Lemma: l(ej,zt) is an unbiased estimator of l(ej,zt)\nK I(P ˆl(e e =roof.Ek,zt)1ket)\natl(ej,zt) =/summationtextPk=1 (a=e) =l(e ,z)P(at=ej) tk j t\nDeﬁnition (Exp 3 Algorithm): Letη >0 be ﬁxed. Deﬁne the exponential weights\nas\n−/summationtextt−1ηˆ(j) exp( l(ej,zs))ps=1\nt+1,j=/summationtextk\nl=1exp(−η/summationtextt−1ˆls=1(ej,zs))\n(Exp3 stands for Exponential weights for Exploration and Exploitation.)\nWe will show that the regret of Exp3 is bounded by√2nKlogK. This bound is√\nK\ntimes bigger than the bound on the regret under the full feedback. The√\nKmultiplier is\nthe price of have smaller information set at the time t. The are methods that allow to get\nrid of log Kterm in this expression. On the other hand, it can be shown that√\n2nKis the\noptimal regret.\n−/summationtextt/summationtextK−1ˆW eProof.LetWt,j= exp(kη l(e ,=jzs)),W1 t=/summationtextWs j=1t,j, andp=j=1t,j j\nt W.t\nW/summationtextK(−η/summationtext−1exptlˆe ,z ηl ˆe ,zt+1 j=1 s=1(j s))exp( ( j t))\nlog( ) = log( ) (5.1)W/summationtextKet−\nxp(−/summationtext−1t ηˆls=1(ej,zj=1 s))\nt−1\nˆ = log(IE J∼ptexp(−η/summationdisplay\nl(eJ,zs))) (5.2)\ns=1\n≤∗ η2\nlog(1−2ηˆIEJ∼ptl(eJ,z ˆs)+IEJ∼ptl(eJ,zs) (5.3)2\n∗ ˆ where inequality is obtained by plugging in IE J∼ptl(eJ,zt) into the inequality\nη2x2\nexpx≥1−ηx+2\n.\nK Kl(ej,zt)1I(a e)ˆIEJ∼ptl(eJzt) =/summationdisplayˆt=j, pt,jl(eJ,zt) =/summationdisplay\npt,j =l(at,zt) (5.4)P(at=ej)j=1 j=1\nK Kl2\n2 (ej,zt)1I(a)ˆ ˆ t=ejIEJ∼ptl(eJ,zt) =/summationdisplay\npt,jl2(eJ,zt) =/summationdisplay\npt,j (5.5)P2(at=ej)j=1 j=1\nl2(ej,zt)1=Pat,t≤ (5.6)Pat,t\nSumming from 1 through n, we get\nlog(Wt+1)≤2log(n\n1)−η/summationtextlt=1(at,zt)+ηW2/summationtext\n1.\nPa ,tt\n2\nFort= 1, we initialize w1,j= 1, soW1=K.\nSince IE1 p\nJ=Pa ,t/summationtextK j,tKj=1= , the expression above becomespt j,t\nIElog(n η2KnWn+1)−logK≤ −η/summationtext\ntl a=1(t,zt)+2ˆ Noting that log( Wn+1) = log(/summationtextK\nj=1exp(−η/summationtextt−1l e ,zs=1(j s))\nand deﬁning j∗= argmin1≤j≤K/summationtextnlt=1(ej,zt), we obtain:\nK t−1 t−1\nlog(Wn+1)≥log(/summationdisplay\nexp(−η/summationdisplay\nl(ej,zs))) =−η/summationdisplay\nl(ej,zs)\nj=1 s=1 s=1\nTogether:\nn nlogKηKn /summationdisplay\nl(at,zt)−min t≤j≤/summationdisplay\nl(ej,z)\nKt t=1≤ +\n1 η2=1\nThe choice of η:=√2logKnKyields the bound Rn√≤2KlogKn.\n3\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "vector", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "6570ac6d662ccb30b130baa4cf97f7bf MIT18 657F15 L20", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "2b1ae0b6-193a-4d35-adef-f68ce0653cbb", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 9\nScribe:Xuhong Zhang Oct. 7, 2015\nRecall that last lecture we talked about convex relaxation of the original problem\nn1ˆh= argmin 1I(h(Xi) =Yi)\nh∈Hn/summationdisplay\ni=1\nby considering soft classiﬁers (i.e. whose output is in [ −1,1] rather than in {0,1}) and\nconvex surrogates of the loss function (e.g. hinge loss, exponential loss, logistic loss):\nn1ˆ ˆ f= argminRϕ,n(f) = argmin ϕ(Yif(Xi))\nf∈F f∈Fn/summationdisplay\ni=1−\nˆ ˆAndh= sign(f) will be used as the ‘hard’ classiﬁer.\nˆ ¯ ¯ We want to bound the quantity Rϕ(f)−Rϕ(f), wheref= argminf∈FRϕ(f).\nˆ ˆ (1)f= argminf∈FRϕ,n(f), thus\nˆ ¯ ˆ¯ˆ¯ˆˆˆˆ ˆ ¯ Rϕ(f) =Rϕ(f)+Rϕ,n(f)−Rϕ,n(f)+Rϕ,n(f)−Rϕ,n(f)+Rϕ(f)−Rϕ(f)\n≤¯ˆ¯ˆˆ ˆ ¯ Rϕ(f)+Rϕ,n(f)−Rϕ,n(f)+Rϕ(f)−Rϕ(f)\n≤¯ ˆ Rϕ(f)+2sup |Rϕ,n(f)−Rϕ(f)\nf∈F|\nˆ (2) Let us ﬁrst focus on E[supf∈F|Rϕ,n(f)−Rϕ(f)|]. Using the symmetrization trick as\nbefore, weknowit isupper-boundedby2 Rn(ϕ◦F), wheretheRademacher complexity\nn1Rn(ϕ◦F) = sup E[sup|/summationdisplay\nσiϕ(−Yif(Xi)) ]\nX1,...,Xn,Y1,...,Ynf∈Fni=1|\nOne thing to notice is that ϕ(0) = 1 for the loss functions we consider (hinge loss,\nexponential loss and logistic loss), but in order to apply contraction inequality later,\nwe requireϕ(0) = 0. Let us deﬁne ψ(·) =ϕ(·)−1. Clearlyψ(0) = 0, and\nn1E[sup|/summationdisplay\n(ϕ(−Yif(Xi))−E[ϕ(−Yif(Xi))]) ]\nf∈Fni=1|\nn1=E[sup|/summationdisplay\n(ψ(−Yif(X)−Ei) [ψ( i\n1−Yif(X))])\nf∈Fni=|]\n≤2Rn(ψ◦F)\n(3) The Rademacher complexity of ψ◦ Fis still diﬃcult to deal with. Let us assume\nthatϕ(·) isL-Lipschitz, (as a result, ψ(·) is alsoL-Lipschitz), apply the contraction\ninequality, we have\nRn(ψ◦F)≤2LRn(F)\n1/ne}ationslash\n(4) LetZi= (Xi,Yi),i= 1,2,...,nand\nn1g(Z1,Z2ˆ ,...,Zn) = sup|Rϕ,n(f)−Rϕ(f) =\nf|sup\n∈F f∈F|n/summationdisplay\n(ϕ(\ni=1−Yif(Xi))−E[ϕ(−Yif(Xi))])|\nSinceϕ(·) ismonotonically increasing, itisnotdiﬃculttoverifythat ∀Z1,Z2,...,Zn,Z′\ni\n1 2L|g(Z1,...,Zi,...,Zn)−g(Z1,...,Z′\ni,...,Zn)| ≤(ϕ(1)−ϕ(n−1))≤n\nThe last inequality holds since gisL-Lipschitz. Apply Bounded Diﬀerence Inequality,\n2t2\nP(|s|ˆ ˆ upRϕ,n(f)−Rϕ(f)|−E[sup|Rϕ,n(f)−Rϕ(f)|]>\nF ∈F|t)≤2exp(\n∈− )\nf/summationtextnf i=(2L)21n\nSet the RHS of above equation to δ, we get:\nlog(2/δ)ˆ ˆ supR E ϕ,n(f)Rϕ(f) [sup Rϕ,n(f)\nf∈F| − | ≤\nf∈F| −Rϕ(f)|]+2L/radicalbigg\n2n\nwith probability 1 −δ.\n(5) Combining (1) - (4), we have\nˆ ¯Rϕ(f)≤Rϕ(f)+8LRn(F)+2L/radicalbigg\nlog(2/δ)\n2n\nwith probability 1 −δ.\n1.4 Boosting\nInthissection, wewillspecializetheaboveanalysistoaparticularlearningmodel: Boosting.\nThe basic idea of Boosting is to convert a set of weak learners (i.e. classiﬁers that do better\nthan random, but have high error probability) into a strong one by using the weighted\naverage of weak learners’ opinions. More precisely, we consider the following function class\nM\nF={/summationdisplay\nθjhj(·) :|θ|1≤1,hj:X /ma√sto→[−1,1],j∈ {1,2,...,Ma\nj=1}re classiﬁers }\nand we want to upper bound Rn(F) for this choice of F.\nn M n1 1Rn(F) = sup E[sup σiYif(Xi) ] = sup E[ supθjYiσihj(Xi) ]\nZ1,...,Znf∈F|n/summationdisplay\nnZ |θ|1≤11|\n1,...,Zi=n|/summationdisplay\nj=1/summationdisplay\ni=1|\nLetg(θ) =|/summationtextM\nj=1θj/summationtextn\ni=1Yiσihj(Xi)|. It is easy to see that g(θ) is a convex function, thus\nsup|θ|1≤1g(θ) is achieved at a vertex of the unit ℓ1ball{θ:/bardblθ/bardbl1≤1}. Deﬁne the ﬁnite set\nY1h1(X1) Y1h2(X1) Y1hM(X1)/braceleftiggY2h1(X2) Y2h2(X2)  Y2hM(X2)\nBX,Y/defines., ,...,.\n±\n.± ±/bracerightigg\nYnh1(Xn)\n \n. ..\nYnh2(Xn)\n\n.. .\nYnhM(Xn)\n2\nThen\nRn(F) = supRn(BX,Y).\nX,Y\nNotice max b∈BX,Yb√| |2≤nand|BX,Y|= 2M. Therefore, using a lemma from Lecture 5,\nwe get\n2log(2B 2 4RX,Y)log(M)\nn(BX,Y)≤max\nb∈BX,Y|b|2/radicalbig\n| |\nn≤/radicalbigg\nn\nThus for Boosting,/bracketleftbig /bracketrightbig\n2log(4M) log(2 /δ)ˆ ¯Rϕ(f)≤Rϕ(f)+8L/radicalbigg\n+2L/radicalbigg\nwith probability 1 - δn 2n\nTo get some ideas of what values Lusually takes, consider the following examples:\n(1) for hinge loss, i.e. ϕ(x) = (1+x)+,L= 1.\n(2) for exponential loss, i.e. ϕ(x) =ex,L=e.\n(3) for logistic loss, i.e. ϕ(x) = log2(1+ex),L=e\n1+elog2(e)≈2.43\nˆ ¯ Now we have bounded Rϕ(f)−Rϕ(f), but this is not yet the excess risk. Excess risk is\nˆ deﬁned asR(f)−R(f∗), wheref∗= argminfRϕ(f). The following theorem provides a\nbound for excess risk for Boosting.\nTheorem: LetF={/summationtextM\nj=1θjhj:/bardblθ/bardbl1≤1,hjsare weak classiﬁers }andϕis anL-\nˆ ˆ ˆ Lipschitz convex surrogate. Deﬁne f= argminf∈FRϕ,n(f) andh= sign(f). Then\nγ γ\n∗/parenleftbig∗/parenrightbigγ/parenleftigg\n2log(4M) log(2/δ)ˆR(h)−R≤2cinfRϕ(f)−Rϕ(f) +2c8L +\nf∈F/radicalbigg\nn/parenrightigg\n2c/parenleftigg\n2L/radicalbigg\n2n/parenrightigg\nwith probability 1 −δ\nProof.\nˆR(h)−R∗≤2c/parenleftbigγRϕ(f)−Rϕ(f∗)/parenrightbig\n/parenleftiggγ\n∗ 2log(4M) log(2 /δ)≤2cinfRϕ(f)Rϕ(f)+8L +2L\nf∈F−/radicalbigg\nn/radicalbigg\n2n/parenrightigg\nγ\n∗γ 2log(4M) log(2/δ)≤2cinfRϕ(f)−Rϕ(f) +2c\nf∈F/parenleftigg\n8L/radicalbigg\nn/parenrightigg\n+2c/parenleftigg\n2L/radicalbigg\n2n/parenrightiggγ/parenleftbig /parenrightbig\nHere the ﬁrst inequality uses Zhang’s lemma and the last one uses the fact that for ai≥0\nandγ∈[0,1], (a1+aγ2+a3)≤aγ\n1+aγ\n2+aγ\n3.\n1.5 Support Vector Machines\nIn this section, we will apply our analysis to another important learning model: Support\nVector Machines (SVMs). We will see that hinge loss ϕ(x) = (1 +x)+is used and the\nassociated function class is F={f:/bardblf/bardblW≤λ}whereWis a Hilbert space. Before\nanalyzing SVMs, let us ﬁrst introduce Reproducing Kernel Hilbert Spaces (RKHS).\n3\n1.5.1 Reproducing Kernel Hilbert Spaces (RKHS)\nDeﬁnition: A function K:X ×X /ma√sto→ IR is called a positive symmetric deﬁnite kernel\n(PSD kernel) if\n(1)∀x,x′∈ X,K(x,x′) =K(x′,x)\n(2)∀n∈Z+,∀x1,x2,...,xn, then\nth×nmatrix with K(xi,xj) as its element in ithrow\nandjcolumn is positive semi-deﬁnite. In other words, for any a1,a2,...,an∈IR,\n/summationdisplay\naiajK(xi,xj) 0\ni,j≥\nLet us look at a few examples of PSD kernels.\nExample 1 LetX= IR,K(x,x′) =/an}bracketle{tx,x′/an}bracketri}htIRdis a PSD kernel, since ∀a1,a2,...,an∈IR\n/summationdisplay\naiaj/an}bracketle{txi,xj/an}bracketri}htIRd=/summationdisplay\n/an}bracketle{taixi,ajxj/an}bracketri}htIRd=/an}bracketle{t/summationdisplay\naixi,/summationdisplay\najxj/an}bracketri}htIRd=/bardbl/summationdisplay\na2ixi/bardblIRd0\ni,j i,j i j i≥\nExample 2 The Gaussian kernel K(x,x′) = exp(−1 2\n2/bardbl′\n2x−x/bardblIRd) is also a PSD kernel.σ\nNote that here and in the sequel, /bardbl · /bardblWand/an}bracketle{t·,·/an}bracketri}htWdenote the norm and inner product\nof Hilbert space W.\nDeﬁnition: LetWbe a Hilbert space of functions X /ma√sto→IR. A symmetric kernel K(·,·)\nis called reproducing kernel ofWif\n(1)∀x∈ X, the function K(x,·)∈W.\n(2)∀x∈ X,f∈W,/an}bracketle{tf(·),K(x,·)/an}bracketri}htW=f(x).\nIf such aK(x,·) exists,Wis called a reproducing kernel Hilbert space (RKHS).\nClaim: IfK(·,·) is a reproducing kernel for some Hilbert space W, thenK(·,·) is a\nPSD kernel.\nProof.∀a1,a2,...,an∈IR, we have\n/summationdisplay\naiajK(xi,xj) =/summationdisplay\naiaj/an}bracketle{tK(xi,·),K(xj,·)/an}bracketri}ht(sinceK(,) is reproducing)\ni,j i,j· ·\n=/an}bracketle{t/summationdisplay\naiK(xi,), ajK(xj,)W\ni·/summationdisplay\nj· /an}bracketri}ht\n=/bardbl/summationdisplay\naiK(xi,\ni·)/bardbl2\nW≥0\n4\nIn fact, the above claim holds both directions, i.e. if a kernel K(·,·) is PSD, it is also a\nreproducing kernel.\nA natural question to ask is, given a PSD kernel K(·,·), how can we build the corresponding\nHilbert space (for which K(·,·) is a reproducing kernel)? Let us look at a few examples.\nExample 3 Letϕ1,ϕ2,...,ϕMbe a set of orthonormal functions in L2([0,1]), i.e. for any\nj,k∈ {1,2,...,M}/integraldisplay\nϕj(x)ϕk(x)dx=\nx/an}bracketle{tϕj,ϕk/an}bracketri}ht=δjk\nLetK(x,x′) =/summationtextM\nj=1ϕj(x)ϕj(x′). We claim that the Hilbert space\nM\nW={/summationdisplay\najϕj( :\n=1·)a1,a2,...,aM\nj∈IR}\nequipped with inner product /an}bracketle{t·,·/an}bracketri}htL2is a RKHS with reproducing kernel K(·,·).\nMProof. (1)K(x,·) =j=1ϕj(x)ϕj(·)∈W. (Chooseaj=ϕj(x)).\n(2) Iff(·) =/summationtextM\nj=1aj/summationtext\nϕj(·),\nM M M\n/an}bracketle{tf(·),K(x,·)/an}bracketri}htL2=/an}bracketle{t/summationdisplay\najϕj(·),/summationdisplay\nϕk(x)ϕk(·)/an}bracketri}htL2=/summationdisplay\najϕj(x) =f(x)\nj=1 k=1 j=1\n(3)K(x,x′) is a PSD kernel: ∀a1,a2,...,an∈IR,\n/summationdisplay\naiajK(x2i,xj) =/summationdisplay\naiajϕk(xi)ϕk(xj) =/summationdisplay\n(/summationdisplay\naiϕk(xi))\ni,j i,j,k ki≥0\nExample 4 IfX= IRd, andK(x,x′) =/an}bracketle{tx,x′/an}bracketri}htIRd, the corresponding Hilbert space is\nW={/an}bracketle{tw,·/an}bracketri}ht:w∈IRd}(i.e. all linear functions) equipped with the following inner product:\niff=/an}bracketle{tw,·/an}bracketri}ht,g=/an}bracketle{tv,·/an}bracketri}ht,/an}bracketle{tf,g/an}bracketri}ht/defines/an}bracketle{tw,v/an}bracketri}htIRd.\nProof. (1)∀x∈IRd,K(x,·) =/an}bracketle{tx,·/an}bracketri}htIRd∈W.\n(2)∀f=/an}bracketle{tw,·/an}bracketri}htIRd∈W,∀x∈IRd,/an}bracketle{tf,K(x,·)/an}bracketri}ht=/an}bracketle{tw,x/an}bracketri}htIRd=f(x)\n(3)K(x,x′) is a PSD kernel: ∀a1,a2,...,an∈IR,\n/summationdisplay\naiajK(xi,xj) =/summationdisplay\naiaj,\n,j i,j/an}bracketle{txixj\ni/an}bracketri}ht=/an}bracketle{t/summationdisplay\naixi,\ni/summationdisplay\najxj\nj/an}bracketri}htIRd=/bardbl/summationdisplay\naix2iIRd0\ni/bardbl ≥\n5\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "convex", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "c8805f7adeb6fe19382e8341878595c8 MIT18 657F15 L9", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "dda4a848-ba12-488c-85ec-d04808b370e8", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 1\nScribe:Philippe Rigollet Sep. 9, 2015\n1. WHAT IS MACHINE LEARNING (IN THIS COURSE) ?\nThis course focuses on statistical learning theory , which roughly means understanding the\namount of data required to achieve a certain prediction accuracy. To better understand\nwhat this means, we ﬁrst focus on stating some diﬀerences between statistics andmachine\nlearning since the two ﬁelds share common goals.\nIndeed,bothseemtotrytousedatatoimprovedecisions. Whiletheseﬁeldshaveevolved\nin the same direction and currently share a lot of aspects, they were at the beginning quite\ndiﬀerent. Statistics was around much before machine learning and statistics was already\na fully developed scientiﬁc discipline by 1920, most notably thanks to the contributions of\nR. Fisher, who popularized maximum likelihood estimation (MLE) as a systematic tool for\nstatistical inference. However, MLErequiresessentially knowingtheprobabilitydistribution\nfromwhichthedataisdraw,uptosomeunknownparameterofinterest. Often, theunknown\nparameter has a physical meaning and its estimation is key in better understanding some\nphenomena. Enabling MLE thus requires knowing a lot about the data generating process:\nthis is known as modeling . Modeling can be driven by physics or prior knowledge of the\nproblem. In any case, it requires quite a bit of domain knowledge.\nMore recently (examples go back to the 1960’s) new types of datasets (demographics,\nsocial, medical,...) have become available. However, modeling the data that they contain\nis much more hazardous since we do not understand very well the input/output process\nthus requiring a distribution free approach. A typical example is image classiﬁcation where\nthe goal is to label an image simply from a digitalization of this image. Understanding\nwhat makes an image a cat or a dog for example is a very complicated process. However,\nfor the classiﬁcation task, one does not need to understand the labelling process but rather\nto replicate it. In that sense, machine learning favors a blackbox approach (see Figure 1).\ninputX outputYblackbox\n y=f(x)+εinputX outputY\nFigure 1: The machine learning blackbox (left) where the goal is to replicate input/output\npairs from past observations, versus the statistical approach that opens the blackbox and\nmodels the relationship.\nThese diﬀerences between statistics and machine learning have receded over the last\ncouple of decades. Indeed, on the one hand, statistics is more and more concerned with\nﬁnite sample analysis, model misspeciﬁcation and computational considerations. On the\nother hand, probabilistic modeling is now inherent to machine learning. At the intersection\nof the two ﬁelds, lies statistical learning theory , a ﬁeld which is primarily concerned with\nsample complexity questions, some of which will be the focus of this class.\n1\n2. STATISTICAL LEARNING THEORY\n2.1 Binary classiﬁcation\nA large partof this class will bedevoted tooneof thesimplest problemof statistical learning\ntheory: binary classiﬁcation (aka pattern recognition [DGL96]). In this problem, weobserve\n(X1,Y1),...,(Xn,Yn) that are nindependent random copies of ( X,Y)∈ X×{0,1}. Denote\nbyPX,Ythe joint distribution of ( X,Y). The so-called featureXlives in some abstract\nspaceX(think IRd) andY∈ {0,1}is called label. For example, Xcan be a collection of\ngene expression levels measured on a patient and Yindicates if this person suﬀers from\nobesity. The goal of binary classiﬁcation is to build a rule to predict YgivenXusing\nonly the data at hand. Such a rule is a function h:X → {0,1}called a classiﬁer . Some\nclassiﬁers are better than others and we will favor ones that have low classiﬁcation error\nR(h) = IP(h(X) =Y). Let us make some important remarks.\nFist of all, since Y∈ {0,1}thenYhas a Bernoulli distribution: so much for distribution\nfree assumptions! However, we will not make assumptions on the marginal distribution of\nXor, what matters for prediction, the conditional distribution of YgivenX. We write,\nY|X∼Ber(η(X)), where η(X) = IP(Y= 1|X) = IE[Y|X] is called the regression function\nofYontoX.\nNext, note that we did not write Y=η(X). Actually we have Y=η(X) +ε, where\nε=Y−η(X) is a“noise” randomvariablethat satisﬁes IE[ ε|X] = 0. Inparticular, this noise\naccounts for the fact that Xmay not contain enough information to predict Yperfectly.\nThis is clearly the case in our genomic example above: it not whether there is even any\ninformation about obesity contained in a patient’s genotype. The noise vanishes if and only\nifη(x)∈ {0,1}for allx∈ X. Figure 2.1 illustrates the case where there is no noise and the\nthe more realistic case where there is noise. When η(x) is close to .5, there is essentially no\ninformation about YinXas theYis determined essentially by a toss up. In this case, it\nis clear that even with an inﬁnite amount of data to learn from, we cannot predict Ywell\nsince there is nothing to learn. We will see what the eﬀect of the noise also appears in the\nsample complexity./ne}ationslash\nFigure 2: The thick black curve corresponds to the noiseless case where Y=η(X)∈ {0,1}\nand the thin red curve corresponds to the more realistic case where η∈[0,1]. In the latter\ncase, even full knowledge of ηdoes not guarantee a perfect prediction of Y.\nIn the presence of noise, since we cannot predict Yaccurately, we cannot drive the\nclassiﬁcation error R(h) to zero, regardless of what classiﬁer hwe use. What is the smallest\nvalue that can beachieved? As a thought experiment, assume to begin with that we have all\n2\nxη(x)\n1\n.5\nthe information that we may ever hope to get, namely we know the regression function η(·).\nFor a given Xto classify, if η(X) = 1/2 we may just toss a coin to decide our prediction\nand discard Xsince it contains no information about Y. However, if η(X) = 1/2, we have\nan edge over random guessing: if η(X)>1/2, it means that IP( Y= 1|X)>IP(Y= 0|X)\nor, in words, that 1 is more likely to be the correct label. We will see that the classiﬁer\nh∗(X) = 1I(η(X)>1/2) (called Bayes classiﬁer ) is actually the best possible classiﬁer in\nthe sense that\nR(h∗) = infR(h),\nh(·)\nwhere the inﬁmum is taken over all classiﬁers, i.e. functions from Xto{0,1}. Note that\nunlessη(x)∈ {0,1}for allx∈ X(noiseless case), we have R(h∗) = 0. However, we can\nalways look at the excess risk E(h) of a classiﬁer hdeﬁned by\nE(h) =R(h)−R(h∗)≥0.\nIn particular, we can hope to drive the excess risk to zero with enough observations by\nmimicking h∗accurately.\n2.2 Empirical risk\nThe Bayes classiﬁer h∗, while optimal, presents a major drawback: we cannot compute it\nbecause we do not know the regression function η. Instead, we have access to the data\n(X1,Y1),...,(Xn,Yn), which contains some (but not all) information about ηand thus h∗.\nIn order to mimic the properties of h∗recall that it minimizes R(h) over all h. But the\nfunction R(·) is unknown since it depends on the unknown distribution PX,Yof (X,Y). We\nˆ estimate it by the empirical classiﬁcation error, or simply empirical risk Rn(·) deﬁned for\nany classiﬁer hby\nn1ˆRn(h) =/summationdisplay\n1I(h(Xi) =Yi).ni=1\nˆ ˆ Since IE[1I( h(Xi) =Yi)] = IP(h(Xi) =Yi) =R(h), we have IE[ Rn(h)] =R(h) soRn(h) is\nanunbiased estimator of R(h). Moreover, for any h, by the law of large numbers, we have\nˆ ˆ Rn(h)→R(h) asn→ ∞, almost surely. This indicates that if nis large enough, Rn(h)\nshould be close to R(h).\nAs a result, in order to mimic the performance of h∗, let us use the empirical risk\nˆ ˆ minimizer (ERM) hdeﬁned to minimize Rn(h) over all classiﬁers h. This is an easy enough\nˆ ˆ task: deﬁne hsuchh(Xi) =Yifor alli= 1,...,nandh(x) = 0 ifx∈/{X1,...,X n}. We\nˆˆ haveRn(h) = 0, which is clearly minimal. The problem with this classiﬁer is obvious: it\ndoes not generalize outside the data. Rather, it predicts the label 0 for any xthat is not in\nˆˆ the data. We could have predicted 1 or any combination of 0 and 1 and still get Rn(h) = 0.\nˆ In particular it is unlikely that IE[ R(h)] will be small.\n3/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash /ne}ationslash\nImportant Remark :Recall that R(h) = IP(h(X)/ne}ationslash=Y).\nˆ ˆ ˆ Ifh(·)\n=h({(X1,Y1),...,(Xn,Yn)};·) is constructed from the data, R(h) denotes\ntheconditional probability\nˆ ˆR(h) = IP(h(X)/ne}ationslash=Y|(X1,Y1),.. .,(Xn,Yn)).\nˆ ˆ ratherthanIP( h(X)/ne}ationslash=Y). As aresult R(h)is arandomvariablesinceit dependsonthe\nrandomness of the data ( X1,Y1),...,(Xn,Yn). One way to view this is to observe that\nˆ we compute the deterministic function R(·) and then plug in the random classiﬁer h.\nThis problem is inherent to any method if we are not willing to make any assumption\non the distribution of ( X,Y) (again, so much for distribution freeness!). This can actually\nbe formalized in theorems, known as no-free-lunch theorems.\nTheorem: ˆ For any integer n≥1, any classiﬁer hbuilt from ( X1,Y1),...,(Xn,Yn) and\nanyε >0, there exists a distribution PX,Yfor (X,Y) such that R(h∗) = 0 and\nˆIER(hn)≥1/2−ε.\nTo be fair, note that here the distribution of the pair ( X,Y) is allowed to depend on\nnwhich is cheating a bit but there are weaker versions of the no-free-lunch theorem that\nessentially imply that it is impossible to learn without further assumptions. One such\ntheorem is the following.\nTheorem: ˆ For any classiﬁer hbuilt from ( X1,Y1),...,(Xn,Yn) and any sequence\n{an}n>0 that converges to 0, there exists a distribution PX,Yfor (X,Y) such that\nR(h∗) = 0 and\nˆIER(hn)≥an,for alln≥1\nIn the above theorem, the distribution of ( X,Y) is allowed to depend on the whole sequence\n{an}n>0 but not on a speciﬁc n. The above result implies that the convergence to zero of\nthe classiﬁcation error may be arbitrarily slow.\n2.3 Generative vs discriminative approaches\nBoth theorems above imply that we need to restrict the distribution PX,Yof (X,Y). But\nisn’t that exactly what statistical modeling is? The is answer is not so clear depending on\nhow we perform this restriction. There are essentially two schools: generative which is the\nstatistical modeling approach and discriminative which is the machine learning approach.\nGenerative: This approach consists in restricting the set of candidate distributions PX,Y.\nThis is what is done in discriminant analysis1where it is assumed that the condition dis-\n1Amusingly, the generative approach is called discriminant analysis but don’t let the terminology fool\nyou.\n4\ntributions of XgivenY(there are only two of them: one for Y= 0 and one for Y= 1) are\nGaussians on X= IRd(see for example [HTF09] for an overview of this approach).\nDiscriminative: This is the machine learning approach. Rather than making assumptions\ndirectly on the distribution, one makes assumptions on what classiﬁers are likely to perform\ncorrectly. In turn, this allows to eliminate classiﬁers such as the one described above and\nthat does not generalize well.\nWhile it is important to understand both, we will focus on the discriminative approach\nin this class. Speciﬁcally we are going to assume that we are given a class Hof classiﬁers\nsuch that R(h) is small for some h∈ H.\n2.4 Estimation vs. approximation\nAssumethat we aregiven a class Hin which weexpect to ﬁnda classiﬁer that performswell.\nThisclassmaybeconstructedfromdomainknowledgeorsimplycomputational convenience.\nˆ We will see some examples in the class. For any candidate classiﬁer hnbuilt from the data,\nwe can decompose its excess risk as follows:\nˆ ˆ ˆ E(hn) =R(hn)−R(h∗) =R(hn)−infR(h)+ infR(h)−R(h∗).\nh∈H h∈H/bracehtipupleft\nestimat/bracehtipdownright\nio/bracehtipdownleft\nn error/bracehtipupright /bracehtipupleft\napproxim/bracehtipdownright\na/bracehtipdownleft\ntion error/bracehtipupright\nOn the one hand, estimation error accounts for the fact that we only have a ﬁnite\namount of observations and thus a partial knowledge of the distribution PX,Y. Hopefully\nwe can drive this error to zero as n→ ∞. But we already know from the no-free-lunch\ntheorem that this will not happen if His the set of all classiﬁers. Therefore, we need to\ntakeHsmall enough. On the other hand, if His too small, it is unlikely that we will\nﬁnd classiﬁer with performance close to that of h∗. A tradeoﬀ between estimation and\napproximation can be made by letting H=Hngrow (but not too fast) with n.\nFor now, assume that His ﬁxed. The goal of statistical learning theory is to understand\nhow the estimation error drops to zero as a function not only of nbut also of H. For the\nﬁrst argument, we will use concentration inequalities such as Hoeﬀding’s and Bernstein’s\ninequalities that allow us to control how close the empirical risk is to the classiﬁcation error\nby bounding the random variable\n/vextendsinglen1/vextendsingle/summationdisplay\n1I(h(X (h /vextendsingle i) =Yi)−IP (X) =Y)/vextendsingle\nn/vextendsingle\ni=1/vextendsingle\nwith high probability. More generally we will be interested in results that allow to quantify\nhow close the average of independent and identically distributed (i.i.d) random variables is\nto their common expected value.\nˆˆˆ Indeed, since by deﬁnition, we have Rn(h)≤Rn(h) for allh∈ H, the estimation error\n¯ can be controlled as follows. Deﬁne h∈ Hto be any classiﬁer that minimizes R(·) overH\n(assuming that such a classiﬁer exist).\nˆ ˆ ¯ R(hn)−infR(h) =R(hn)−R(h)\nh∈H\nˆˆˆ¯ ˆ ˆ ¯ ¯ =Rn(hn)−Rn(h)+R(hn)−Rˆn(h)+Rˆn n(h)−R(h)/bracehtipupleft\n≤/bracehtipdownright/bracehtipdownleft\n0/bracehtipupright\n≤/vextendsingle/vextendsingleˆˆ ˆ ˆ¯ ¯ Rn(hn)−R(hn)/vextendsingle/vextendsingle+/vextendsingle/vextendsingleRn(h)−R(h)/vextendsingle/vextendsingle.\n5/ne}ationslash /ne}ationslash\n¯ ˆ¯ ¯ Sincehis deterministic, we can use a concentration inequality to control/vextendsingle/vextendsingleRn(h)−R(h)/vextendsingle/vextendsingle.\nHowever,\nn1ˆˆ ˆ Rn(hn) =/summationdisplay\n1I(hn(Xi) =Yi)ni=1\nisnot ˆ the average of independent random variables since hndepends in a complicated\nmanner on all of the pairs ( Xi,Yi),i= 1,...,n. To overcome this limitation, we often use\nˆ a blunt, but surprisingly accurate tool: we “sup out” hn,\n/vextendsingle/vextendsingleˆˆ ˆ ˆˆ ˆ Rn(hn)−R(hn)/vextendsingle/vextendsingle≤sup/vextendsingle\nh∈/vextendsingleRn(hn)−R(hn)/vextendsingle\nH/vextendsingle.\nControlling this supremum falls in the scope of suprema of empirical processes that we will\nstudy in quite a bit of detail. Clearly the supremum is smaller as His smaller but Hshould\nbe kept large enough to have good approximation properties. This is the tradeoﬀ between\napproximation and estimation. It is also know in statistics as the bias-variance tradeoﬀ.\nReferences\n[DGL96] L. Devroye, L. Gy¨ orﬁ, and G. Lugosi, A probabilistic theory of pattern recognition ,\nApplications of Mathematics (New York), vol. 31, Springer-Verlag, New York,\n1996. MR MR1383093 (97d:68196)\n[HTF09] Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The elements of statis-\ntical learning , second ed., Springer Series in Statistics, Springer, New York, 2009,\nData mining, inference, and prediction. MR 2722294 (2012d:62081)\n6/ne}ationslash\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "erm", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "7d229ed907a6d1410a3736c98a9d78d8 MIT18 657F15 L1", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "9ef317cf-61ad-40ca-bd50-6abeb45624fb", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 23\nScribe: Jonathan Weed Dec. 2, 2015\n1. POTENTIAL BASED APPROACHABILITY\nLast lecture, we saw Blackwell's celebrated Approachability Theorem, which establishes a\nprocedure by which a player can ensure that the average (vector) payo\u000b in a repeated game\napproaches a convex set. The central idea was to construct a hyperplane separating the\n\u0016 convex set from the point `t1, the average loss so far. By projecting perpendicular to\u0000\nthis hyperplane, we obtained a scalar-valued problem to which von Neumann's minimax\ntheorem could be applied. The set Sis approachable as long as we can always \fnd a \\silver\nbullet,\" a choice of action atfor which the loss vector `tlies on the side of the hyperplane\ncontainingS. (See Figure 1.)\nFigure 1: Blackwell approachability\nConcretely, Blackwell's Theorem also implied the existence of a regret-minimizing algo-\nrithm for expert advice. Indeed, if we de\fne the vector loss `tby (`t)i=`(at;zt)\u0000`(ei;zt),\nthen the average regret at time tis equivalent to the sup-norm distance between the average\n\u0016loss`tand the negative orthant. Approaching the negative orthant therefore corresponds\nto achieving sublinear regret.\nHowever, this reduction yielded suboptimal rates. To bound average regret, w\nthe sup-norm distance by the Euclidean distance, which led to an extra factor ofpe replaced\nkappear-\ning in our bound. In the sequel, we develop a more sophisticated version of approachability\nthat allows us to adapt to the geometry of our problem. (Much of what follows resem-\nbles out development of the mirror descent algorithm, though the two approaches di\u000ber in\ncrucial details.)\n1.1 Potential functions\nWe recall the setup of mirror descent, \frst described in Lecture 13. Mirror descent achieved\naccelerated rates by employing a potential function which was strongly convex with respect\n1\nto the given norm. In this case, we seek what is in some sense the opposite: a function\nwhose gradient does not change too quickly. In particular, we make the following de\fnition.\nDe\fnition: A function \b : I Rd!I R is a potential forS2I R if it satis\fes the following\nproperties:\n\u000f\b is convex.\n\u000f\b(x)\u00140 forx2S.\n\u000f\b(y) = 0 fory2@S.\n\u000f\b(y)\u0000\b(x)\u0000hr\b(x);y\u0000xi\u0014hx2k \u0000yk2, where by abuse of notation we use\nr\b(x) to denote a subgradient of \b at x.\nGiven such a function, we recall two associated notions from the mirror descent algo-\nrithm. The Bregman divergence associated to \b is given by\nD\b(y;x) = \b(y)\u0000\b(x)\u0000hr\b(x);y\u0000xi:\nLikewise, the associated Bregman projection is\n\u0019(x) = argmin D\b(y;x):\ny2S\nWe aim to use the function \b as a stand-in for the Euclidean distance that we employed\nin our proof of Blackwell's theorem. To that end, the following lemma establishes several\nproperties that will allow us to generalize the notion of a separating hyperplane.\nLemma: For any convex, closed set Sandz2S,x2SC, the following properties\nhold.\n\u000f hz\u0000\u0019(x);r\b(x)i\u0014 0;\n\u000f hx\u0000\u0019(x);r\b(x)i\u0015 \b(x):\nIn particular, if \b is positive on SC, thenH:=fyjhy\u0000\b(x);r\b(x)i = 0g is a\nseparating hyperplane.\nOur proof requires the following proposition, whose proof appears in our analysis of the\nmirror descent algorithm and is omitted here.\nProposition: For allz2S, it holds\nhr\b(\u0019 (x))\u0000r\b(x);\u0019 (x)\u0000zi\u00140:\nProof of Lemma. Denote by\u0019the projection \u0019(x). The \frst claim follows upon expanding\nthe expression on the left-hand side as follows\nhz\u0000\u0019;r\b(x)i =hz\u0000\u0019;r\b(x)\u0000r\b(\u0019 )i+hz\u0000\u0019;r\b(\u0019 )i:\n2\nThe above Proposition implies that the \frst term is nonpositive. Since the function \b is\nconvex, we obtain\n0\u0015\b(z)\u0015\b(\u0019) +hz\u0000\u0019;r\b(\u0019 )i:\nSince\u0019lies on the boundary of S, by assumption \b( \u0019) = 0 and the claim follows.\nFor the second claim, we again use convexity:\n\b(\u0019)\u0015\b(x) +h\u0019\u0000x;r\b(x)i:\nSince \b(\u0019 ) = 0, the claim follows.\n1.2 Potential based approachability\nWith the de\fnitions in place, the algorithm for approachability is essentially the same as it\nbefore we introduced the potential function. As before, we will use a projection de\fned by\nthe hyperplane H=fyjhy\u0000\u0016 \u0016\u0019(`t\u00001);r\b(`t= 0 and von Neumann's minmax theorem\u00001i g\nto \fnd a \\silver bullet\" a\u0003\ntsuch that`t=`(a\u0003\nt;zt) satis\fes\nh`t\u0000 \u0016\u0019t;r\b(`t\u00001)i\u0014 0:\nAll that remains to do is to analyze this procedure's performance. We have the following\ntheorem.\nTheorem: Ifk`(a;z )k\u0014Rholds for all z2A;z2Z and all assumptions above are\nsatis\fed, then\n4R2hlogn\u0016\b(`n)\u0014 :n\nProof. The de\fnition of the potential \b required that \b be upper bounded by a quadratic\nfunction. The proof below is a simple application of that bound.\nAs before, we note the identity\n`\u0016 \u0016t t\u00001`t=`t\u00001+ :t\nThis expression and the de\fnition of \b imply.\n1 h\u0016\u0014 \u0016 )h \u0016 \b(`t\b(`t\u00001) +`t\u0000\u0016 \u0016`2t1; )\u0000r\b(`t1i+k`\u0000 t`t 22\u0000t\u00001tk:\n\u0016 The last term is the easiest to control. By assumption, `tand`t1are contained in a ball\u0000\nof radiusR, sok`t\u0000\u0016`t\u00001k2\u00144R2.\nTo bound the second term, write\n1 1 1h \u0016 `t\u0000\u0016 \u0016 \u0016 \u0016 `t1;r\b(`t1)i=h`t\u0000\u0019t;r\b(`t1) +\u0019 ` ; \b(` ):t\u0000 \u0000 \u0000ithtt\u0000t\u00001rt\u00001i\nThe \frst term is nonpositive by assumption, since this is how the algorithm constructs\nthe silver bullet. By the above Lemma, the inner product in the second term is at most\n\u0000\u0016\b(`t\u00001).\nWe obtain \u0012t\u00001 2\u0016\b(`t)\u0014\u0013hR2\u0016\b(`t)\u00001+:t t2\n3\u0000\u0016`\n\u0016 De\fningut=t\b(`t) and rearranging, we obtain the recurrence\n2hR2\nut\u0014ut\u00001+;t\nSon\nun=Xn\nut\u0000ut\u00001\u00142hR2X1\ntt=1 t=1\nApplying the de\fnition of unproves the claim.\n1.3 Application to regret minimization\nWe now show that potential based approachability providespan improved bound on regret\nminimization. Our ultimate goal is to replace the bound nk(which we proved last lecture)\nbypnlogk(which we know to be the optimal bound for prediction with expert advice).\nWe will be able to achieve this goal up to logarithmic terms in n. (A more careful analysis\nof the potential de\fned below does actually yields an optimal rate.)\nRecall thatRn \u0016 =d(`n;On K\u0000), whereRnis the cumulative regret after nrounds and O\u00001 K\nis the negative orthant. It is not hard to see that d=kx+k, wherex+is the positive 11\npart of the vector x.\nWe de\fne the following potential function:\nK1 1\b(x) = log\u00110\nKX\ne\u0011(xj)+\nj=11\n:\nThe function \b is a kind of \\soft max\" of the@\npositive entriesA\nofx. (Note that this de\fnition\ndoes not agree with the use of the term soft max in the literature|the di\u000berence is the\npresence of the factor1.) The terminology soft max is justi\fed by noting thatK\n1 1kx+k= max(x log+logK logK\nj)+\u0014max e\u0011(xj)+ :\u0011\u0014\b(x) + 1j j\u0011 K \u0011\nThe potential function therefore serves as an upper bound on the sup distance, up to an\nadditive logarithmic factor.\nThe function \b de\fned in this way is clearly convex and zero on the negative orthant.\nTo verify that it is a potential, it remains to show that \b can be bounded by a quadratic.\nAway from the negative orthant, \b is twice di\u000berentiable and we can compute the\nHessian explicitly:\nr2\b(x) =\u0011diag(r\b(x))\u0000\u0011r\br\b>:\nFor any vector usuch thatkuk2= 1, we therefore have\nK K\nu>r2\b(x)u =\u0011X\nu2\nj(r\b(x ))j\u0000\u0011(u>\njr\b(x))2\u0014\u0011\n=1X\nu2\nj(\nj=1r\b(x))j\u0014\u0011;\nsincekuk2= 1 andkr\b(x)\n2k1\u00141.\nWe conclude that r\b(x)\u0016\u0011I, which for nonnegative xandyimplies the bound\n\u0011\b(y)\u0000\b(x)\u0000hr\b(x);y\u0000xi\u00142ky\u0000xk2:\n4\u00144hR2logn:\nIn fact, this bound holds everywhere. Therefore \b is a valid potential function for the\nnegative orthant, with h=\u0011.\nThe above theorem then implies that we can ensure\nRn logK 4R2\u0011lognlogK\u0014 \u0016\b(`n) +\u0014 +:n \u0011 n \u0011\nTo optimize this bound, we pick \u0011=1q\nnlogKand obtain the bound2R logn\nRn\u00144Rp\nnlognlogK:\nAs alluded to earlier, a more careful analysis can remove the log nterm. Indeed, for this\nparticular choice of \b, we can modify the above Lemma to obtain the sharper bound\nhx\u0000\u0019(x);r\b(x)i\u0015 2\b(x):\nWhen we substitute this expression into the above proof, we obtain the recurrence\nrelationt\u0016\b(`t)\u00002 c\u0014 \u0016\b(`t\u00001) +:t t2\nThis small change is enough to prevent the appearance of log nin the \fnal bound.\n5\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "convex", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "007788a35f2e22aecf29dd1db77a6d42 MIT18 657F15 L23", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "e4bbf804-b9e7-46ab-95b5-e864e2924e94", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 3\nScribe:James Hirst Sep. 16, 2015\n1.5 Learning with a ﬁnite dictionary\nRecall from the end of last lecture our setup: We are working with a ﬁnite dictionary\nH={h1,...,h M}of estimators, andwewouldliketounderstandthescaling ofthis problem\nwith respect to Mand the sample size n. GivenH, one idea is to simply try to minimize\nˆ the empirical risk based on the samples, and so we deﬁnethe empirical risk minimizer, herm,\nby\nˆ ˆ herm∈argminRn(h).\nh∈H\nˆ ˆ In what follows, we will simply write hinstead of hermwhen possible. Also recall the\n¯ deﬁnition of the oracle, h, which (somehow) minimizes the true risk and is deﬁned by\n¯h∈argminR(h).\nh∈H\nˆ ¯ The following theorem shows that, although hcannot hope to do better than hin\ngeneral, the diﬀerence should not be too large as long as the sample size is not too small\ncompared to M.\nˆ Theorem: The estimator hsatisﬁes\nˆ ¯R(h)≤R(h)+/radicalbigg\n2log(2M/δ)\nn\nwith probability at least 1 −δ. In expectation, it holds that\n/radicalbigg\n2log(2M)ˆ ¯IE[R(h)]≤R(h)+ .n\nProof. ˆ ˆˆˆ¯ From the deﬁnition of h, we have Rn(h)≤Rn(h), which gives\nˆ ¯ ˆ¯ ¯ ˆ ˆˆ R(h)≤R(h)+[Rn(h)−R(h)]+[R(h)−Rn(h)].\nThe only term here that we need to control is the second one, but since we don’t have\n¯ any real information about h, we will bound it by a maximum over Hand then apply\nHoeﬀding:\nlog(2M/δ)ˆ¯ ¯ ˆ ˆˆ ˆ [Rn(h)−R(h)]+[R(h)−Rn(h)]≤2max|Rn(hj)−R(hj)| ≤2\nj/radicalbigg\n2n\nwith probability at least 1 −δ, which completes the ﬁrst part of the proof.\n1\nTo obtain the bound in expectation, we start with a standard trick from probability\nwhich bounds a max by its sum in a slightly more clever way. Here, let {Zj}jbe centered\nrandom variables, then\n/bracketleftbigg /bracketrightbigg1/parenleftbigg /bracketleftbigg1IE max|Zj|= logexp sIE max|Zj|/bracketrightbigg/parenrightbigg\n≤logIE/bracketleftbigg\nexp/parenleftbigg\nsmax|Zj|\nj s j s j/parenrightbigg/bracketrightbigg\n,\nwhere the last inequality comes from applying Jensen’s inequality to the convex function\nexp(·). Now we bound the max by a sum to get\n2M1 )≤log/summationdisplay 1 s2log(2M sIE[exp(sZj)]≤log/parenleftbigg\n2Mexp/parenleftbigg /parenrightbigg/parenrightbigg\n= + ,s s 8n s 8nj=1\nˆ where we used Zj=Rn(hj)−R(hj) in our case and then applied Hoeﬀding’s Lemma. Bal-\nancing terms by minimizing over s, this gives s= 2/radicalbig\n2nlog(2M) and plugging in produces\n/bracketleftbigglog(2M)ˆ IE max|Rn(hj)−R(hj)| ≤\nj/bracketrightbigg/radicalbigg\n,2n\nwhich ﬁnishes the proof.\n2. CONCENTRATION INEQUALITIES\nConcentration inequalities are results that allow us to bound the deviations of a function of\nrandomvariablesfromitsaverage. Theﬁrstofthesewewillconsiderisadirect improvement\nto Hoeﬀding’s Inequality that allows some dependence between the random variables.\n2.1 Azuma-Hoeﬀding Inequality\nGiven a ﬁltration {Fi}iof our underlying space X, recall that {∆i}iare called martingale\ndiﬀerences if, for every i, it holds that ∆ i∈ Fiand IE[∆ i|Fi] = 0. The following theorem\ngives a very useful concentration bound for averages of bounded martingale diﬀerences.\nTheorem (Azuma-Hoeﬀding): Suppose that {∆i}iare margingale diﬀerences with\nrespect to the ﬁltration {Fi}i, and let Ai,Bi∈ Fi−1satisfyAi≤∆i≤Bialmost surely\nfor every i. Then\nIP/bracketleftBigg\n1/summationdisplay 2n∆i> t/bracketrightBigg\n2t2\n≤expni/parenleftbigg\n−/summationtextn\ni=1/bardblBi−Ai/bardbl2∞/parenrightbigg\n.\nIn comparison to Hoeﬀding’s inequality, Azuma-Hoeﬀding aﬀords not only the use of\nnon-uniform boundedness, but additionally requires no independence of the random vari-\nables.\nProof.We start with a typical Chernoﬀ bound.\nIP/bracketleftBigg /bracketrightBigg/summationdisplay\n∆i> t≤IE/bracketleftBig\nes/summationtext∆i/bracketrightBig\ne−st= IE\ni/bracketleftBig\nIE/bracketleftBig\nes/summationtext∆i|Fn−1/bracketrightBig/bracketrightBig\ne−st\n2\nn−1 n−1 2 2= IE/bracketleftBig\nes/summationtext∆iIE[es∆n|Fn1]e−st− ≤IE[es/summationtext∆i·es(Bn−An)/8]e−st,\nwhere we have used the fact that the ∆/bracketrightBig\ni,i < n, are allFnmeasureable, and then applied\nHoeﬀding’s lemma on the inner expectation. Iteratively isolating each ∆ ilike this and\napplying Hoeﬀding’s lemma, we get\nIP/bracketleftBiggn/summationdisplay s2\n∆> t/bracketrightBigg\n≤exp/parenleftBigg/summationdisplay\n/bardblB−A/bardbl2/parenrightBigg\ne−sti i i8∞.\ni i=1\nOptimizing over sas usual then gives the result.\n2.2 Bounded Diﬀerences Inequality\nAlthough Azuma-Hoeﬀding is a powerful result, its full generality is often wasted and can\nbe cumbersome to apply to a given problem. Fortunately, there is a natural choice of the\n{Fi}iand{∆i}i, giving a similarly strong result which can be much easier to apply. Before\nwe get to this, we need one deﬁnition.\nDeﬁnition (Bounded Diﬀerences Condition): Letg:X →IR and constants cibe\ngiven. Then gis said to satisfy the bounded diﬀerences condition (with constants ci) if\nsup|g(x ,...,x )−g(x ,...,x′1 n 1 i,...,x n)| ≤ci\nx′1,...,xn,xi\nfor every i.\nIntuitively, gsatisﬁes the bounded diﬀerences condition if changing only one coordinate\nofgat a time cannot make the value of gdeviate too far. It should not be too surprising\nthat these types of functions thus concentrate somewhat strongly around their average, and\nthis intuition is made precise by the following theorem.\nTheorem (Bounded Diﬀerences Inequality): Ifg:X →IR satisﬁes the bounded\ndiﬀerences condition, then\n2t2\nIP[|g(X1,...,X n)−IE[g(X1,...,X n)|> t]≤2exp/parenleftbigg\n−/summationtext\nic2\ni/parenrightbigg\n.\nProof.Let{Fi}ibe given by Fi=σ(X1,...,X i), and deﬁne the martingale diﬀerences\n{∆i}iby\n∆i= IE[g(X1,...,X n)|Fi]−IE[g(X1,...,X n)|Fi−1].\nThen\nIP/bracketleftBigg\n|/summationdisplay\n∆i|> t/bracketrightBigg\n= IP/vextendsingle\ng(X1,...,X n)−IE[g(X1,...,X n)\ni/vextendsingle\n> t ,\nexactly the quantity we want to bou/bracketleftbig/vextendsingle\nnd. Now, note that/vextendsingle/bracketrightbig\n∆i≤IE/bracketleftbigg\nsupg(X1,...,x i,...,X n)|Fi−IE[g(X1,...,X n)|Fi−1]\nxi/bracketrightbigg\n3\n= IE/bracketleftbigg\nsupg(X1,...,x i,...,X n)−g(X1,...,X n)|Fi−1\nxi/bracketrightbigg\n=:Bi.\nSimilarly,\n∆i≥IE/bracketleftbigg\ninfg(X1,...,x i,...,X n)−g(X1,...,X n)|Fi−1=:Ai.\nxi/bracketrightbigg\nAt this point, our assumption on gimplies that /bardblBi−Ai/bardbl∞≤cifor every i, and since\nAi≤∆i≤BiwithAi,Bi∈ Fi−1, an application of Azuma-Hoeﬀding gives the result.\n2.3 Bernstein’s Inequality\nHoeﬀding’s inequality is certainly a powerful concentration inequality for how little it as-\nsumes about the random variables. However, one of the major limitations of Hoeﬀding is\njust this: Since it only assumes boundedness of the random variables, it is completely obliv-\nious to their actual variances. When the random variables in question have some known\nvariance, an ideal concentration inequality should capture the idea that variance controls\nconcentration to some degree. Bernstein’s inequality does exactly this.\nTheorem (Bernstein’s Inequality): LetX1,...,X nbe independent, centered ran-\ndom variables with |X| ≤cfor every i, and write σ2=n−1i iVar(Xi) for the average\nvariance. Then/summationtext\nIP/bracketleftBigg\n1/summationdisplay nt2\nXi> t/bracketrightBigg\n≤exp/parenleftBigg\n−n 2σ2+2tci 3/parenrightBigg\n.\nHere, one should think of tas being ﬁxed and relatively small compared to n, so that\nstrength of the inequality indeed depends mostly on nand 1/σ2.\nProof.The idea of the proof is to do a Chernoﬀ bound as usual, but to ﬁrst use our\nassumptions on the variance to obtain a slightly better bound on the moment generating\nfunctions. To this end, we expand\n∞(sk ∞X) skck−2iIE[esXi] = 1+IE[ sXi]+IE/bracketleftBigg /bracketrightBigg/summationdisplay\n≤1+Var(Xi)/summationdisplay\n,k! k!k=2 k=2\nwhere we have used IE[ Xk\ni]≤IE[X2\ni|Xi|k−2]≤Var(Xk−2i)c. Rewriting the sum as an\nexponential, we get\nesc\nsXi2 −sc−1IE[e]≤sVar(Xi)g(s), g(s) := .c2s2\nThe Chernoﬀ bound now gives\nIP/bracketleftBigg\n1/summationdisplay\nXi> t/bracketrightBigg\n≤exp/parenleftBigg\ninf[s2(/summationdisplay\nVar(Xi))g(s)−nst]/parenrightBigg\n= exp/parenleftbigg\nn·inf[s2σ2g(s)−st],n s>0 s>0i i/parenrightbigg\nand optimizing this over s(a fun calculus exercise) gives exactly the desired result.\n4\n3. NOISE CONDITIONS AND FAST RATES\nˆ To measure the eﬀectiveness of the estimator h, we would like to obtain an upper bound\nˆ ˆ on the excess risk E(h) =R(h)−R(h∗). It should be clear, however, that this must depend\nsigniﬁcantly on the amount of noise that we allow. In particular, if η(X) is identically equal\nˆ to 1/2, then we should not expect to be able to say anything meaningful about E(h) in\ngeneral. Understanding this trade-oﬀ between noise and rates will be the main subject of\nthis chapter.\n3.1 The Noiseless Case\nA natural (albeit somewhat na¨ ıve) case to examine is the completely noiseless case. Here,\nwe will have η(X)∈ {0,1}everywhere, Var( Y|X) = 0, and\nE(h) =R(h)−R(h∗) = IE[|2η(X)−1|1I(h(X) =h∗(X))] = IP[h(X) =h∗(X)].\nLet us now denote\n¯ ˆ Zi= 1I(h(Xi) =Yi)−1I(h(Xi) =Yi),\n¯ and write Zi=Zi−IE[Zi]. Then notice that we have\nˆ ¯ |Zi|= 1I(h(Xi) =h(Xi)),\nand\nVar(Zi)≤IE[Z2ˆ ¯i] = IP[h(Xi) =h(Xi)].\nˆ For any classiﬁer hj∈ H, we can similarly deﬁne Zi(hj) (by replacing hwithhjthrough-\nout). Then, to set up an application of Bernstein’s inequality, we can compute\nn1/summationdisplay¯ Var(Zi(hj))≤IP[hj(Xi) =h(Xi)] =:σ2\nnj.\ni=1\nAt this point, we will make a (fairly strong) assumption about our dictionary H, which\n¯ is thath∗∈ H, which further implies that h=h∗. Since the random variables Zicompare\n¯ ˆ toh, this will allow us to use them to bound E(h), which rather compares to h∗. Now,\n¯ applying Bernstein (with c= 2) to the {Zi(hj)}ifor every jgives\n/bracketleftBiggn1/bracketrightBigg/summationdisplay nt2δ¯ IP Zi(hj)> t≤exp/parenleftBigg\n− =2σ2\ni=1 j+4t3/parenrightBigg\n:,n M\nand a simple computation here shows that it is enough to take\n/radicalBigg\n2σ2\njlog(M/δ)4t≥max ,log(M/δ)n 3n\n=:t0(j)\n¯ for this to hold. From here, we may use the assumption h=h∗to conclude that\nˆ ˆ IP/bracketleftBig\nE(h)> t0(ˆj)/bracketrightBig\n≤δ, hˆ=h.j\n5/ne}ationslash /ne}ationslash\n/ne}ationslash /ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash\nˆ However, we also know that σ2\nˆ≤ E(h), which implies thatj\n/radicalBigg\nˆ2E(h)log(M/δ)4ˆE(h)≤max ,log(M/δ)n 3n\n\nˆ with probability 1 −δ, and solving for E(h) gives the improved rate\nlog(M/δ)ˆE(h)≤2 .n\n6\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "convex", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "e1c43cdb5ad222e94adbdf639b12b395 MIT18 657F15 L3", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "4c93c7a8-927c-49ea-bc23-efc8fb246660", "text": "18.657. Fall 2105\nRigollet September 27, 2015\nProblem set #1 (due Wed., October 7)\nProblem 1. Discriminant analysis\nLet (X,Y)∈IRd×{0,1}be a random pair such that IP( Y=k) =πk>0 (π0+π1= 1)\nand the conditional distribution of XgivenYisX|Y∼ N(µ ,Σ ), where µ/ne}ationslash=µ∈IRdY Y 0 1\nand Σ 0,Σ1∈IRd×dare mean vectors and covariance matrices respectively.\n1. What is the (unconditional) density of X?\n2. AssumethatΣ 0= Σ1= Σisapositivedeﬁnitematrix. ComputetheBayes classiﬁer\nh∗as a function of µ0,µ1,π0,π1and Σ. What is the nature of the sets {h∗= 0}and\n{h∗= 1}?\n3. Assume now that Σ 0/ne}ationslash= Σ1are two positive deﬁnite matrices. What is the nature of\nthe sets{h∗= 0}and{h∗= 1}?\nProblem 2. VC dimensions\n1. LetCbe the class of convex polygons in IR2withdvertices. Show that VC(C) =\n2d+1.\n2. LetCbe the class of convex compact sets in IR2. Show that VC(C) =∞.\n3. LetCbe ﬁnite. Show that VC(C)≤log2(cardC).\n4. Give an example of a class Csuch that card C=∞andVC(C) = 1.\nProblem 3. Glivenko-Cantelli Theorem\nLetX1,...,X nbeni.i.d copies of Xthat has cumulative distribution function (cdf)\nF(t) = IP(X≤t). Theempirical cdf ofXis deﬁned by\n1ˆFn(t) =n/summationdisplay\n1I(Xi.\ni=1≤t)n\nFˆnt ˆ 1. Compute the mean and the variance of ( ) and conclude that Fn(t)→F(t) as\nn→ ∞almost surely (hint: use Borel-Cantelli).\npage 1 of 2\n2. Show that for n≥2\nˆsupFn(t)F(t)C\nt∈I R/vextendsingle /vextendsingle/vextendsingle−/vextendsingle≤/radicalbigg\nlog(n/δ)\nn\nwith probability 1 −δ.\nProblem 4. Concentration\n1. LetX1,...,X nbeni.i.d copies of X∈[0,1]. Each Xirepresents the size of a\npackages to be shipped. The shipping containers are bins of size 1 (so that each bin\ncan hold a set of packages whose sizes sum to at most 1). Let Bnbe the minimal\nnumber of bins needed to store the npackages. Show that\n2\nIP(|Bn−2IE[Bn]| ≥t)≤2et−n.\n2. LetX1,...,X nbeni.i.d copies of X∈IRd,IE[X] = 0 and assume that /bardblXi\nX¯/bardbl ≤1\nalmost surely for all i. Let denote the average of the Xis. Prove the following\ninequalities (the constant Cmay change from one inequality to the other)\n(a)IP/bracketleftBig/vextenddouble¯/vextenddoubleX/vextenddouble/vextenddouble−IE/vextenddouble C/vextenddoubleX¯/vextenddouble/vextenddouble≥t/bracketrightBig\n≤e−Cnt2,(b)IE/vextenddouble\nX¯/vextenddouble/vextenddouble /vextenddouble≤√,(c)IPn/bracketleftBig/vextenddouble¯/vextenddouble/bracketrightBig\n−Cnt2/vextenddoubleX/vextenddouble≥t≤2e\n3. LetX1,...,X nbeniid random variables, i.e. such that Xiand−Xihave the same\n¯ distribution. Let Xdenote the average of the Xis andV=n−1/summationtextn2\ni=1Xi. Show\nthat\nIP/bracketleftBigX¯\n√2\n> t\nV/bracketrightBig\n≤ent−2.\n[Hint: introduce Rademacher random variables].\npage 2 of 2\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "convex", "section_heading": "18.657. Fall 2105", "source_title": "83e771a9b96fa08a6bac7bc783411491 MIT18 657F15 PS1", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "e1a12190-74d8-41b7-809f-1d2988e0bc03", "text": "18.657 PS 1 SOLUTIONS\n1.Problem 1\n(1) We expand in terms of conditional probabilities:\nP(X2U) =P(X2UjY= 0)P(Y= 0) + P(X2UjY= 1)P(Y= 1)\n=\u00190P(X2UjY= 0) +\u00191P(X2UjY= 1):\nPassing to densities:\npX=\u00190pXjY=0+\u00191pXjY=1\n1=\u00190(2\u0019)\u0000d=2(det \u0006 0)\u00001=2exp\u0012\n\u0000(x2\u0000\u00160)>\u0006\u00001\n0(x\u0000\u00160)\nd=2 1=2\u00121\u0013\n+\u00191(2\u0019)\u0000(det \u0006 1)\u0000exp\u0000(x\u0000\u00161)>\u0006\u00001\n1(x\u0000\u00161)2\u0013\n:\n(2) By Bayes' rule, we have P(Y =yjX=x)/pX Y=y(x)P(Y =y). Thus, given X=x, the j\nvalue of the Bayes classi\fer is 0 if\n\u00190pXjY=0>\u00191pXjY=1;\nand is otherwise 1. Cancelling (2\u0019 )\u0000d=2(det \u0006)\u00001=2on both sides, and taking logs, the in-\nequality reads\n1log\u00190\u00001(x2\u0000\u00160)>\u0006\u00001(x\u0000\u00160)>log\u00191\u0000(x\u0000\u00161)>\u0006\u00001(x2\u0000\u00161):\nThis asks whether xis at least log \u00190\u0000log\u00191units closer to \u00160than\u00161, in distance measured\nby the quadratic form1\u0006\u00001. By the PSD assumption, this distance matches Euclidean2\ndistance after a linear transformation (given by the Cholesky factorization of \u0006\u00001). Then,\ngeometrically, the boundary forms a hyperplane, and the two regions fh\u0003= 0g andfh\u0003= 1g\nform half-spaces.\n(3) The inequality de\fning the Bayes classi\fer remains quadratic in x, so the hypersurface sepa-\nratingfh\u0003= 0g fromfh\u0003= 1g is a quadric. For example, with\n1 0 2 0\u00060=\nthe\u0012\n;2\u0013\n\u00061=0\u0012\n0 1\u0013\n; \u0019 06=\u00191;\nseparating curve is a hyperbola.\n1\n2 18.657 PS 1 SOLUTIONS\n2.Problem 2\n(1) We see thatCcan shatter 2d + 1 points on a circle: proceeding clockwise around the circle,\nwe pass a line from last of each run of consecutive `included' points to start of the next such\nrun, for a total of at most dlines, and de\fne our polygon by these lines (which extend beyond\ntheir de\fning points).\nSuppose we have 2 d+ 2 points. If any point is in the convex hull of the others, then C\nfails to shatter: any set in Cis convex, and thus can not include the extreme points while\nexcluding an interior point. Otherwise, number the points in their clockwise order as vertices\nof the convex hull, and consider an alternating sign pattern. In order to shatter, there must\nbe a face of the polygon from Cpassing between each excluded point and its two included\nneighbors; these d+ 1 faces must all be distinct, or else we could show that the given points\nviolated our convexity assumption; but no convex polygon with dvertices has more than d\nfaces, a contradiction. So Ccan not shatter 2 d+ 2 points.\n(2) The convex compact sets include in particular the convex polygons of the previous section,\nwhich we see can shatter any number of points (once we allow arbitrarily many polygon\nvertices). Thus the VC dimension is in\fnite.\n(3) We know that Cshatters some n=VC(C ) points. Then some set in\nnCachieves each of the 2n\ninclusion patterns. In particular there exist at least 2 sets in C. Son\u0014log2(cardC).\n(4) On the space R, the class of intervals ( \u00001;t] fort2Ris in\fnite. This class certainly shatters\nthe point 0 (the intervals (\u00001; \u00001] and (\u00001; 0] su\u000ece), while it fails to shatter any two\npointsa<b , since no such interval contains bbut nota. Thus the VC dimension is 1.\n18.657 PS 1 SOLUTIONS 3\n3.Problem 3\n(1) We begin by computing mean and variance:\nXnXn1 1E^[Fn(t)] = P(Xi\u0014t) =F(t) =F(t);n ni=1 i=1\nn1^Var[Fn(t)] =n2X\nVar[ 1(Xi\ni=1\u0014t)]\n1= Var[ 1(X 1n\u0014t)]\n1 1= (E[1(X 1\u0014t)2]\u0000E[1(X2\n1\u0014t)] ) = (F (t)n n\u0000F(t)2):\n^ Let\">0 be given. As the Fn(t) are averages of independent 0{1 random variables, we can\napply Hoe\u000bding's inequality:\nP^(jFn(t)\u0000F(t)j\u0015\")\u00142 exp(\u00002n\"2);\nX1 12P(j^Fn(t)\u0000F(t)j\u0015\")\u0014X\n2 exp(\u00002n\") = < ;1 2=1 n=1\u0000exp(n\u0000\")1\n^ so that by Borel{Cantelli, almost surely only \fnitely many of the events jFn(t)\u0000F(t)j\u0015\"\noccur. Thus Fn(t)F(t) almost surely.\n^!\n(Alternative: Fn(t) is an average of niid samples of Bern(F (t)); the result follows from the\nstrong law of large numbers. Of course, the proof of that law uses Borel{Cantelli.)\n(2) We can view this is a question of empirical measure versus true measure on the class Cof\nhalf-line intervals ( 1;t], which have VC dimension 1 (see the solution to 2(d)). The VC\ninequality now asserts that\nsupj^Fn(t)\u0000F(t) = sup\u0016n(A)\u0016(A)\ntj\nAj \u0000\n2Rj\n2C\n\u00142r\n2 log(2en)+nr\nlog(2=\u000e )\n2n\n\u0014 \n2s\n2 log 4e 1 (+logp\n2!r\nlogn=\u000e)\n2 n\nfor alln\u00152.\n4 18.657 PS 1 SOLUTIONS\n4.Problem 4\n(1) Changing the value Xican only change Bnby at most 1: putting the ith item in a new bin\nis always an option. Applying the bounded di\u000berences inequality yields exactly the result:\nP(jBn\u0000EBnj>t)\u00142 exp(\u00002t2=n):\n\u0016 (2) (a) By the triangle inequality, changing Xiwill changekXkby at most 2 =n. By the (one-\nsided) bounded di\u000berences inequality,\n2t2\nP(k\u0016 \u0016X E\u0000k\u0000 kXk\u0015t)\u0014exp\u0012\n=2\n(2=n)2\u0013\nexp(\u0000nt =2):n\n(b) We apply Jensen's inequality:\nE\u0016[kXk]\u0014E[k\u0016Xk2]1=2\n X!1=2d\n\u0016 = E[X(i)2]:\ni=1\nBy the assumption E[X] = 0, the cross terms of the squared average will cancel in\nexpectation:\n1E\u0016[X(i)2] =Xn\nn2E[Xj(i)2 1] =\nj=1E[X(i)2]:n\nReturning to our computation:\nE[k\u0016Xk]\u0014 Xd11\nE[X(i)2]ni=1\n=\u00121E[nkXk2]\u00131=2\n1\u0014p:n\n(c) Combining (a) and (b), we have\nP\u0016(pkXk\u0015t)\u0014exp(\n\u0012\u0000n(t\u00001= n)2=2)\n1 1= exp\u0000(tpn)2+tpn2\u00002\u0013\n:\nThe polynomial\u00001x2+x\u00001is bounded above by1\u0000x2=4. Thus,2 2 2\nP(k\u0016Xk\u0015t)\u0014e1=2exp(\u0000nt2=4)\u00142 exp(\u0000nt2=4):\n(3) Let\u001b1;:::;\u001bnbe independent symmetric Rademacher random variables. By symmetry of\n\u0016 Xi, the distribution of Xis the same as that of h\u001b;Xi=n. Conditioned on X, Hoe\u000bding's\ninequality yields\nP\u0012h\u001b;Xip>t\u0013\n\u0014exp(\u0000nt2\n\u001b =2):\nn V\nBy the law of total\n\u0012probabilit\n\u0013y,\n\u0016X\u0012h\u001b;XPXp>t =PX;\u001bi\nV np>t =EXP\u001bh\u001b;Xi>t\nV\u0013 \u0012\nnp\nV\u0013\n\u0014EXexp(\u0000nt2=2)\n= exp(\u0000nt2=2):!=2\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "convex", "section_heading": "18.657 PS 1 SOLUTIONS", "source_title": "d61d6c8e64ed20efff3e6ca4fc88742b MIT18 657F15 PS1 Sol", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "ed150026-cb33-434f-a092-a976e61e0418", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 13\nScribe:Mina Karzand Oct. 21, 2015\nPreviously, we analyzed the convergence of the projected gradient descent algorithm.\nWe proved that optimizing the convex L-Lipschitz function fon a closed, convex set Cwith\ndiam(C)≤Rwith step sizes ηs=R√would give us accuracy of f(x)≤f(x∗)+LR\nL k√afterk\nkiterations.\nAlthough it might seem that projected gradient descent algorithm provides dimension-\nfree convergence rate, it is not always true. Reviewing the proof of convergence rate, we\nrealize that dimension-free convergence is possible when the objective function fand the\nconstraint set Care well-behaved in Euclidean norm (i.e., for all x∈ Candg∈∂f(x), we\nhave that |x|2and|g|2are independent of the ambient dimension). We provide an examples\nof the cases that these assumptions are not satisﬁed.\n•Consider the diﬀerentiable, convex function fon the Euclidean ball B2,nsuch that\n/ba∇dbl∇f(x)/ba∇dbl ≤1,∀x∈B2,n. This implies that f(x)√nand the projected ∞ |∇ | 2≤\ngradient descent converges to the minimum of finBn2,/radicalignat ratek. Using the\nlog(n)method of mirror descent we can get convergence rate of/radicalbig\nk\nTo get better rates of convergence in the optimization problem, we can use the Mirror\nDescent algorithm. The idea is to change the Euclidean geometry to a more pertinent\ngeometry to a problem at hand. We will deﬁne a new geometry by using a function which\nis sometimes called potential function Φ( x). We will use Bregman projection based on\nBregman divergence to deﬁne this geometry.\nThe geometric intuition behind the mirror Descent algorithm is the following: The\nprojected gradient described in previous lecture works in any arbitrary Hilbert space Hso\nthat thenormof vectors isassociated withaninnerproduct. Now, supposeweareinterested\nin optimization in a Banach space D. In other words, the norm (or the measure of distance)\nthat we use does not derive from an inner product. In this case, the gradient descent does\nnot even make sense since the gradient ∇f(x) are elements of dual space. Thus, the term\nx−η∇f(x) cannot be performed. (Note that in Hilbert space used in projected gradient\ndescent, the dual space of His isometric to H. Thus, we didn’t have any such problems.)\nThe geometric insight of the Mirror Descent algorithm is that to perform the optimiza-\ntion in the primal space D, one can ﬁrst map the point x∈ Din primal space to the dual\nspaceD∗, then perform the gradient update in the dual space and ﬁnally map the optimal\npoint back to the primal space. Note that at each update step, the new point in the primal\nspaceDmight be outside of the constraint set C ⊂ D, in which case it should be projected\ninto the constraint set C. The projection associate with the Mirror Descent algorithm is\nBergman Projection deﬁned based on the notion of Bergman divergence.\nDeﬁnition (Bregman Divergence): Forgivendiﬀerentiable, α-stronglyconvexfunc-\ntion Φ(x) :D →R, we deﬁne the Bregman divergence associated with Φ to be:\nDΦ(y,x) = Φ(y)−Φ(x)−∇Φ(x)T(y−x)\n1\nWe will usetheconvex openset D ⊂nRwhoseclosure contains theconstraint set C ⊂ D.\nBregman divergence is the error term of the ﬁrst order Taylor expansion of the function Φ\ninD.\nAlso, note that the function Φ( x) is said to be α-strongly convex w.r.t. a norm /ba∇dbl./ba∇dblif\nΦ(y)−Φ(x)−∇Φ(x)T α(y−x)≥2/ba∇dbly−x/ba∇dbl2.\nWe used the following property of the Euclidean norm:\n2a⊤b=/ba∇dbla/ba∇dbl2+/ba∇dblb/ba∇dbl2−/ba∇dbla−b/ba∇dbl2\nin the proof of convergence of projected gradient descent, where we chose a=xs−ys+1and\nb=xs−x∗.\nTo prove the convergence of the Mirror descent algorithm, we use the following property\noftheBregmandivergence inasimilarfashion. Thispropositionshowsthat theBregmandi-\nvergenceessentially behaves astheEuclideannormsquaredintermsof projections:\nProposition: Givenα-strongly diﬀerentiable convex function Φ : D →R, for all\nx,y,z∈ D,\n[∇Φ(x)−∇Φ(y)]⊤(x−z) =DΦ(x,y)+DΦ(z,x)−DΦ(z,y).\nAs described previously, the Bregman divergence is used in each step of the Mirror descent\nalgorithm to project the updated value into the constraint set.\nDeﬁnition (Bregman Projection): Givenα-strongly diﬀerentiable convex function\nΦ :D →Rand for all x∈ Dand closed convex set C ⊂D\nΠΦ(x) = argmin DC Φ(z,x)\nz∈C∩D\n2.4.2 Mirror Descent Algorithm\nAlgorithm 1 Mirror Descent algorithm\nInput:x1∈argmin Φ( x),ζ:d dR R such that ζ(x) = Φ(x)C∩D → ∇\nfors= 1,···,kdo\nζ(ys+1) =ζ(xs)−ηgsforgs∈∂f(xs)\nxs+1= ΠΦ(yCs+1)\nend for\nreturn Eitherx=1\nk/summationtextk\ns=1xsorx◦∈argminx x1, ,xkf(x)∈{ ··· }\nProposition: Letz∈ C ∩D, then∀y∈ D,\n(∇Φ(π(y)−∇Φ(y))⊤(π(y)−z)≤0\n2\nMoreover, DΦ(z,π(y))≤DΦ(z,y).\nProof.Deﬁneπ= ΠΦ(y) andh(t) =DΦ(π+t(z−π),y).Sinceh(t) is minimized at t= 0C\n(due to the deﬁnition of projection), we have\nh′(0) =∇xDΦ(x,y)|x=π(z−π)≥0\nwhere suing the deﬁnition of Bregman divergence,\n∇xDΦ(x,y) =∇Φ(x)−∇Φ(y)\nThus,\n(∇Φ(π)−∇Φ(y))⊤(π−z)≤0.\nUsing proposition 1, we know that\n(∇Φ(π)−∇Φ(y))⊤(π−z) =DΦ(π,y)+DΦ(z,π)−DΦ(z,y)≤0,\nand since DΦ(π,y)≥0, we would have DΦ(z,π)≤DΦ(z,y).\nTheorem: Assume that fis convex and L-Lipschitz w.r.t. /ba∇dbl./ba∇dbl. Assume that Φ is\nα-strongly convex on C ∩ Dw.r.t./ba∇dbl./ba∇dbland\nR2= sup Φ( x) m\nx−in Φ(x)\n∈C∩D x∈C∩D\nta/radicaligkex1= argminxΦ(x) (assume that it exists). Then, Mirror Descent with η=∈C∩D\nR2α\nL Rgives,\n2 2f(x)−f(x∗)≤RL/radicalbigg\nandf(x◦)αk−f(x∗)≤RL/radicalbigg\n,αk\nProof.Takex♯∈ C ∩D. Similar to the proof of the projected gradient descent, we have:\n(i)\nf(xs)−f(x♯)≤gs⊤(xs−x♯)\n(ii)1=(ζ(x♯s)−ζ(ys+1))⊤(xs)η−x\n(iii)1=( Φ(xs) Φ(ys+1))⊤(xsx♯)η∇ −∇ −\n(iv)1=/bracketleftig\nD♯Φ(xs,ys+1)+D♯Φ(x ,xs)η−DΦ(x ,ys+1)/bracketrightig\n(v)1≤/bracketleftig\nDΦ(xs,ys+1)+DΦ(x♯,xs)−DΦ(x♯,xs+1)η/bracketrightig\n(vi)ηL21≤+/bracketleftig\nDΦ(x♯,xs)2α2η−DΦ(x♯,xs+1)/bracketrightig\nWhere (i) is due to convexity of the function f.\n3\nEquations (ii) and (iii) are direct results of Mirror descent algorithm.\nEquation (iv) is the result of applying proposition 1.\nInequality (v) is a result of the fact that x= ΠΦ ♯s+1 (yCs+1), thus for x\n♯ ♯∈ C ∩ D, we have\nDΦ(x ,ys+1)≥DΦ(x ,xs+1).\nWe will justify the following derivations to prove inequality (vi):\n(a)DΦ(xs,ys+1) = Φ(xs)−Φ(ys+1)−∇Φ(ys+1)⊤(xs−ys+1)\n(b) α≤[∇Φ(x2s)−∇Φ(ys+1)]⊤(xs−ys+1)−2/ba∇dblys+1−xs/ba∇dbl\n(c) α≤η/ba∇dblgs/ba∇dbl∗/ba∇dblxs−ys+1/ba∇dbl−2/ba∇dblys+1−xs/ba∇dbl2\n(d)η2L2\n≤.2α\nEquation (a) is the deﬁnition of Bregman divergence.\nTo show inequality (b), we used the fact that Φ is α-strongly convex which implies that\nΦ(ys+1)−Φ(xs)≥ ∇Φ(xs)T(ys+1−xs)α\n2/ba∇dbly2s+1−xs/ba∇dbl.\nAccording to the Mirror descent algorithm, ∇Φ(xs)− ∇Φ(ys+1) =ηgs. We use H¨ older’s\ninequality to show that gs⊤(xs−ys+1)≤ /ba∇dblgs/ba∇dbl∗/ba∇dblxs−ys+1/ba∇dbland derive inequality (c).\nLookingatthequadraticterm ax−bx2fora,b >0,itisnothardtoshowthatmax ax\na−bx2=\n2. We use this statement with x=/ba∇dblys+1−xs/ba∇dbl,a=η gb/ba∇dbls/ba∇dblL4 ∗≤andb=αto derive2\ninequality (d).\nAgain, we use telescopic sum to get\nk1/summationdisplay ηL2D(x♯Φ,x1)[f(xs)f(x♯)] + . (2.1)k 2α kηs=1− ≤\nWe use the deﬁnition of Bregman divergence to get\nDΦ(x♯,x1) = Φ(x♯)−Φ(x1)−∇Φ(x1)(x♯−x1)\n≤Φ(x♯)−Φ(x1)\n≤sup Φ(x) min Φ( x)\nx x−\n∈C∩D ∈C∩D\n≤R2.\nWhere we used the fact x1∈argmin Φ( x) in the description of the Mirror Descent\n♯C∩D\nalgorithm to prove ∇Φ(x1)(x−x1)≥0. We optimize the right hand side of equation (2.1)\nforηto get\nk1/summationdisplay\n(x♯ 2[f(xs)−f)]ks=≤RL\n1/radicalbigg\n.αk\nTo conclude the proof, let x♯→x∗∈ C.\nNote that with the right geometry, we can get projected gradient descent as an instance\nthe Mirror descent algorithm.\n4\n2.4.3 Remarks\nThe Mirror Descent is sometimes called Mirror Prox. We can write xs+1as\nxs+1= argmin DΦ(x,ys+1)\nx∈C∩D\n= argminΦ( x)\nx−∇Φ⊤(ys+1)x\n∈C∩D\n= argminΦ( x) xs\nx−[∇Φ( )−ηgs]⊤x\n∈C∩D\n= argmin η(gs⊤x)+Φ(x)\nx−∇Φ⊤(xs)x\n∈C∩D\n= argmin η(gs⊤x)+DΦ(x,xs)\nx∈C∩D\nThus, we have\nxs+1= argmin η(gs⊤x)+DΦ(x,xs).\nx∈C∩D\nTo getxs+1, in the ﬁrst term on the right hand side we look at linear approximations\nclose toxsin the direction determined by the subgradient gs. If the function is linear, we\nwould just look at the linear approximation term. But if the function is not linear, the\nlinear approximation is only valid in a small neighborhood around xs. Thus, we penalized\nby adding the term DΦ(x,xs). We can penalized by the square norm when we choose\nDΦ(x,xs) =/ba∇dblx−xs/ba∇dbl2. In this case we get back the projected gradient descent algorithm\nas an instance of Mirror descent algorithm.\nBut if we choose a diﬀerent divergence DΦ(x,xs), we are changing the geometry and we\ncan penalize diﬀerently in diﬀerent directions depending on the geometry.\nThus, using the Mirror descent algorithm, we could replace the 2-norm in projected\ngradient descent algorithm by another norm, hoping to get less constraining Lipschitz con-\nstant. On the other hand, the norm is a lower bound on the strong convexity parameter.\nThus, there is trade oﬀ in improvement of rate of convergence.\n2.4.4 Examples\nEuclidean Setup:\nΦ(x) =1x2, =dR, Φ(x) =ζ(x) =x. Thus, the updates will be similar to2/ba∇dbl /ba∇dbl D ∇\nthe gradient descent.\n1DΦ(y,x) =/ba∇dbly/ba∇dbl21− /ba∇dblx2\n2/ba∇dbl2x2−⊤y+/ba∇dblx/ba∇dbl\n1=/ba∇dblx−y/ba∇dbl2.2\nThus, Bregman projection with this potential function Φ( x) is the same as the usual Eu-\nclidean projection and the Mirror descent algorithm is exactly the same as the projected\ndescent algorithm since it has the same update and same projection operator.\nNote that α= 1 since D1Φ(y,x)≥2/ba∇dblx−y/ba∇dbl2.\nℓ1Setup:\nWe look at D=dR+\\{0}.\n5\nDeﬁne Φ( x) to be the negative entropy so that:\nd\nΦ(x) =/summationdisplay\nxilog(xi), ζ(x) =∇Φ(x) ={1+log(xdi)\ni=1}i=1\n(s+1)∇(s)−(s+1)Thus, looking at the update function y= Φ(x)ηgs, we get log( yi) =\n(s)log(xi)−(s) (s+1) ( s) (s)ηgiand for all i= 1,···,d, we have yi=xiexp(−ηgi). Thus,\ny(s)=x(s)exp(−ηg(s)).\nWe call this setup exponential Gradient Descent or Mirror Descent with multiplicative\nweights.\nThe Bregman divergence of this mirror map is given by\nDΦ(y,x) = Φ(y)−Φ(x)−∇Φ⊤(x)(y−x)\n/summationdisplayd /summationdisplayd d\n=yilog(yi)−xilog(xi)(1+log( xi))(yixi)\ni 1/summationdisplay\ni−\n=1 i−\n= =1\n/summationdisplaydyi=yilog()+xii=1/summationdisplayd\n(yi\ni=1−xi)\nNote that/summationtextd\ni=1yilog(yi\ni) is call the Kullback-Leibler divergence (KL-div) between yx\nandx.\nWe show that the projection with respect to this Bregman divergence on the simplex\n∆d={x∈dR:d\ni=1xi= 1,xi≥0}amounts to a simple renormalization y/ma√sto→y/|y|1. To\nprove so, we prov/summationtext\nide the Lagrangian:\n/summationdisplayd /summationdisplayd dyLi=yilog()+(xixii=1 i=−yi)+λ(\n1/summationdisplay\nxi\ni=1−1).\nTo ﬁnd the Bregman projection, for all i= 1,···,dwe write\n∂ yL −i= +1+ λ= 0∂xixi\nThus, for all i, we have xi=γyi. We know that/summationtextd\ni=1xi= 1. Thus, γ=1/summationtextyi.\nThus, we have ΠΦ y\n∆d(y) =\n1. The Mirror Descent algorithm with this update and|y|\nprojection would be:\nys+1=xsexp(−ηgs)\nyxs+1=.|y|1\nTo analyze the rate of convergence, we want to study the ℓ1norm on ∆ d. Thus, we have\nto show that for some α, Φ isα-strongly convex w.r.t |·|1on ∆d.\n6\nDΦ(y,x) =KL(y,x)+/summationdisplay\n(xi\ni−yi)\n=KL(y,x)\n1≥2|x−y|2\n1\nWhere we used the fact that x,y∈∆dto showi(xi−yi) = 0 and used Pinsker\ninequality show the result. Thu/summationtexts, Φ is 1-strongly conve/summationtext\nx w.r.t.|·|1on ∆d.\nRemembering that Φ( x) =d\ni=1xilog(xi) was deﬁned to be negative entropy, we know\nthat−log(d)≤Φ(x)≤0 forx∈∆d. Thus,\nR2= maxΦ( x)\nx∈∆d−min Φ(x) = log(d).\nx∈∆d\nCorollary: Letfbe a convex function on ∆ dsuch that\n/ba∇dblg/ba∇dbl∞≤L,∀g∈∂f(x),∀x∈∆d.\n2log(d)Then, Mirror descent with η=1/radicalig\ngivesL k\n2log(d) 2log(d)f(xk)−f(x∗)≤L/radicalbigg\n, f(x◦\nk)−f(x∗)k≤L/radicalbigg\nk\nBoosting: For weak classiﬁers f1(x),···,fN(x) andα∈∆n, we deﬁne\nN f1(x)\n.fα=\n/summationdisplay\nαjfjandF(x) =\nj=1..\nfN(x)\nso thatfα(x) is the weighted majority vote classiﬁer. Note that |F|∞≤1.\nAs shown before, in boosting, we have:\nn\ng=∇R/hatwide1\nn,φ(fα) =/summationdisplay\nφ′(−yifα(xi))(−yi)F(xi),ni=1\nSince|F| ≤1 and|y| ≤1, then|g| ≤LwhereLis the Lipschitz constant of φ ∞ ∞ ∞\n(e.g., a constant like eor 2).\n/radicalbigg\n/hatwide /hatwide2log(N)Rn,φ(fα◦\nk)−minRn,φ(fα)L\nα∈∆n≤k\nWe need the number of iterations k≈n2log(N).\nThe functions fj’s could hit all the vertices. Thus, if we want to ﬁt them in a ball, the\n/radicaligball has to be radius√\nN. This is why the projected gradient descent would give the rate of\nN\nk. But by looking at the gradient we can determine the right geometry. In this case, the\ngradient is bounded by sup-norm which is usually the most constraining norm in projected\n7\ngradient descent. Thus, using Mirror descent would be most beneﬁcial.\nOther Potential Functions:\nThere are other potential functions which are strongly convex w.r.t ℓ1norm. In partic-\nular, for\n1Φ(x) =|x|p 1\np, p= 1+p log(d)\nthen Φ is c/radicalbig\nlog(d)-strongly convex w.r.t ℓ1norm.\n8\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "gradient descent", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "c0633c5305ddcf87b0bd61f69b2cbdb5 MIT18 657F15 L13", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "95ce702f-0307-425b-b651-2cec53cf581d", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 6\nScribe:Ali Makhdoumi Sep. 28, 2015\n5. LEARNING WITH A GENERAL LOSS FUNCTION\nIn the previous lectures we have focused on binary losses for the classiﬁcation problem and\ndeveloped VCtheory forit. Inparticular, theriskforaclassiﬁcation function h:X → {0,1}\nand binary loss function the risk was\nR(h) = IP(h(X) =Y) = IE[1I( h(X) =Y)].\nIn this lecture we will consider a general loss function and a general regression model where\nYis not necessarily a binary variable. For the binary classiﬁcation problem, we then used\nthe followings:\n•Hoeﬀding’s inequality: it requires boundedness of the loss functions.\n•Bounded diﬀerence inequality: again it requires boundedness of the loss functions.\n•VC theory: it requires binary nature of the loss function.\nLimitations of the VC theory:\n•Hard to ﬁnd the optimal classiﬁcation: the empirical risk minimization optimization,\ni.e.,\nn1min\nhn/summationdisplay\n1I(h(Xi) =Yi)\ni=1\nis a diﬃcult optimization. Even though it is a hard optimization, there are some\nalgorithms that try to optimize this function such as Perceptron and Adaboost.\n•This is not suited for regression. We indeed know that classiﬁcation problem is a\nsubset of Regression problem as in regression the goal is to ﬁnd IE[ Y|X] for a general\nY(not necessarily binary).\nIn this section, we assume that Y∈[−1,1] (this is not a limiting assumption as all the\nresults can bederived for any bounded Y) and we have a regression problem where ( X,Y)∈\nX ×[−1,1]. Most of the results that we preset here are the analogous to the results we had\nin binary classiﬁcation. This would be a good place to review those materials and we will\nrefer to the techniques we have used in classiﬁcation when needed.\n5.1 Empirical Risk Minimization\n5.1.1 Notations\nLoss function: In binary classiﬁcation the loss function was 1I( h(X) =Y). Here, we\nreplace this loss function by ℓ(Y,f(X)) which we assume is symmetric, where f∈ F,\nf:X →[−1,1] is the regression functions. Examples of loss function include\n1/\\e}atio\\slash /\\e}atio\\slash\n/\\e}atio\\slash\n/\\e}atio\\slash\n•ℓ(a,b) = 1I( a=b) ( this is the classiﬁcation loss function).\n•ℓ(a,b) =|a−b|.\n•ℓ(a,b) = (a−b)2.\n•ℓ(a,b) =|a−b|p,p≥1.\nWe further assume that 0 ≤ℓ(a,b)≤1.\nRisk: risk is the expectation of the loss function, i.e.,\nR(f) = IEX,Y[ℓ(Y,f(X))],\nwhere the joint distribution is typically unknown and it must be learned from data.\nData: we observe a sequence ( X1,Y1),...,(Xn,Yn) ofnindependent draws from a joint\ndistribution PX,Y, where ( X,Y)∈ X ×[−1,1]. We denote the data points by Dn=\n{(X1,Y1),...,(Xn,Yn)}.\nEmpirical Risk : the empirical risk is deﬁned as\nn1ˆRn(f) =n/summationdisplay\nℓ(Yi,f(Xi)),\ni=1\nˆ ˆ and the empirical risk minimizer denoted by ferm(orf) is deﬁned as the minimizer of\nempirical risk, i.e.,\nˆargminRn(f).\nf∈F\nˆ In order to control the risk of fwe shall compare its performance with the following oracle:\n¯f∈argminR(f).\nf∈F\nNote that this is an oracle as in order to ﬁnd it one need to have access to PXYand then\nˆ optimize R(f) (we only observe the data Dn). Since fis the minimizer of the empirical\nˆˆˆ¯ risk minimizer, we have that Rn(f)≤Rn(f), which leads to\nˆR(f)≤ˆR(f)−ˆˆˆˆˆ¯ˆ¯ ¯ ¯ Rn(f)+Rn(f)−Rn(f)+Rn(f)−R(f)+R(f)\n≤¯ ˆ ˆˆˆ¯ ¯ ¯ ˆ R(f)+R(f)−Rn(f)+Rn(f)−R(f)≤R(f)+2sup\nf∈F|Rn(f)−R(f)|.\nTherefore, the quantity of interest that we need to bound is\nsup|ˆRn(f)−R(f)\nf∈F|.\nMoreover, from theboundeddiﬀerence inequality, we know that sincethe loss function ℓ(·,·)\nˆ is bounded by 1, supf∈F|Rn(f)−R(f)|has the bounded diﬀerence property with ci=1\nn\nfori= 1,...,n, and the bounded diﬀerence inequality establishes\nP/bracketleftigg\n2t2\nsup|ˆ ˆ Rn(f)−R(f)|−IE/bracketleftigg\nsup|Rn(f)−R(f)\nf∈F|/bracketrightigg\n≥t\nf∈F/bracketrightigg\n−≤exp/parenleftbigg\nx2\ni i/parenrightbigg\n= e pc−2nt2,\nwhich in turn yields/parenleftbig /parenrightbig/summationtext\nlog(1/delta)|ˆsupRn(f)−R(f)| ≤I|ˆE/bracketleftigg\nsupRn(f)−R(f) δ\nf∈F f|/bracketrightigg\n+\n∈/radicalbigg\n,w.p. 1\nF 2n−.\nˆ As a result we only need to bound the expectation IE[supf∈F|Rn(f)−R(f)|].\n2/\\e}atio\\slash\n5.1.2 Symmetrization and Rademacher Complexity\nSimilar to the binary loss case we ﬁrst use symmetrization technique and then intro-\nduce Rademacher random variables. Let Dn={(X1,Y1),...(Xn,Yn)}be the sample set\nand deﬁne an independent sample (ghost sample) with the same distribution denoted by\nD′\nn={(X′\n1,Y′\n1),...(X′\nn,Y′\nn)}( for each i, (X′\ni,Y′\ni) is independent from Dnwith the same\ndistribution as of ( Xi,Yi)). Also, let σi∈ {−1,+1}be i.i.d. Rad(1) random variables2\nindependent of DnandD′\nn. We have\nIE/bracketleftiggn1sup|ℓ i\nf∈Fn/summationdisplay\n(Yi,f(X))\ni=1−IE[ℓ(Yi,f(Xi))]|/bracketrightigg\nn n1 1= IE/bracketleftigg\nsup ℓ(Yi,f(X ℓ(Y′i)) IEi,f(X′\ni))Dn\nf∈F|n/summationdisplay\ni=1−/bracketleftigg\nn/summationdisplay\ni=1|/bracketrightigg\n|/bracketrightigg\nn n1 1= IE/bracketleftigg\nsup|IE/bracketleftigg/summationdisplay\nℓ(Yi,f(X′i)) ℓ(Yi,f(X′\ni))Dn\nf∈Fni=1−n/summationdisplay\ni=1|/bracketrightigg\n|/bracketrightigg\nn(a)\n≤IE/bracketleftiggn1 1supIE/bracketleftigg\n|/summationdisplay\nℓ(Yi,f(X′i))−/summationdisplay\nℓ(Y ,f(X′\ni i))| |Dn\nf∈Fn ni=1 i=1/bracketrightigg/bracketrightigg\n≤IE/bracketleftiggn n1sup|/summationdisplay 1ℓ(Yi,f(Xi)) ℓ(Y′\ni,f(X′\nf∈Fn ni))\ni=1−/summationdisplay\ni=1|/bracketrightigg\n(b) 1= IE/bracketleftiggn\nsup|/summationdisplay\nσi/parenleftbig\nℓ(Yi,f(Xi))−ℓ(Y′X\nfFni,f(′\ni))\n∈i=1/parenrightbig\n|/bracketrightigg\nn(c) 1≤2IE/bracketleftigg\nsup\nf∈F|n/summationdisplay\nσiℓ(Yi,f(Xi))\ni=1|/bracketrightigg\nn\n≤2supIE/bracketleftigg\n1sup|/summationdisplay\nσiℓ(yi,f(xi))\nDnf∈Fni=1|/bracketrightigg\n.\nwhere (a) follows from Jensen’s inequality with convex function f(x) =x, (b) follows from\nthe fact that ( X ,Y) and (X′ ′| |\ni i i,Yi) has the same distributions, and (c) follows from triangle\ninequality.\nRademacher complexity: of a class Fof functions for a given loss function ℓ(·,·) and\nsamplesDnis deﬁned as\nn1Rn(ℓ◦F) = supIE/bracketleftigg\nsup|/summationdisplay\nσiℓ(yi,f(xi)).\nDnf∈Fni=1|/bracketrightigg\nTherefore, we have\nIE/bracketleftiggn1sup|/summationdisplay\nℓ(Yi,f(Xi))\nf∈Fni=1−IE[ℓ(Yi,f(Xi))]|/bracketrightigg\n≤2Rn(ℓ◦F)\nand we only require to bound the Rademacher complexity.\n5.1.3 Finite Class of functions\nSuppose that the class of functions Fis ﬁnite. We have the following bound.\n3\nTheorem: Assume that Fis ﬁnite and that ℓtakes values in [0 ,1]. We have\n/radicalbigg\n2log(2Rn(ℓ◦F)|F|)≤ .n\nProof.From the previous lecture, for B⊆nR, we have that\nn1 2log(2B)Rn(B) = IE/bracketleftigg\nmax\nb∈B|n/summationdisplay\nσibi\ni=1|/bracketrightigg\n| |≤max\nb∈B|b|2/radicalbig\n.n\nHere, we have\nℓ(y(x 1,f1))\n.B= .,.\nℓ(yn,f(xn)f∈ F\n\n.\n)\nSinceℓtakes values in [0 ,1], this\nim\npliesB\n⊆ {b:|b|2√≤\nn}. Plugging this bound in the\nprevious inequality completes the proof.\n5.2 The General Case\nRecall that for the classiﬁcation problem, we had F ⊂ {0,1}X. We have seen that the\ncardinality of the set {(f(x1),...,f(xn)),f\nˆerm∈ F}plays an important role in bounding the\nrisk off(this is not exactly what we used but the XOR argument of the previous lecture\nallows us to show that the cardinality of this set is the same as the cardinality of the set\nthat interests us). In this lecture, this set might be uncountable. Therefore, we need to\nintroduce a metric on this set so that we can treat the close points in the same manner. To\nthis end we will deﬁne covering numbers (which basically plays the role of VC dimension\nin the classiﬁcation).\n5.2.1 Covering Numbers\nDeﬁnition: Given a set of functions Fand a pseudo metric donF((F,d) is a metric\nspace) and ε >0. Anε-netof (F,d) is a set Vsuch that for any f∈ F, there exists\ng∈Vsuch that d(f,g)≤ε. Moreover, the covering numbers of (F,d) are deﬁned by\nN(F,d,ε) = inf{|V|:Vis anε-net}.\nFor instance, for the Fshown in the Figure 5.2.1 the set of points {1,2,3,4,5,6}is a\ncovering. However, the covering number is 5 as point 6 can be removed from Vand the\nresulting points are still a covering.\nDeﬁnition: Givenx= (x1,...,x n), theconditional Rademacher average of a class of\n4\nfunctions Fis deﬁned as\nRˆx\nn= IE/bracketleftiggn1sup σ\nf∈F/vextendsingle/vextendsingle\nn/summationdisplay\nif(xi)\ni=1/bracketrightigg\n/vextendsingle/vextendsingle.\nNote that in what follows we consider a general class of functions F. However, for\napplying the results in order to bound empirical risk minimization, we take xito be (xi,yi)\nandFto beℓ◦F. We deﬁne the empirical l1distance as\nn\ndx 1\n1(f,g) =n/summationdisplay\ni\n=1|f(x)\ni−g(xi)|.\nTheorem: If 0≤f≤1 for all f∈ F, then for any x= (x1,...,x n), we have\nˆRx\nn(F)≤inf\nε≥0/radicalbigg\nx /braceleftbig2log(2N(F,dε+1,ε))\nn/bracerightbig\n.\nProof.Fixx= (x1,...,x n) andε >0. LetVbe a minimal ε-net of ( F,dx\n1). Thus,\nby deﬁnition we have that |V|=N(F,dx\n1,ε). For any f∈ F, deﬁnef◦∈Vsuch that\n56\n5 432 1 F\nǫ\ndx\n1(f,f◦)≤ε. We have that\nn1Rx\nn(F) = IE/bracketleftigg\nsup σif(xi)\nf∈F|n/summationdisplay\ni=1|/bracketrightigg\n≤IE/bracketleftiggn n1 1sup|/summationdisplay\nσi(f(xi)f◦(xi)) +IE sup σif◦(xi)\nf∈Fn f∈Fni=1− |/bracketrightigg /bracketleftigg\n|/summationdisplay\ni=1|/bracketrightigg\n≤ε+IE/bracketleftiggn1max σif(xi)\nf∈V|n/summationdisplay\ni=1|/bracketrightigg\n/radicalbigg\n2log(2≤ε+|V|)\nn/radicalbigg\n2log(2N(=ε+F,dx\n1,ε)).n\nSince the previous bound holds for any ε, we can take the inﬁmum over all ε≥0 to obtain\nx/radicalbigg\n/braceleftbig2log(2N(F,dx\nRn(F)≤infε+1,ε))\nε≥0 n/bracerightbig\n.\nThe previous bound clearly establishes a trade-oﬀ because as εdecreases N(F,dx\n1,ε) in-\ncreases.\n5.2.2 Computing Covering Numbers\nAs a warm-up, we will compute the covering number of the ℓ2ball of radius 1 indRdenoted\nbyB2. We will show that the covering is at most (3\nε)d. There are several techniques to\nprove this result: one is based on a probabilistic method argument and one is based on\ngreedily ﬁnding an ε-net. We will describe the later approach here. We select points in V\none after another so that at step k, we have uk∈B2\\∪k\nj=1B(uj,ε). We will continue this\nprocedure until we run out of points. Let it be step N. This means that V={u1,...,u N}\nis anε-net. We claim that the balls B(ui,ε) andB(uj,ε) for any i,j12 2∈ {,...,N}are\ndisjoint. The reason is that if v∈B(ui,ε)∩B(uj,ε), then we would have2 2\nε ε/bardblui−uj/bardbl2≤ /bardblui−v/bardbl2+/bardblv−uj/bardbl2≤+ =ε,2 2\nwhich contradicts the way we have chosen the points. On the other hand, we have that\n∪N\nj=1B(uj,ε)⊆(1+ε)B2. Comparing the volume of these two sets leads to2 2\nε ε|V|( )dvol(B2)≤(1+ )dvol(B2),2 2\nwherevol(B2) denotes the volume of the unit Euclidean ball in ddimensions. It yields,\n|V| ≤/parenleftbig\n1+εd\n22d3d\n= +1 . /parenleftbigε ε\n2/parenrightbig/parenrightbigg\n/parenrightbigd/parenleftbigg\n≤/parenleftbigg\nε/parenrightbigg\n6\nFor anyp≥1, deﬁne\n1\ndx\np(f,g) =/parenleftiggn1p /summationdisplay\n|f(xi)g(x)pi,ni=1− |/parenrightigg\nand forp=∞, deﬁne\ndx\n∞(f,g) = max |f(xi)−g(xi)\ni|.\nˆ Using the previous theorem, in order to bound Rx\nnwe need to bound the covering number\nwithdx\n1norm. We claim that it is suﬃcient to bound the covering number for the inﬁnity-\nnorm. In order to show this, we will compare the covering number of the norms dx\np(f,g) =\n1 /parenleftbig1\nn/summationtextn\ni=1|f(xpi)−g(xi)|/parenrightbig\npforp≥1 and conclude that a bound on N(F,dx\n∞,ε) implies a\nbound on N(F,dx\np,ε) for any p≥1.\nProposition: For any 1 ≤p≤qandε >0, we have that\nN(F,dx\np,ε)≤N(F,dx\nq,ε).\nProof.First note that if q=∞, then the inequality evidently holds. Because, we have\nn1(/summationdisplay 1\n|zi|p)p≤maxn ii=1|zi|,\nwhich leads to B(f,dx\n∞,ε)⊆B(f,dx\np,ε) andN(f,d∞,ε)≥N(f,dp,ε). Now suppose that\n1≤p≤q <∞. Using H¨ older’s inequality with r=q\np≥1 we obtain\n/parenleftigg /parenrightigg 1/parenleftigg /parenrightigg(11)1/parenleftigg /parenrightigg 1/parenleftigg /parenrightigg 1− n n n1p r p pr n1q\n1n/summationdisplay 1\n|z|pi≤−np/summationdisplay\ni1/summationdisplay\ni=1|zi|pr=ni=1 =/summationdisplay\n.\ni|zi|q\n=1\nThis inequality yeilds\nB(f,dx\nq,ε) ={g:dx\nq(f,g)≤ε} ⊆B(f,dx\np,ε),\nwhich leads to N(f,dq,ε)≥N(f,dp,ε).\nUsing this propositions we only need to bound N(F,dx\n∞,ε).\nLet the function class be F={f(x) =/a\\}bracketle{tf,x/a\\}bracketri}ht,f∈Bd,x∈Bd}, where1 1\np q + = 1. Thisp q\nleads to|f| ≤1.\nClaim: N(F,dx\n∞,ε)≤(2)d.ε\nThis leads to\nx/radicalbigg\n2dlog(4/ε)ˆRn(F)≤inf\n0{ε+ .\nε> n}\nTakingε=O(/radicalig\ndlogn), we obtainn\nˆRx d\nn(F)≤O(/radicalbigg\nlogn).n\n7\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "convex", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "5ebb42429b252cbd2f711cd03e01b97f MIT18 657F15 L6", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "bc033ce7-8648-4218-aa70-484de4245972", "text": "18.657. Fall 2105\nRigollet October 18, 2015\nProblem set #2 (due Wed., October 21)\nShould be typed in LAT XE\nProblem 1. Rademacher Complexities and beyond\nLetFbe a class of functions from Xto IR and let X1,...,X nbe iid copies of a\nrandom variable X∈ X. Moreover, let σ1,...,σ nbeni.i.d.Rad(1/2) random variables\nand letg1,...,g nbeni.i.d.N(0,1). Assume that all these random variables are mutually\nindependent.\n1. Prove the desymmetrization inequality :\n1IE/bracketleftBig\nsup\nf∈F/vextendsingle/vextendsingle/vextendsinglen/summationdisplay 1σi/bracketleftbig\nf(Xi)−IE[f(X)]\ni=1/bracketrightbig/vextendsingle/vextendsingle/bracketrightBig\n≤2IE/bracketleftBig\nsupn/vextendsingle\nf∈F/vextendsingle/vextendsingle/vextendsinglen\nn/summationdisplay\ni=1/bracketleftbig\nf(Xi)−IE[f(X)]/bracketrightbig/vextendsingle/vextendsingle/bracketrightBig\n/vextendsingle\n2. Prove the Rademacher/Gaussian process comparison inequality\nn\nIE/bracketleftBig\nsup\nf∈F/summationdisplay\nσif(Xi)\ni=1/bracketrightBig\n≤/radicalbiggπn\nIE/bracketleftBig\nsup2f∈F/summationdisplay\ngif(Xi)\ni=1/bracketrightBig\n=/bracketleftBig1DeﬁneRn(F) IE sup\nf∈F/vextendsinglen/vextendsingle/summationdisplay\nσif(Xi)/vextendsingle/vextendsingle/bracketrightBig\n. LetFandGbe two set of functions from tnXo\nIR and recall that + =/vextendsingle\ni=1/vextendsingle\nF G {f+g:f∈ F,g∈ G}.\n3. Leth∈IRXbe a given function and deﬁne F+h={f+h:f∈ F}. Show that\nh∞Rn(F+{h})≤Rn(F)+/bardbl /bardbl√,n\nwhere/bardblh/bardbl∞= supx∈X|h(x)|.\n4. LetF1,...,Fkbeksets of functions from Xto IR. Show that\nk\nRn(F1+···,Fk)≤/summationdisplay\nRn(\nj=1Fj).\n5. Show that this inequality derived in 4. is in fact an equality when the Fjs are the\nsame.\npage 1 of 4\nProblem 2. Covering and packing\nDeﬁnition: AsetP⊂Tiscalledan ε-packing ofthemetricspace( T,d)ifd(f,g)>ε\nfor everyf,g∈P,f/ne}ationslash=g. The largest cardinality of an ε-packing of ( T,d) is called\nthepacking number of (T,d):\nD(T,d,ε) = sup/braceleftbig\ncard(P) :Pis anεpacking of ( T,d)/bracerightbig\nRecall that N(T,d,ε) denotes the ε-covering number of ( T,d).\n1. Show that\nD(T,d,2ε)≤N(T,d,ε)≤D(T,d,ε)\nLetMbe ann×mrandom matrix with entries that are i.i.d Rad(1/2) entries. We\nare interested in its operator norm\n/bardblM/bardbl= sup u⊤Mv.\nu∈I Rn:|u|2≤1\nv∈I Rm:|v|2≤1\n2. Show that\n/bardblM/bardbl ≤2 maxu⊤Mv,\nu∈Nn\nv∈Nm\nwhereNnandNmare1-nets of the unit Euclidean balls of IRnand IRm\n4respectively.\n3. Conclude that\nIE/bardblM/bardbl ≤C√/parenleftbig\nm+√n/parenrightbig\n.\nProblem 3. Chaining\nLetFbe the class of all nondecreasing functions from [0,1] to [0,1].\n1. Show that for any x= (x1,...,x n)∈[0,1]n, the covering number of ( F,dx\n∞) satisfy:\nN(F,dx 2\n∞,ε)≤n/ε.\n2. Using the chaining bound, show that\nRn(F)≤C/radicalbigg\nlogn\nn\npage 2 of 4\n3. Show that there is indeed a strict improvement over the bound obtained using the\ntheorem in section 5.2.1\nProblem 4. Kernel ridge regression\nConsider the regression model:\nYi=f(xi)+ξi, ,i= 1,...,n\nwherex1,...,x nare ﬁxed design points in IRd,ξ= (ξ1,...,ξnn)∼ N(0,Σ)∈IR with\nknown covariance matrix Σ ≻0 andf: IRd→IR is an unknown regression function.\nLetWbe an RKHS on IRdwith reproducing kernel k. Deﬁne Y= (Y1,...,Y n)⊤and\ng ˆ = [g(x1),...,g(xn)]⊤for any function g. Deﬁne the estimator foffby\nˆf= argmin/braceleftbig\nψ(Yg\ng∈W−)+µ/bardblg/bardbl2\nW/bracerightbig\nwhere/bardbl · /bardblWdenotes the Hilbert norm on W,ψ(x) =x⊤Σ−1/2xandµ >0 is a tuning\nparameter to be chosen later.\n1. Prove the representer theorem, i.e., that there exists a vector θ∈IRnsuch that\nn\nˆf(x) =/summationdisplay\nθik(xi,x),for anyx\ni=1∈IRd\nˆ ˆ 2. Prove that the vector fˆ= [f(x1),...,f(x⊤n)] satisﬁes\n(KΣ−1/2+µIn)−1/2fˆ=KΣY,\nwhereInis the identity matrix of IRnandKdenotes the symmetric n×nmatrix\nwith elements Ki,j=k(xi,xj).\n3. Prove that the following inequality holds\nψ(f−21fˆ)≤inf\ng∈W/braceleftbig\nψ(f−g)+2µ/bardblg/bardblW/bracerightbig\n+n/vextenddouble/summationdisplay/vextenddoubleZik(xi,µi=1·)/vextenddouble2/vextenddouble,W\nwhereZ1,...,Z nare iidN(0,1).\n4. Conclude that\n1IEψ(f− /bardbl2fˆ)≤inf\ng∈W/braceleftbig\nψ(f−g)+2µ/bardblgW/bracerightbig\n+Tr(K),µ\nwhereTr(K) denotes the trace of K.\npage 3 of 4\n5. Assume now that kis the Gaussian kernel:\nk(x,x′) =e−|x−x′|2\n2\nShow that there exists a choice of µfor which\nIEψ(f−fˆ) 2√\n≤ /bardblf/bardblW2n.\npage 4 of 4\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "norm", "section_heading": "18.657. Fall 2105", "source_title": "8235bec2d7664a3481322938bcff0225 MIT18 657F15 PS2", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "0e22ef62-37df-41d3-96cc-d6703b6076ee", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 22\nScribe:Aden Forrow Nov. 30, 2015\n7. BLACKWELL’S APPROACHABILITY\n7.1 Vector Losses\nDavid Blackwell introduced approachability in 1956 as a generalization of zero sum game\ntheory to vector payoﬀs. Born in 1919, Blackwell was the ﬁrst black tenured professor at\nUC Berkeley and the seventh black PhD in math in the US.\nRecall our setup for online linear optimization. At time t, we choose an action at∈∆K\nand the adversary chooses zt∈B∞(1). We then get a loss ℓ(at,zt) =/an}bracketle{tat,zt/an}bracketri}ht. In the full\ninformation case, where we observe ztand not just ℓ(at,zt), this is the same as prediction\nwith expert advice. Exponential weights leads to a regret bound\nRn≤/radicalbiggn\n2log(K).\nTh\ne setup of a zero sum game is nearly identical:\n•Player 1 plays a mixed strategy p∈∆n.\n•Player 2 plays q∈∆m.\n•Player 1’s payoﬀ is p⊤Mq.\nHereMis the game’s payoﬀ matrix.\nTheorem: Von Neumann Minimax Theorem\nmax min⊤p Mq= min max⊤p Mq.\np∈∆nq∈∆m q∈∆mp∈∆n\nThe minimax is called the value of the game. Each player can prevent the other from doing\nany better than this. The minimax theorem implies that if there is a good response pqto\nany individual q, then there is a silver bullet strategy pthat works for any q.\nCorollary: If∀q∈∆n,∃psuch that p⊤Mq≥c, then∃psuch that ∀q,p⊤Mq≥c.\nVon Neumann’s minimax theorem can be extended to more general sets. The following\ntheorem is due to Sion (1958).\nTheorem: Sion’s Minimax Theorem LetAandZbe convex, compact spaces, and\nf:A×Z→R. Iff(a,·) is upper semicontinuous and quasiconcave on Z∀a∈Aand\n1\nf(·,z) is lower semicontinuous and quasiconvex on A∀z∈Z, then\ninf supf(a,z) = sup inf f(a,z).\na∈Az∈Z z∈aZ∈A\n(Note - this wasn’t given explicitly in lecture, but we do use it later.) Quasiconvex and\nquasiconcave are weaker conditions than convex and concave respectively.\nBlackwell looked at the case with vector losses. We have the following setup:\n•Player 1 plays a∈A\n•Player 2 plays z∈Z\n•Player 1’s payoﬀ is ℓ(a,z)∈dR\nWe suppose AandZare both compact and convex, that ℓ(a,z) is bilinear, and that\n/bardblℓ(a,z)/bardbl ≤R∀a∈A,z∈Z. All norms in this section are Euclidean norms. Can we\ntranslate the minimax theorem directly to this new setting? That is, if we ﬁx a set S⊂dR,\nand if∀z∃asuch that ℓ(a,z)∈S, does there exist an asuch that ∀z ℓ(a,z)∈S?\nNo. We’ll construct a counterexample. Let A=Z= [0,1],ℓ(a,z) = (a,z), and\nS={(a,z)∈[0,1]2:a=z}. Clearly, for any z∈Zthere is an a∈Asuch that a=zand\nℓ(a,z)∈S, but there is no a∈Asuch that ∀z,a=z.\nInstead of looking for a single best strategy, we’ll play a repeated game. At time t,\nplayer 1 plays at=at(a1,z1,...,at−1,zt−1) and player 2 plays zt=zt(a1,z1,...,a t−1,zt−1).\nPlayer 1’s average loss after niterations is\n1ℓ¯n=n/summationdisplay\nℓ(at,zt)nt=1\nLetd(x,S) be the distance between a point x∈dRand the set S, i.e.\nd(x,S) = inf\ns∈S/bardblx−s/bardbl.\nIfSis convex, the inﬁmum is a minimum attained only at the projection of xinS.\nDeﬁnition: AsetSisapproachable ifthereexistsastrategy at=at(a1,z1,...,a t−1,zt−1)\n¯ such that lim n→∞d(ℓn,S) = 0.\nWhether a set is approachable depends on the loss function ℓ(a,z). In our example, we can\nchoosea0= 0 and at=zt−1to get\nn1ℓ¯limn= lim/summationdisplay\n(zt−1,zt) = (z¯,z¯)∈S.\nn→∞ n→∞nt=1\nSo thisSis approachable.\n7.2 Blackwell’s Theorem\nWe have the same conditions on A,Z, andℓ(a,z) as before.\n2\nTheorem: Blackwell’s Theorem LetSbe a closed convex set of2Rwith/bardblx/bardbl ≤R\n∀x∈S. If∀z,∃asuch that ℓ(a,z)∈S, thenSis approachable.\nMoreover, there exists a strategy such that\n2Rd ℓ¯(n,S)≤√n\nProof.We’ll prove the rate; approachability of Sfollows immediately. The idea here is to\ntransform the problem to a scalar one where Sion’s theorem applies by using half spaces.\nSuppose we have a half space H={x∈dR:/an}bracketle{tw,x/an}bracketri}ht ≤c}withS⊂H. By assumption,\n∀z∃asuch that ℓ(a,z)∈H. That is, ∀z∃asuch that /an}bracketle{tw,ℓ(a,z)/an}bracketri}ht ≤c, or\nmaxmin/an}bracketle{tw,ℓ(a,z)/an}bracketri}ht ≤c.\nz∈Z a∈A\nBy Sion’s theorem,\nminmax/an}bracketle{tw,ℓ(a,z)\na∈A z∈Z/an}bracketri}ht ≤c.\nSo∃a∗\nHsuch that ∀z ℓ(a,z)∈H.\nThis works for any Hcontaining S. We want to choose Htso thatℓ(at,zt) brings the\n¯averageℓ ¯tcloser to Sthanℓt−1. An intuitive choice is to have the hyperplane Wbounding\nH ¯tbe the separating hyperplane between Sandℓt−1closest to S. This is Blackwell’s\n¯ strategy: let Wbe the hyperplane through πt∈argminµ∈S/bardblℓt−1−µ/bardblwith normal vector\nℓ¯t−1−πt. Then\nH={x∈dR:/an}bracketle{tx−πt,ℓ¯t−1−πt/an}bracketri}ht ≤0}.\nFinda∗\nHand play it.\nWe need one more equality before proving convergence. The average loss can be ex-\npanded:\nt1ℓ¯t t−1 tt t\nt=−1 t1 1¯(ℓt−1−πt)+−πt+ℓtt t t\nNow we look at the distance of the average from S, using the above equation and the\ndeﬁnition of πt+1:\nd¯(ℓ ,S)2ℓ¯t=/bardblt−πt+1/bardbl2\n≤ /bardblℓ¯t−πt/bardbl2\n=/vextenddouble/vextenddouble/vextenddouble2t−1 1¯(ℓt−1−πt)+ (ℓt−πt)t/vextenddouble\n/vextenddoublet\nℓt/vextenddouble\n=/parenleftbiggt−1/parenrightbigg2\nd¯(ℓ2 π2tt1¯t−1,S) +/bardbl −/vextenddouble/vextenddouble\n/bardbl+2−/an}bracketle{tℓt−πt,ℓt t2 t2t−1−πt/an}bracketri}ht\nSinceℓt∈H, the last term is negative; since ℓtandπtare both bounded by R, the middle\n2term is bounded by4R\n2. Letting µ2\nt=t2d(ℓ¯2t,S) , we have a recurrence relationt\nµ2\nt≤µ2\nt−1+4R2,\n3=−¯ℓ+1ℓ\nimplying\nµ2\nn≤4nR2.\nRewriting in terms of the distance gives the desired bound,\n2Rd ℓ¯(t,S)≤√n\nNote that this proof fails for nonconvex S.\n7.3 Regret Minimization via Approachability\nConsider the case A= ∆KK,Z=B∞(1). As we showed before, exponential weights Rn≤\nc/radicalbig\nnlog(K). We can get the same dependence on nwith an approachability-based strategy.\nFirst recall that\nn n1 1Rn=/summationdisplay 1ℓ(at,zt)−min ℓ(ej,zt)n n jnt=1/summationdisplay\nt=1\nn n\n= m x/bracketleftBigg\n1a/summationdisplay 1ℓ(at,zt) ℓ(ej,zt)\njn nt=1−/summationdisplay\nt=1/bracketrightBigg\nIf we deﬁne a vector average loss\nn1ℓ¯n=/summationdisplay\n(ℓ(at,zt)−ℓ(e1,zt),...,ℓ(aKR t,zt)e\nt=1−ℓ(K,zt))n∈,\nRn ¯ ¯\nn→0 if and only if all components of ℓnare nonpositive. That is, we need d(−ℓn,OK)→0,\nwhere−O={x∈KR:−1≤xi≤0, iK∀ }is the nonpositive orthant. Using Blackwell’s\napproachability strategy, we get\nRn≤d(ℓ¯−n,O/radicalbigg\nK)≤c .nKn\nTheKdependence is worse than exponential weights,√\nKinstead of log( K).\nHow do we ﬁnd a∗\nH? As a concrete example, let K= 2. We need a/radicalbig\n∗\nHtp satisfy\n/an}bracketle{t∗w,ℓ(∗aH,z)/an}bracketri}ht=/an}bracketle{tw,/an}bracketle{taH,z/an}bracketri}hty−z/an}bracketri}ht ≤c\nfor allz. Hereyis the vector of all ones. Note that c≥0 since 0 is in Sand therefore in\nH. Rearranging,\n/an}bracketle{t∗aH,z/an}bracketri}ht/an}bracketle{tw,y/an}bracketri}ht ≤ /an}bracketle{tw,z/an}bracketri}ht+c,\nChoosing a∗\nH=wwill work; the inequality reduces to/angbracketleftw,y/angbracketright\n/an}bracketle{tw,z/an}bracketri}ht ≤ /an}bracketle{tw,z/an}bracketri}ht+c.\nApproachability in the banditsetting with only partial feedback is still an openproblem.\n4\nReferences\n[Bla56] D. Blackwell, An analog of the minimax theorem for vector payoﬀs , Paciﬁc J.\nMath. 6 (1956), no. 1, 1–8\n[Sio58] M. Sion, On general minimax theorems . Paciﬁc J. Math. 8 (1958), no. 1, 171–176.\n5\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "convex", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "b21de17384706de8db8078cd767d459e MIT18 657F15 L22", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "63bf2d85-59aa-4e09-8b2e-697775a303cf", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 8\nScribe:Quan Li Oct. 5, 2015\nPart II\nConvexity\n1. CONVEX RELAXATION OF THE EMPIRICAL RISK MINIMIZATION\nˆ In the previous lectures, we have proved upper bounds on the excess risk R(herm)−R(h∗)\nof the Empirical Risk Minimizer\nˆherm 1= argmin\nh∈Hn\n1I(Yi=h(Xi)). (1.1)n/summationdisplay\ni=1/ne}ationslash\nHowever due to the nonconvexity of the objective function, the optimization problem\n(1.1) in general can not be solved eﬃciently. For some choices of Hand the classiﬁcation\nerror function (e.g. 1I( ·)), the optimization problem can be NP-hard. However, the problem\nwe deal with has some special features:\n1. Since the upper bound we obtained on the excess risk is O(/radicalBig\ndlogn), we only need ton\napproximate the optimization problem with error up to O(/radicalBig\ndlogn\nn).\n2. The optimization problem corresponds to the average case problem where the data\ni.i.d(Xi,Yi)∼PX,Y.\n3.Hcan be chosen to be some ’natural’ classiﬁers, e.g. H={half spaces }.\nThese special features might help us bypass the computational issue. Computational\nissueinmachinelearninghavebeenstudiedforquitesometime(see, e.g. [Kea90]), especially\nin the context of PAC learning. However, many of these problems are somewhat abstract\nand do not shed much light on the practical performance of machine learning algorithms.\nToavoid thecomputational problem, thebasicideais to minimizea convex upperbound\nof the classiﬁcation error function 1I( ·) in (1.1). For the purpose of computation, we shall\nalso require that the function class Hbe a convex set. Hence the resulting minimization\nbecomes a convex optimization problem which can be solved eﬃciently.\n1.1 Convexity\nDeﬁnition: A setCis convex if for all x,y∈Candλ∈[0,1],λx+(1−λ)y∈C.\n1\nDeﬁnition: A function f:D→IR on a convex domain Dis convex if it satisﬁes\nf(λx+(1−λ)y)≤λf(x)+(1−λ)f(y),∀x,y∈D,andλ∈[0,1].\n1.2 Convex relaxation\nThe convex relaxation takes three steps.\nStep 1: Spinning.\nUsing a mapping Y/ma√sto→2Y−1, the i.i.d. data ( X1,Y1),(X2,Y2),...,(Xn,Yn) is transformed\nto lie inX ×{−1,1}. These new labels are called spinnedlabels. Correspondingly, the task\nbecomes to ﬁnd a classiﬁer h:X /ma√sto→ {−1,1}. By the relation\nh(X)/ne}ationslash=Y⇔−h(X)Y >0,\nwe can rewrite the objective function in (1.1) by\nn n1/summationdisplay 11I(h(Xi) =Yi) =/summationdisplay\nϕ1 I(hn ni=1 i=1−(Xi)Yi) (1.2)\nwhereϕ1 I(z) = 1I(z >0).\nStep 2: Soft classiﬁers.\nThe setHof classiﬁers in (1.1) contains only functions taking values in {−1,1}. As a result,\nit is non convex if it contains at least two distinct classiﬁers. Soft classiﬁers provide a way\nto remedy this nuisance.\nDeﬁnition: Asoft classiﬁer is any measurable function f:X →[−1,1]. The hard\nclassiﬁer (or simply “classiﬁer”) associated to a soft classiﬁer fis given by h= sign(f).\nLetF ⊂IRXbe aconvexset soft classiﬁers. Several popular choices for Fare:\n•Linear functions:\nF:={/an}bracketle{ta,x/an}bracketri}ht:a∈ A}.\nfor some convex set A ∈IRd. The associated hard classiﬁer h= sign(f) splits IRdinto\ntwo half spaces.\n•Majority votes: given weak classiﬁers h1,...,h M,\nM M\nF:=/braceleftBig/summationdisplay\nλjhj(x) :λj\nj=≥0,/summationdisplay\nλj= 1/bracerightBig\n.\n1 j=1\n•Letϕj,j= 1,2,...a family of functions, e.g., Fourier basis or Wavelet basis. Deﬁne\n∞\nF:={/summationdisplay\nθjϕj(x) : (θ1,θ2,...)\nj=1∈Θ},\nwhere Θ is some convex set.\n2/ne}ationslash\nStep 3: Convex surrogate.\nGiven a convex set Fof soft classiﬁers, using the rewriting in (1.2), we need to solve that\nminimizes the empirical classiﬁcation error\n1min\nf∈Fn\nϕ1 I(f(Xi)Yi),n/summationdisplay\ni=1−\nHowever, while we are now working with a convex constraint, our objective is still not\nconvex: we need a surrogate for the classiﬁcation error.\nDeﬁnition: A function ϕ: IR/ma√sto→IR+is called a convex surrogate if it is a convex\nnon-decreasing function such that ϕ(0) = 1 and ϕ(z)≥ϕ1 I(z) for allz∈IR.\nThe following is a list of convex surrogates of loss functions.\n•Hinge loss: ϕ(z) = max(1+ z,0).\n•Exponential loss: ϕ(z) = exp(z).\n•Logistic loss: ϕ(z) = log2(1+exp( z)).\nTo bypass the nonconvexity of ϕ1 I(·), we may use a convex surrogate ϕ(·) in place of\nˆ ϕ1 I(·) and consider the minimizing the empirical ϕ-riskRn,ϕdeﬁned by\n1ˆRn,ϕ(f) =nn/summationdisplay\ni=1ϕ(−Yif(Xi))\nI\nt is the empirical counterpart of the ϕ-riskRϕdeﬁned by\nRϕ(f) = IE[ϕ(−Yf(X))].\n1.3ϕ-risk minimization\nIn this section, we will derive the relation between the ϕ-riskRϕ(f) of a soft classiﬁer fand\nthe classiﬁcation error R(h) = IP(h(X) =Y) of its associated hard classiﬁer h= sign(f)\nLet\nf∗\nϕ= argmin E[ϕ(Y\nf∈IRX−f(X))]\nwhere the inﬁmum is taken over all measurable functions f:X →IR.\nTo verify that minimizing the ϕserves our purpose, we will ﬁrst show that if the convex\nsurrogate ϕ(·) is diﬀerentiable, then sign( f∗\nϕ(X))≥0 is equivalent to η(X)≥1/2 where\nη(X) = IP(Y= 1|X). Conditional on {X=x}, we have\nIE[ϕ(−Yf(X))|X=x] =η(x)ϕ(−f(x))+(1−η(x))ϕ(f(x)).\nLet\nHη(α) =η(x)ϕ(−α)+(1−η(x))ϕ(α) (1.3)/ne}ationslash\n3\nso that\nf∗\nϕ(x) = argmin H∗η(α),andRϕ= minRϕ(f) = minHη)\nα f∈IRX(x)(α .\nα∈IR ∈IR\nSinceϕ(·) is diﬀerentiable, setting thederivative of H∗η(α) to zero gives fϕ(x) =α¯, where\nH′\nη(α¯) =−η(x)ϕ′(−α¯)+(1−η(x))ϕ′(α¯) = 0,\nwhich gives\nη(x)ϕ′(α¯)=1−η(x)ϕ′(−α¯)\nSinceϕ(·)isaconvex function, itsderivative ϕ′(·) isnon-decreasing. Thenfromtheequation\nabove, we have the following equivalence relation\n1η(x)≥ ⇔α¯≥0⇔sign(f∗\n2ϕ(x))≥0. (1.4)\nSince the equivalence relation holds for all x∈ X,\n1η(X)≥ ⇔sign(f∗\nϕ(X))2≥0.\nThe following lemma shows that if the excessϕ-riskR(f)−R∗ϕ ϕof a soft classiﬁer fis\nsmall, then the excess-risk of its associated hard classiﬁer sign( f) is also small.\nLemma (Zhang’s Lemma [Zha04]): Letϕ: IR/ma√sto→IR+be a convex non-decreasing\nfunction such that ϕ(0) = 1. Deﬁne for any η∈[0,1],\nτ(η) := inf Hη(α).\nα∈IR\nIf there exists c >0 andγ∈[0,1] such that\n1|η−c2| ≤(1−τ(η))γ,∀η∈[0,1], (1.5)\nthen\nR(sign(f))−R∗≤2c(Rϕ(f)−R∗\nϕ)γ\nProof.Note ﬁrst that τ(η)≤Hη(0) =ϕ(0) = 1 so that condition (2.5) is well deﬁned.\nNext, let h∗= argminh∈{−1,1}XIP[h(X) =Y] = sign(η−1/2) denote the Bayes classiﬁer,\nwhereη= IP[Y= 1|X=x], . Then it is easy to verify that\nR(sign(f))−R∗= IE[|2η(X)−1|1I(sign(f(X)) =h∗(X))]\n= IE[|2η(X)−1|1I(f(X)(η(X)−1/2)<0)]\n≤2cIE[((1−τ(η(X)))1I(f(X)(η(X)−1/2)<0))γ]\n≤2c(IE[(1−τ(η(X)))1I(f(X)(η(X)−1/2)<0)])γ,\nwhere the last inequality above follows from Jensen’s inequality.\n4/ne}ationslash\n/ne}ationslash\nWe are going to show that for any x∈ X, it holds\n(1−τ(η))1I(f(x)(η(x)−1/2)<0)]≤IE[ϕ(−Yf(x))|X=x]−R∗\nϕ.(1.6)\nThis will clearly imply the result by integrating with respect to x.\nRecall ﬁrst that\nIE[ϕ(−Yf(x))|X=x] =Hη(x)(f(x)) and R∗\nϕ= minHη(x)(α) =τ(η(x)).\nα∈IR\nso that (2.6) is equivalent to\n(1−τ(η))1I(f(x)(η(x)−1/2)<0)]≤Hη(x)(α)−τ(η(x))\nSince the right-hand side above is nonnegative, the case where f(x)(η(x)−1/2)≥0 follows\ntrivially. If f(x)(η(x)−1/2)<0, (2.6) follows if we prove that Hη(x)(α)≥1. The convexity\nofϕ(·) gives\nHη(x)(α) =η(x)ϕ(−f(x))+(1−η(x))ϕ(f(x))\n≥ϕ(−η(x)f(x)+(1−η(x))f(x))\n=ϕ((1−2η(x))f(x))\n≥ϕ(0) = 1,\nwhere the last inequality follows from the fact that ϕis non decreasing and f(x)(η(x)−\n1/2)<0. This completes the proof of (2.6) and thus of the Lemma.\nIT is not hard to check the following values for the quantities τ(η),candγfor the three\nlosses introduced above:\n•Hinge loss: τ(η) = 1−|1−2η|withc= 1/2 andγ= 1.\n•Exponential loss: τ(η) = 2/radicalbig\nη(1−η) withc= 1/√\n2 andγ= 1/2.\n•Logistic loss: τ(η) =−ηlogη−(1−η)log(1−η) withc= 1/√\n2 andγ= 1/2.\nReferences\n[Kea90] Michael J Kearns. The computational complexity of machine learning . PhD thesis,\nHarvard University, 1990.\n[Zha04] Tong Zhang. Statistical behavior and consistency of classiﬁcation methods based\non convex risk minimization. Ann. Statist. , 32(1):56–85, 2004.\n5\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "convex", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "a86e2ffbc8a505b53f9051b60587763c MIT18 657F15 L8", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "507aab47-babf-4d40-bc60-25c70af47728", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 7\nScribe: Zach Izzo Sep. 30, 2015\nIn this lecture, we continue our discussion of covering numbers and compute upper\n^ bounds for speci\fc conditional Rademacher averages Rx\nn(F). We then discuss chaining and\nconclude by applying it to learning.\nRecall the following de\fnitions. We de\fne the risk function\nR(f) = I E[`(X;f (X))]; (X;Y )2X\u0002 [\u00001;1];\nfor some loss function `(\u0001;\u0001). The conditiona Rademacher average that we need to control\nis\nn1R(`l\u000eF) = sup I E sup \u001bi`(yi;f(xi)):\n(x1;y1);:::;(x n;yn)\"\nf\f\f\f\f\fn2FX\ni=1\f\f#\nFurthermore, we de\fned the conditional Rademacher average for a poin\f\f\ntx= (x1;:::;x n)\nto be\f\nR^x\nn(F) = I E\"\nsup\nf2F\f\f n1\u001bif(xi)ni=1\f\f#\n:\nLastly, we de\fne the \"-covering number N(X\nF;d;\f\f\n\") to be the m\f\f\ninimum number of balls (with\nrespect to the metric d) of radius\"needed to\f\ncover\f\nF. We proved the following theorem:\nTheorem: Assumejfj\u00141 for allf2F. Then\n2 log(2N(;dx;\"))R^x\nn(F) +r\nF\u0014inf1\n\">0(\n\"n)\n;\nwheredx\n1is given by\nn\ndx 1\n1(f;g) =nX\ni=1jf(xi)\u0000g(xi)j:\nWe make use of this theorem in the following example. De\fne Bd\np=fx2I Rd:jxjp\u00141g.\nThen takef(x) =ha;xi, setF=fha;\u0001i:a2Bdg, andX=Bd\n1. By H older's inequality,1\nwe have\njf(x)j\u0014jajx1j j1\u00141;\nso the theorem above holds. We need to compute the covering number N(F;dx\n1;\"). Note\nthat for all a2Bd, there exists v= (v1;:::;v n) such that vi=g(xi) and1\nn1\nnX\na;xivi\"\ni=1jh i\u0000 j\u0014\nfor some function g. For this case, we will take g(x) =hb;xi, sovi=hb;x ii. Now, note the\nfollowing. Given this de\fnition of g, we have\nn n\ndx 1 1\n1(f;g) = a;x 1b;xi=a b;x ia bnX\nni=1jh i\u0000h ijX\ni=1jh \u0000 ij\u0014j \u0000 j 1\n1\nby H older's inequality and the fact that jxj1= 1. So ifja\u0000bj1\u0014\", we can take vi=hb;x ii.\nWe just need to \fnd a set of fb1;:::;b Mg\u001aI Rdsuch that, for any athere exists bjsuch\nthatja\u0000bjj<1. We can do this by dividing Bdinto cubes with side length \"and 1 1\ntaking theb's to be the set of vertices of these cubes. Then any a2Bdj must land in one1\nof these cubes, so ja\u0000bjj \u0014\"as desired. There are c=\"dof suchb 1 j's for some constant\nc>0. Thus\nN(Bd;dx\n1 1;\")\u0014c=\"d:\nWe now plug this value into the theorem to obtain\nR^x 2 log(c=\"d)\nn(F)\u0014inf :\n\"(\n\"+\n\u00150r\nn)\nOptimizing over all choices of \"gives\nr\ndlog(n)\"\u0003=cn) R ^x\nn(F)\u0014cr\ndlog(n):n\nNote that in this \fnal inequality, the conditional empirical risk no longer depends on\nx, since we \\sup'd\" xout of the bound during our computations. In general, one should\nignorexunless it has properties which will guarantee a bound which is better than the sup.\nAnother important thing to note is that we are only considering one granularity of Fin our\n\fnal result, namely the one associated to \"\u0003. It is for this reason that we pick up an extra\nlog factor in our risk bound. In order to remove this term, we will need to use a technique\ncalled chaining.\n5.4 Chaining\nWe have the following theorem.\nTheorem: Assume thatjfj\u00141 for allf2F. Then\n\u001a12Z1\nR^x\nn\u0014inf 4\" +\n\">0p log(N (;dx))dt :n2;t\n\"q\nF\u001b\n(Note that the integrand decays with t.)\nProof. Fixx= (x1;:::;x n), and for all j= 1;:::;N , letVjbe a minimal 2\u0000j-net ofF\nunder thedx\n2metric. (The number Nwill be determined later.) For a \fxed f2F, this\nprocess will give us a \\chain\" of points fi\u000ewhich converges to f:dx\n2(fi\u000e;f)\u00142\u0000j.\nDe\fneF=f(f(x1);:::;f (xn))>; f2Fg\u001a [\u00001;1]n. Note that\nR^x 1\nn(F) = I E supn f2Fh\u001b;fi\nwhere\u001b= (\u001b1;:::;\u001b n). Observe that for all N, we can rewriteh\u001b;fias a telescoping sum:\nh\u001b;fi=h\u001b;f\u0000fN\u000ei+h\u001b;fN\u000e\u0000fN\u000e\n\u00001i+:::+h\u001b;f1\u000e\u0000f0\u000ei\n2\nwheref0\u000e:= 0. Thus\nN\nR^x 1 1\nn(F)\u0014I E supjh\u001b;f\u0000fN\u000eij+ I fn f2FX\nE sup\u001b;nj\u000efj\u000e\nf F\u00001:\nj=1jh \u0000 ij\n2\nWe can control the two terms in this inequality separately. Note \frst that by the Cauchy-\nSchwarz inequality,\n1 dx\n2(f;fN\u000e)I E supjh\u001b;f\u0000fN\u000eij\u0014j\u001bj2n fp:n 2F\nSincej\u001bj2=pnanddx\n2(f;fN\u000e)\u00142\u0000N, we have\n1I E supjh\u001b;f\u0000fN\u000e2n f2Fij\u0014\u0000N:\nNow we turn our attention to the second term in the inequality, that is\nN\nS=X1I E supjh\u001b;fj\u000e\u0000fj\u000e\nn fj=12F\u00001ij:\nNote that since fj\u000e2Vjandfj\u000e\n\u000012Vj V \u00001, there are at most jjjjVj\u00001jpossible di\u000berences\nfj\u000e\u0000fj\u000e.\u00001SincejV2j1j\u0014jV\u0000 jj=2,jVjjjVj1j\u0014jVjj=2 and we \fnd ourselves in the \fnite\u0000\ndictionary case. We employ a risk bound from earlier in the course to obtain the inequality\np\n2 log(2Rn(B)\u0014max\nb Bjbj2jBj):\n2 n\nIn the present case, B=ffj\u000e\u0000fj\u000e\n\u00001;f2Fgso thatjBj\u0014jV jj2=2. It yields\n2j22 log(jVj) logR2 Vj\nn(B)j\u0001q\nj\u0014r = 2rn\u0001p\n;n\nwherer= supf2Fjfj\u000e\u0000fj\u000e\n\u00001j2. Next, observe that\njfj\u000e\u0000fj\u000e\n1j2=pn d\u0000\u0001x\n2(fj\u000e;fj\u000e\n\u00001)p p\u0014n(dx\n2(fj\u000e;f) +dx\n2(f;fj\u000e)) 3 2\u0000jn:\u00001\u0014 \u0001\nby the triangle inequality and the fact that dx(f\u000e;f)\u00142\u0000j\n2j . Substituting this back into our\nbound forRn(B), we have\nlog(B)jVj6 2\u0000jnr\njj ;2j ( ))\u0014 \u0001 = 6n\u0001\u0000r\nlog(NFdx\n2;2\u0000j\nRn\nsinceVjjwas chosen to be a minimal 2\u0000-net.\nThe proof is almost complete. Note that 2\u0000j= 2(2\u0000j\u00002\u0000j\u00001) so that\nN6pXN122\u0000jq\nlog(N (F;dx\n2;2\u0000j)) =pX\n(2\u0000j\u00002\u0000j\u00001)q\nlog(N (F;dx\n2;2\u0000j)):n nj=1 j=1\nNext, by comparing sums and integrals (Figure 1), we see that\nXN\n(2\u0000j\nj=1\u00002\u0000j\u00001)q 1\nlog(N (F;dx\n2;2\u0000j))\u0014Z=2\nlog(N (;dx\n2;t))dt:\n2\u0000(N+1)q\nF\n3\nFigure 1: A comparison of the sum and integral in question.\nSo we choose Nsuch that 2\u0000(N +2)\u0014\"\u00142\u0000(N +1), and by combining our bounds we obtain\n121=2 1\nR^x)\u00142\u0000N\nn(F +pnZq\nlog(N (F;dx\n2;t))dt\n2\u0000( +1)\u00144\"+\nNZp\nlog(N;\n\"F;t)dt\nsince the integrand is non-negative. (Note: this integral is known as the \\Dudley Entropy\nIntegral.\")\nReturning to our earlier example, since N(F;dx\n2;\")\u0014c=\"d, we have\n1\nR^x\nn(F)\u0014inf\u001a124\"+pZq\nlog((c0=t)d)dt\n\">0 n\"\u001b\n:\nSinceR1p\nlog(c=t)dt =c\u0016 is \fnite, we then have0\nR^x\nn(F)\u001412c\u0016p\nd=n:\nUsing chaining, we've been able to remove the log factor!\n5.5 Back to Learning\nWe want to bound\nn1Rn(`\u000eF) = sup I E sup \u001bi`(yi;f(xi)):\n(x1;y1);:::;(x n;yn)\"\nf2F\f\f\f\nnX\ni\nx\f\n=1\f\f#\n\f\nR^ We considern(\b\u000eFn) = I E i\b\f\n\u0002\nsupf\f\f1P\ni=1\u001b\u000ef(x)2F ifor someLn\f\f\n-Lipschitz function\n\b, that isj\b(a)\u0000\b(b)j\u0014Lja\u0000bjfor alla;b2[\u00001;1]. We\f\f\u0003\nhave the following lemma.\n4\nTheorem: (Contraction Inequality) Let \b be L-Lipschitz and such that \b(0) = 0,\nthen\nR^x\nn(\b\u000eF)\u0014 R ^ 2L\u0001x\nn(F):\nThe proof is omitted and the interested reader should take a look at [LT91, Kol11] for\nexample.\nAs a \fnal remark, note that requiring the loss function to be Lipschitz prohibits the use\nofR-valued loss functions, for example `(Y;\u0001) = (Y\u0000\u0001)2:\nReferences\n[Kol11] Vladimir Koltchinskii. Oracle inequalities in empirical risk minimization and sparse\n\u0013 \u0013 recovery problems. Ecole d'Et\u0013 e de Probabilit\u0013 es de Saint-Flour XXXVIII-2008. Lec-\nture Notes in Mathematics 2033. Berlin: Springer. ix, 254 p. EUR 48.10 , 2011.\n[LT91] Michel Ledoux and Michel Talagrand. Probability in Banach spaces, volume 23 of\nErgebnisse der Mathematik und ihrer Grenzgebiete (3) [Results in Mathematics and\nRelated Areas (3)] . Springer-Verlag, Berlin, 1991. Isoperimetry and processes.\n5\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "erm", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "af336bcec45f238d85d84cea1a04fd3b MIT18 657F15 L7", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "fa027d37-408c-490c-b122-3662e634b208", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 14\nScribe:Sylvain Carpentier Oct. 26, 2015\nIn this lecture we will wrap up the study of optimization techniques with stochastic\noptimization. The tools that we are going to develop will turn out to be very eﬃcient in\nminimizing the ϕ-risk when we can bound the noise on the gradient.\n3. STOCHASTIC OPTIMIZATION\n3.1 Stochastic convex optimization\nWeareconsideringrandomfunctions x/ma√sto→ℓ(x,Z)wherexistheoptimization parameterand\nZa random variable. Let PZbe the distribution of Zand let us assume that x/ma√sto→ℓ(x,Z) is\nconvexPZa.s. In particular, IE[ ℓ(x,Z)] will also be convex. The goal of stochastic convex\noptimization is to approach min xIE[ℓ(x,Z)] when is convex. For our purposes, will ∈C C C\nbe a deterministic convex set. However, stochastic convex optimization can be deﬁned more\nbroadly. The constraint can be itself stochastic :\nC={x,IE[g(x,Z)]≤0}, gconvexPZa.s.\nC={x,IP[g(x,Z)≤0]≥1−ε},“chance constraint”\nThe second constraint is not convex a priori but remedies are possible (see [NS06, Nem12]).\nIn the following, we will stick to the case where Xis deterministic. A few optimization\nproblems we tackled can be interpreted in this new framework.\n3.1.1 Examples\nBoosting. Recall that the goal in Boosting is to minimize the ϕ-risk:\nminIE[ϕ(f\n∈Λ−Yα(X))],\nα\nwhere Λ is the simplex of IRd. Deﬁne Z= (X,Y) and the random function ℓ(α,Z) =\nϕ(−Yfα(X)), convex PZa.s.\nLinear regression. Here the goal is the minimize the ℓ2risk:\nmin IE[(Y−f2α(X)) ].\nα∈IRd\nDeﬁneZ= (X,Y) and the random function ℓ(α,Z) = (Y−fα(X))2, convex PZa.s.\nMaximum likelihood. We consider samples Z1,...,Z niid with density pθ,θ∈Θ. For\ninstance, ZN(θ,1). The likelihood functions associated to this set of samples is θ ∼ /ma√sto→/producttextn\n=1pθ(Zi). Letp∗(Z) denote the true density of Zi (it does not have to be of the form pθ\nforsomeθ∈Θ. Then\nn1/productdisplay/integraldisplayp∗(z)IE[log pθ(Zi)] =−log( ) p∗(z)dz+C=n pθ(z)i=1−KL(p∗,pθ)+C\n1\nwhereCis a constant in θ. Hence maximizing the expected log-likelihood is equivalent to\nminimizing the expected Kullback-Leibler divergence:\nn\nmaxIE[log/productdisplay\npθ(Zi)]\nθi=1⇐⇒KL(p∗,pθ)\nExternal randomization. Assume that we want to minimize a function of the form\n1f(x) =n/summationdisplay\nfi(x),ni=1\nwhere the functions f1,...,fnare convex. As we have seen, this arises a lot in empirical\nrisk minimization. In this case, we treat this problem as deterministic problem but inject\nartiﬁcial randomness as follows. Let Ibe a random variable uniformly distributed on\n[n] =:{1,...,n}. We have the representation f(x) = IE[fI(x)], which falls into the context\nof stochastic convex optimization with Z=Iandℓ(x,I) =fI(x).\nImportant Remark :There is a key diﬀerence between the case where we assume that\nwearegivenindependentrandomvariablesandthecasewherewegenerateartiﬁcialran-\ndomness. LetusillustratethisdiﬀerenceforBoosting. Wearegiven( X1,Y1),...,(Xn,Yn)\ni.i.d from some unknown distribution. In the ﬁrst example, our aim is to minimize\nIE[ϕ(−Yfα(X))] based on these nobservations and we will that the stochastic gradient\nallows to do that by take one pair ( Xi,Yi) in each iteration. In particular, we can use\neach pair at most once. We say that we do one pass on the data.\nWe could also leverage our statistical analysis of the empirical risk minimizer from\nprevious lectures and try to minimize the empirical ϕ-risk\n1Rˆn,ϕ(fα) =n/summationdisplay\nϕ(α\ni=1−Yif(Xi))n\nby generating kindependent random variables I1,...,Ikuniform over [ n] and run the\nstochasticgradientdescenttousonerandomvariable Ijineachiteration. Thediﬀerence\nhereisthat kcanbearbitrarylarge, regardlessofthenumber nofobservations(wemake\nmultiple passes onthedata). However, minimizingIE I[ϕ(−YIfα(XI))|X1,Y1,...,X n,Yn]\nwill perform no better than the empirical risk minimizer whose statistical performance\nis limited by the number nof observations.\n3.2 Stochastic gradient descent\nIf the distribution of Zwas known, then the function x/ma√sto→IE[ℓ(x,Z)] would be known and\nwe could apply gradient descent, projected gradient descent or any other optimization tool\nseen before in the deterministic setup. However this is not the case in reality where the\ntrue distribution PZis unknown and we are only given the samples Z1,...,Z nand the\nrandom function ℓ(x,Z). In what follows, we denote by ∂ℓ(x,Z) the set of subgradients of\nthe function y/ma√sto→ℓ(y,Z) at point x.\n2\nAlgorithm 1 Stochastic Gradient Descent algorithm\nInput:x1∈ C, positive sequence {ηs}s1, independent random variables Z ,...,Z ≥ 1 k\nwith distribution PZ.\nfors= 1 tok−1do\nys+1=xs−ηsg˜s, g˜s∈∂ℓ(xs,Zs)\nxs+1=π(yCs+1)\nend for\n1return x¯k=k\nk/summationdisplay\nxs\ns=1\nNote the diﬀerence here with the deterministic gradient descent which returns either\nx¯korx◦\nk= argmin f(x). In the stochastic framework, the function f(x) = IE[ℓ(x,ξ)] is\nx1,...,xn\ntypically unknown and x˚kcannot be computed.\nTheorem: LetCbea closed convex subset of IRdsuch that diam( C)≤R. Assume that\nhe convex function f(x) = IE[ℓ(x,Z)] attains its minimum on Catx∗∈IRd. Assume\nthatℓ(x,Z) is convex PZa.s. and that IE /ba∇dblg˜/ba∇dbl2≤L2for allg˜∈∂ℓ(x,Z) for allx. Then\nifηs≡η=R\nL√,k\nLRIE[f(x¯k)]−f(x∗)≤√\nk\nProof.\nf xsf x g sxsx\n= IE[g˜s⊤(xs−x∗)|xs]\n1= IE[(ys+1xs)⊤(xsx∗)xs]η− − |\n1=IE[/ba∇dblx2 2s−y2s+1/ba∇dbl+η/ba∇dblxs−x∗\n2/ba∇dbl −/ba∇dblys+1−x∗/ba∇dbl |xs]\n1≤(η2IE[/ba∇dblg˜s/ba∇dbl2|xs]+IE[/ba∇dblx2s−x∗/ba∇dbl |xs]−IE[/ba∇dblxs+1−x∗xη/ba∇dbl2\n2|s]\nTaking expectations and summing over swe get\nk1/summationdisplay ηL2R2\nf(xs) (\ns=1−f x∗)k≤+.2 2ηk\nUsing Jensen’s inequality and chosing η=R\nL√, we getk\nLRIE[f(x¯k)]−f(x∗)≤√\nk\n3( )−(∗)≤⊤(−∗)\n3.3 Stochastic Mirror Descent\nWe can also extend the Mirror Descent to a stochastic version as follows.\nAlgorithm 2 Mirror Descent algorithm\nInput: x1∈argmin Φ( x),ζ:dR→dRsuch that ζ(x) =∇Φ(x), independentC∩D\nrandom variables Z1,...,Z kwith distribution PZ.\nfors= 1,···,kdo\nζ(ys+1) =ζ(xs)−ηg˜sforg˜s∈∂ℓ(xs,Zs)\nxΦs+1= Π (yCs+1)\nend for\nreturn x=1k\nk/summationtext\ns=1xs\nTheorem: Assume that Φ is α-strongly convex on C ∩ Dw.r.t./ba∇dbl·/ba∇dbland\nR2= sup Φ( x) Φ x)\nx−min (\n∈C∩D x∈C∩D\ntakex1= argminxΦ(x) (assume that it exists). Then, Stochastic Mirror Descent∈C∩D\nwithη=R\nL/radicalBig\n2αxRoutputs ¯ k, such that\nIE[f(x¯k)]−f(x∗)≤RL/radicalbigg\n2.αk\nProof.We essentially reproduce the proof for the Mirror Descent algorithm.\nTakex♯∈ C ∩D. We have\nf(xs ss\nIE[g˜s⊤(xs−x∗)|xs]\n1=IE[(ζ(xs)η−ζ(ys+1))⊤(xs−x♯)|xs]\n1=IE[(∇Φ(xs)−∇Φ(ys+1))⊤(xsη−x♯)|xs]\n1=IE/bracketleftBig\nD♯Φ(xs,ys+1)+DΦ(x♯,xs)−DΦ(x ,ys+1)η/vextendsingle/vextendsinglexs/bracketrightBig\n1≤IE/bracketleftBig\nDΦ(x♯s,ys+1)+DΦ(x ,xs)−DΦ(x♯,xs+1)xη/vextendsingle/vextendsingles/bracketrightBig\nη≤2IE[/ba∇dblg˜s/ba∇dbl21|xs]+ IE2α∗η/bracketleftBig\nDΦ(x♯,xs)−DΦ(x♯,xs+1)/vextendsingle/vextendsinglexs/bracketrightBig)−f(x♯)≤g⊤(x−x♯)\n4\nwhere the last inequality comes from\nDΦ(xs,ys+1) = Φ(xs)−Φ(ys+1)−∇Φ(ys+1)⊤(xs−ys+1)\nα≤[∇Φ(x2s)−∇Φ(ys+1)]⊤(xs−ys+1)−2/ba∇dblys+1−xs/ba∇dbl\nα≤η/ba∇dblg˜s/ba∇dblx y y x2∗/ba∇dbls−s+1/ba∇dbl−2/ba∇dbls+1−s/ba∇dbl\nη2/ba∇dblg˜≤s/ba∇dbl2\n∗.2α\nSumming and taking expectations, we get\nk1/summationdisplay ηL2DΦ(x♯,x1)[f(x♯s) ]\ns=1≤+k−f(x) . (3.1)2α kη\nWe conclude as in the previous lecture.\n3.4 Stochastic coordinate descent\nLetfbe a convex L-Lipschitz and diﬀerentiable function on IRd. Let us denote by ∇ifthe\npartial derivative of fin the direction ei. One drawback of the Gradient Descent Algorithm\nis that at each step one has to update every coordinate ∇ifof the gradient. The idea of\nthe stochastic coordinate descent is to pick at each step a direction ejuniformly and to\nchoose that ejto be the direction of the descent at that step. More precisely, of Iis drawn\nuniformly on [ d], then IE[ d∇If(x)eI] =∇f(x). Therefore, the vector d∇If(x)eIthat has\nonly one nonzero coordinate is an unbiased estimate of the gradient ∇f(x). We can use\nthis estimate to perform stochastic gradient descent.\nAlgorithm 3 Stochastic Coordinate Descent algorithm\nInput: x1∈ C, positive sequence {ηs}s1, independent random variables I ,...,I ≥ 1 k\nuniform over [ d].\nfors= 1 tok−1do\nys+1=xs−ηsd∇If(x)eI, g˜s∈∂ℓ(xs,Zs)\nxs+1=π(ys+1) C\nend for\nk1return x¯k=k/summationdisplay\nxs\ns=1\nIf we apply Stochastic Gradient Descent to this problem for η=R/radicalBig\n2, we directlyL dk\nobtain\n2dIE[f(x¯k)]−f(x∗)≤RL/radicalbigg\nk\nWe are in a trade-oﬀ situation where the updates are much easier to implement but where\nwe need more steps to reach the same precision as the gradient descent alogrithm.\n5\nReferences\n[Nem12] Arkadi Nemirovski, On safe tractable approximations of chance constraints , Euro-\npean J. Oper. Res. 219(2012), no. 3, 707–718. MR 2898951 (2012m:90133)\n[NS06] Arkadi Nemirovski and Alexander Shapiro, Convex approximations of chance\nconstrained programs , SIAM J. Optim. 17(2006), no. 4, 969–996. MR 2274500\n(2007k:90077)\n6\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "gradient descent", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "41f810c4abefea4b511832aa26a437d0 MIT18 657F15 L14", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "20210ae9-85cd-44ab-9598-4b85f6f48e68", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 2\nScribe: Jonathan Weed Sep. 14, 2015\nPart I\nStatistical Learning Theory\n1. BINARY CLASSIFICATION\nIn the last lecture, we looked broadly at the problems that machine learning seeks to solve\nand the techniques we will cover in this course. Today, we will focus on one such problem,\nbinary classi\fcation , and review some important notions that will be foundational for the\nrest of the course.\nOur present focus on the problem of binary classi\fcation is justi\fed because both binary\nclassi\fcation encompasses much of what we want to accomplish in practice and because the\nresponse variables in the binary classi\fcation problem are bounded. (We will see a very\nimportant application of this fact below.) It also happens that there are some nasty surprises\nin non-binary classi\fcation, which we avoid by focusing on the binary case here.\n1.1 Bayes Classi\fer\nRecall the setup of binary classi\fcation: we observe a sequence ( X1;Y1);:::; (Xn;Yn) ofn\nindependent draws from a joint distribution PX;Y. The variable Y(called the label) takes\nvalues inf0;1g, and the variable Xtakes values in some space Xrepresenting \\features\" of\nthe problem. We can of course speak of the marginal distribution PXofXalone; moreover,\nsinceYis supported onf0;1g, the conditional random variable YjXis distributed according\nto a Bernoulli distribution. We write YjX\u0018Bernoulli(\u0011 (X)), where\n\u0011(X) = I P(Y = 1jX ) = I E[YjX]:\n(The function \u0011is called the regression function .)\nWe begin by de\fning an optimal classi\fer called the Bayes classi\fer. Intuitively, the\nBayes classi\fer is the classi\fer that \\knows\" \u0011|it is the classi\fer we would use if we had\nperfect access to the distribution YjX.\nDe\fnition: The Bayes classi\fer ofXgivenY, denotedh\u0003, is the function de\fned by the\nrule\n(h\u001a1 if\u0011 x)>1=2 \u0003(x) =0 if\u0011(x)\u00141=2.\nIn other words, h\u0003(X) = 1 whenever I P(Y = 1jX)>I P(Y = 0jX).\nOur measure of performance for any classi\fer h(that is, any function mapping Xto\nf0;1g) will be the classi\fcation error :R(h) = I P(Y=h(X)). The Bayes risk is the value\nR\u0003=R(h\u0003) of the classi\fcation error associated with the Bayes classi\fer. The following\ntheorem establishes that the Bayes classi\fer is optimal with respect to this metric.\n16\nTheorem: For any classi\fer h, the following identity holds:\nR(h)\u0000R(h\u0003) =Z\nj2\u0011(x)\u00001jPx(dx) = I E X[j2\u0011(X)\u00001j1(h(X ) =h\u0003(X))] (1.1)\nh=h\u0003\nWhereh=h\u0003is the (measurable) set fx2Xjh(x) =h\u0003(x)g.\nIn particular, since the integrand is nonnegative, the classi\fcation error R\u0003of the\nBayes classi\fer is the minimizer of R(h) over all classi\fers h.\nMoreover,\n1R(h\u0003) = I E[min(\u0011 (X);1\u0000\u0011(X))]\u0014: (1.2)2\nProof. We begin by proving Equation (1.2). The de\fnition of R(h) implies\nR(h) = I P(Y=h(X)) = I P(Y = 1;h(X ) = 0) + I P( Y= 0;h(X ) = 1);\nwhere the second equality follows since the two events are disjoint. By conditioning on X\nand using the tower law, this last quantity is equal to\nI E[I E[1(Y = 1;h(X ) = 0)jX ]] + I E[I E[1(Y = 0;h(X ) = 1)jX]]\nNow,h(X) is measurable with respect to X, so we can factor it out to yield\nI E[1(h(X ) = 0)\u0011(X) +1(h(X ) = 1)(1\u0000\u0011(X))]]; (1.3)\nwhere we have replaced I E[ YjX] by\u0011(X).\nIn particular, if h=h\u0003, then Equation 1.3 becomes\nI E[1(\u0011 (X)\u00141=2)\u0011 (X) +1(\u0011(x)>1=2)(1\u0000\u0011(X))]:\nBut\u0011(X)\u00141=2 implies \u0011(X)\u00141\u0000\u0011(X) and conversely, so we \fnally obtain\nR(h\u0003) = I E[1(\u0011 (X)\u00141=2)\u0011 (X) +1(\u0011(x)>1=2)(1\u0000\u0011(X))]\n= I E[(1(\u0011 (X)\u00141=2) + 1(\u0011(x)>1=2)) min(\u0011(X);1\u0000\u0011(X))]\n= I E[min(\u0011 (X);1\u0000\u0011(X))];\nas claimed. Since min(\u0011 (X);1\u0000\u0011(X))\u00141=2, its expectation is also certainly at most 1 =2\nas well.\nNow, given an arbitrary h, applying Equation 1.3 to both handh\u0003yields\nR(h)\u0000R(h\u0003) = I E[ 1(h(X ) = 0)\u0011 (X) +1(h(X ) = 1)(1\u0000\u0011(X))]\n\u00001(h\u0003(X) = 0)\u0011 (X) +1(h\u0003(X) = 1)(1\u0000\u0011(X))]];\nwhich is equal to\nI E[(1(h(X ) = 0)\u00001(h\u0003(X) = 0))\u0011 (X) + (1(h(X ) = 1)\u00001(h\u0003(X) = 1))(1\u0000\u0011(X))]:\nSinceh(X) takes only the values 0 and 1, the second term can be rewritten as \u0000(1(h(X ) =\n0)\u00001(h\u0003(X) = 0)). Factoring yields\nI E[(2\u0011 (X)\u00001)(1(h(X ) = 0)\u00001(h\u0003(X) = 0))]:\n266\n6\n6\nThe term 1(h(X ) = 0)\u00001(h\u0003(X) = 0) is equal to \u00001, 0, or 1 depending on whether h\nandh\u0003agree. When h(X) =h\u0003(X), it is zero. When h(X) =h\u0003(X), it equals 1 whenever\nh\u0003(X) = 0 and\u00001 otherwise. Applying the de\fnition of the Bayes classi\fer, we obtain\nI E[(2\u0011 (X)\u00001)1(h(X ) =h\u0003(X)) sign(\u0011\u00001=2)] = I E[j2\u0011(X)\u00001j1(h(X ) =h\u0003(X))];\nas desired.\nWe make several remarks. First, the quantity R(h)\u0000R(h\u0003) in the statement of the\ntheorem above is called the excess risk ofhand denotedE(h). (\\Excess,\" that is, above\nthe Bayes classi\fer.) The theorem implies that E(h)\u00150.\nSecond, the risk of the Bayes classi\fer R\u0003equals 1=2 if and only if \u0011(X) = 1=2 almost\nsurely. This maximal risk for the Bayes classi\fer occurs precisely when Y\\contains no\ninformation\" about the feature variable X. Equation (1.1) makes clear that the excess risk\nweighs the discrepancy between handh\u0003according to how far \u0011is from 1=2. When \u0011is\nclose to 1=2, no classi\fer can perform well and the excess risk is low. When \u0011is far from\n1=2, the Bayes classi\fer performs well and we penalize classi\fers that fail to do so more\nheavily.\nAs noted last time, linear discriminant analysis attacks binary classi\fcation by putting\nsome model on the data. One way to achieve this is to impose some distributional assump-\ntions on the conditional distributions XjY= 0 andXjY= 1.\nWe can reformulate the Bayes classi\fer in these terms by applying Bayes' rule:\nI P(X =xY= 1)I P(Y= 1)\u0011(x) = I P(Y = 1jjX=x) = :I P(X =xjY= 1)I P(Y= 1) + I P(X=xjY= 0)I P(Y= 0)\n(In general, when PXis a continuous distribution, we should consider in\fnitesimal proba-\nbilities I P(X2dx).)\nAssume that XjY= 0 andXjY= 1 have densities p0andp1, and I P(Y = 1) =\u0019is\nsome constant re\necting the underlying tendency of the label Y. (Typically, we imagine\nthat\u0019is close to 1=2, but that need not be the case: in many applications, such as anomaly\ndetection,Y= 1 is a rare event.) Then h\u0003(X) = 1 whenever \u0011(X)\u00151=2, or, equivalently,\nwhenever\np1(x) 1\u0000\u0019:p0(x)\u0015\u0019\nWhen\u0019= 1=2, this rule amounts to reporting 1 or 0 by comparing the densities p1\nandp0. For instance, in Figure 1, if \u0019= 1=2 then the Bayes classi\fer reports 1 whenever\np1\u0015p0, i.e., to the right of the dotted line, and 0 otherwise.\nOn the other hand, when \u0019is far from 1=2, the Bayes classi\fer is weighed towards the\nunderlying bias of the label variable Y.\n1.2 Empirical Risk Minimization\nThe above considerations are all probabilistic , in the sense that they discuss properties of\nsome underlying probability distribution. The statistician does nothave access to the true\nprobability distribution PX;Y; she only has access to i.i.d. samples (X 1;Y1);:::; (Xn;Yn).\nWe consider now this statistical perspective. Note that the underlying distribution PX;Y\nstill appears explicitly in what follows, since that is how we measure our performance: we\njudge the classi\fers we produced on future i.i.d. draws from PX;Y.\n36\n6 6\nFigure 1: The Bayes classi\fer when \u0019= 1=2.\nGiven dataDn=f ^ (X1;Y1);:::; (Xn;Yn)g, we build a classi\fer hn(X), which is random\nin two senses: it is a function of a random variable Xand also depends implicitly on the\n^ random dataDn. As above, we judge a classi\fer according to the quantity E(hn). This is\na random variable: though we have integrated out X, the excess risk still depends on the\ndataDn. We therefore will consider bounds both on its expected value and bounds that\n^ hold in high probability. In any case, the bound E(hn)\u00150 always holds. (This inequality\ndoes not merely hold \\almost surely,\" since we proved that R(h)\u0015R(h\u0003) uniformly over\nall choices of classi\fer h.)\nLast time, we proposed two di\u000berent philosophical approaches to this problem. In\nparticular, generative approaches make distributional assumptions about the data, attempt\nto learn parameters of these distributions, and then plug the resulting values into the model.\nThe discriminative approach|the one taken in machine learning|will be described in great\ndetail over the course of this semester. However, there is some middle ground, which is worth\nmentioning brie\ny. This middle ground avoids making explicit distributional assumptions\naboutXwhile maintaining some of the \navor of the generative model.\nThe central insight of this middle approach is the following: since by de\fnition h\u0003(x) =\n^ 1(\u0011(X)>1=2), we estimate \u0011by some\u0011^nand thereby produce the estimator hn=\n1(\u0011^n(X)>1=2). The result is called a plug-in estimator.\nOf course, achieving good performance with a plug-in estimator requires some assump-\ntions. (No-free-lunch theorems imply that we can't avoid making an assumption some-\nwhere!) One possible assumption is that \u0011(X) is smooth; in that case, there are many\nnonparamteric regression techniques available (Nadaraya-Watson kernel regression, wavelet\nbases, etc.).\nWe could also assume that \u0011(X) is a function of a particular form. Since \u0011(X) is only\nsupported on [0; 1], standard linear models are generally inapplicable; rather, by applying\nthe logit transform we obtain logistic regression , which assumes that \u0011satis\fes an identity\nof the form\nlog\u0012\u0011(X)\n1\u0000\u0011(X)\u0013\n=\u0012TX:\nPlug-in estimators are called \\semi-paramteric\" since they avoid making any assumptions\nabout the distribution of X. These estimators are widely used because they perform fairly\nwell in practice and are very easy to compute. Nevertheless, they will not be our focus here.\nIn what follows, we focus here on the discriminative framework and empirical risk min-\nimization. Our benchmark continues to be the risk function R(h) = I E1(Y =h(X)), which\n46\nis clearly not computable based on the data alone; however, we can attempt to use a na \u0010ve\nstatistical \\hammer\" and replace the expectation with an average.\nDe\fnition: The empirical risk of a classi\fer his given by\nn1^Rn(h) =X\n1(Yi=h(Xi)):ni=1\nMinimizing the empirical risk over the family of all classi\fers is useless, since we can\nalways minimize the empirical risk by mimicking the data and classifying arbitrarily other-\nwise. We therefore limit our attention to classi\fers in a certain family H.\n^ De\fnition: The Empirical Risk Minimizer (ERM) overHis any element1hermof the set\n^ argminhRn(h).2H\nIn order for our results to be meaningful, the class Hmust be much smaller than the\n^ space of all classi\fers. On the other hand, we also hope that the risk of hermwill be close\nto the Bayes risk, but that is unlikely if His too small. The next section will give us tools\nfor quantifying this tradeo\u000b.\n1.3 Oracle Inequalities\nAn oracle is a mythical classi\fer, one that is impossible to construct from data alone but\n\u0016 whose performance we nevertheless hope to mimic. Speci\fcally, given Hwe de\fnehto be\nan element of argminhR(h)|a classi\fer in2H Hthat minimizes the true risk. Of course,\n\u0016 we cannot determine h, but we can hope to prove a bound of the form\n^R(h)\u0014 \u0016R(h) + something small: (1.4)\n\u0016 Sincehis the best minimizer in Hgiven perfect knowledge of the distribution, a bound of\n^ the form given in Equation 1.4 would imply that hhas performance that is almost best-in-\nclass. We can also apply such an inequality in the so-called improper learning framework,\n^ where we allow hto lie in a slightly larger class H0\n^\u001bH; in that case, we still get nontrivial\n\u0016 guarantees on the performance of hif we know how to control R(h)\nThere is a natural tradeo\u000b between the two terms on the right-hand side of Equation 1.4.\nWhenH \u0016 is small, we expect the performance of the oracle hto su\u000ber, but we may hope\n\u0016 to approximate hquite closely. (Indeed, at the limit where His a single function, the\n\\something small\" in Equation 1.4 is equal to zero.) On the other hand, as Hgrows the\noracle will become more powerful but approximating it becomes more statistically di\u000ecult.\n(In other words, we need a larger sample size to achieve the same measure of performance.)\n^ SinceR(h) is a random variable, we ultimately want to prove a bound in expectation\nor tail bound of the form\n^ I P(R(h)\u0014 \u0016R(h) + \u0001n;\u000e(H))\u00151\u0000\u000e;\nwhere \u0001n;\u000e(H) is some explicit term depending on our sample size and our desired level of\ncon\fdence.\n1In fact, even an approximate solution will do: our bounds will still hold whenever we produce a classi\fer\n^ ^^ hsatisfying Rn(h)\u0014infhR2H n(h) + \".\n56\nIn the end, we should recall that\nE^ ^\u0000\u0003 ^\u0000 \u0016 \u0016 (h) =R(h)R(h) = (R(h)R(h)) + (R (h)\u0000R(h\u0003)):\nThe second term in the above equation is the approximation error, which is unavoidable\nonce we \fx the class H. Oracle inequalities give a means of bounding the \frst term, the\nstochastic error.\n1.4 Hoe\u000bding's Theorem\nOur primary building block is the following important result, which allows us to understand\nhow closely the average of random variables matches their expectation.\nTheorem (Hoe\u000bding's Theorem): LetX1;:::;Xnbenindependent random vari-\nables such that Xi2[0;1] almost surely.\nThen for any t>0,\nI P \f\fn1X\nXiI EXini=1\u0000\f\f\n\f>!\n\f\f\f2t\u00142e\u00002nt:\nIn other words, deviations from\f\nthe mean deca\f\ny exponentially fast in nandt.\nProof. De\fne centered random variables Zi=Xi\u0000I EXi. It su\u000eces to show that\n\u00121X\u0013\n\u0014\u00002nt2I PZi>t e ;n\nsince the lower tail bound follows analogously. (Exercise!)\nWe apply Cherno\u000b bounds. Since the exponential function is an order-preserving bijec-\ntion, we have for any s>0\nI P\u00121X\nZstni>t\u0013\n= I P\u0010\nexp\u0010\nsX\nZstn s Z ii ]n\u0011\n>e\u0011\n\u0014e\u0000I E[eP\n(Markov)\n=e\u0000stnI E[esZi]; (1.5)\nwhere in the last equality we have used the independence of theY\nZi.\nWe therefore need to control the term I E[ esZi], known as the moment-generating func-\ntion ofZi. If theZiwere normally distributed, we could compute the moment-generating\nfunction analytically. The following lemma establishes that we can do something similar\nwhen theZiare bounded.\nLemma (Hoe\u000bding's Lemma): IfZ2[a;b] almost surely and I EZ = 0, then\n2 2\n\u0014s(b\u0000a)I EesZe 8:\nProof of Lemma. Consider the log-moment generating function (s) = log I E[esZ], and note\nthat it su\u000eces to show that (s)\u0014s2(b\u0000a)2=8. We will investigate by computing the\n6\n\frst several terms of its Taylor expansion. Standard regularity conditions imply that we\ncan interchange the order of di\u000berentiation and integration to obtain\nI E[ZesZ] 0(s) = ;I E[esZ]\n2I E[Z2esZ]I E[esZ] I E[ZesZ]2esZesZ\n 00(s) =\u0000= I E\u0014\nZ2\u0015\n\u0000\u0012\nI EZI E[esZ]2 I E[esZ]\u0014\nI E[esZ]\u0015\u0013\n:\nSinceesZ\nsZintegrates to 1, we can interpret 00(s) as the variance of Zunder the probabilityI E[e]\nmeasuredF=esZ\nsZdI E. We obtainI E[e]\n 00(s) = var F(Z) = var F\u0012a+bZ\u00002\u0013\n;\nsince the variance is una\u000bected under shifts. But jZ\u0000a+b\n2j\u0014b\u0000aalmost surely since2\nZ2[a;b] almost surely, so\nvarF\u0012+bZ\u00002\u0013\n\u0014F\"\u00122a a+bZ\u00002\u0013#\n(b\u0000a)2\n\u0014:4\nFinally, the fundamental theorem of calculus yields\ns us2(b a)2\n (s) =Z\n(u du\n0Z\n 00)\u0000:\n0\u00148\nThis concludes the proof of the Lemma.\nApplying Hoe\u000bding's Lemma to Equation (1.5), we obtain\nI P\u00121X2Z >t\u0013\n\u0014e\u0000stnY\nes2=8=ens =8stni\u0000;n\nfor anys>0. Plugging in s= 4t>0 yields\nI P\u00121X\nZi>t\u0013\n\u0014e\u00002nt2;n\nas desired.\nHoe\u000bding's Theorem implies that, for any classi\fer h, the bound\nlog(2=\u000e )j^Rn(h)\u0000R(h)j\u0014r\n2n\nholds with probability 1 \u0000\u000e. We can immediately apply this formula to yield a maximal\ninequality: ifHis a \fnite family, i.e., H=fh1;:::;hMg, then with probability 1 \u0000\u000e=M\nthe bound\nlogj^Rn(hj)\u0000R(hj)j\u0014r\n(2M=\u000e)\n2n\n7\n^ holds. The event that max jjRn(hj)\u0000R(hj)j ^ >tis the union of the events jRn(hj)\u0000R(hj)j>\ntforj= 1;:::;M , so the union bound immediately implies that\nlog(2M=\u000e )maxj^Rn(hj)\u0000R(hj)\njj\u0014r\n2n\nwith probability 1 \u0000\u000e. In other words, for such a family, we can be assured that the empirical\nrisk and the true risk are close. Moreover, the logarithmic dependence on Mimplies that\nwe can increase the size of the family Hexponentially quickly with nand maintain the\nsame guarantees on our estimate.\n8\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "norm", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "86f311c7073869c5e0c199008787d5c9 MIT18 657F15 L2", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "3da17347-a0cf-49dd-8fe5-f4cab3665b17", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 4\nScribe:Cheng Mao Sep. 21, 2015\nIn this lecture, we continue to discuss the eﬀect of noise on the rate of the excess risk\nˆ ˆ ˆ E(h) =R(h)−R(h∗) wherehis the empirical risk minimizer. In the binary classiﬁcation\nmodel, noise roughly means how close the regression function ηis from1\n2.In particular, if\nη=1then we observe only noise, and if η∈ {0,1}we are in the noiseless case which has2\nbeen studied last time. Especially, we achieved the fast ratelogMin the noiseless case byn¯ assuming h∗∈ Hwhich implies that h=h∗. This assumption was essential for the proof\nand we will see why it is necessary again in the following section.\n3.2 Noise conditions\nThe noiseless assumption is rather unrealistic, so it is natural to ask what the rate of excess\nrisk is when the noise is present but can be controlled. Instead of the condition η∈ {0,1},\nwe can control the noise by assuming that ηis uniformly bounded away from1\n2, which is\nthe motivation of the following deﬁnition.\nDeﬁnition (Massart’s noise condition): The noise in binary classiﬁcation is said\nto satisfy Massart’s condition with constant γ∈(0,1\n2]if|η(X)−1| ≥γalmost surely.2\nOnce uniform boundedness is assumed, the fast rate simply follows from last proof with\nappropriate modiﬁcation of constants.\nˆ ˆ ˆ Theorem: LetcE(h) denote the excess risk of the empirical risk minimizer h=herm.\nIf Massart’s noise condition is satisﬁed with constant γ, then\nlog(M/δ)ˆE(h)≤γn\nwith probability at least 1 −δ. (In particular γ=1gives exactly the noiseless case.)2\nProof. ¯ ¯ DeﬁneZi(h) = 1I(h(Xi) =Yi)−1I(h(Xi) =Yi). By the assumption h=h∗and the\nˆ ˆ deﬁnition of h=herm,\nˆ ˆ ¯E(h) =R(h)−R(h)\nˆˆˆ¯ˆ¯ˆˆ ¯ ˆ =Rn(h)−Rn(h)+Rn(h)−Rn(h)−R(h)−R(h) (3.1)\nn1ˆ ≤ )/parenleftbig /parenrightbig\n/summationdisplay/parenleftbigˆ Zi(h)−IE[Zi(h]/parenrightbig\n. (3.2)ni=1\nHence it suﬃces to bound the deviation of/summationtext\niZifrom its expectation. To this end, we\nhope to apply Bernstein’s inequality. Since\nVar[Zi(h)]≤IE[Z2 ¯i(h) ] = IP[h(Xi) =h(Xi)],\n1/ne}ationslash /ne}ationslash\n/ne}ationslash\nwe have that for any 1 ≤j≤M,\nn1/summationdisplay¯ Var[Zi(hj)]≤IP[hj(X) =h(X)] =:σ2\nnj.\ni=1\nBernstein’s inequality implies that\nn/bracketleftbig1/summationdisplay /bracketrightbig /parenleftbig nt2\nIP ( Zi(hj)−IE[Zi(hj)])> t≤exp−n 2σ2\ni=1 j+2\n3t/parenrightbigδ=:.M\nApplying a union bound over 1 ≤j≤Mand taking\n2σ2\njlog(M/δ)2log(M/δ)t=t0(j) := max/radicalBigg\n/parenleftbig\n,n 3n/parenrightbig\n,\nwe get that\nn1/summationdisplay\n(Zi(hj)−IE[Zi(hj)])≤t0(j) (3.3)ni=1\nfor all 1≤j≤Mwith probability at least 1 −δ.\nˆ Suppose h=hˆ. It follows from (3.2) and (3.3) that with probability at least 1 −δ,j\nˆE(h)≤tˆ0(j).\n(Note that so far the proof is exactly the same as the noiseless case.) Since |η(X)−1\n2| ≥γ\n¯ a.s. and h=h∗,\nˆ ˆ ¯ E(h) = IE[|2η(X)−1|1I(h(X) =h∗(X))]≥2γIP[hˆ(X) =h(X)] = 2γσ2\nˆ.j j\nTherefore,/radicalBigg\nˆE(h)log(M/δ)2log(M/δ)ˆE(h)≤max , , (3.4)γn 3n\nso we conclude that with probabilit/parenleftbig\ny at least 1 −δ,/parenrightbig\nlog(M/δ)ˆE(h)≤ .γn\n¯ The assumption that h=h∗was used twice in the proof. First it enables us to ignore\nthe approximation error and only study the stochastic error. More importantly, it makes\nthe excess risk appear on the right-hand side of (3.4) so that we can rearrange the excess\nrisk to get the fast rate.\nMassart’s noise condition is still somewhat strong because it assumes uniform bounded-\nness ofηfrom1\n2. Instead, we can allow ηto be close to1\n2but only with small probability,\nand this is the content of next deﬁnition.\n2/ne}ationslash\n/ne}ationslash /ne}ationslash\nDeﬁnition (Tsybakov’s noise condition or Mammen-Tsybakov noise condi-\ntion):The noise in binary classiﬁcation is said to satisfy Tsybakov’s condition if there\nexistsα∈(0,1),C10>0 andt0∈(0,2] such that\n1 αIP[|η(X)−| ≤t]≤C10t−α\n2\nfor allt∈[0,t0].\nαIn particular, as α→1,t1−α→0\nα, so this recovers Massart’s condition with γ=t0and\nwe have the fast rate. As α→0,t1−α→1, so the condition is void and we have the slow\nrate. In between, it is natural to expect fast rate (meaning faster than slow rate) whose\norder depends on α. We will see that this is indeed the case.\nLemma: Under Tsybakov’s noise condition with constants α,C0andt0, we have\nIP[h(X) =h∗(X)]≤CE(h)α\nfor any classiﬁer hwhereC=C(α,C0,t0) is a constant.\nProof.We have\nE(h) = IE[|2η(X)−1|1I(h(X) =h∗(X))]\n1≥IE[|2η(X)−1|1I(|η(X)−|> t)1I(h(X) =h∗(X))]2\n1≥2tIP[|η(X)− |> t,h(X) =h∗(X)]2\n1≥2tIP[h(X) =h∗(X)]−2tIP[|η(X)−| ≤t]2\n1≥2tIP[h(X) =h∗(X)]−2C0t1−α\n1where Tsybakov’s condition was used in the last step. Take t=cIP[h(X) =h∗(X)]−α\nαfor\nsome positive c=c(α,C0,t0) to be chosen later. We assume that c≤t0to guarantee that\nt∈[0,t0]. Sinceα∈(0,1),\nE(h)≥2cIP[h(X) =h∗(X)]1/α1−2C c1−αIP[h(X) =h∗10 (X)]/α\n≥cIP[h(X) =h∗(X)]1/α\nby selecting csuﬃciently small depending on αandC0. Therefore\n1IP[h(X) =h∗(X)]≤E(h)α\ncα\nand choosing C=C(α,C0,t0) :=c−αcompletes the proof.\nHaving established the key lemma, we are ready to prove the promised fast rate under\nTsybakov’s noise condition.\n3/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash /ne}ationslash\n/ne}ationslash\n/ne}ationslash\nTheorem: If Tsybakov’s noise condition is satisﬁed with constant α,C0andt0, then\nthere exists a constant C=C(α,C0,t0) such that\nl )ˆEh)≤C/parenleftbigog(M/δ(1\nn/parenrightbig\n2−α\nwith probability at least 1 −δ.\nThis rate of excess risk parametrized by αis indeed an interpolation of the slow ( α→0)\nˆ and the fast rate ( α→1). Futhermore, note that the empirical risk minimizer hdoes not\ndepend on the parameter αat all! It automatically adjusts to the noise level, which is a\nvery nice feature of the empirical risk minimizer.\nProof.The majority of last proof remains valid and we will explain the diﬀerence. After\nestablishing that\nˆE(h)≤t0(ˆj),\nwe note that the lemma gives\nˆσ2 ¯ ˆˆ= IP[h(X)/ne}ationslash=h(X)]≤CE(h)α.j\nIt follows tha t /radicalBigg\nˆ /parenleftbig2CE(h)αlog(M/δ)2log(M/δ)ˆE(h)≤max ,n 3n/parenrightbig\n/parenleftBig2ClogM2ˆ1\nE(h)≤max/parenleftbigδ/parenrightbig\n2−αlog(M/δ),/parenrightBig\n.n 3na\nnd thus\n4. VAPNIK-CHERVONENKIS (VC) THEORY\nThe upper bounds proved so far are meaningful only for a ﬁnite dictionary H, because if\nM=|H|is inﬁnite all of the bounds we have will simply be inﬁnity. To extend previous\nresults to the inﬁnite case, we essentially need the condition that only a ﬁnite number of\nelements in an inﬁnite dictionary Hreally matter. This is the objective of the Vapnik-\nChervonenkis (VC) theory which was developed in 1971.\n4.1 Empirical measure\nRecall from previous proofs (see (3.1) for example) that the key quantity we need to control\nis\nˆ2sup/parenleftbig\nRn(h)−R(h).\nh∈H\nInstead of the union bound which would not work in th/parenrightbig\ne inﬁnite case, we seek some bound\nthat potentially depends on nand the complexity of the set H. One approach is to consider\nsome metric structure on Hand hope that if two elements in Hare close, then the quantity\nevaluated at these two elements are also close. On the other hand, the VC theory is more\ncombinatorial and does not involve any metric space structure as we will see.\n4\nBy deﬁnition\nn1ˆRn(h)−R(h) =/summationdisplay/parenleftbig\n1I(h(Xi) =Yi)−IE[1I(h(Xi) =Yi)]ni=1/parenrightbig\n.\nLetZ= (X,Y) andZi= (Xi,Yi), and let Adenote the class of measurable sets in the\nsample space X ×{0,1}. For a classiﬁer h, deﬁneAh∈ Aby\n{Zi∈Ah}={h(Xi) =Yi}.\nMoreover, deﬁne measures µnandµonAby\nn1µn(A) =/summationdisplay\n1I(Zi∈A) and µ(A) = IP[Zi∈A]ni=1\nforA∈ A. With this notation, the slow rate we proved is just\nlog(2|A|/δ)ˆsupRn(h)−R(h) = sup|µn(A)−µ(A)| ≤\nh∈H A∈A/radicalbigg\n.2n\nSince this is not accessible in the inﬁnite case, we hope to use one of the concentration\ninequalities to give an upperbound. Note that µn(A) is a sum of random variables that may\nnot be independent, so the only tool we can use now is the bounded diﬀerence inequality.\nIf we change the value of only one ziin the function\nz1,...,zn/mapsto→sup|µn(A)−µ(A)|,\nA∈A\nthe value of the function will diﬀer by at most 1 /n. Hence it satisﬁes the boundeddiﬀerence\nassumption with ci= 1/nfor all 1≤i≤n. Applying the bounded diﬀerence inequality, we\nget that\n/vextendsinglelog(2/δ) /vextendsinglesup|µn(A)−µ(A)|−IE[sup|µn(A)−µ(A)|]≤\nA∈A A∈A/radicalbigg\n2n\nwith probability/vextendsingle\nat least 1 −δ. Note that this already preclu/vextendsingle/vextendsingle\n/vextendsingle\ndes any fast rate (faster than\nn−1/2). Toachieve fast rate, weneedTalagrand inequality andlocalization techniques which\nare beyond the scope of this section.\nIt follows that with probability at least 1 −δ,\nlog(2/δ)sup|µn(A)−µ(A)| ≤IE[sup|µn(A)−µ(A)|]+\nA A∈A/radicalbigg\n.\nA∈ 2n\nWe will now focus on bounding the ﬁrst term on the right-hand side. To this end, we need\na technique called symmetrization, which is the subject of the next section.\n4.2 Symmetrization and Rademacher complexity\nSymmetrization is a frequently used technique in machine learning. Let D={Z1,...,Z n}\nbe the sample set. To employ symmetrization, we take another independent copy of the\nsample set D′={Z′\n1,...,Z′\nn}. This sample only exists for the proof, so it is sometimes\nreferred to as a ghost sample. Then we have\nn n1 1µ(A) = IP[Z∈A] = IE[/summationdisplay\n1I(Z′\ni∈A)] = IE[ 1I( Z′\ni∈A)|D] = IE[µ′\nn nn(A)|D]\ni=1/summationdisplay\ni=1\n5/ne}ationslash /ne}ationslash\n/ne}ationslash\nnwhereµ′\nn:=1/summationtext\ni=11I(Z′\ni∈A). Thus by Jensen’s inequality,n\nIE[sup|µn(A)−µ(A)|] = IE/bracketleftbig\nsup/vextendsingle/vextendsingle\nµn(A)−IE[µ′\nn(A)|D]\nA∈A A∈A\n≤IE sup IE[ |µn(A)−µ′\nn(A)||D/vextendsingle/vextendsingle\n]/bracketrightbig\n≤/bracketleftbig\nA∈A\nIE/bracketleftbig\nsup|µ′n(A)−µn(A)|/bracketrightbig\nA∈A\nn1/bracketrightbig\n= IE/bracketleftbig\nsup/vextendsingle/summationdisplay/parenleftbig\n1I(Z′i∈A)−1I(Z\nA∈Ani∈A)\ni=1/parenrightbig/vextendsingle/bracketrightbig\n.\nSinceD′has the same distribution of D, by sy/vextendsingle\nmmetry 1I( Zi∈A)−1I(Z′/vextendsingle\ni∈A) has the same\ndistribution as σi/parenleftbig\n1I(Zi∈A)−1I(Z′\ni∈A)/parenrightbig\nwhereσ1,...,σ nare i.i.d. Rad(1\n2), i.e.\n1IP[σi= 1] = IP[ σi=−1] =,2\nandσi’s are taken to be independent of both samples. Therefore,\nn\nIE[sup|µn(A)−µ(A)|]≤IE\nAA/bracketleftbig\nsup′\n∈ A∈A/vextendsingle/vextendsingle1/summationdisplay\nσi/parenleftbig\n1I(Zi∈A)−1I(Zni∈A)\ni=1\nn/parenrightbig/vextendsingle/bracketrightbig\n≤2IE/bracketleftbig1/vextendsingle\nsup/vextendsingle/summationdisplay\nσi1I(Zi∈A).\nA∈Ani=1/vextendsingle/bracketrightbig\n(4.5)\nUsingsymmetrization we have boundedIE[sup/vextendsingle\nA∈A|µn(A)−µ(A)|/vextendsingle\n] by amuch nicer quantity.\nYet we still need an upper bound of the last quantity that depends only on the structure\nofAbut not on the random sample {Zi}. This is achieved by taking the supremum over\nallzi∈ X ×{0,1}=:Y.\nDeﬁnition: The Rademacher complexity of a family of sets Ain a space Yis deﬁned\nto be the quantity\nn\nRn(A) = sup sup/vextendsingle1IE/bracketleftbig /summationdisplay\nσi1I(zi∈A)\nz1,...,zn∈YA∈Ani=1/vextendsingle/bracketrightbig\n.\nThe Rademacher complexity of a set B⊂I/vextendsingle\nRnis deﬁned to b/vextendsingle\ne\nn1Rn(B) = IE/bracketleftbig\nsup\nb∈B/vextendsingle/vextendsingle\nn/summationdisplay\nσibi\ni=1/vextendsingle/vextendsingle/bracketrightbig\n.\nWe conclude from (4.5) and the deﬁnition that\nIE[sup|µn(A)−µ(A)|]≤2Rn(A).\nA∈A\nnIn the deﬁnition of Rademacher complexity of a set, the quantity1sni=1σibimeasure\nhow well a vector b∈Bcorrelates with a random sign pattern {σi}. The more complex\nBis, the better some vector in Bcan replicate a sign pattern. In/vextendsingle/vextendsingle\npa/summationtext\nrticular, i/vextendsingle/vextendsingle\nfBis the\nfull hypercube [ −1,1]n, thenRn(B) = 1. However, if B⊂[−1,1]ncontains only k-sparse\n6\nvectors, then Rn(B) =k/n. Hence Rn(B) is indeed a measurement of the complexity of\nthe setB.\nThe set of vectors to our interest in the deﬁnition of Rademacher complexity of Ais\nT(z) :={(1I(z1∈A),...,1I(zn∈A))T,A∈ A}.\nThus the key quantity here is the cardinality of T(z), i.e., the number of sign patterns these\nvectors can replicate as Aranges over A. Although the cardinality of Amay be inﬁnite,\nthe cardinality of T(z) is bounded by 2n.\n7\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "vector", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "f9bb2571a80299876669c67fd1aff897 MIT18 657F15 L4", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "bab2efbb-a908-4d88-9bcc-102b647b48a3", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture 12\nScribe:Michael Traub Oct. 19, 2015\n2.3 Projected Gradient Descent\nIn the original gradient descent formulation, we hope to optimize min xf(x) where nC Ca d ∈\nfare convex, but we did not constrain the intermediate xk. Projected gradient descent will\nincorporate this condition.\n2.3.1 Projection onto Closed Convex Set\nFirst we must establish that it is possible to always be able to keep xkin the convex set C.\nOne approach is to take the closest point π(xk)∈ C.\nDeﬁnition: LetCbe a closed convex subset of IRd. Then∀x∈IRd, letπ(x)∈ Cbe\nthe minimizer of\n/ba∇dblx−π(x)/ba∇dbl= minx z\nz∈C/ba∇dbl − /ba∇dbl\nwhere/ba∇dbl·/ba∇dbldenotes the Euclidean norm. Then π(x) is unique and,\n/an}b∇acketle{tπ(x)−x,π(x)−z/an}b∇acket∇i}ht ≤0∀z∈ C (2.1)\nProof.From the deﬁnition of π:=π(x), we have /ba∇dblx−π/ba∇dbl2≤ /ba∇dblx−v/ba∇dbl2for anyv∈ C. Fix\nw∈ Cand deﬁne v= (1−t)π+twfort∈(0,1]. Observe that since Cis convex we have\nv∈ Cso that\n/ba∇dbl − /ba∇dbl2≤ /ba∇dbl − /ba∇dbl2 2x π x v =/ba∇dblx−π−t(w−π)/ba∇dbl\nExpanding the right-hand side yields\n/ba∇dbl2 2 2x−π/ba∇dbl ≤ /ba∇dblx−π/ba∇dbl −2t/an}b∇acketle{tx−π,w−π/an}b∇acket∇i}ht+t2/ba∇dblw−π/ba∇dbl\nThis is equivalent to\n/an}b∇acketle{tx−π,w− /an}b∇acket∇i}ht ≤2π t/ba∇dblw−π/ba∇dbl\nSince this is valid for all t∈(0,1), letting t→0 yields (2.1).\nProof of Uniqueness. Assumeπ1,π2∈ Csatisfy\n/an}b∇acketle{tπ1−x,π1−z/an}b∇acket∇i}ht ≤0∀z∈C\n/an}b∇acketle{tπ2−x,π2−z/an}b∇acket∇i}ht ≤0∀z∈C\nTakingz=π2in the ﬁrst inequality and z=π1in the second, we get\n/an}b∇acketle{tπ1−x,π1−π2/an}b∇acket∇i}ht ≤0\n/an}b∇acketle{tx−π2,π1−π2/an}b∇acket∇i}ht ≤0\nAdding these two inequalities yields /ba∇dblπ1−π2/ba∇dbl2≤0 so that π1=π2.\n1\n2.3.2 Projected Gradient Descent\nAlgorithm 1 Projected Gradient Descent algorithm\nInput:x1∈ C, positive sequence {ηs}s≥1\nfors= 1 tok−1do\nys+1=xs−ηsgs, gs∈∂f(xs)\nxs+1=π(ys+1)\nend for\nk1return Eitherx¯ =/summationdisplay\nxsorx◦∈argmin f(x)kxs={x1,...,x1∈ k}\nTheorem: LetCbe a closed, nonempty convex subset of IRdsuch that diam( C)≤R.\nLetfbe a convex L-Lipschitz function on\nRCsuch that x∗∈argminxf(x) exists.∈C\nThen ifηs≡η=L√thenk\nLR LRf(x¯)−f(x∗)≤√andf(x¯◦)−f(x∗)\nk≤√\nk\nMoreover, if ηs=R√, then∃c >0 such thatL s\nLR LRf(x¯)−f(x∗)≤c√andf(x¯◦)f(x∗)\nk− ≤ c√\nk\nProof.Again we will use the identity that 2 a⊤b=/ba∇dbla/ba∇dbl2+/ba∇dblb/ba∇dbl2−/ba∇dbl2a−b/ba∇dbl.\nBy convexity, we have\nf(xs)−f(x∗)≤gs⊤(xs−x∗)\n1= (xs−ys+1)⊤(xsη−x∗)\n1=2η/bracketleftBig\n/ba∇dbl2xs−ys+1/ba∇dbl+/ba∇dblxs−x∗/ba∇dbl2−/ba∇dblys+1−x∗/ba∇dbl2/bracketrightBig\nNext,\n/ba∇dblys+1−x∗/ba∇dbl2=/ba∇dbl2ys+1−xs+1/ba∇dbl+/ba∇dblxs+1−x∗/ba∇dbl2+2/an}b∇acketle{tys+1\n2−xs+1,xs+1−x∗/an}b∇acket∇i}ht\n=/ba∇dblys+1−xs+1/ba∇dbl+/ba∇dbl2xs+1−x∗/ba∇dbl+2/an}b∇acketle{tys+1−π(ys+1),π(ys+1)−x∗/an}b∇acket∇i}ht\n≥ /ba∇dblxs+1−x∗/ba∇dbl2\nwhere we used that /an}b∇acketle{tx−π(x),π(x)−z/an}b∇acket∇i}ht ≥0∀z∈ C, andx∗\n2 2∈ C. Also notice that\n/ba∇dblxs−2y2 2s+1/ba∇dbl=η/ba∇dblgs/ba∇dbl ≤η LsincefisL-Lipschitz with respect to /ba∇dbl·/ba∇dbl. Using this\nwe ﬁnd\nk k1\nk/summationdisplay 1 1( )−(∗)≤/summationdisplay2 2+∗2 ∗2f xsf x η L x sx x s+1xk2ηs=1 s=1/bracketleftBig\n/ba∇dbl − /ba∇dbl −/ba∇dbl − /ba∇dbl/bracketrightBig\nηL212ηL2R2\n≤+x2k/ba∇dbl1−x∗\n2η/ba∇dbl ≤ +2 2ηk\n2\n2 2Minimizing over ηwe getL=R\n2=⇒η=R√, completing the proof22η k L k\nRLf(x¯)−f(x∗)≤√\nk\n2\nMoreover, the proof of the bound for f(/summationtextk\nkxs)−f(x∗) is identical because xkxs=/vextenddouble/vextenddouble\n2−∗\n2/vextenddouble/vextenddouble≤\nR2as well./vextenddouble /vextenddouble\n2.3.3 Examples\nSupport Vector Machines\nThe SVM minimization as we have shown before is\nn1minn/summationdisplay\nmax(0,1Yifα(Xi))\nαR\n⊤∈In\n≤2i=1−\nαIKα C\nwherefα(Xi) =α⊤IKei=/summationtextn\n=1αjK(X ,j jXi). Forconvenience, call gi(α) = max(0 ,1−Yifα(Xi)).\nIn this case executing the projection onto the ellipsoid {α:α⊤IKα≤C2}is not too hard,\nbut we do not know about C,R, orL. We must determine these we can know that our\nbound is not exponential with respect to n. First we ﬁnd Land start with the gradient of\ngi(α):\n∇gi(α) = 1I(1−Yifα(Xi)≥0)YiIKei\nˆ With this we bound the gradient of the ϕ-riskRn,ϕ(fα) =1\nn\nn n/summationtextn\n=1gi(αi).\n/vextenddouble/vextenddouble∂ 1 1/vextenddoubleRˆn,ϕ(fα)/vextenddouble/vextenddouble/summationdisplay/vextenddouble=/vextenddouble/vextenddouble/vextenddouble∇gi(α)∂α/vextenddouble/vextenddoublen/vextenddouble\n/vextenddoublei=1/vextenddouble/vextenddouble≤/summationdisplay\nIKe/vextenddouble /vextenddouble in2\ni=1/ba∇dbl /ba∇dbl\nby the triangle inequality and the fact that that 1I(1/vextenddouble\n−Yifα(Xi)≥0)Yi≤1. We can now\nuse the properties of our kernel K. Notice that /ba∇dblIKei\n1\n2/ba∇dblis theℓ2norm of the ithcolumn so\n/ba∇dblIKei/ba∇dbln\n2=/parenleftBig/summationtext\nj=1K(Xj,Xi)2/parenrightBig\n. We also know that\nK(Xj,Xi)2=/an}b∇acketle{tK(X2j,·),K(Xi,·)/an}b∇acket∇i}ht ≤ /ba∇dblK(Xj,·)/ba∇dblKH/ba∇dbl(Xi,·)/ba∇dblH≤kmax\nCombining all of these we get\n1/vextenddouble n n2/vextenddouble∂ 1/vextenddoubleRˆn,ϕ(fα)/vextenddouble\n≤max\n/summationdisplay /summationdisplay/vextenddouble/vextenddoublek2=kmax√n=L/vextenddouble∂α/vextenddoubleni=1j=1\nTo ﬁndRwe try to evaluate diam {α⊤IKα≤C2}= 2 max√\nα\nα⊤⊤α. We can use the\nIKα≤C2\ncondition to put bounds on the diameter\nC2 2C≥α⊤IKα≥λmin(IK)α⊤α=⇒diam{α⊤IKα≤C2} ≤/radicalbig\nλmin(IK)\nWe need to understand how small λmincan get. While it is true that these exist random\nsamples selected by an adversary that make λmin= 0, we will consider a random sample of\n3\ni.i.dX1,...,X n∼ N(0,Id). This we can write these d-dimensional samples as a d×nmatrix\nX. We can rewrite the matrix IK with entries IK ij=K(Xi,Xj) =/an}b∇acketle{tXi,Xj/an}b∇acket∇i}htIRdas a Wishart\nmatrix IK = X⊤X(in particular,1Xd⊤Xis Wishart). Using results from random matrix\ntheory, if we take n,d→ ∞but holdnas a constant γ, thenλ(IK 2min) (d→1√−γ) . Takingd\nan approximation since we cannot take n,dto inﬁnity, we get\nλmin(IK)≃d/parenleftbiggn d1−2/radicalbigg\nd/parenrightbigg\n≥2\nusing the fact that d≫n. This means that λminbecoming too small is not a problem when\nwe model our samples as coming from multivariate Gaussians.\nNow we turn our focus to the number of iterations k. Looking at our bound on the\nexcess risk\nnRˆn,ϕ(f ˆα◦\nR)≤minRn,ϕ(fα)+C/radicalbigg\nkmax\nα⊤IKα≤C2 kλmin(IK)\nwe notice that our all of the constants in our stochastic term can be computed given the\nnumber of points and the kernel. Since statistical error is often √1, to be generous we wantn\nto have precision up to1\nnto allow for fast rates in special cases. This gives us\nn3k2C2\nk≥max\nλmin(IK)\nwhich is not bad since nis often not very big.\nIn [Bub15], the rates for many a wide rage of problems with various assumptions are\navailable. For example, if we assume strong convexity and Lipschitz we can get an exponen-\ntial rate so k∼logn. If gradient is Lipschitz, then we get get1\nkinstead of √1in the bound.k\nHowever, often times we are not optimizing over functions with these nice properties.\nBoosting\nWe already know that ϕisL-Lipschitz for boosting because we required it before.\nRemember that our optimization problem is\nn1min/summationdisplay\nϕ(−Yifα(Xi))\nαRNn\n|α∈I\n|1≤i=11\nwherefα=N\nj=1αjfjandfjis thejthweak classiﬁer. Remember before we had some rate\nlike/radicalBig\nlogNc/summationtext\nnand we would hope to get some other rate that grows with log NsinceNcan\nbe very large. Taking the gradient of the ϕ-loss in this case we ﬁnd\nN1∇Rˆn,ϕ(fα) =/summationdisplay\nϕ′(−Yifα(Xi))(−Yi)F(Xi)ni=1\nwhereF(x) is the column vector [ f1(x),...,fN(x)]⊤. Since|Yi| ≤1 andϕ′≤L, we can\nbound the ℓ2norm of the gradient as\n/vextenddouble nL /vextenddouble∇Rˆ/vextenddoublen,ϕ(fα)/vextenddouble/vextenddouble/vextenddouble\n2≤n/vextenddouble /vextenddouble/vextenddouble/vextenddouble/summationdisplay\nF(X/vextenddouble i)/vextenddoublei=1/vextenddouble\nn/vextenddouble\nL/vextenddouble/vextenddouble\n≤n/summationdisplay\n)\ni=/ba∇dblF(Xi\n1/ba∇dbl ≤L√\nN\n4\nusing triangle inequality and the fact that F(Xi) is aN-dimensional vector with each\ncomponent bounded in absolute value by 1.\nUsing the fact th√at the diameter of the ℓ1ball is 2, R= 2 and the Lipschitz associated\nwith our ϕ-risk isL NwhereLis the Lipschitz constant for ϕ. Our stochastic termR√L\nk\nbecomes 2 L/radicalBig\nN\nk. Imposing the same1\nnerror as before we ﬁnd that k∼N2n, which is very\nbad especially since we want log N.\n2.4 Mirror Descent\nBoosting is an example of when we want to do gradient descent on a non-Euclidean space,\nin particular a ℓ1space. While the dual of the ℓ2-norm is itself, the dual of the ℓ1norm is\ntheℓor sup norm. We want this appear if we have an ℓ1constraint. The reason for this ∞\nis not intuitive because we are taking about measures on the same space IRd, but when we\nconsider optimizations on other spaces we want a procedure that does is not indiﬀerent to\nthe measure we use. Mirror descent accomplishes this.\n2.4.1 Bregman Projections\nDeﬁnition: If/ba∇dbl·/ba∇dblis some norm on IRd, then/ba∇dbl·/ba∇dblis its dual norm.∗\nExample: If dual norm of the ℓpnorm/ba∇dbl·/ba∇dblpis theℓqnorm/ba∇dbl·/ba∇dblq, then1\np+1\nq= 1. This is the\nlimiting case of H¨ older’s inequality.\nIn general we can also reﬁne our bounds on inner products in IRdtox⊤y≤ /ba∇dblx/ba∇dbl/ba∇dbly/ba∇dblif∗\nwe consider xto be the primal and yto be the dual. Thinking like this, gradients live in\nthe dual space, e.g. in gs⊤(x−x∗),x−x∗is in the primal space, so gsis in the dual. The\ntranspose of the vectors suggest that these vectors come from spaces with diﬀerent measure,\neven though all the vectors are in IRd.\nDeﬁnition: Convex function Φ on a convex set Dis said to be\n(i) L-Lipschitz with respect to /ba∇dbl·/ba∇dblif/ba∇dblg/ba∇dbl∗≤L∀g∈∂Φ(x)∀x∈D\n(ii)α-strongly convex with respect to /ba∇dbl·/ba∇dblif\nαΦ(y)≥Φ(x)+g⊤(y−x)+2/ba∇dbly−x/ba∇dbl2\nfor allx,y∈Dand forg∈∂f(x)\nExample: If Φ is twice diﬀerentiable with Hessian Hand/ba∇dbl·/ba∇dblis theℓ2norm, then all\neig(H)≥α.\nDeﬁnition (Bregman divergence): For a given convex function Φ on a convex set\nDwithx,y∈ D, the Bregman divergence of yfromxis deﬁned as\nDΦ(y,x) = Φ(y)−Φ(x)−∇Φ(x)⊤(y−x)\n5\nThis divergence is the error of the function Φ( y) from the linear approximation at x.\nAlso note that this quantity is not symmetric with respect to xandy. If Φ is convex then\nDΦ(y,x)≥0 because the Hessian is positive semi-deﬁnite. If Φ is α-strongly convex then\nDΦ(y,x)≥α\n2/ba∇dbly−x/ba∇dbl2and if the quadratic approximation is good then this approximately\nholds in equality and this divergence behaves like Euclidean norm.\nProposition: Given convex function Φ on Dwithx,y,z∈ D\n(∇Φ(x)−∇Φ(y))⊤(x−z) =DΦ(x,y)+DΦ(z,x)−DΦ(z,y)\nProof.Looking at the right hand side\n= Φ(x)−Φ(y)−∇Φ(y)⊤(x−y)+Φ(z)−Φ(x)−∇Φ(x)⊤(z−x)\n−/bracketleftBig\nΦ(z)−Φ(y)−∇Φ(y)⊤(z−y)/bracketrightBig\n=∇Φ(y)⊤(y−x+z−y)−∇Φ(x)⊤(z−x)\n= (∇Φ(x)−∇Φ(y))⊤(x−z)\nDeﬁnition (Bregman projection): Givenx∈IRd, Φaconvex diﬀerentiablefunction\nonD ⊂ D¯ IRdand convex C⊂, the Bregman projection of xwith respect to Φ is\nπΦ(x)∈argminDφ(x,z)\nz∈C\nReferences\n[Bub15] S´ ebastien Bubeck, Convex optimization: algorithms and complexity , Now Publish-\ners Inc., 2015.\n6\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "gradient descent", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "d3fe2b7a6b61edf9bd44ea8495e5c52a MIT18 657F15 L12", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "2fbf35c3-2ab3-48f5-a396-28d7c4cc9370", "text": "18.657: Mathematics of Machine Learning\nLecturer: Philippe Rigollet Lecture\nScribe:Vira Semenova andPhilippe Rigollet Sep. 23, 2015\nIn this lecture, we complete the analysis of the performance of the empirical risk mini-\nmizer under a constraint on the VC dimension of the family of classiﬁers. To that end, we\nwill see how to control Rademacher complexities using shatter coeﬃcients. Moreover, we\nwill see how the problem of controlling uniform deviations of the empirical measure µnfrom\nthe true measure µas done by Vapnik and Chervonenkis relates to our original classiﬁcation\nproblem.\n4.1 Shattering\nRecall from the previous lecture that we are interested in sets of the form\nT(z) :=/braceleftbig\n(1I(z1∈A),...,1I(zn∈A)),A∈ A, z= (z1,...,zn). (4.1)\nIn particular, the cardinality of T(z), i.e., the numbe/bracerightbig\nr of binary patterns these vectors\ncan replicate as Aranges over A, will be of critical importance, as it will arise when\ncontrolling the Rademacher complexity. Although the cardinality of Amay be inﬁnite, the\ncardinality of T(z) is always at most 2n. When it is of the size 2n, we say that Ashatters\nthe setz1,...,zn. Formally, we have the following deﬁnition.\nDeﬁnition: A collection of sets Ashatters the set of points {z1,z2,...,zn}\ncard{(1I(z1∈A),...,1I(zn∈A)),A∈ A}= 2n.\nThe sets of points {z1,z2,...,zn}that we are interested are realizations of the pairs Z1=\n(X1,Y1),...,Z n= (Xn,Yn) and may, in principle take any value over the sample space.\nTherefore, we deﬁne the shatter coeﬃcient to be the largest cardinality that we may obtain.\nDeﬁnition: Theshatter coeﬃcients of a class of sets Ais the sequence of numbers\n{SA(n)}n≥1, where for any n≥1,\nSA(n) = sup card (1I( z1A),...,1I(znA)),A\nz1,...,zn{ ∈ ∈ ∈ A}\nand the suprema are taken over the whole sample space.\nBydeﬁnition,the nthshattercoeﬃcient SA(n)isequalto2nifthereexistsaset {z1,z2,...,zn}\nthatAshatters. The largest of such sets is precisely the Vapnik-Chervonenkis or VC di-\nmension.\nDeﬁnition: The Vapnik-Chervonenkis dimension, or VC-dimension of\ndVCAis the largest\nintegerdsuch that SA(d) = 2 . We write ( A) =d.\n15\nIfSA(n) = 2nfor all positive integers n, thenVC(A) :=∞\nIn words, Ashatters someset of points of cardinality dbut shatters noset of points of\ncardinality d+1. In particular, Aalso shatters no set of points of cardinality d′> dso that\nthe VC dimension is well deﬁned.\nInthesequel, wewillseethattheVCdimensionwillplaytherolesimilartoofcardinality,\nbut on an exponential scale. For interesting classes Asuch that card( A) =∞, we also may\nhaveVC(A)<∞. For example, assume that Ais the class of half-lines ,A={(−∞,a],a∈\nIR}∪ {[a,∞),a∈IR}, which is clearly inﬁnite. Then, we can clearly shatter a set of size\n2 but we for three points z1,z2,z3,∈IR, if for example z1< z2< z3, we cannot create the\npattern (0 ,1,0) (see Figure 4.1). Indeed, half lines can can only create patterns with zeros\nfollowed by ones or with ones followed by zeros but not an alternating pattern like (0 ,1,0).\n00\n10\n01\n11000\n100\n110\n111\n001\n011\n101\nFigure 1: If A={halﬂines}, then any set of size n= 2 is shattered because we can\ncreate all 2n= 4 0/1 patterns (left); if n= 3 the pattern (0 ,1,0) cannot be reconstructed:\nSA(3) = 7<23(right). Therefore, VC(A) = 2.\n4.2 The VC inequality\nWe have now introducedall the ingredients necessary tostate themain result of this section:\nthe VC inequality.\nTheorem (VC inequality): For any family of sets Awith VC dimension VC(A) =d,\nit holds /radicalbigg\n2dlog(2en/d)IE sup|µn(A)−µ(A)| ≤2\nA∈A n\nNotethatthisresultholdsevenif AisinﬁniteaslongasitsVCdimensionisﬁnite. Moreover,\nobserve that log( |A|) has been replaced by a term of order dlog 2en/d.\nTo prove the VC inequality, we proceed in three steps:/parenleftbig /parenrightbig\n2\n1. Symmetrization, to bound the quantity of interest by the Rademacher complexity:\nIE[sup|µn(A)−µ(A)|]≤2Rn( )\nA∈AA.\nWe have already done this step in the previous lecture.\n2. Control of the Rademacher complexity using shatter coeﬃcients. We are going to\nshow that\ngR(A)≤/radicaligg\n2lo\nn/parenleftbig\n2SA(n)\nn/parenrightbig\n3. We are going to need the Sauer-Shelah lemma to bound the shatter coeﬃcients by\nthe VC dimension. It will yield\nS(n)≤/parenleftigen/parenrightigd\n, d=VC A (dA).\nPut together, these three steps yield the VC inequality.\nStep 2: Control of the Rademacher complexity\nWe prove the following Lemma.\nLemma: For anyB⊂IRn, such that |B|<∞:, it holds\nn/bracketleftbig/vextendsingle1 2σ/vextendsingle )B/bracketrightbig (Rn( ) = IE max /vextendsingle/summationdisplay log 2B\nibi/vextendsingle≤max| |\nb∈Bn b∈Bi=1|b|2/radicalbig\nn\nwhere|·|2denotes the Euclidean norm.\nProof.Note that\n1Rn(B) = IEn/bracketleftbig\nmaxZb,\nb∈B|\nwhereZb=/summationtextn\ni=1σibi. In particular, since/vextendsingle/vextendsingle/bracketrightbig\n−|bi| ≤σi|bi| ≤ |bi|, a.s., Hoeﬀding’s lemma\nimplies that the moment generating function of Zbis controlled by\nn n\nIE/bracketleftbig\nexp(sZb)/bracketrightbig\n=/productdisplay\nIE\ni=1/bracketleftbig\nexp(sσibi)/bracketrightbig\n≤/productdisplay\nexp(s2b2\ni/2) = exp( s2b2\n2/2) (4.2)\ni=1| |\nNext, to control IE max b∈BZb|, we use the same technique as in Lecture 3, section 1.5.\n¯ To that end, deﬁne/bracketleftbig\nB=B∪/vextendsingle/vextendsingle\n{−B/bracketrightbig\n}and observe that for any s >0,\nIE/bracketleftbigg1max|Zb|/bracketrightbigg\n= IE/bracketleftbigg\nmaxZb/bracketrightbigg\n= logexp/parenleftbigg\nsIE/bracketleftbigg\nmaxZb/bracketrightbigg/parenrightbigg1≤logIE exp smaxZb,\nb∈B b∈B¯ s b¯∈B s/bracketleftbigg /parenleftbigg\nb¯∈B/parenrightbigg/bracketrightbigg\nwhere the last inequality follows from Jensen’s inequality. Now we bound the max by a\nsum to get/bracketleftbigg /bracketrightbigg1/summationdisplay log|¯B|s b2\nIE max|Zb| ≤log IE[exp( sZb)]≤ +| |2,\nb∈B s s2n\nb∈B¯\nwhere in the last inequality, we used (4.2). Optimizing over s >0 yields the desired\nresult.\n3\nWe apply this result to our problem by observing that\nRn(A) = sup (\n,Rn(T z))\nz1,... zn\nwhereT(z) is deﬁned in (4.1). In particular, since T(z)⊂ {0,1}, we have |b√|2≤n\nfor allb∈T(z). Moreover, by deﬁnition of the shatter coeﬃcients, if B=T(z), then\n|¯B| ≤2|T(z)| ≤2SA(n). Together with the above lemma, it yields the desired inequality:\n/radicalbigg\n2log(2SA(n))Rn(A)≤ .n\nStep 3: Sauer-Shelah Lemma\nWe need to use a lemma from combinatorics to relate the shatter coeﬃcients to the VC\ndimension. A priori, it is not clear from its deﬁnition that the VC dimension may be at\nall useful to get better bounds. Recall that steps 1 and 2 put together yield the following\nbound\n2log(2S(n))IE[sup n(A)\nA−µ(A) ]\nA∈|µ | ≤A2/radicalbigg\n(4.3)n\nIn particular, if SA(n) is exponential in n, the bound (4.3) is not informative, i.e., it does\nnot imply that the uniform deviations go to zero as the sample size ngoes to inﬁnity. The\nVC inequality suggest that this is not the case as soon as VC(A)<∞but it is not clear a\npriori. Indeed, it may be the case that\nVCSA(n) = 2nforn≤dandSA(n) = 2n−1 forn > d,\nwhich would imply that ( A) =d <∞but that the right-hand side in (4.3) is larger than\n2 for alln. It turns our that this can never be the case: if the VC dimension is ﬁnite, then\nthe shatter coeﬃcients are at most polynomial inn. This result is captured by the Sauer-\nShelah lemma, whose proof is omitted. The reading section of the course contains pointers\nto various proofs, speciﬁcally the one based on shiftingwhich is an important technique in\nenumerative combinatorics.\nLemma (Sauer-Shelah): IfVC(A) =d, then∀n≥1,\nd\nSA(n)≤/summationdisplay/parenleftbiggn en d\n.k/parenrightbigg\n≤dk=0/parenleftig /parenrightig\nTogether with (4.3), it clearly yields the VC inequality. By applying the bounded diﬀerence\ninequality, we also obtain the following VC inequality that holds with high probability. This\nis often the preferred from for this inequality in the literature.\nCorollary (VC inequality): For any family of sets Asuch that VC(A) =dand any\nδ∈(0,1), it holds with probability at least 1 −δ,\n/radicalbigg\n2dlog(2en/d)/radicalbigg\nlog(2/δ)supµnA)−µ(A)| ≤2 +\nA∈A|( .n 2n\n4\nNote that the logarithmic term log(2 en/d) is actually superﬂuous and can be replaced\nby a numerical constant using a more careful bounding technique. This is beyond the scope\nof this class and the interested reader should take a look at the recommending readings.\n4.3 Application to ERM\nThe VC inequality provides an upper bound for supA∈A|µn(A)−µ(A)|in terms of the VC\ndimension of the class of sets A. This result translates directly to our quantity of interest:\n2VC( )log2en)ˆsup|Rnh)−VC(A)log(2/δ(R(h) 2n/parenrightbig\n+\nh∈H≤/radicaligg\nA\n|/parenleftbig /radicalbigg\n(4.4)2n\nwhereA={Ah:h∈ H}andAh={(x,y)∈ X ×{0,1}:h(x) =y}. Unfortunately, the\nVC dimension of this class of subsets of X ×{0,1}is not very natural. Since, a classiﬁer h\nis a{0,1}valued function, it is more natural to consider the VC dimension of the family\nA=/braceleftbig\n{h= 1}:h∈ H/bracerightbig\n.\nDeﬁnition: LetHbe a collection of classiﬁers and deﬁne\nA¯={h= 1}:h∈ H\nWe deﬁne the VC d/braceleftbig\nimension VC( ) o/bracerightbig\n=/braceleftbig\nA:∃h∈ H,h(·) = 1I(· ∈A).\nH ¯ fHto be the VC dimension of/bracerightbig\nA.\n¯ ¯ It is not clear how VC(A) relates to the quantity VC(A), where A={Ah:h∈ H}and\nAh={(x,y)∈ X ×{0,1}:h(x) =y}that appears in the VC inequality. Fortunately, these\ntwo are actually equal as indicated in the following lemma.\nLemma: Deﬁne the two families for sets: = AX×h:h 2{0,1}where\n{ ¯A { ∈ H} ∈\nAh= (x,y)∈ X ×{0,1}:h(x) =y}andA=/braceleftbig\n{h= 1 :h 2X.\nS S ≥ VCA¯} ∈ H ∈\nThen,A¯(n) =A¯(n) for alln1. It implies ( ) = VC(A/bracerightbig\n).\nProof.Fixx= (x1,...,xn)∈ Xnandy= (y1,y2,...,yn)∈ {0,1}nand deﬁne\nT(x,y) ={(1I(h(x1) =y1),...,1I(h(xn) =yn)),h∈ H}\nand\n¯T(x) ={(1I(h(x1) = 1),...,1I(h(xn) = 1)),h∈ H}\nTo that end, ﬁx v∈ {0,1}and recall the XOR (exclusive OR) boolean function from {0,1}\nto{0,1}deﬁned by u⊕v= 1I(u=v). It is clearly1a bijection since ( u⊕v)⊕v=u.\n1One way to see that is to introduce the “spinned” variables u˜ = 2u−1 andv˜ = 2v−1 that live in\n/tildewider {−1,1}. Thenu⊕v=u˜·v˜, and the claim follows by observing that ( u˜·v˜)·v˜ =u˜. Another way is to simply\nwrite a truth table.\n5/ne}ationslash\n/ne}ationslash\n/ne}ationslash\n/ne}ationslash /ne}ationslash\n/ne}ationslash\nWhen applying XOR componentwise, we have\n\n1I(h(x1) =y1)\n1I(h(x1= 1) y\n ..\n) 1 ..\n .\n1I(h(xi) .\n\n=yi) = 1I(h(xi) = 1)\n..\n.⊕\n.\n .\n.\n1I(h(xn) =yn)\n \n \n1I(h(xn) = 1)...\nyi\n ...\nyn \n¯\nSince XOR is a bijection, we must have card[ T(x,y)] = card[ T(x)]. The lemma follows\nby taking the supremum on each side of the equality.\nIt yields the following corollary to the VC inequality.\nCorollary: LetHbe a family of classiﬁers with VC dimension d. Then the empirical\nˆ risk classiﬁer hermoverHsatisﬁes\nerm/radicalbigg\n2dlog(2en/d)ˆR(h)≤minR(h)+4 +\nh∈H n/radicalbigg\nlog(2/δ)\n2n\nwith probability 1 −δ.\nProof.Recall from Lecture 3 that\nˆR(herm)−min ) ≤ ˆ R(h2sup\nh∈H h∈H\nThe proof follows directly by applyi/vextendsingle/vextendsingleRn(h)−R(h)/vextendsingle\nng (4.4) and the above lemma./vextendsingle\n6/ne}ationslash\n/ne}ationslash\n/ne}ationslash\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "norm", "section_heading": "18.657: Mathematics of Machine Learning", "source_title": "c804dde9f0fcaac2abc68696147b3ee6 MIT18 657F15 L5", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "675dfb86-ebd7-4527-9503-f771cdb1011e", "text": "18.657 PS SOLUTIONS\n1.Problem 1\n(1) Symmetry is clear. Let K1andK2be the PSD Gram matrices of k1andk2, respectively.\nThen the Gram matrix Kofkis simply the Hadamard (or Schur) product K1•K2; we wish\nto see that this is PSD.\nThe Kronecker product K1⊗K2is PSD: its eigenvalues are simply the pair products of an\neigenvalue of K1with an eigenvalue from K2, as is easily seen from the identity\n(K1⊗K2)(v⊗w) = (K1v)⊗(K2w)\nwhenvandware eigenvectors of K1andK2, respectively. Now the Hadamard product\nK=K1•K2is a principal submatrix of A⊗B, and a principal submatrix of a PSD matrix\nis PSD.\n(There are many good approaches to this part.)\n(2) Treating gas a real vector indexed by C, we have K=gg⊤, and a matrix of this form is\nalways PSD.\n(3) The Gram matrix Kofkis simply Q(K1), where K1is the Gram matrix of k1, and where the\nmultiplication in the polynomial is Hadamard product. From (1), the PSD matrices are closed\nunder Hadamard product, and it is a common fact that they are closed under positive scaling\nand addition; thus they are closed under the application of polynomials with non-negative\ncoeﬃcients.\n(4) LetTr(x) be therth Taylor approximation to exp( x) about 0, a polynomial with non-negative\ncoeﬃcients. Then Tr(k1) converges to k= exp(k1) asr→ ∞; equivalently, Tr(K1) converges\ntoK= exp(K1), where KandK1are the Gram matrices of kandk1. Here exp is the\nentry-wise exponential function, not the “matrix exponential”.. From (3), Tr(K1) is PSD. As\nthe PSD cone is a closed subset ofnR×n(it’s deﬁned by non-strict inequalities x⊤Mx≥0),\nthe limit Kis PSD, and this is the Gram matrix of K.\n(5) Applying (2) to the function f(u) = exp( −/ba∇dblu/ba∇dbl2), the kernel k1(u,v) = exp( −/ba∇dblu/ba∇dbl2− /ba∇dblv/ba∇dbl2)\nis PSD. Moreover the kernel k2(u,v) = 2u⊤vis PSD: if Cis a set of vectors, and we let the\nmatrixUbe deﬁned by taking the elements of Cas columns, then the Gram matrix of k2is\n2U⊤Uwhich is PSD. By (4), the kernel k⊤3(u,v) = exp(2 u v) is PSD. By (1), the kernel\nk(u,v) =k1(u,v)k3(u,v) = exp(−/ba∇dblu/ba∇dbl2+2u⊤v−/ba∇dblv/ba∇dbl2)exp(−/ba∇dblu−v/ba∇dbl2)\nis PSD.\nDate: Fall 2015.\n13\n2 18.657 PS 2 SOLUTIONS\n2.Problem 2\nn(1) Suppose x∈dR; we wish to ﬁnd y∈Cminimizing /ba∇dblx−y/ba∇dbl2=/summationtext2\ni=1(xi−yi) . As the\nconstraints deﬁning Capply to each yiseparately, the problem amounts to ﬁnding, for each\ni, a value −1≤yi≤1 minimizing ( x2i−yi) . This is clearly achieved at\niy=/braceleftBigg\nxi/|x\ni|if|xi|>1,\nxiotherwise .\nThis formula is exact, so there is no convergence issue; eﬀectively the method converges\nperfectly after one update on each coordinate.\n(2) Letz∈dRbe given; we want x∈∆ minimizing the ℓ2distance f(x) =/ba∇dblx−z/ba∇dbl.We apply\nmirror descent, following the corollary on page 7 of the Lecture 13 notes. The objective\nis clearly 1-Lipschitz, with gradient ∇f(x) = (x−z)//ba∇dblx−z/ba∇dbl, so we obtain the following\nconvergence guarantee at iteration k:\nf(x◦\nk)−f(x∗)≤/radicalbigg\n2logd.k\n(3) (a) §+\nnis convex: certainly any convex combination of symmetric matrices is symmetric, and\nifA,B∈ §+\nn, then\nx⊤(λA+(1−λ)B)x=λx⊤Ax+(1−λ)x⊤Bx\nisaconvexcombinationofnon-negativereals,thus non-negative,soaconvexcombination\nof PSD matrices is PSD.\n§+\nnis closed inn×nRas it is deﬁned as an intersection of a linear subspace (the symmetric\nmatrices) and the half-spaces /angb∇acketleftM,v⊤v/angb∇acket∇ight ≥0 forv∈nR, all of which are closed; an\nintersection of closed sets is closed.\n(b) LetA∈ §n. As§+\nnis convex and closed, and the function f(B) =1\n2/ba∇dblA−B/ba∇dbl2\nFis convex, a\nmatrixBminimizes foverS+\nniﬀ it satisﬁes ﬁrst-order optimality. Speciﬁcally, we must\nhave that ∇f(B) =/summationtext\niµ⊤ixixi, for some µi≥0 and some xifor which Bis tight for the\nconstraint x⊤\niBxi≥0 deﬁning S+\nn.\nWe compute the gradient:\n1∇(1A2/ba∇dbl −B/ba∇dbl2\nF) =∇tr(A2−2AB+B2) =B2−A.\nThus, if we can write B−A=iµixix⊤\nias above, then we certify Bas optimal.\nWrite the eigendecomposition A=UΣU⊤, and let B=UΣ+U⊤, where Σ +replaces the\nnegative entries of Σ by zero. T/summationtext\nhen\nB−A=U(Σ−Σ)U⊤Σ⊤+ =\ni:Σ/summationdisplay\n(ii)UiUi,\nii<0−\nand we have U⊤\niBUi= (Σ+)ii= 0, thus fulﬁlling ﬁrst-order optimality.\n(4) We begin with the ﬁrst claim. In class, we proved that\n/angb∇acketleftπ(x)−x,π(x)−z/angb∇acket∇ight ≤0,\nwhenever z∈ C. Applying this with z=π(y), we have\n/angb∇acketleftπ(x)−x,π(x)−π(y)/angb∇acket∇ight ≤0.\nSumming a copy of this inequality with the same thing with xandyreversed, we have\n/angb∇acketleftπ(x)−π(y)−(x−y),π(x)−π(y)/angb∇acket∇ight ≤0,\nπ(x)−π(y)/ba∇dbl2≤ /angb∇acketleftx−y,π(x)−π(y)/angb∇acket∇ight ≤ /ba∇dblx−y/ba∇dbl/ba∇dblπ(x)−π(y)/ba∇dbl,\nby Cauchy–Schwartz. Cancelling /ba∇dblπ(x)−π(y)/ba∇dblfrom both sides yields the ﬁrst claim.\nThe second claim follows by specializing to the case y∈ C, for which π(y) =y.\n18.657 PS 2 SOLUTIONS 3\n3.Problem 3\n(1) (a) By deﬁnition, f∗(y) = supx>0xy−1. Wheny >0, a suﬃciently large choice of xmakesx\nthe objective arbitrarilylarge, and the supremum is inﬁnite. When y≤0, the objective is\nbounded above by 0; for y= 0, this is achieved as x→ ∞, whereas for y <0, ﬁrst-order\noptimality conditions show that the optimum is achieved at x= (−y)−1/2, at which\nf∗(y) =−2√−y. We have D= (−∞,0].\n(b) By deﬁnition, f∗(y) = sup1\nx∈dy⊤\nRx−2\n2|x|2. The gradient of the objective is y−x, so\nﬁrst-order optimality conditions are satisﬁed at x=y, and we have f∗(y) =12\n2|x|2(fis\nself-conjugate). Here D=dR.\n⊤ d(c) By deﬁnition, f∗(y) = supx∈Rdy x−log/summationtext\nj=1exp(xj). The partial derivative in xiof\nthe objective function isexpxiyi− ,\njexpxj\nso the gradient may be made zero w/summationtext\nheneverylies in the simplex ∆, by taking xi= logyi.\nFor such y, we thus have f∗(y) =iyilogyi.\nWe next rule out all y/negationslash∈∆, so that/summationtext\nD= ∆. Consider xofthe form ( λ,λ,...,λ ), for which\nthe objective value is λiyi−λ−logd. Wheniyi/negationslash= 1, this can be made arbitrarily\nla\nrge, by taking an extreme value of λ. On the other hand, if any coordinate yiofyis\nnegative, then taking x/summationtext\nto be supported only on t/summationtext\nheith coordinate, the objective value is\nyixi−log((d−1)+expxi), which is arbitrarily large when we take xito be a suﬃciently\nlarge negative number. So ymust lie in the simplex ∆.\n(2) For all x∈Candy∈D, we have f∗(y)≥y⊤x−f(x), so that y⊤x−f∗(y)≤f(x). Thus\nf∗∗(x) = supy∈Dy⊤x\ndR−f∗(y)≤f(x).\n(3) Here C= , so that the supremum is either achieved in some limit of arbitrarily distant\npoints, or else at at point satisfying ﬁrst-order optimality. The ﬁrst case can actually occur,\ne.g. when d= 1,f(x) =−exp(x), andy= 0. In the other case, ﬁrst-order optimality is that\n0 =∇x(y⊤x−f(x)) =y\n∗−∇f(x),\nso that∇f(x) =y.\n(4) We will need the gradient of f∗. Asfis strictly convex, y=∇f(x) is an injective function of\nx, so we can write x= (∇f)−1(y). Then\nf∗(y) =y⊤x∗−f(x∗)\n=y⊤(∇f)−1(y)−f((∇f)−1(y)),\n∇f∗(y) = (∇f)−1(y)+D(∇f)−1(y)[y]\n−1−D(∇f)−1(y)[∇f((∇f)−1)(y))]\n= (∇f) (y),\nwhereDf(a)[b] denotes the Jacobian of f, taken at a, applied to the vector b.\nWe now compute the Bregman divergence:\nDf∗(∇f(y),∇f(x)) =f∗(∇f(y))−f∗(∇f(x))−(∇f∗(∇f(x)))⊤(∇f(y)−∇f(x))\n=∇f(y)⊤y−f(y)−∇f(x)⊤x+f(x)−x⊤(∇f(y)−∇f(x))\n=f(x)−f(y)−(∇f(y))⊤(x−y)\n=Df(x,y).\n4 18.657 PS 2 SOLUTIONS\n4.Problem 4\n(1) Adapting the proof of convergence of projected subgradient descent from the lecture notes,\nwe have:\nf(xs)−f(x∗)≤g⊤\ns(xs−x∗)\n=/ba∇dblgs/ba∇dbl(xsη−ys+1)⊤(xs−x∗)\n/ba∇dblg≤s/ba∇dbl(2η/ba∇dblxs−ys+1/ba∇dbl2+/ba∇dblxs−x∗/ba∇dbl2−/ba∇dblys+1−x∗/ba∇dbl2)\nη=/ba∇dblgs/ba∇dblg+/ba∇dbls/ba∇dbl\n2(2η/ba∇dblxs−x∗/ba∇dbl2−/ba∇dblys+1−x∗/ba∇dbl2),\nasxs−ys+1has norm η,\nη/ba∇dblg≤s/ba∇dblg+/ba∇dbls/ba∇dbl\n2(sη/ba∇dblxs−∗/ba∇dbl2−/ba∇dblxs+12−x∗/ba∇dbl2),\nas the projection operator is a contraction.\nSumming over s, and using the bounds /ba∇dblx1−x∗/ba∇dbl ≤Rand/ba∇dblgs/ba∇dbl ≤L, we have\n∗ηL LR2\nf(x¯s)−f(x)≤+.2 2ηk\nTakingη=R/√\nk, we obtain the rate LR/√\nk.\n(2) (a) Starting from the deﬁnition of β-smoothness:\nβf(xs+1)−f(xs)≤ ∇f(xs)⊤(xs+1−xs)+2/ba∇dblxs+1−xs/ba∇dbl2\nβ=γs∇f(x⊤s) (ys−xs)+γ2\n2s/ba∇dblys−xs/ba∇dbl2\n⊤ ∗ β≤γs∇f(xs) (x−xs)+γ2R2,2s\nasf(xs)⊤y⊤s≤f(xs)yfor ally∈C,\nβ≤γs(f(x∗)−f(xs))+γ2\n2sR2,\nby convexity.\n(b) We induct on k. Continuing from the inequality above by subtracting f(x∗)−f(xs) from\nboth sides, we have\nβf(xs+1)−f(x∗)≤(1−γs)(f(xs)−f(x∗))+γ2\n2sR2,\nor, if we deﬁne δs=f(x∗s)−f(x),\nβδs+1≤(1−γs)δs+γ2\n2sR2.\n18.657 PS 2 SOLUTIONS 5\nSpecializing to s= 1, and noting that γ1= 1, we have that δ2=βR2/2≤2βR2/3, so\nthat the base case of k= 2 is satisﬁed. Now proceeding inductively, we have\nβδk≤(1−γk−1)δk−1+γ2 2\n2k−1R\n2(k−2)βR22βR2\n≤k2+k2,\nby induction and the deﬁnition of γk−1,\n2(k−1)βR2\n=k2\n2βR2\n≤,k+1\ncompleting the induction.\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.657 Mathematics of Machine Learning\nFall 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .", "topic": "Math for ML", "subtopic": "gradient descent", "section_heading": "18.657 PS SOLUTIONS", "source_title": "285d7726dad0e91cf447da611b84e6d6 MIT18 657F15 PS3 Sol", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "e187c17a-a729-40e5-9bc0-bcd1fb61043e", "text": "� �� � 1 6.867 Mac hine learning , lectur e 11 (Jaak kola) \nLecture topics: model selection criteria \n• Minim um descript ion length (MDL) \n• Feature (subs et) selection \nModel selection criteria: Minimum description length (MDL) \nThe minim um description length criterion (MDL) turns the model selectio n problem into a \ncomm unica tion problem. The basic idea is that the best model should lead to the best way \nof compr essing the availabl e data. This view equa tes learning with the ability to compr ess \ndata. This is indeed a sensible view since learning has to do with the ability to make \nprediction s and eﬀective compr ession is precisely based on such ability. So, the more we \nhave learned, the better we can compress the observed data. \nIn a classiﬁc ation context the communicatio n problem is deﬁned as follows. We have a \nsende r and a receiver. The sender has acces s to the training data, both examples x1,..., xn \nand labels y1,...,yn, and needs to comm unica te the labels to the receiv er. The receiv er \nalready has the example s but not the labels. \nAny classiﬁe r that perfectly classiﬁes the training examples would permit the receiver to \nrepro duce the labels for the training exam ples. But the sender (say we) would have to \ncomm unica te the classiﬁer to the receiver before they can run it on the training examples. \nIt is this part that pertains to the complexit y of the set of classiﬁers we are ﬁtting to the \ndata. The more choices we have, the more bits it takes to comm unica te the speciﬁc choice \nthat perfectly classiﬁe d the training examples. We can also consider classiﬁers that are not \nperfectly accurate on the training examples by communicat ing the errors that they make. \nAs a result, the comm unication cost, the number of bits we have to send to the receiver, \ndepends on two thing s: the cost of errors on the training examples (descript ion length of \nthe data given the model), and the cost of describing the selected classiﬁer (descript ion \nlength of the model), both measured in bits. \nSimple two-part MDL. Let’s start by deﬁning the basic concepts in a simpler context \nwhere we just have a sequence of labels. You could view the example s xi in this case \njust specifying the index es of the corresp onding labels. Consider the following sequence of \nmostly 1’s: \nn labels \n111 −111111 −11 ... (1) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "� �� � 1 6.867 Mac hine learning , lectur e 11 (Jaak kola) ", "source_title": "dd0fdf90cb4bb7f2a1b04358588ca506 lec11", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "3d32302f-fe02-4cc7-b42a-bc27209105e3", "text": "of labels. You could view the example s xi in this case just specifying the index es of the corresp onding labels. Consider the following sequence of mostly 1’s: n labels 111 −111111 −11 ... (1) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� � � � 2 6.867 Mac hine learning , lectur e 11 (Jaak kola) \nIf we assume that each label in the sequence is an indep enden t sample from the same \ndistributio n, then each yt ∈ {−1, 1} is a sample from \nP (yt|θ)= θδ(yt,1)(1 − θ)δ(yt,−1) (2) \nfor some θ. Here δ(y1, 1) is an indica tor such that δ(yt, 1) = 1 if yt = 1 and zero otherwise . \nIn our model P (yt =1|θ)= θ. We don’t necessarily know that this is the correct model \nfor the data. But whatev er model (here a set of distr ibutions) we choose, we can evaluate \nthe resulting comm unicat ion cost. \nNow, the question is: given this model for the data, how many bits do we need to send \nthe observ ed sequence of n labels to the receiv er? This can be answ ered generally . Any \ndistributio n over the data can be used as a basis for a coding sceme . Moreover, the op­\ntima l coding scheme, assuming the data really did follow our distribut ion, would require \n− log2 P (y1,...,yn|θ) bits (base two logarithm) or − log P (y1,...,yn|θ) nats (natur al log­\narithm). We will use the natura l logarithm for consistency with other material . You can \nalways convert the answ er to bits by multiplying nats by log(2). \nSince our model assumes that the labels are indep ende nt of each other, we get \nn\n− log P (y1,...,yn|θ)= − log P (yt|θ) (3) \nt=1 \nIt is now tempting to minimize this encoding cost with respect to θ (maximizing the log-\nlikeliho od): \nn n\nmin log P (ytθ) = log P (ytθˆ) (4) \nθ − | − |\nt=1 t=1 \n= −l(y1,...,yn; θˆ) (5) \nwhere l(D; θ) is again the log-likeliho od of data D and θˆis the maxim um likeliho od setting \nof the parameters. This corresponds to ﬁnding the distribution in our model (one corre­\nsponding to θˆ) that minimize s the encoding length of the data. The problem is that the \nreceiv er needs to be able to decode what we send. This is only possible if they have acces s \nto the same distributio n (be able to recreat e the code we use). So the receiver would have \nto know θˆ(in addition to the model we are considering but we will omit this part, assuming \nthat the receiv er alrea dy knows the type of distributio ns we are have). \nThe solution is to encode the choice of θˆas well. This way the receiver can recreate our \nsteps to decode the bits we have sent. So how do we encode θˆ? It is a continuous parameter \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "if ", "section_heading": "of labels. You could view the example s xi in this case just specifying the index es of the corresp onding labels. Consi", "source_title": "dd0fdf90cb4bb7f2a1b04358588ca506 lec11", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 1}
{"id": "7ad8c249-a8b7-405f-a1b6-e218a4b14e8d", "text": "The solution is to encode the choice of θˆas well. This way the receiver can recreate our steps to decode the bits we have sent. So how do we encode θˆ? It is a continuous parameter Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3 6.867 Mac hine learning , lectur e 11 (Jaak kola) \nand it migh t seem that we need an inﬁnite number of bits to encode a real number. But \nnot all θ can arise as ML estimates of the parameters. In our case, the ML estimates of \nthe parameters θ have the following form \nθˆ= nˆ(1) (6) n \nwhere ˆn(1) is the number of 1’s in the observ ed sequence. There are thus only n+1 possible \nvalues of θˆcorrespondin g to ˆn(1) ∈{0,...,n}. Since the rec eiver already kno ws that w e are \nencoding a sequence of length n, then we can just send θ by deﬁning a distributio n over its \npossible discrete values, say Q(θ = k/n)= 1 for simplic ity (i.e., a uniform distr ibution n+1 \nover possible discrete values). As before the cost in bits (nats) is − log Q(θˆ) = log(n + 1). \nThe total cost we have to send to the receiv er is therefore \nDL of data given modelDL of model� �� � ���� \nDL = −l(y1,...,yn; θˆ) + log(n + 1) (7) \nThis is known as the description length of the sequence. The minim um description length \ncriterio n simple ﬁnds the model that leads to the shortest description length. Asympt oti­\ncally, the simple two-par t MDL principle is essentially the same as BIC. \nUniversal coding, normalized maximum likelihood. The problem with the simple \ntwo part coding is that the descript ion length we associate with the second part, the model, \nmay seem a bit arbitrar y. We can correct this by ﬁnding a distribu tion that is in some \nsense universally closest to just encoding the data with the best ﬁtting distribut ion in our \nmodel: \n− log PNML(y1,...,yn) ≈ min −l(y1,...,yn; θ)= − max l(y1,...,yn; θ) (8) \nθ θ \nIn other words, we don’t wish to deﬁne a prior over the parameters in our model so as to \nencode the parameter values. While it won’t be possible to achieve the above appr oximate \nequalit y with equa lity since the right hand side, if exponentiated, wouldn’t deﬁne a valid \ndistributio n (because of the ML ﬁtting) . The distribution that minimizes the maximum \ndeviat ion in Eq.(8) is given by \nexp(max θ l(y1,...,yn; θ))PNML(y1,...,yn)= � (9) \ny1�,...,y�exp(max θ l(y1�,...,yn�; θ)) \nn \nand is known as the normalize d maximum likeliho od distribution. Using this distribut ion \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "The solution is to encode the choice of θˆas well. This way the receiver can recreate our steps to decode the bits we ha", "source_title": "dd0fdf90cb4bb7f2a1b04358588ca506 lec11", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "0bdbb33f-8a70-4da6-a06f-a29654042a59", "text": "minimizes the maximum deviat ion in Eq.(8) is given by exp(max θ l(y1,...,yn; θ))PNML(y1,...,yn)= � (9) y1�,...,y�exp(max θ l(y1�,...,yn�; θ)) n and is known as the normalize d maximum likeliho od distribution. Using this distribut ion Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � � 4 6.867 Mac hine learning , lectur e 11 (Jaak kola) \nto encode the sequence leads to a minimax optimal coding lengt h \nParametric complexit y of model � � ��� �� \n− log PNML(y1,...,yn)= − max l(y1,...,yn; θ) + log exp max l(y1�,...,yn�; θ) (10) \nθ θ \ny1�,...,y�n \nNote that the result is again in the same form, the negative log-lik elihood with a ML param­\neter setting and a complexit y penalt y. The new parametric complexit y penalt y depends on \nthe length of the sequence as well as the model we use. While the criterio n is theoretica lly \nnice, it is hard to evaluate the parametric comple xity penalty in practice. \nFeature subset selecti on \nFeature selec tion is a pr oblem where w e wis h to iden tify componen ts of feature v ector s most \nuseful for classiﬁca tion. For example, some comp onents may be very noisy and theref ore \nunreliable. Depending on how such featur es1 would be treated in the classiﬁcatio n metho d, \nit may be best not to include them. Here we assume that we have no prior knowledge of \nwhich features migh t be useful for solving the classiﬁc ation task and instead have to provide \nan automa ted solutio n. This is a type of model selectio n problem, where the models are \nidentiﬁed with subse ts of the features we choose to include (or the number of features we \nwish to include). Note that kernels do not solve nor avoid this problem. By taking an inner \nproduct between feature vector s that may contain many noisy or irrelev ant features results \nin an unnec essarily varying kernel value. \nInstead of selecting a subset of the features we could also try to weight them according to \nhow useful they are. This is a related feature weighting problem and we will get to this \nlater on. This is also the metho d typically used with kernels (cf. kernel optimiza tion). \nLet’s address the featur e selection problem a bit more concretely . Supp ose our input \nfeatures are d−dimensional binary vectors x =[x1,...,xd]d where xi ∈ {−1, 1} and labels \nare binary y ∈ {−1, 1} as before. We will begin by using a particular type of probability \nmodel known as Naive Bayes model to solve the classiﬁcation problem. This will help tie \nour MDL discussion to the feature selection problem. In the Naiv e Bayes model, we assume \nthat the featur es are conditio nally indep endent given the label so that \nd\nP (x,y)= P (xi|y) P (y) (11) \ni=1 \n1We will refer to the comp onen ts of the feature/inp ut vectors as “features”. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "minimizes the maximum deviat ion in Eq.(8) is given by exp(max θ l(y1,...,yn; θ))PNML(y1,...,yn)= � (9) y1�,...,y�exp(ma", "source_title": "dd0fdf90cb4bb7f2a1b04358588ca506 lec11", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "4c93f538-261d-42b3-9d62-685003ab93f8", "text": "featur es are conditio nally indep endent given the label so that d P (x,y)= P (xi|y) P (y) (11) i=1 1We will refer to the comp onen ts of the feature/inp ut vectors as “features”. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � � \n� � � � � \n� � � \n� � � � � 5 6.867 Mac hine learning , lectur e 11 (Jaak kola) \nPut another way, in this model, the features are correlated only throug h the labels. We \nwill contrast this model with a “null model” that assumes that none of the features are \nrelev ant for the label: \nd\nP0(x,y)= P (xi) P (y) (12) \ni=1 \nIn other words, in this model we assume that the featur es and the label are all indep enden t \nof each other. \nThe Naiv e Bayes model (as well as the null model) are gener ative models in the sense that \nthey can genera te all the data, not just the labels. Thes e models are typically trained using \nthe log-likeliho od of all the data as the estima tion criterion. Let us follow this here as well. \nWe will discuss later on how this aﬀects the results. So, we aim to maximize \nn n d\nlog P (xt,yt) = log P (xti|yt) + log P (yt) (13) \nt=1 t=1 i=1 \nd n n\n= log P (xti|yt) + log P (yt) (14) \ni=1 t=1 t=1 \nd\n= nˆiy(xi,y)log P (xi|y)+ nˆy(y)log P (yt) (15) \ni=1 xi,y y \nwhere we have used counts ˆniy(xi,y) and ˆny(y) to summa rize the availab le data for the \npurp ose of estimating the model parameters. nˆiy(xi,y) indic ates the number of training \ninstances that have value xi for the ith feature and y for the label. These are called suﬃcient \nstatistics as they are the only thing s from the data that the model cares about (the only \nnumbers computed from the data that are required to estima te the model parameters). The \nML parameter estimat es, values that maximize the above expression, are simply fractions \nof these coun ts: \nPˆ(y)= nˆy(y) (16) n \nnˆiy(xi,y)Pˆ(xi|y)= nˆy(y) (17) \nAs a result, we also have Pˆ(xi,y)= Pˆ(xi|y)Pˆ(y)= nˆiy(xi,y)/n. If we plug these back into \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "featur es are conditio nally indep endent given the label so that d P (x,y)= P (xi|y) P (y) (11) i=1 1We will refer to t", "source_title": "dd0fdf90cb4bb7f2a1b04358588ca506 lec11", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "dc25223a-fd57-4d2e-935b-3b886189698d", "text": "es, values that maximize the above expression, are simply fractions of these coun ts: Pˆ(y)= nˆy(y) (16) n nˆiy(xi,y)Pˆ(xi|y)= nˆy(y) (17) As a result, we also have Pˆ(xi,y)= Pˆ(xi|y)Pˆ(y)= nˆiy(xi,y)/n. If we plug these back into Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� � � � \n� � � \n� � � � 6 6.867 Mac hine learning , lectur e 11 (Jaak kola) \nthe expression for the log-likeliho od, we get \nn\nˆl(Sn) = log Pˆ(xt,yt) (18) \nt=1 \nd\n= n �� nˆiy(\nnxi,y) log Pˆ(xi|y)+ � nˆy\nn (y) log Pˆ(yt) (19) \ni=1 xi,y y⎡ ⎤ \nH(Y ) −Hˆ(Xi|Y ) − ˆ\nd� �� �����⎢�� � ⎥ \n= n ⎢⎣ Pˆ(xi,y)log Pˆ(xi|y)+ Pˆ(y)log Pˆ(yt)⎥⎦ (20) \ni=1 xi,y y \nd\n= n − Hˆ(Xi|Y ) − Hˆ(Y ) (21) \ni=1 \nwhere Hˆ(Y ) is the Shannon entropy (uncerta inty) of y relat ive to distribut ion Pˆ(y) and \nHˆ(Xi|Y ) is the conditional entropy of Xi given Y . Entropy H(Y ) measures how many bits \n(here nats) we would need on average to encode a value y chosen at random from Pˆ(y). It \nis zero when y is deterministic and takes the largest value (one bit) when Pˆ(y)=1/2 in \ncase of binar y labels. Similarly , the condit ional entropy Hˆ(Xi|Y ) measures how uncerta in \nwe would be about Xi if we already knew the value for y and the values for these variables \nwere sampled from Pˆ(xi,y). If xi is perfectly predicta ble from y accor ding to Pˆ(xi,y), then \nHˆ(Xi|Y ) = 0. \nWe can perform a similar calculat ion for the null model and obtain \nn d\nˆl0(Sn) = log Pˆ0(xt,yt)= n − Hˆ(Xi) − Hˆ(Y ) (22) \nt=1 i=1 \nNote that the condition ing on y is gone since the features were assumed to be indep enden t \nof the label. By taking a diﬀerence of these two, we can gauge how much we gain in terms \nof the resulting log-lik eliho od if we assume the features to depend on the label: \n�d� � \nˆl(Sn) − lˆ0(Sn)= n Hˆ(Xi) − Hˆ(Xi|Y ) (23) \ni=1 \nThe expression \nIˆ(Xi; Y )= Hˆ(Xi) − Hˆ(Xi|Y ) (24) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "es, values that maximize the above expression, are simply fractions of these coun ts: Pˆ(y)= nˆy(y) (16) n nˆiy(xi,y)Pˆ(", "source_title": "dd0fdf90cb4bb7f2a1b04358588ca506 lec11", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "7cdfd782-b2e2-42a9-8e9c-70409585c0c9", "text": "resulting log-lik eliho od if we assume the features to depend on the label: �d� � ˆl(Sn) − lˆ0(Sn)= n Hˆ(Xi) − Hˆ(Xi|Y ) (23) i=1 The expression Iˆ(Xi; Y )= Hˆ(Xi) − Hˆ(Xi|Y ) (24) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n7 6.867 Mac hine learning , lectur e 11 (Jaak kola) \nis known as the mutual infor mation between xi and y, relative to distri bution Pˆ(xi,y). \nMutual inform ation is non-ne gative and symmetric. In other words, \nIˆ(Xi; Y )= Hˆ(Xi) − Hˆ(Xi|Y )= Hˆ(Y ) − Hˆ(Y |Xi) (25) \nTo understa nd mutual informa tion in more detail, please consult, e.g., Cover and Tomas \nlisted on the course website. \nSo, as far as featur e selec tion is concerned, our calculation so far suggests that we should \nselect the featur es to include in the decrea sing order of Iˆ(Xi; Y ) (the higher the mutual \ninforma tion, the more we gain in terms of log-likelihood). It may seem a bit odd, however, \nthat we can select the features indiv idually , i.e., not consider them in speciﬁc combinations. \nThis is due to the simple Naive Bayes assumpt ion, and our use of the log-likeliho od of all \nthe data to deriv e the selectio n criterio n. The mutua l infor matio n criterio n is neverthele ss \nvery common and it is useful to understa nd how it comes about. We will now turn to \nimpro ving this a bit with MDL. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "resulting log-lik eliho od if we assume the features to depend on the label: �d� � ˆl(Sn) − lˆ0(Sn)= n Hˆ(Xi) − Hˆ(Xi|Y ", "source_title": "dd0fdf90cb4bb7f2a1b04358588ca506 lec11", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "724f1815-aa01-4504-9f9d-d13d6cdbb51e", "text": "Massa chusetts Institu te of Techn ology \n6.867 Machine Lear ning, Fall 2006 \nProblem Set 2: Solutions \n1. (a) (5 points) From the lecture notes (Eqn 14, Lecture 5), the optimal parameter values for linear \nregres sion given the matri x of trainin g examp les X and the corresponding response variables y is: \nθ =(XT X)−1XT y \nThe quan tity (XT X)−1XT is also known as the pseudo-in verse of X, and often occurs when dealing \nwith linear systems of equation s. When X is a squar e matrix and invertible, it is exactly the same \nas the inverse of X. \nMATLAB provides many ways of achievin g our desired goal. We can directly write out the expres­\nsion above or use the function pinv. Here are the function s linear regress and linear pred: \nfunction theta = linear_regress(y,X)\ntheta = pinv(X)*y;\nfunction y_hat = linear_pred(theta,X)\ny_hat = X*theta;\nIn linear regress, note that we are not calculating θ0 separately . This diﬀers from the description \nin the lecture notes where the training exampl es were expli citly padded with 1’s, allowing us to \nintroduce an oﬀset θ0. Inste ad, we will use a feature mapping to achieve the same eﬀect. \n(b) (2 points) Before we describe the soluti on, we ﬁrst describe how the datase t was created. This \nmay help you appreciate why some feature mappi ngs may work better than others. The x values \nwere created by sampling each coordinate uniformly at random from (-1,1): \nX in = rand(1000,3)*2 -1 \nGiven a particular x, the corresponding ytrue and ynoisy values were created as follows: \nytrue = −10log x12 − 15log x22 − 7.5log x2 +2 3 \nynoisy = ytrue + �� ∼ N(0, 100) \nTo evaluate the performance of linear regression on given trainin g and test sets, we created the \nfunction test linear reg which combines the regres sion, prediction, and evaluati on steps. You \nmay, of course , do it in some other way: \nfunction err = test_linear_reg(y_train, X_train, y_test, X_test)\n% train linear regression using X_train and y_train\n% evaluate the mean squared prediction error on X_test and y_test\ntheta = linear_regress(y_train, X_train);\ny_hat = linear_pred(theta,X_test);\nyd = y_hat -y_test;\nerr = sum(yd.^2)/length(yd); %err = Mean Squared Prediction Error\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "Massa chusetts Institu te of Techn ology ", "source_title": "be6c46f4bb8a2e709c04533b7979d51f hw2 soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "b0df6c74-b70f-445a-81e2-98abf972fdfa", "text": "regression using X_train and y_train % evaluate the mean squared prediction error on X_test and y_test theta = linear_regress(y_train, X_train); y_hat = linear_pred(theta,X_test); yd = y_hat -y_test; err = sum(yd.^2)/length(yd); %err = Mean Squared Prediction Error Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nUsing this, we can now calculate the mean squared prediction error (MSP E) for the two feature \nmapp ings: \n>> X1 = feature_mapping(X_in,1);\n>> test_linear_reg(y_noisy, X1, y_true, X1)\nans = 1.5736e+003\n>> X2 = feature_mapping(X_in,2);\n>> test_linear_reg(y_noisy, X2, y_true, X2)\nans = 0.5226\nThe two sets of error s are 1573.6 (φ1) and 0.5226 (φ2), respectively. Unsurprisingly , the mapp ing \nφ2 performs much better than φ1– it is exactly the space in which the relation ship between X and \ny is linear. \n(c) (10 points) Recall from the notes (Eqn 8, lecture 6), that the desired quantity we need to maximize \nis \nvT AAv \n1+ vT Av \nwhere A =(XT X)−1 . In the notes , an oﬀset paramete r is explicitly assume d so that v =[xT , 1]T . \nHowever, in our case this is not necessary and so v = x. \nActiv e learning may, in general, select the same point x to be observed repeatedly . Each of \nthese observation s corresponds to a diﬀerent ynoisy. However, due to practical limitation s, we had \nsupp lied you with only one set of ynoisy values. Thus, if some xi occurs repeatedl y in idx, you will \nneed to use the same ynoisy, i for each occurrence of xi. Alternatively, you could change your code \nso as to disallow repti tions in idx. This is the option we have chosen here. \nGiven any feature space, the criterion above will aim to ﬁnd points far apart in that space. However, \nthese points may not be far apart in the featur e space where classiﬁcation actually occurs. This is \nof particular conce rn when the latter featur e space migh t not be easily accessible (e.g., when using \na kernel like the radial basis function kernel). \nHere’s the active learning code: \nfunction idx = active_learn(X,k1,k2)\nidx = 1:k1;\nN = size(X,1);\nfor i=1:k2\nvar_reduction = zeros(N,1);\nX1 = X(idx,:);\nA = inv(X1’*X1);\nAA = A*A;\nfor j=1:N\nif ismember(j,idx) %this is the part where we disallow repititions \ncontinue;\nend\nv = X(j,:);\na = v*AA*v’ / ( 1 + (v*A*v’));\nvar_reduction(j) = a;\nend\n[a, aidx] = max(var_reduction);\nidx(end+1) = aidx;\nend \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Classical Search", "subtopic": "a*", "section_heading": "regression using X_train and y_train % evaluate the mean squared prediction error on X_test and y_test theta = linear_re", "source_title": "be6c46f4bb8a2e709c04533b7979d51f hw2 soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "f0d2662a-d984-4d7e-a989-04637269be7d", "text": "j=1:N if ismember(j,idx) %this is the part where we disallow repititions continue; end v = X(j,:); a = v*AA*v’ / ( 1 + (v*A*v’)); var_reduction(j) = a; end [a, aidx] = max(var_reduction); idx(end+1) = aidx; end Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nUsing it, we can now compute the desired quantities: \n>> idx1 = active_learn(X1,5,10)\nidx1 = 1 2 3 4 5 437 270 928 689 803 670 979 932 558 490\n>> test_linear_reg(y_noisy(idx1), X2(idx1,:), y_true, X2)\nans = 2.5734e+003\n>> idx2 = active_learn(X2,5,10)\nidx2 = 1 2 3 4 5 955 803 270 558 628 490 283 592 761 414\n>> test_linear_reg(y_noisy(idx2), X2(idx2,:), y_true, X2)\nans = 24.4643\nThus, the MSP E when using φ1 for active learni ng chooses points that are not well-placed (for this \nparticular dataset); φ2 performs much better. Note that the feature mapp ing used to perform the \nregres sion (and evaluati on) is the same for both (φ2) so the performance diﬀerence is due to the \npoints chosen by activ e learning. \nThe answers change when repititions are allowed (MSPE for φ1= 117.47 ; for φ2 = 25.25), but they \nstill illustrate our concept.\nFor completeness’ sake, the answers for the original version of the problem are: (φ1: 2618.9 (re­\npeats), 2618.6 (no repeats)) and (φ2: 25.2546 (no repeats) and 24.4643 (repeats)).\n(d) (4 points) This error will vary, depending upon the number of iterations you perform and the \nrandom points selected. In my simulations, the value of error (using mapp ing φ2 for regres sion and \nevaluation ) was 33.694. When this simulation was run for a 1000 runs, the error was close to 25.34 \n. Clearly, it seems to be much better to perform random sampl ing than perform activ e learning \nin φ1’s space . This may seem surprising at ﬁrst, but is completely understand able: in the space \nwhere regression is performe d (φ2) the points chosen by performin g active learni ng in the φ1 space \nare not far apart at all, and are thus particularl y bad points to be sampled. \nFor complete ness’ sake, the answ er for the origin al version of the problem is: (φ1: 2218. 9 , φ2: \n33.694). \n(e) (4 points) The ﬁgures are shown in Fig 1. For clarity’s sake, we have only plotted the last 10 \npoints of idx (since the ﬁrst 5 are the same across all cases).\nIn the origin al featur e space (ﬁg a), the points selected by activ e learning in the φ1 space are spread\nfar apart, as expected. However, as part (b) showed, a better ﬁt to data is obtai ned by using φ2.\nIn this space (ﬁg b), the points selected by active learni ng in the φ1 space are very closely bunched\ntogether. The points selected by activ e learn ing in the φ2 space are, in contrast, spread far apart.\nThis helps explain why the points learn ed using active learning on the φ1 space did not lead to\ngood performa nce in the regres sion step.\n2. (a) 5 points The function f(t)= −βt2/2 monotoni cally decrease s with t (for β> 0). The function \ng(t)= et monoton ically increas es with t. Thus, the function gof(t)= h(t)= e−βt2/2 monoton ically \ndecre ases with t. As such, the RBF kernel K(x, x�)= e− β \n2 �x−x��2 deﬁn es a Gram matrix that \nsatisﬁes the cond itions of Michelli’s theorem and is hence invertib le. \n(b) 5 points Let A =(λI + K)−1y. Then, \nlim A = K−1y \nλ0→\nSince K is always invertible, this limit is always well-deﬁn ed. Now, we have ˆαt = λA t where At is \nthe t-th element of A. We then have: \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Classical Search", "subtopic": "a*", "section_heading": "j=1:N if ismember(j,idx) %this is the part where we disallow repititions continue; end v = X(j,:); a = v*AA*v’ / ( 1 + (", "source_title": "be6c46f4bb8a2e709c04533b7979d51f hw2 soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "11a4303a-f8f3-4eae-abb3-64a5cf342d06", "text": "K)−1y. Then, lim A = K−1y λ0→ Since K is always invertible, this limit is always well-deﬁn ed. Now, we have ˆαt = λA t where At is the t-th element of A. We then have: Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� \n� \n� \n� \n� \n−101\n−1 −0.8−0.6−0.4−0.20 0.20.40.60.81−1−0.8−0.6−0.4−0.200.20.40.60.81\n−12\n−10\n−8\n−6\n−4\n−2\n0−16−14−12−10−8−6−4−20−15−10−50(a) In φ1 space (b) In φ2 space \nFigure 1: The red circles corres pond to points chosen by performin g active learning on the φ1 space and the\nblue ones correspond to those chosen by performin g activ e learning on the φ2 space .\nOnly the last 10 points of idx are shown for each; the ﬁrst 5 points are the same.\ny(x) = � n\nt=1(ˆαt/λ)K (xt, x), or \ny(x) = �tn \n=1(λA t/λ)K (xt, x), or \ny(x) = tn \n=1 AtK(xt, x) \nWe then have, \nlimλ→0 y(x) = � n\nnt=1(limλ→0 At)K(xt, x), or \nlimλ0 y(x) = t=1 BtK(xt, x), where B = K−1y →\nThus, in the requi red limit, the function y(x)= in \n=1 BtK(xt, x) \n(c) 10 points To prove that the training error is zero, we need to prove that y(xt)= yt for t =1,...,n. \nFrom part (b), we have \ny(xt)= � n\ni=1 B�iK(xi, xt), or \ny(xt)= n ( jn \n=1 yj K−1(i,j))K(xi, xt) or�i=1� y(xt)= � n\nj=1 yj n\ni=1 K−1(i,j)K(xi, xt), or \ny(xt)= jn \n=1 yj δ(j,t), or \ny(xt)= yt \nwhere K−1(i,j)=(i,j)-th entry of K−1 and \n0 for i = jδ(i,j)= �\n1 for i = j \nHere, we made use of the fact that K(xi, xt)= K(xt, xi) and that if A = B−1 then i A(t,i)B(i,j)= \nδ(t,j). \n(d) 5 points Samp le code for this problem is shown below: \nNtrain = size(Xtrain,1);\nNtest = size(Xtest,1);\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "function", "section_heading": "K)−1y. Then, lim A = K−1y λ0→ Since K is always invertible, this limit is always well-deﬁn ed. Now, we have ˆαt = λA t w", "source_title": "be6c46f4bb8a2e709c04533b7979d51f hw2 soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 3}
{"id": "ca18e1b7-75dd-4921-86f0-653c8178d0c1", "text": "of the fact that K(xi, xt)= K(xt, xi) and that if A = B−1 then i A(t,i)B(i,j)= δ(t,j). (d) 5 points Samp le code for this problem is shown below: Ntrain = size(Xtrain,1); Ntest = size(Xtest,1); Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� for i=1:length(lambda), \nlmb = lambda(i); \nalpha = lmb * ((lmb*eye(Ntrain) + K)^-1) * Ytrain;\nAtrain = (1/lmb) * repmat( alpha’, Ntrain, 1);\nyhat_train = sum(Atrain.*K,2);\nAtest = (1/lmb) * repmat( alpha’, Ntest, 1);\nyhat_test = sum(Atest.*(Ktrain_test’), 2);\nE(i,:) = [mean((yhat_train-Ytrain).^2),mean((yhat_test-Ytest).^2)]; \nend; \n00.10.20.30.40.50.60.70.80.9 1010203040506070\nβTrain Error\nTest Error\nFigure 2: Trainin g and test error for Prob # 2(e) \nThe resulting plot is shown in Fig 2. As can be seen the training error is zero at λ = 0 and increas es \nas λ increases. The test error initially decreases, reaches a minimum around 0.1, and then increas es \nagain . This is exactly as we would expect. λ ≈ 0 results in over-ﬁttin g (the model is too powerful). \nOur regression function has a low bias but high variance. By increas ing λ we constrain the model, \nthus increas ing the training error. While the regular ization increases bias, the variance decrease s \nfaster, and we generalize better. High values of λ result in under-ﬁt ting (high bias, low variance ) \nand both training error and test errors are high. \n3. (a) Observe that θ is only a sum of ytφ(X t)’s, so we can just store the coeﬃcients: \nn\nθ = wtytφ(X t). \nt=1 \nWe can update wt’s by increm enting wt by one when a mistak e is made on exam ple t. Class ifying \nnew exampl es means evaluatin g: \nn\nθT φ(X)= wtytK(Xt,X), \nt=1 \nwhich only involves kernel operation s.\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "regression", "section_heading": "of the fact that K(xi, xt)= K(xt, xi) and that if A = B−1 then i A(t,i)B(i,j)= δ(t,j). (d) 5 points Samp le code for thi", "source_title": "be6c46f4bb8a2e709c04533b7979d51f hw2 soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "97d71db1-f1b3-4bc0-9ff5-cfbfb5050c0b", "text": "wt’s by increm enting wt by one when a mistak e is made on exam ple t. Class ifying new exampl es means evaluatin g: n θT φ(X)= wtytK(Xt,X), t=1 which only involves kernel operation s. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n(b) The most straigh tforward proof: use the regres sion argumen t to show that you can ﬁt the points \nexactly and not only achieve the correct sign, but the value of the discriminan t function can be \nmade ±1 for every training example. \n(c) Here is my solution. \n% kernel=‘(1 + trans pose(x i)xj )d’; d = 5; % polynomial kernel \n% kernel=‘exp(−trans pose(x i − xj )(xi − xj )/(2s2))’; s = 3; % radial basis function kernel \nfunct ion α=tra in kernel perceptron(X , y, kernel) \nn,d = size(X ); · \nK = []; · \nfor i =1: n · \nxi = X(i, :)�; ·· \nfor j =1: n ·· \n··· xj = X(j, :)�; \nK(i,j)= eval(kernel); ··· \nend ·· \nend· \n· \nα = zeros (n, 1); · \nmistakes = 1; · \nwhile mistakes > 0 · \nmistakes = 0; ·· \nfor i =1: n · · \n· · · · · · if α�K(:, i)y(i) ≤ 0 \n· α(i) = α(i) + y(i); \n· · · · mistakes = mistak es + 1; \n· · · end \nend ·· \nend· \nfunct ion f =discrimina nt functi on(α, X, kernel, Xtest) \nn,d = size(X ); · \nK = []; · \nfor i =1: n · \nxi = X(i, :)�; ·· \nxj = Xtest; ·· \nK(i)= eval(kernel); ·· \nend· \nf = α�K; · \n(d) The original dataset requires d = 4; the new datase t requires d = 2. An RBF will easily separate \neither datase t. \n� load p3 a \n“X” and “y” loaded. \n� kernel = ‘(1 + trans pose(x i) ∗ xj )2’; \n� α = train kernel perceptron(X ,y, kernel); \n� ﬁgur e \n� hold on \n� plot(X(1 : 1000, 1),X(1 : 1000, 2), ‘rs’) \n� plot(X(1001 : 2000, 1),X(1001 : 2000, 2), ‘bo’) \n� plot dec boundar y(α,X, kernel, [−4, −2], [0.5, 0.5], [2, 4]) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "perceptron", "section_heading": "wt’s by increm enting wt by one when a mistak e is made on exam ple t. Class ifying new exampl es means evaluatin g: n θ", "source_title": "be6c46f4bb8a2e709c04533b7979d51f hw2 soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "79d092c2-e174-4aa9-913e-82ed220d46bf", "text": "� ﬁgur e � hold on � plot(X(1 : 1000, 1),X(1 : 1000, 2), ‘rs’) � plot(X(1001 : 2000, 1),X(1001 : 2000, 2), ‘bo’) � plot dec boundar y(α,X, kernel, [−4, −2], [0.5, 0.5], [2, 4]) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n−4 −3 −2 −1 0 1 2 3−3−2−101234\n � kernel = ‘exp(−trans pose(x i − xj ) ∗ (xi − xj )/18)’; \n� α = train kernel perceptron(X ,y, kernel); \n� ﬁgur e \n� hold on \n� plot(X(1 : 1000, 1),X(1 : 1000, 2), ‘rs’) \n� plot(X(1001 : 2000, 1),X(1001 : 2000, 2), ‘bo’) \n� plot dec boundar y(α,X, kernel, [−4, −2], [0.5, 0.5], [2, 4]) \n−3 −2 −1 0 1 2−2−10123\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "perceptron", "section_heading": "� ﬁgur e � hold on � plot(X(1 : 1000, 1),X(1 : 1000, 2), ‘rs’) � plot(X(1001 : 2000, 1),X(1001 : 2000, 2), ‘bo’) � plot ", "source_title": "be6c46f4bb8a2e709c04533b7979d51f hw2 soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "3bedcd16-990d-478b-934c-61f3b837e209", "text": "6.867 Machine learning and neural networks \nFALL 2001 – Final exam \nDecem ber 11, 2001 \n(2 points) Your name and MIT ID #: \n(4 points) The grade you would give to yourself + brief justiﬁcat ion. If you \nfeel that there’s no question that your grade should be A (and you feel we agree with \nyou) then just write “A”. \nProblem 1 \n1. (T/F – 2 points) The sequence of output symbols sampled from a \nhidden Markov model satisﬁes the ﬁrst order Markov property F \n2. (T/F – 2 points) Increasing the number of values for the the hid­\nden states in an HMM has much great er eﬀect on the comput ational \ncost of forward-backward algorithm than increasing the length of the \nobserv ation sequence. T \n1\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "hmm", "section_heading": "6.867 Machine learning and neural networks ", "source_title": "85d4ef5363a3445d33ca956c394beec6 final f01soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "be2cf538-e8ae-4165-a76b-96dbfe511d4a", "text": "number of values for the the hid­ den states in an HMM has much great er eﬀect on the comput ational cost of forward-backward algorithm than increasing the length of the observ ation sequence. T 1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3. (T/F – 2 points) In HMMs , if there are at least two distinct most T \nlikely hidden state sequences and the two state sequence s cross in the \nmiddle (share a single state at an intermediate time point), then there \nare at least four most likely state sequences . \n4. (T/F – 2 points) One advantage of Boosting is that it does not overﬁt. F \n5. (T/F – 2 points) Supp ort vector machines are resistan t to outliers, F \ni.e., very noisy examples drawn from a diﬀerent distribution. \n6. (T/F – 2 points) Activ e learning can substan tially reduce the number T \nof training examples that we need. \nProblem 2 \nConsider two classiﬁe rs: 1) an SVM with a quadra tic (second order polynomial) kernel \nfunction and 2) an unconstra ined mixture of two Gaussians model, one Gaussian per class \nlabel. These classiﬁers try to map examples in R2 to binary labels. We assume that the \nproblem is separable, no slack penalties are added to the SVM classiﬁer, and that we have \nsuﬃcie ntly many training examples to estimate the covariance matrices of the two Gauss ian \ncomp onents. \n1. (T/F – 2 points) The two classiﬁe rs have the same VC-dimension. T \n2. (4 points) Supp ose we evaluated the structural risk minimizatio n score for the two \nclassiﬁers. The score is the bound on the expected loss of the classiﬁer, when the \nclassiﬁer is estimated on the basis of n training examples . Whic h of the two classiﬁers \nmigh t yield the better (lower) score? Provide a brief justiﬁcat ion. \nThe SVM would proba bly get a better score. Both classiﬁers have the same com­\nplexit y penalt y but SVM would better optimize the train ing error resulting in a \nlower (or equal) overall score. \n2\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "number of values for the the hid­ den states in an HMM has much great er eﬀect on the comput ational cost of forward-bac", "source_title": "85d4ef5363a3445d33ca956c394beec6 final f01soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "cc8fbebe-a990-4b8f-97b6-ae89113d8483", "text": "SVM would proba bly get a better score. Both classiﬁers have the same com­ plexit y penalt y but SVM would better optimize the train ing error resulting in a lower (or equal) overall score. 2 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3. (4 points) Supp ose now that we regularize the estima tion of the covariance matr ices \nfor the mixture of two Gaussians. In other words, we would estimate each class \nconditio nal covariance matrix accor ding to \nn n�ˆ ˆ Σreg = Σ+ S (1) n + n� n + n� \nwhere n is the number of training examples , ˆΣ is the unregularized estima te of the \ncovariance matri x (sample covariance matrix of the examples in one class), S is our \nprior covariance matr ix (same for both classes ), and n� the equiv alent sample size \nthat we can use to balance between the prior and the data. \nIn computing the VC-dimension of a classiﬁer, we can choose the set of points that \nwe try to “shatter” . In particular, we can scale any k points by a large facto r and \nuse the resulting set of points for shatt ering. In light of this, would you expect our \nregula rization to chang e the VC-dimension? Why or why not? \nNo. We can scale the points so that the sample covariance matrix becomes very\nlarge in compa rison to the prior, essentially washing away any eﬀect from the prior.\n4. (T/F – 2 points) Regulariza tion in the above sense would impro ve \nthe structur al risk minimizatio n score for the mixtur e of two Gaussians. F\n3\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "SVM would proba bly get a better score. Both classiﬁers have the same com­ plexit y penalt y but SVM would better optimi", "source_title": "85d4ef5363a3445d33ca956c394beec6 final f01soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "f846b58a-930b-4951-b194-7c2f406b8e39", "text": "away any eﬀect from the prior. 4. (T/F – 2 points) Regulariza tion in the above sense would impro ve the structur al risk minimizatio n score for the mixtur e of two Gaussians. F 3 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 3 \nThe problem here is to predict the identity of a person based on a single handwritten \ncharacter. The observed characters (one of ’a’, ’b’, or ’c’) are transfor med into binary 8 \nby 8 pixel images. There are four diﬀerent people we need to identify on the basis of such \ncharacters. To do this, we have a training set of about 200 examples, where each example \nconsists of a binary 8x8 image and the identity of the person it belongs to. You can assume \nthat the overall number of occurences of each person and each character in the training set \nis roughly balanced. \nWe would like to use a mixture of experts architecture to solve this problem. \n1. (2 points) How might the experts be useful ? Suggest what task each expert might \nsolve. \nEach expert would be responsible for classifying by a single digit; this corresp onds \nto a mixture of 3 experts. \n2. (4 points) Draw a graphical model that describ es the mixture of experts architecture \nin this context. Indicate what the variables are and the number of values that they \ncan take. Shade any nodes corresp onding to variables that are always observed. \nyi\nx x x1 2 64\ni ∈ 1, 2, 3 -the digit type; y ∈ 1, 2, 3, 4 -the \nidentity; xj -pixels (binar y). \n4\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "for ", "section_heading": "away any eﬀect from the prior. 4. (T/F – 2 points) Regulariza tion in the above sense would impro ve the structur al ris", "source_title": "85d4ef5363a3445d33ca956c394beec6 final f01soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 3}
{"id": "06398318-4801-49d3-9418-39e7d57ebbe7", "text": "nodes corresp onding to variables that are always observed. yi x x x1 2 64 i ∈ 1, 2, 3 -the digit type; y ∈ 1, 2, 3, 4 -the identity; xj -pixels (binar y). 4 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3. (4 points) Before implemen ting the mixture of experts architecture, we need to know \nthe parametric form of the conditio nal proba bilities in your graphica l model. Provide \na reasonable speciﬁcat ion of the relevant conditiona l proba bilities to the exten t that \nyou could ask your class- mate to impleme nt the classiﬁer. \nWe need to specify P (i|x) and P (y|i,x). P (i|x) is a softmax regression model, taking \nas input x represen ted as a binar y vector. P (yi,x) is also a softmax regression |\nmodel, where we have a diﬀeren t set of weights for each value of i. \n4. (4 points) So we imple mented your metho d, ran the estima tion algorithm once, \nand measured the test performance. The method was unfortuna tely performing at \na chance level. Provide two possible expla natio ns for this. (there may be multiple \ncorrect answers here) \nProblem 1: there are too few training examples. We have 200/(4*3) which is ap­\nproxima tely 10 training examples per expert.\nProblem 2: like in mixtur e models, we migh t converge to a locally optima l solutio n.\n5. (3 points) Would we get anything reaso nable out of the estimatio n if, initially , \nall the experts were identical while the parameters of the gating network would be \nchosen randomly? By reaso nable we mean training performance. Provide a brief \njustiﬁcatio n. \n5\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "nodes corresp onding to variables that are always observed. yi x x x1 2 64 i ∈ 1, 2, 3 -the digit type; y ∈ 1, 2, 3, 4 -", "source_title": "85d4ef5363a3445d33ca956c394beec6 final f01soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "b573427c-bda2-459f-b5d7-cd19e7019648", "text": "of the estimatio n if, initially , all the experts were identical while the parameters of the gating network would be chosen randomly? By reaso nable we mean training performance. Provide a brief justiﬁcatio n. 5 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nYes, the training examples would be assigned to experts with diﬀerent probabilities \nbecause of the gating network. This would ensure that the M-step of the EM \nalgorithm would make the experts diﬀeren t. Note that the posterior probabilit y of \nassigning examples to diﬀeren t experts is based on both the gating network and how \nwell each expert can predict the output from the input. \n6. (3 points) Would we get anything reasonable out the estimat ion if now the gating \nnetwork is initia lly set so that it assigns each training example uniformly to all experts \nbut the experts themselv es are initia lized with rando m parameter values? Again, \nreaso nable refers to the trainin g error. Provide a brief justiﬁcatio n. \nYes. Again, the posterior is based on both the gating network and the experts. In \nthis case, the experts would make diﬀerent predictio ns resulting in diﬀerent posterio r \nassignmen ts of examples to experts. The gating network would be modiﬁed on the \nbasis of these assignmen ts. \nProblem 4 \nConsider the following pair of observed sequence s: \nSequence 1 (st): AATTGGCC AATTGGCC ... \nSequence 2 (xt): 11221122 11221122 ... \nPositio n t: 0 1 2 3 4 ... \nwhere we assume that the pattern (highligh ted with the spaces) will continue forever. Let \nst ∈{A,G,T,C}, t =0, 1, 2,... denote the variables associated with the ﬁrst sequence, \nand xt ∈{1, 2}, t =0, 1, 2,... the variables characterizing the second sequence . So, for \nexample, given the sequence s above, the o bserved values for t hese variables are s 0 = A,s1 = \nA,s2 = C,..., and, similarly, x0 =1,x1 =1,x2 =2,.... \n1. (4 points) If we use a simple ﬁrst order homogeneous mark ov model to predict the \nﬁrst sequence (values for st only), what is the maximum likeliho od solutio n that \nwe would ﬁnd? In the transition diagr am below, please draw the relevant tran­\nsitions and the associated probabilit ies (this should not require much calculatio n) \n6\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "if ", "section_heading": "of the estimatio n if, initially , all the experts were identical while the parameters of the gating network would be ch", "source_title": "85d4ef5363a3445d33ca956c394beec6 final f01soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 5}
{"id": "ded32cfa-ec98-497c-80b7-3f10a8e004d8", "text": "is the maximum likeliho od solutio n that we would ﬁnd? In the transition diagr am below, please draw the relevant tran­ sitions and the associated probabilit ies (this should not require much calculatio n) 6 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nstATC\nG\nATC\nG\nst−1. . . . . .All the transitio n probabilities are 1/2. \nstATC\nG\nATC\nG\nst−1. . . . . .\n2. (T/F – 2 points) The resulting ﬁrst order Markov model is ergodic T \n3. (4 points) To impro ve the Mark ov model a bit we would like to deﬁne a graphica l \nmodel that predicts the value of st on the basis of the previo us observed values \nst−1,st−2,... (looking as far back as needed). The model parameters/structur e are \nassumed to rema in the same if we shift the model one step. In other words, it is the \nsame graphical model that predicts st on the basis of st−1,st−2,... as the model that \npredicts st−1 on the basis of st−2,st−3,.... In the graph below, draw the minimum \nnumb er of arrows that are needed to predict the ﬁrst observ ed sequence perfectly \n(disrega rding the ﬁrst few symbols in the sequenc e). Since we slide the model along \nthe sequence, you can draw the arrows only for st. \nstst−1st−2st−3st−4. . . . . .\n7\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "markov", "section_heading": "is the maximum likeliho od solutio n that we would ﬁnd? In the transition diagr am below, please draw the relevant tran­", "source_title": "85d4ef5363a3445d33ca956c394beec6 final f01soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "cfe0e119-dda8-49ac-bb4d-fb976906ed48", "text": "ed sequence perfectly (disrega rding the ﬁrst few symbols in the sequenc e). Since we slide the model along the sequence, you can draw the arrows only for st. stst−1st−2st−3st−4. . . . . . 7 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n4. Now, to incorporate the second observ ation sequence, we will use a standard hidden \nMark ov model: \nstst−1st−2st−3st−4\n. . . . . .\n. . . . . .\nxtxt−1xt−2xt−3xt−4\nwhere again st ∈{A,G,T,C} and xt ∈{1, 2}. We will estimate the parameters of \nthis HMM in two diﬀerent ways. \n(I) Treat the pair of observ ed sequences (st,xt) (given above) as complete observa­\ntions of the variables in the model and estimate the parameters in the maximum \nlikeliho od sense. The initial state distribut ion P0(s0) is set according to the over­\nall frequency of symbols in the ﬁrst observed sequence (uniform). \n(II) Use only the second observ ed sequence (xt) in estimating the parameters, again \nin the maximum likeliho od sense. The initia l state distribut ion is again uniform \nacross the four symbols. \nWe assume that both estimatio n processe s will be success full relative to their criteria . \na) (3 points) What are the observation probabilities P (x|s)(x ∈{1, 2}, s ∈ \n{A,G,T,C}) resulting from the ﬁrst estimatio n approa ch? (should not require \nmuch calculat ion) \nP (x =1|s = A) = 1, P (x =1|s = G) = 1, P (x =2|s = T ) = 1, P (x =2|s = C)= \n1, all other probabilit ies are zero. \nb) (3 points) Whic h estima tion approach is likely to yield a more accurate model \nover the second observ ed sequence (xt)? Brieﬂy expla in why. \nThe second one (II) since we can use the available four states to exactly capture the \nvariability in the xt sequenc e. Using the observ ation proba bilities above, we’d get \na model which assigns probabilit y 1/4 to each observed sequence of the above type \n(the only thing to predict is the starting state). \n8\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "hmm", "section_heading": "ed sequence perfectly (disrega rding the ﬁrst few symbols in the sequenc e). Since we slide the model along the sequence", "source_title": "85d4ef5363a3445d33ca956c394beec6 final f01soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 7}
{"id": "3660c7d2-81fb-4906-9972-d7f30a87eacc", "text": "sequenc e. Using the observ ation proba bilities above, we’d get a model which assigns probabilit y 1/4 to each observed sequence of the above type (the only thing to predict is the starting state). 8 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n5. Consider now the two HMM s resulting from using each of the estimat ion appro aches \n(approa ches I and II above). These HMMs are estimated on the basis of the pair of \nobserv ed sequence s given above. We’d like to evaluate the proba bility that these two \nmodels assign to a new (diﬀeren t) observ ation sequenc e 1212, i.e., x0 =1,x1 = \n2,x2 =1,x3 = 2. For the ﬁrst model, for which we have some idea about what the \nst variables will capture, we also want to know the the associated most likely hidden \nstate sequenc e. (these should not require much calculatio n) \na) (2 points) What is the proba bility that the ﬁrst model (approach I) assigns to \nthis new sequence of observ ations? \n1/16\nb) (2 points) What is the probabilit y that the second model (appr oach II) gives \nto the new sequence of observ ations? \nzero (generates only repeated symbol sequences of the type 1 1 2 2 ...)\nc) (2 points) What is the most likely hidde n state sequence in the ﬁrst model \n(from approa ch I) associated with the new observ ed sequence? \nAT GC\n6. (4 points) Finally , let’s assume that we observ e only the second sequenc e (xt) (the \nsame sequence as given above). In building a graphical model over this sequence we \nare no longer limiting ourselves to HMM s. However, we only consider models whose \nstructure/ parameters remain the same as we slide along the sequenc e. The variables \nst are included as before as they might come handy as hidden variables in predicting \nthe observ ed sequence. \na) In the ﬁgure below, draw the arrows that any reasonable model selection criterion \nwould ﬁnd given an unlimited supply of the observed sequence xt,xt+1,.... You \n9\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "hmm", "section_heading": "sequenc e. Using the observ ation proba bilities above, we’d get a model which assigns probabilit y 1/4 to each observed", "source_title": "85d4ef5363a3445d33ca956c394beec6 final f01soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 8}
{"id": "168ee3fd-d4e2-489c-91ed-c8d6b8252756", "text": "as hidden variables in predicting the observ ed sequence. a) In the ﬁgure below, draw the arrows that any reasonable model selection criterion would ﬁnd given an unlimited supply of the observed sequence xt,xt+1,.... You 9 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nonly need to draw the arrows for the last pair of variables in the graphs, i.e., \n(st,xt)). \nstst−1st−2st−3st−4\n. . . . . .\n. . . . . .\nxtxt−1xt−2xt−3xt−4\nb) Given only a small number of obervations, the model selection criterion migh t \nselect a diﬀeren t model. In the ﬁgure below, indicat e a possible alternat e model \nthat any reaso nable model selection criterion would ﬁnd given only a few ex­\namples. You only need to draw the arrows for the last pair of variables in the \ngraphs, i.e., (st,xt)). \nstst−1st−2st−3st−4\n. . . . . .\n. . . . . .\nxtxt−1xt−2xt−3xt−4\nProblem 5\nx1x2\nx3x4x5\nx6B A\nFigur e 1: Decision makers A and B and their “context” \nThe Bayesian network in ﬁgure 1 claims to model how two people, call them A and B, make \ndecis ions in diﬀeren t contexts. The context is speciﬁed by the setting of the binar y context \nvariables x1,x2,...,x6. The values of these variables are not known unless we speciﬁcally \nask for such values. \n10 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "bayesian network", "section_heading": "as hidden variables in predicting the observ ed sequence. a) In the ﬁgure below, draw the arrows that any reasonable mod", "source_title": "85d4ef5363a3445d33ca956c394beec6 final f01soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 9}
{"id": "b8feccc0-ae68-4b32-95c5-8398a9112361", "text": "decis ions in diﬀeren t contexts. The context is speciﬁed by the setting of the binar y context variables x1,x2,...,x6. The values of these variables are not known unless we speciﬁcally ask for such values. 10 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n1. (6 points) We are interested in ﬁnding out what informat ion we’d have to acquire \nto ensure that A and B will make their decisions indep enden tly from one another. \nSpecify the smal lest set of context variables whose insta ntiation would render A and \nB independen t. Brieﬂy expla in your reasoning (there may be more than one strateg y \nfor arriving at the same decision) \nContext variables = {x5 and x1} or {x5 and x2}.\nSince x6 is unobserved, it “drops out”. We have to ﬁnd the remaining context\nvariables that serve as common causes for the decisions. x5 is one but knowing it’s\nvalue would render x1 and x2 dependen t. So we have to additio nally observe one of\nthem.\nYou could also solve this by formally using the d-separ ation criterio n.\n2. (T/F – 2 points) We can in general achieve indep endence with less \ninforma tion, i.e., we don’t have to fully instan tiate the selected context \nvariables but provide some evidence about their values F\n11\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3. (4 points) Could your choice of the minim al set of context variables chang e if we \nalso provided you with the actua l proba bility values associated with the dependencies \nin the graph? Provide a brief justiﬁca tion. \nOur answ ers could change. The proba bility values migh t imply additiona l indep en­\ndencies and therefore reduc e the number of context variables we have to know the \nvalues for. \nAddi tional set of ﬁgures\nstATC\nG\nATC\nG\nst−1. . . . . .\nstst−1st−2st−3st−4. . . . . .\nstst−1st−2st−3st−4\n. . . . . .\n. . . . . .\nxtxt−1xt−2xt−3xt−4\n12\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nstst−1st−2st−3st−4\n. . . . . .\n. . . . . .\nxtxt−1xt−2xt−3xt−4\nstst−1st−2st−3st−4\n. . . . . .\n. . . . . .\nxtxt−1xt−2xt−3xt−4\nx1x2\nx3x4x5\nx6B A13\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "if ", "section_heading": "decis ions in diﬀeren t contexts. The context is speciﬁed by the setting of the binar y context variables x1,x2,...,x6. ", "source_title": "85d4ef5363a3445d33ca956c394beec6 final f01soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 10}
{"id": "3b091380-b61d-4214-a8d5-c6f3f127dd18", "text": "Massa chusetts Institu te of Techn ology \n6.867 Machine Lear ning , Fall 2006 \nProbl em Set 3 \nDue Date: Monda y, Oct 30, 1:05pm\nYou may submit your solutions in class or in the box.\n1. We will exam ine here leave-one-out cross-validation as a model selection tool. Let Sn = {(x1,y1),..., (xn,yn) \ndenot e our training set and Sn−i the corresp onding set with the ith training example and label removed. \nLOOCV is perfor med as follows: for each (xi,yi) in the train ing set, we train the classiﬁer on the re­\nmaini ng n − 1 points S−i and test our prediction on the left-out pair (xi,yi). More formall y, when using n\nthe squared loss, we deﬁn e errorLOOCV as\nn\nerrorLOOCV (Sn)= 1 � \n(yi − fˆ −i(xi))2 (1) n i=1 \nwhere fˆ −i is the estimator trained on S−i .n \n(a) Let’s start with a simpler strategy. We only leave out the ﬁrst point, i.e., train with Sn−1, and test \non (x1,y1). The error is now \nerror1(Sn)=(y1 − fˆ −1(x1))2 (2) \nAssuming each training exampl e and label is sampled independently from some underly ing distri­\nbution P (x,y), show that \nE{error1(Sn)} = E{(y − fˆSn−1 (x))}2 (3) \nwhere the expectation on the left is over all random quantities and, on the right hand side, it is \nover both (x,y) (test exampl e) as well as a datase t Sn−1 of size n − 1 sampled from the same \ndistribution. In other words, on average , error1(Sn) gives us the test error ! \n(b) Now, using the above result, show that errorLOOCV (Sn) also has this property, i.e., \nE{errorLOOCV (Sn)} = E{(y − fˆSn−1 (x))2} (4) \n(c) Parts a) and b) seem to indicate that both LOOCV and the single test set approximation are unbi­\nased estimates of the test error based on n − 1 training examp les. Are the variances of errorLOOCV \nand error1 the same as well? \nWe now consider a situati on where cross-validation can be ineﬀe ctive as a model selection strategy . Sup­\npose we have a classiﬁcation task where we have binary-v alued labels and binary-valued d-dime nsional \nexam ples. In other words, xi ∈ {−1, 1}d and yi ∈ {−1, 1}. There are d models, each making use of \nonly one feature (coordinate) of x. Model Mk corres ponding to coordinate k can produce one of two \npossible estimators: \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "for ", "section_heading": "Massa chusetts Institu te of Techn ology ", "source_title": "436d96cb61c2075ad9ebb31a4edb484d hw3", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 0}
{"id": "6512507e-7418-40bf-b5ad-fa10c6ef8eb9", "text": "∈ {−1, 1}d and yi ∈ {−1, 1}. There are d models, each making use of only one feature (coordinate) of x. Model Mk corres ponding to coordinate k can produce one of two possible estimators: Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nMk = {fk \nflip} (5)keep,fk \nfk (x) = xk (6)keep\nfk \nflip(x) = −xk (7) \nwhere xk is the k-th featur e/coordinate of x.\nGiven any datase t Sn, the model Mk chooses the estimator (fk or fk ) which results in the lowest\nflip keep\ntraining error, i.e., the ﬁnal estimator fˆk is selected on the basis of whether xk or −xk better agree s Sn \nwith y. \nNow, suppose that the data was generated is as follows: Pr(xk = 1) = 0.5 for all k =1,...,d, i.e, \nthe coordinates of x are sampled uniformly at random from {−1, 1}d . The y values are generated from \nthe x values (based on only one coordinate) in a probabilistic fashion , the details of which are not known. \nOur goal is to use LOOCV to identify the best model Mk for classiﬁcation. \n(d) What is the probability that model Mr relying on an irrelev ant coordinate r will produce an \nestimator with zero training error? \nHint: What value(s) must the vector (x1r,x2r,...,xnr)take to ensur e that no mistakes occur during \ntraining? Here xir is the rth coordinate of the ith exampl e. \n(e) For any Mr with training error � (where � �\nHint: what can you say about the estimator fˆk 1 \n2) show that the trainin g error = errorLOOCV . \n−i produced in a LOO CV step? \n1 (f) How many dimension s d do we need so that with probability 1/2 at least one model Mr \nan irrelev ant coordinate would have errorLOOCV <� (again , assume � �based on\n2)? An upper bound\nsuﬃc es.\n2. We will explore here the use of margin al likelihood for feature selection with a simple “voting” classiﬁer. \nSuppose we have d−dimension al binary input examples x where each coordinate xj ∈ {−1, 1}. Our goal \nis to select a subset of featur es (coordinates of x) so as to ensure that the classiﬁer generalizes well. In \nparticular , we are interes ted in how margi nal likelihood might be able to guide us in this process. If J\ndenot es the indexes of featur es we have chosen, then the discriminan t function takes the form \n1 � \nf(x; θ, J )= θj xj (8) |J| j∈J \nWe will assume for simplicity that the parameters are also binary so that θj ∈ {−1, 1}. The product \nθj xj in this case speciﬁes the ±1 label that the jth coordinate is voting for. We can deﬁne a probability \ndistribution over y based on the value of the discrimin ant function according to \n1� � 1 � \nP (y|x,θ, J )= 2 1+ yf(x; θ, J ) = (1+ yθj xj )/2 (9)|J| j∈J \nIn other words, the probability distribution over the labels is based on counting how many of the \ncoordinate prediction s agree with the label. Note that (1 + yθj xj )/2 = 1 if the jth coordinate prediction \nagree s with y and zero otherwise . \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "∈ {−1, 1}d and yi ∈ {−1, 1}. There are d models, each making use of only one feature (coordinate) of x. Model Mk corres ", "source_title": "436d96cb61c2075ad9ebb31a4edb484d hw3", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "1abcc98c-04e1-488f-bd3c-142cea5581a2", "text": "based on counting how many of the coordinate prediction s agree with the label. Note that (1 + yθj xj )/2 = 1 if the jth coordinate prediction agree s with y and zero otherwise . Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n��\n� � We will try to use the marginal likelihood to select the appropriate coordinates , i.e., to ﬁnd J . Before \nwe can evaluate the margin al likelihood we have to specify a prior distrib ution P (θ) over the binary \nparameters θ. If we have no reason to prefer one set of parame ter values over anoth er, we can just \nassume that all binary parame ter vectors are equall y likely: \nP (θ|J )= 1 |J | \n(10)2 \nNow, given a train ing set Sn = {(x1,y1),..., (xn,yn)} of exampl es and labels, we can in principl e \nevaluate the margin al likeliho od for any subset J ⊆{1,...,d}: \nn\nP (Sn|J )= P (θ|J ) P (yt|xt,θ, J ) (11) \nθ∈{−1,1}|J | t=1 \n⎡ ⎤ �� n� 1 |J | � 1 � \n= ⎣ (1 + ytθj xtj )/2⎦ (12) \nθ∈{−1,1}|J | 2 t=1 |J| j∈J \nwhere xtj is the jth coordinate of xt. \n(a) Evaluate an expression for P (Sn|J ) when J is a singleton set, i.e., when J = {l} for some \nl ∈{1,...,d}. \n(b) Do you see a problem ? Can you propose how we should ﬁx it? \n(c) In directory hw3 you can ﬁnd a MATLAB function logmarlikel(X,y,idx) that evaluates the \nlogar ithm of the above margi nal likelihood (and includ es a particular “ﬁx” to the problem). The \nmatrix argumen t X contains the input vectors as rows, y is a vector of corres ponding labels, and idx \nspeciﬁes the columns of X we care about. Let’s try to see that our score behaves reasonably. Load \nXrand.dat and yrand.dat. These are randomly generated and should not contain any real rela­\ntionship between x and y. Plot the log-margin al likelihoods corresponding to the following feature \nsets (1), (1:2), ..., (1:10). What can you say about how the log-margin al likelihood behaves as a \nfunction of the size of the set? Is the behavior reasonable? Try using logmarlikel(X,y,idx,0.4) \n(assuming high label noise ). \n(d) It is hard to search over all possible subsets of features but we can do it sequentially. That is, we \ncan ﬁnd the best single feature ﬁrst, then ﬁnd another one to add that works best in combination \nwith the ﬁrst, and so on. When would we stop? \n(e) Set Xrand(:,2) = Yrand so that the second column of Xrand now contains the labels to be pre­\ndicted. Rerun part a) with this data. Explai n the result. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "based on counting how many of the coordinate prediction s agree with the label. Note that (1 + yθj xj )/2 = 1 if the jth", "source_title": "436d96cb61c2075ad9ebb31a4edb484d hw3", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "b3a3eb72-2451-4410-96cd-50f6567448ef", "text": "Massa chusetts Institute of Technology \n6.867 Machine Learning, Fall 2006 \nProblem Set 5: Solutions \n1. (a) For the LM, there is no need to iterate; the maxim um likelihood estimates are easy to derive; they \nare simply the normalized counts.\nThe updates for T and D are:\n1 � D(aj = i | j,�,m)T (fj (k) | ei (k)) \nT �(f | e)= ZT (e) i,j,k ��\ni�=0 D(aj = i� | j,�,m)T (fj (k) , e(\ni� k)) \ne (k)=e, fj (k)=fi \nD�(aj = i | j,�,m)= 1 �\n�� D(aj = i | j,�,m)T (fj (k) | \n(e\nki (\n) k)) \n(k) , ZD(j,�,m) k i�=0 D(aj = i� | j,�,m)T (fj , ei� ) \n|e(k)|=�, |f (k)|=m \nwhere ZT and ZD are normalization constan ts. One can derive this easily using the formal EM \nformulation; however, just using the soft counts is ﬁne, as well. \n(b) Generally , a choice that has lots of zeros will be bad. Other choices that depend on a lot of \nsymmetry in the data will also cause problems. This model is not convex by a long shot, so there \nare plenty of local extrema; it is nontrivial to ﬁnd a nontrivial initial setting, but a trivial one is \nﬁne here. \n(c) My update to the code was this:\nLM:\nLM(m2m1,english(i,j)) = LM(m2m1,english(i,j)) + 1;\n...\nLM(m2m1,i) = LM(m2m1,i)/LMc(m2,m1);\nEM updates: \nfor j=1:m\na = [];\nfor i=1:l\na(i) = T(deutsch(idx,j),english(idx,i)) * D(indexpack(j,l,m),i); \nend \na = a / sum(a); \nfor i=1:l \nTn(deutsch(idx,j),english(idx,i)) = Tn(deutsch(idx,j),english(idx,i)) + a(i); \nDn(indexpack(j,l,m),i) = Dn(indexpack(j,l,m),i) + a(i); \nend \nend \nnormalization: \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "convex", "section_heading": "Massa chusetts Institute of Technology ", "source_title": "d76013505b6ecdd0be9abf1817c42d07 hw5 soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "490e2c92-2670-4a3e-8083-cc9b8636b157", "text": "LM(m2m1,i)/LMc(m2,m1); EM updates: for j=1:m a = []; for i=1:l a(i) = T(deutsch(idx,j),english(idx,i)) * D(indexpack(j,l,m),i); end a = a / sum(a); for i=1:l Tn(deutsch(idx,j),english(idx,i)) = Tn(deutsch(idx,j),english(idx,i)) + a(i); Dn(indexpack(j,l,m),i) = Dn(indexpack(j,l,m),i) + a(i); end end normalization: Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nT = sparse(de_vocab, en_vocab);\nD = sparse((mmax+1) * 50 * 50, lmax);\nfor enword=1:en_vocab\nnfactor = sum(Tn(:,enword));\nfor deword=find(Tn(:,enword))\nT(deword, enword) = Tn(deword, enword)/nfactor;\nend\nend\nDnfactor = sum(Dn,2) * sparse(ones(1,size(D,2)));\nidxpack = find(Dnfactor);\nD(idxpack) = Dn(idxpack)./Dnfactor(idxpack);\nHere is the output I got: \nDear Klaus !\nI have you . locked a small room .\nYou can read the books .\nIt\nSome problems with this is that it tends to be somewhat sensitiv e to the start point; random \nwas not the best choice, but it is ok. A better choice is to choose uniform for D and to use co­\noccurrence counts for T . In practice, one would train a simpler model (IBM Model 1) and transfer \nthe probabilities. Also, a NULL word on the English side would help to “explain” common particle \nwords in the German side. The last sentence is seven words long in German; this size does not \nappear in the corpus, so the translation is ridiculous. All in all, this metho d has many limitations, \nbut considering its simplicit y, the results are quite good and using this type of metho d is very \nappealing. \n(d) (i) It will reorder more freely. \n(ii) It will reorder less. \n(iii) This is (essen tially) the same as (i). If there are spots that tend to have attractiv e words on \nthe English side (words that explain a lot of the German words), then all of the words will try \nto reorder there. In general, however, it will have a smoothing eﬀect on the alignmen ts. \n2. (a) 4 points If the kernel is deﬁned so as to encode a high covariance between two points x1 and x2, \nthen the observ ations at those two values should be highly correlated (i.e. should have roughly \nsimilar values). Thus, if the covariance between neighboring points is high along a long stretch of \nthe x-axis, the function value will remain roughly constan t along that stretch. \nUsing this intuition, we have the following set of matches: 1-d, 2-b, 3-a, 4-c. The kernels can also \ncapture periodicity. For example, kernel (3) indicates that points which are t − t� = 2 apart will \nhave low covariances while points which are t − t� = 4 will have higher covariances (i.e. roughly \nsimilar values). This introduces periodicity in the function values, as observ ed in Fig (a). Also, \nthe (tt�)2 component of kernel (4) implies that the covariance between neighboring points increases \nwith t, resulting in the curve moving in a single direction. \n(b) 5 points The log-lik elihood of observing a set of values y1,...,yr at time-p oints t1,...,tr is simply \nthe probabilit y of sampling the r-dimensional vector (y1,...,yr) from N(0,G) where G is the Gram \nmatrix corresp onding to the time-p oints. The code looks as follows: \nfunction ll = log_likelihood_gp(params, t, Yobs) \n%Gram matrix \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "LM(m2m1,i)/LMc(m2,m1); EM updates: for j=1:m a = []; for i=1:l a(i) = T(deutsch(idx,j),english(idx,i)) * D(indexpack(j,l", "source_title": "d76013505b6ecdd0be9abf1817c42d07 hw5 soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "354b899f-1e6c-40ed-aac3-f35fd6789df0", "text": "the probabilit y of sampling the r-dimensional vector (y1,...,yr) from N(0,G) where G is the Gram matrix corresp onding to the time-p oints. The code looks as follows: function ll = log_likelihood_gp(params, t, Yobs) %Gram matrix Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nG = ... \nGinv = inv(G);\ndetG = det(G);\nll = zeros(q,1); \nfor i=1:q\ny = reshape(Yobs(i,:), r, 1);\na = 0.5*(y’*Ginv*y);\nb = (r/2) * log(2*pi) + (0.5*log(detG));\nl = -a -b;\nll(i) = l;\nend \n(c) 7 points The Expectation code is shown below. Recall that Wij is the probabilit y of gene i \nbelonging to cluster j. \nfunction W_new = Expectation(t, Y_obs, k, W, V)\nP = sum(W);\nP = P/sum(P);\n[n,T] = size(Y_obs);\nW_new = zeros(n,k);\nfor j=1:k,\n% get likelihood\nll = log_likelihood_gp(V(j,:), t, Y_obs);\n% get posterior assignment probability\nW_new(:,j) = ll + log(P(j));\nend \n%normalize posterior\n% a more sophisticated handling of very small\n% number will be better, but the simplest\n% normalization method will suffice\n% for grading purposes\nfor i=1:n\nW_new(i,:) = exp(W_new(i,:)) / sum(exp(W_new(i,:)));\nend\n(d) 5 points The best results are obtained with k = 3 clusters. With higher k, some of the clusters \nare either empty or two diﬀeren t clusters have similar curves. With lower k (e.g. k = 2), one of \nthe clusters contains two diﬀeren t kinds of curves. \n(e) 4 points There are a few diﬀeren t ways of dealing with missing values. One approac h would be to \nuse the function curves estimated in the previous iteration of EM to compute the expected value \nof gene’s expression at the missing time-p oint (in the ﬁrst iteration, we could just use the mean of \nthe adjacen t, non-missing values). \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "the probabilit y of sampling the r-dimensional vector (y1,...,yr) from N(0,G) where G is the Gram matrix corresp onding ", "source_title": "d76013505b6ecdd0be9abf1817c42d07 hw5 soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "b2510d64-68a6-48aa-86d5-a360d8983766", "text": "estimated in the previous iteration of EM to compute the expected value of gene’s expression at the missing time-p oint (in the ﬁrst iteration, we could just use the mean of the adjacen t, non-missing values). Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� \n� � � Here’s an alternativ e approac h. Recall that the observ ations y1,...,yr play a role in our compu­\ntation only when we compute the likelihood of sampling that set of values from a multi-variate \nGaussians. Clearly , if some values are missing, this probabilit y can be computed on a reduced \nset. In such a case, we simply use whatev er values are available (i.e., the non-missing values) to \nconstruct the Gram matrix and then evaluate the likelihood of seeing that set of values. The Gram \nmatrix in such a case would have a smaller size. \nFor example, if there are 10 time-p oints in total, you’d typically construct a 10x10 Gram matrix and \nthen evaluate the probabilit y of sampling a particular 10-dimensional vector from the corresp onding \n10-dimensional Gaussian. If, on the other hand, you only have 8 points, you’d construct a 8x8 Gram \nmatrix and then evaluate the probabilit y of sampling a particular 8-dimensional vector from the \ncorresp onding 8-dimensional Gaussian. \n(f) Optional: 4 points There are many possible ways to guess an initial estimate of the gene clusters \n(for grading purposes, any reasonable approac h is ﬁne). Here we describ e one such approac h: \n• Between each pair of genes g1 and g2, deﬁne a distance measure. This distance measure may be \nthe Euclidean distance between the observ ations: �r\ni=1(y1i − y2i)2 . Another measure might \nbe the Pearson correlation between these sets of observ ations. \n• Using these pairwise distances, perform Hierarc hical Agglomerativ e Clustering between the \ngenes, i.e., start with each gene as a singleton cluster and at each step, merge the two most \nsimilar clusters. For example, if we perform complete linkage clustering, the distance between \ntwo clusters will deﬁned as the largest distance between any pair of genes, one from each \ncluster. \n(g) Optional: 6 points For notational convenience, we deﬁne the marginal likelihood for the case of \na single cluster. The generalization to multiple clusters is straigh tforward.\nSuppose there are r timep oints: t1,...,tr and let f(t) be the cluster mean curve. Then, for any\ngene, the probabilit y of seeing the observ ations [y(1),...,y(r)] in the cluster is the probabilit y that\neach y(l) is sampled from the Normal distribution N(f(tl),�n2 ):\nr \nN(y(l); f(tl),�n2 ), or \nl=1 \nr � 1 (y(l) − f(tl))2 \n2��n exp(−�2 ) \nl=1 n \nThe prior probabilit y of seeing a cluster mean curve f(t) is given by the probabilit y of sampling \nthe r-dimensional vector f =[f(t1),...f(tr)] from the r-dimensional Gaussian formed by using the \nGram matrix constructed from t1,...,tr. \nN(f; 0,G) (1) \nwhere G is the Gram matrix as per the ﬁxed GP.\nThe marginal likelihood is then:\n� r \nN(y(l); f(tl),�n2 ) N(f; 0,G) df \nl=1 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "estimated in the previous iteration of EM to compute the expected value of gene’s expression at the missing time-p oint ", "source_title": "d76013505b6ecdd0be9abf1817c42d07 hw5 soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "914b01fa-a019-4a69-b1a5-8907cc91510e", "text": "� 1 6.867 Mac hine learning , lectur e 22 (Jaak kola) \nLecture topics: \n• Learning Bayesian networks from data \n– maximum likeliho od, BIC \n– Bayesian, margina l likeliho od \nLearning Bayesian networks \nThere are two problems we have to solve in order to estimat e Bayesian networks from \navailable data. We have to estimate the parameters given a speciﬁc structu re, and we have \nto searc h over possible structures (model selection) . \nSupp ose now that we have d discrete variables, x1,...,xd, where xi ∈{1,...,ri}, and n \ncomplete observ ations D = {(xt \n1,...,xt\nd),t =1,...,n}. In other words, each observ ation \ncontains a value assignmen t to all the variables in the model. This is a simpliﬁc ation and \nmodels in practice (e.g., HMMs) have to be estimat ed from incomplete data. We will \nalso assume that the condit ional probabilities in the models are fully parameterized. This \nmeans, e.g., that in P (x1|x2) we can selec t the probabilit y distribution over x1 separa tely \nand witho ut constra ints for each possible value of the paren t x2. Models used in practice \noften do have parametr ic constr aints. \nMaxi mum likelihood parameter estimation \nGiven an acyclic graph G over d variables, we know from previo us lecture that we can write \ndown the associated joint distribut ion as \nd\nP (x1,...,xd)= P (xi|xpai ) (1) \ni=1 \nThe parameters we have to learn are therefore the condit ional distributions P (xi|xpai ) in \nthe product. For later utilit y we will use P (xi|xpai )= θxi|xpai to specify the parameters. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "margin", "section_heading": "� 1 6.867 Mac hine learning , lectur e 22 (Jaak kola) ", "source_title": "0b2ddf0b3af319e4b2dd204fec91da56 lec22", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "70d99e1f-2b99-452c-ad9b-aeacd50f17a0", "text": ") (1) i=1 The parameters we have to learn are therefore the condit ional distributions P (xi|xpai ) in the product. For later utilit y we will use P (xi|xpai )= θxi|xpai to specify the parameters. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� � 2 6.867 Mac hine learning , lectur e 22 (Jaak kola) \nGiven the complet e data D, the log-likeliho od function is \nl(D; θ, G) = log P (D|θ) \nn� (2) \n= \nt=1 log P (x t \n1, . . . , x t d|θ) (3) \nn� n� \n= \nt=1 i=1 log θxt \ni|xt pai (4) \nn� � \n= n(x i, xpai ) log θxi|xpai (5) \ni=1 xi,xpai \nwhere we have again collapsed the available data into counts n(x i,xpai ), the number of \nobserv ed insta nces with a particula r setting of the variable and its parents. These are the \nsuﬃcient statistics we need from the data in order to estimate the parameters. This will \nbe true in the Bayesian setting as well (disc ussed below). Note that the statistics we need \ndepend on the model structure or graph G. The parameters θˆxi|xpai that maximize the \nlog-likeliho od have simple closed form expressions in terms of empir ical fractions: \nˆ = � n(x i,xpai ) (6) θxi|xpai r\nxi \n�\ni=1 n(x�\ni,xpai ) \nThis simplic ity is due to our assumption that θxi|xpai can be chosen freely for each setting \nof the parents xpai . The parameter estimat es are likely not going to be particula rly good \nwhen the number of paren ts increases . For example, just to provide one observ ation per \na conﬁgura tion of paren t variables would requir e j∈pai rj instances. Introducing some \nregula rization is clearly important, at least in the fully parameterized case. We will provide \na Bayesian treatmen t of the parameter estimat ion problem shortly . \nBIC and structure estimation \nGiven the ML parameter estima tes θˆxi|xpai we can evaluate the resulting maxim um value \nof the log-likeliho od l(D;ˆθ,G) as well as the corresp onding BIC score: \nBIC(G)= l(D; θ,Gˆ) − dim(G) log(n) (7) 2 \nwhere dim(G) speciﬁes the number of (indep enden t) parameters in the model. In our case \nthis is given by \nd\ndim(G) = (ri − 1) rj (8) \ni=1 j∈pai \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": ") (1) i=1 The parameters we have to learn are therefore the condit ional distributions P (xi|xpai ) in the product. For ", "source_title": "0b2ddf0b3af319e4b2dd204fec91da56 lec22", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "164c3100-eccc-4e4a-acce-1d8dcf65d823", "text": "θ,Gˆ) − dim(G) log(n) (7) 2 where dim(G) speciﬁes the number of (indep enden t) parameters in the model. In our case this is given by d dim(G) = (ri − 1) rj (8) i=1 j∈pai Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� 3 6.867 Mac hine learning , lectur e 22 (Jaak kola) \nwhere each term in the sum corresp onds to the size of the probabilit y table P (xi|xpai ) \nminus the associated normaliza tion constra ints x�\ni P (x�\ni|xpai ) = 1. \nBIC and likelihood equivalence \nSupp ose we have two diﬀeren t graphs G and G� that nevertheles s make exactly the same \nindep endence assumptions about the variables involved. For example, neither graph in \nx2 x1 x2 x1 \nmakes any indep endence assumptio ns and are therefore equivalent in this sense. The re­\nsulting BIC scores for such graphs are also identical. The principle that equiv alent graphs \nshould receive the same score is known as likeliho od equivalenc e. How can we determine if \ntwo graphs are equivalent? In principle this can be done by deriving all the possible inde­\npendence statemen ts from the graphs and compa ring the resulting lists but there are easier \nways. Two graphs are equiv alent if they diﬀer only in the direction of arcs and possess the \nsame v-structur es, i.e., they have the same set of converging arcs (two or more arcs pointing \nto a single node). This criterion captur es most equiv alences . Figure 1 provides a list of all \nequiv alence classes of graphs over three variables. Only one repres entativ e of each class is \nshown and the number next to the graph indic ates how many graphs there are that are \nequiv alent to the represe ntative. \nEquiv alence of graphs and the associated scores highligh t why we should not interpret the \narcs in Bayesian networks as indicat ing the directio n of causa l inﬂue nce. While models are \noften drawn based on one’s causa l understanding , when learning them from the available \ndata we can only distinguish between models that make diﬀeren t proba bilistic assumptions \nabout the variables involved (diﬀeren t indep endence prop erties), not based on which way \nthe arcs are pointing. It is nevertheless possible to estimat e causal Bayesian networks, \nmodels where we can interpret the arcs as causal inﬂue nces. The diﬃculty is that we \nneed interventional data to do so, i.e., data that corresp ond to explicitly setting some of \nthe variables to speciﬁc values (controlled experimen ts) rather than simply observing the \nvalues they take. \nBayesian estimation \nThe idea in Bayesian estimat ion is to avoid reducing our knowledge about the parameters \ninto point estimates (e.g., ML estimates) but instead retain all the information in a form of \na distribut ion over the possible parameter values. This is advantageous when the available \ndata are limited and the number of parameters is large (e.g., only a few data points per \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "θ,Gˆ) − dim(G) log(n) (7) 2 where dim(G) speciﬁes the number of (indep enden t) parameters in the model. In our case thi", "source_title": "0b2ddf0b3af319e4b2dd204fec91da56 lec22", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "f8d075e1-a3dd-47d5-a07e-9b7cff013800", "text": "information in a form of a distribut ion over the possible parameter values. This is advantageous when the available data are limited and the number of parameters is large (e.g., only a few data points per Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n4 6.867 Mac hine learning , lectur e 22 (Jaak kola) \n(2) x1 x2 \nx3 (6) \nx1 x2 \nx3 x1 x2 \nx3 x1 x2 \nx3 (1) (1) (1) \nx1 x2 \nx3 (2) x1 x2 \nx3 (2) x1 x2 \nx3 x1 x2 \nx3 (3) x1 x2 \nx3 (3) (3) \nx3 x1 x2 \n(1) x3 x1 x2 \nFigur e 1: Equiv alence classes of graphs over three variables. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "for ", "section_heading": "information in a form of a distribut ion over the possible parameter values. This is advantageous when the available dat", "source_title": "0b2ddf0b3af319e4b2dd204fec91da56 lec22", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 3}
{"id": "cd685464-9be2-4bcb-8cbe-9f84962da5ad", "text": "x3 (2) x1 x2 x3 (2) x1 x2 x3 x1 x2 x3 (3) x1 x2 x3 (3) (3) x3 x1 x2 (1) x3 x1 x2 Figur e 1: Equiv alence classes of graphs over three variables. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� 5 6.867 Mac hine learning , lectur e 22 (Jaak kola) \nparameter to estima te). The Bayesian framew ork requires us to also articulate our knowl­\nedge about the parameters prior to seeing any data in a form of a distribution, the prior \ndistributio n. Consider the following simple graph with three variables \nx1 x2 \nx3 \nThe parameters we have to estima te are {θx1 }, {θx2 }, and {θx3|x1,x2 }. We will assume that \nthe parameters are a prior i indep endent for eac h variable and acro ss diﬀ erent conﬁg urations \nof paren ts (parameter indep endence assumption) : \nP (θ)= P ({θx1 }x1=1,...r1 ) P ({θx2 }x2=1,...,r2 ) P ({θx3|x1,x2 }x3=1,...,r3 ) (9) \nx1,x2 \nWe will also assume that we will use the same prior distributio n over the same parameters \nshould they appear in diﬀeren t graphs (para meter modularit y). For example, since x1 has \nno parents in G and G’ given by \nG G’ \nx1 x2 x1 x2 \nx3 x3 \nwe need the same parameter {θx1 } in both models. The parameter modularit y assumptio n \ncorresponds to using the same prior distribution P ({θx1 }x1=1,...,r1 ) for both models (other \nparameters would have diﬀerent prior distributions since, e.g., θx3|x1,x2 does not appear \nin graph G�). Finally, we would like the marg inal likeliho od score to satisfy likeliho od \nequiv alence simila rly to BIC. In other words, if G and G� are equivalent, then we would \nlike P (D|G)= P (D|G�) where, e.g., \n� \nP (D|G)= P (D|θ,G)P (θ)dθ (10) \nIf we agree to these three assumptions (parameter indep endenc e, modularit y, and likelihood \nequiv alence), then we can only choose one type of prior distribution over the parameters, \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "if ", "section_heading": "x3 (2) x1 x2 x3 (2) x1 x2 x3 x1 x2 x3 (3) x1 x2 x3 (3) (3) x3 x1 x2 (1) x3 x1 x2 Figur e 1: Equiv alence classes of grap", "source_title": "0b2ddf0b3af319e4b2dd204fec91da56 lec22", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 4}
{"id": "69cbf7f9-519f-45d0-bd8c-54e4bc85ec9e", "text": "P (D|G)= P (D|θ,G)P (θ)dθ (10) If we agree to these three assumptions (parameter indep endenc e, modularit y, and likelihood equiv alence), then we can only choose one type of prior distribution over the parameters, Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� � \n� \n� \n� � 6 6.867 Mac hine learning , lectur e 22 (Jaak kola) \nthe Dirichlet distribution. To specify and use this prior distributio n, it will be helpful \nto chang e the notation slightly. We will denote the parameters by θijk where i speciﬁes \nthe variable, j the parent conﬁg uratio n (see below), and k the value of the variable xi. \nClearly , kri \n=1 θijk = 1 for all i and j. The parent conﬁg urations are simply indexed from \nj =1,...,qi as in \nj x1 x2 \n1 1 1 \n2 2 1 (11) \n· · · · · · · · · \nq r1 r2 \nwhere q = r1r2. When xi has no parents we say there is only one “parent conﬁg uration” so \nthat P (xi = k)= θi1k. Note that writing parameters as θijk is graph speciﬁc; the paren ts \nof each variable, and therefore also parent conﬁgura tions, vary from one graph to another. \nWe will deﬁne θij = {θijk}k=1,...,ri so we can talk about all the parameters for xi given a \nﬁxed paren t conﬁg uration. \nNow, the prior distribut ion of each θij has to be a Dirichlet: \nΓ( αijk) ri\nαijk−1P (θij )= � k θijk = Diric hlet(θ ij ; αij1,...,αijri ) (12) \nk Γ(α ijk) k=1 \nwhere, for integers, Γ(z +1) = z!. The mean of this distribution is \nP (θij ) θijk dθij = � αijk (13) \nk� αijk� \nand it is more concentrated around the mean the larger the value of k� αijk� . We can \nfurther write the hyper-par ameters αijk > 0 in the form αijk = n�p�\nijk where n� is the \nequiv alent sample size specifying how many observ ations we need to balance the eﬀect of \nthe data on the estima tes in compa rison to the prior . There are two subtleties here. First, \nthe number of available observ ations for estima ting θij varies with j, i.e., depends on how \nmany times the paren t conﬁgur ations appear in the data. To keep n� as an equivalent \nsample size across all the parameters, we will have to accoun t for this variatio n. The \nparameters p�\nijk are theref ore not norma lized to one across the values of variable xi but \nacross its values and the parent conﬁgura tions: jqi \n=1 kri \n=1 p�\nijk = 1 so we can interpret \np�\nijk as a distribut ion over (xi,xpaj ). In other words, they include the expectat ion of how \nmany times we would see a particula r parent conﬁg uratio n in n� observ ations. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "P (D|G)= P (D|θ,G)P (θ)dθ (10) If we agree to these three assumptions (parameter indep endenc e, modularit y, and likeli", "source_title": "0b2ddf0b3af319e4b2dd204fec91da56 lec22", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "a015356e-b8fe-4e88-b97d-1cc35b6c008f", "text": "interpret p� ijk as a distribut ion over (xi,xpaj ). In other words, they include the expectat ion of how many times we would see a particula r parent conﬁg uratio n in n� observ ations. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� \n� � \n� \n� � 7 6.867 Mac hine learning , lectur e 22 (Jaak kola) \nThe second subtlety further constrains the values p�\nijk, in addit ion to the normaliza tion. \nIn order for the likeliho od equiv alence to hold, p�\nijk should be possible to interpret as \nmarginals P �(xi,xpai ) of some common distribution over all the variables P �(x1,...,xd) \n(commo n to all the graphs we conside r). For example, simply normalizing p�\nijk across the \nparent conﬁgura tions and the values of the variables does not ensure that they can be \nviewed as marg inals from some commo n joint distributio n P �. This subtlety does not often \narise in practice. It is typical and easy to set them based on a unifo rm distribution so that \n1 1 n� \np�\nijk = � = or αijk = (14) ri l∈pai rl riqi riqi \nThis leaves us with only one hyper-parameter to set: n�, the equiv alent sample size. \nWe can now combine the data and the prior to obtain posterio r estimates for the parameters. \nThe prior facto rs across the variables and across paren t conﬁgura tions. Moreover, we \nassume that each observ ation is complete, containing a value assignm ent for all the variables \nin the model. As a result, we can evaluate the posterior probabilit y over each θij separa tely \nfrom others. Speciﬁcally , for each θij = {θijk}k=1,...,ri , where i and j are ﬁxed, we get \n⎡ ⎤ \nP (θij |D,G) ∝ ⎣ P (xit|xpat \ni ,θij )⎦ P (θij) (15) \n� t: xpat \ni →j � ri\nnijk = θijk P (θij ) (16) \n�k=1 �� � ri ri\n∝ θnijk θαijk−1 (17) ijk ijk \nk=1 k=1 \nri\n= θnijk+αijk−1 (18) ijk \nk=1 \nwhere the product in the ﬁrst line picks out only observ ations where the paren t conﬁg urati on \nmaps to j (otherwis e the case would fall under the doma in of another parameter vector ). \nnijk speciﬁes the number of observations where xi had value k and its parents xpai were \nin conﬁgur ation j. Clearly, q\nji \n=1 r\nki \n=1 nijk = n. The posterio r has the same form as the \nprior1 and is therefore also Diric hlet, just with updated hyper-para meters: \nP (θij |D,G) = Dirichlet(θij ; αij1 + nij1,...,αijri + nijri ) (19) \n1Dirichlet is a conju gate prior for the multi-nomial distribution. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "interpret p� ijk as a distribut ion over (xi,xpaj ). In other words, they include the expectat ion of how many times we ", "source_title": "0b2ddf0b3af319e4b2dd204fec91da56 lec22", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "f611b409-848f-48cc-84ee-f0d0d59138d9", "text": "the prior1 and is therefore also Diric hlet, just with updated hyper-para meters: P (θij |D,G) = Dirichlet(θij ; αij1 + nij1,...,αijri + nijri ) (19) 1Dirichlet is a conju gate prior for the multi-nomial distribution. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� 8 6.867 Mac hine learning , lectur e 22 (Jaak kola) \nThe normaliza tion constan t for the posterio r in Eq.(15) is given by \n⎡ ⎤ � � ri\n � \nipai ⎦ P (θij )dθ ij Γ( αijk) � Γ(α ijk + nijk)\n⎣ P (x t|x t ,θij ) =Γ( � αijk k \n+ � nijk) Γ(α ijk) (20) \nt: xt k k \npai →j k=1 \nThis is also the marg inal lik eliho od of data pertaining to x i when xpai are in conﬁgu ratio n j. \nSince the observ ations are complete, and the prior is indep enden t for each set of parameters, \nthe margina l likeliho od of all the data is simply a product of these local norma lizatio n terms. \nThe product is taken across variables and across diﬀeren t parent conﬁgura tions: \nn qi ri �� Γ( αijk) � Γ(α ijk + nijk)P (D|G)= Γ( � k � (21) \nk αijk + k nijk) Γ(α ijk)i=1 j=1 k=1 \nWe would now ﬁnd a graph G that maximizes P (D|G). Note that Eq.(2 1) is easy to \nevaluate for any particular graph by recomputing some of the coun ts nijk. We can further \npenalize graphs that involve a large number of paramet ers (or edges) by assignin g a prior \nprobabilit y P (G) over the graphs, and maximizing instead P (DG)P (G). For example, the |\nnprior could be some function of the number of parameters in the model or (ri − 1)qi i=1\nsuch as \n1 P (G) ∝ � (ri − 1)qi (22) n \ni=1\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "the prior1 and is therefore also Diric hlet, just with updated hyper-para meters: P (θij |D,G) = Dirichlet(θij ; αij1 + ", "source_title": "0b2ddf0b3af319e4b2dd204fec91da56 lec22", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 7}
{"id": "bb9b4398-5e9a-46e8-9dd8-9be30ba822bf", "text": "6.867 Machine learning \nFinal exam (Fall 2003) \nDecem ber 10, 2003 \nProblem 1: your information \n1.1. Your name and MIT ID: \n1.2. The grade you would give to yourself + brief justiﬁcati on (if you feel that \nthere’s no question your grade should be an A, then just say A): \n1\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "if ", "section_heading": "6.867 Machine learning ", "source_title": "438614c2e5f2a2871b7ef89ffb506c45 final f03", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 0}
{"id": "33d30918-b246-49ac-b63f-1deebd5c8b7a", "text": "1.1. Your name and MIT ID: 1.2. The grade you would give to yourself + brief justiﬁcati on (if you feel that there’s no question your grade should be an A, then just say A): 1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 2 \n2.1. (3 points) Let F be a set of classiﬁers whose VC-dime nsion is 5. Suppose we have \nfour training examples and labels, {(x1,y1),..., (x4,y4)}, and select a classiﬁer fˆ \nfrom F by minimizing classiﬁcat ion error on the training set. In the absenc e of any \nother informat ion about the set of classiﬁers F, can we say that the predic tion fˆ(x5) \nfor a new example x5 has any relation to the training set? Brieﬂy justify your answer. \n2.2. (T/F – 2 points) Consider a set of classiﬁers that includes all linear \nclassiﬁers that use diﬀerent choices of strict subsets of the components \nof the input vectors x ∈Rd . Claim: the VC-dimens ion of this combined \nset cannot be more than d + 1. \n2.3. (T/F – 2 points) Structu ral risk minimiz ation is based on compa ring \nupper bounds on the genera lizatio n error, where the bounds hold with \nprobabilit y 1 − δ over the choice of the training set. Claim: the value \nof the conﬁdenc e parameter δ canno t aﬀect model selectio n decisions. \n2.4. (6 points) Supp ose we use class- conditiona l Gaussians to solve a binary classiﬁcatio n \ntask. The covariance matr ices of the two Gaussians are constrained to be σ2 I, where \nthe value of σ2 is ﬁxed and I is the identity matr ix. The only adjustable parame­\nters are theref ore the means of the class conditiona l Gaussians, and the prior class \nfrequencies . We use the maxim um likeliho od criterion to train the model. Chec k all \nthat apply. \n( ) For any three distinct training points and suﬃc iently small σ2 , the classiﬁer \nwould have zero classiﬁcat ion error on the training set \n( ) For any three training points and suﬃcien tly large σ2, the classiﬁer would always \nmake one classiﬁcatio n error on the training set \n( ) The classiﬁc ation error of this classiﬁer on the training set is always at least that \nof a linear SVM, whether the points are linearly separable or not \n2\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "1.1. Your name and MIT ID: 1.2. The grade you would give to yourself + brief justiﬁcati on (if you feel that there’s no ", "source_title": "438614c2e5f2a2871b7ef89ffb506c45 final f03", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "81e4a48a-b4aa-4982-82a0-155105406856", "text": "error on the training set ( ) The classiﬁc ation error of this classiﬁer on the training set is always at least that of a linear SVM, whether the points are linearly separable or not 2 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 3\n3.1. (T/F – 2 points) In the AdaBo ost algorithm, the weights on all the\nmisclassiﬁe d points will go up by the same multiplica tive factor .\n3.2. (3 points) Provide a brief rationa le for the following observ ation about AdaBo ost. \nThe weighted error of the kth weak classiﬁer (measured relativ e to the weights at the \nbeginning of the kth iteration) tends to increase as a function of the iteratio n k. \nConsider a text classiﬁcat ion problem, where documen ts are represe nted by binar y (0/1) \nfeature vectors φ =[φ1,...,φm]T ; here φi indicat es whether word i appears in the documen t. \nWe deﬁne a set of weak classiﬁers, h(φ; θ)= yφi, parameterized by θ = {i,y} (the choice \nof the comp onent, i ∈{1,...,m}, and the class label, y ∈ {−1, 1}, that the component \nshould be associated with). There are exactly 2m possible weak learners of this type. \nWe use this boosting algorithm for feature selection. The idea is to simply run the boosting \nalgorithm and select the features or comp onents in the order in which they were identiﬁed \nby the weak learners. We assume that the boosting algorithm ﬁnds the best available weak \nclassiﬁer at each iteratio n. \n3.3. (T/F – 2 points) The boosting algorithm described here can select \nthe exact same weak classiﬁer more than once. \n3.4. (4 points) Is the ranking of features generated by the boosting algorithm likely to \nbe more useful for a linear classiﬁer than the ranking from simple mutual informatio n \ncalculat ions (estimates Iˆ(y; φi)). Brieﬂy justify your answ er. \n3\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "error on the training set ( ) The classiﬁc ation error of this classiﬁer on the training set is always at least that of ", "source_title": "438614c2e5f2a2871b7ef89ffb506c45 final f03", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "befea926-e408-4ba9-9cf2-019f59af4654", "text": "ranking of features generated by the boosting algorithm likely to be more useful for a linear classiﬁer than the ranking from simple mutual informatio n calculat ions (estimates Iˆ(y; φi)). Brieﬂy justify your answ er. 3 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 4\n0 1 2 3 4 5 600.511.522.533.544.55\ntime step tobservation x\n0 1 2 3 4 5 600.511.522.533.544.55\ntime step tobservation x\nFigur e 1a) Figure 1b) \nFigur e 1: Time dependen t observations. The data points in the ﬁgure are generated as \nsets of ﬁve consecutiv e time dependen t observations, x1,...,x5. The clusters come from \nrepeatedly generating ﬁve consecutiv e samples . Each visible cluster consists of 20 points, \nand has appro xima tely the same variance. The mean of each cluster is shown with a large \nX. \nConsider the data in Figure 1 (see the captio n for details). We begin by modeling this data \nwith a three state HMM, where each state has a Gaussian output distribut ion with some \nmean and variance (means and variances can be set indep enden tly for each state). \n4.1. (4 points) Draw the state transitio n diagram and the initial state distribution for \na three state HMM that models the data in Figure 1 in the maximum likelihood \nsense. Indic ate the possible transitions and their proba bilities in the ﬁgure below \n(whether or not the state is reachable after the ﬁrst two steps). In order words, your \ndrawing should characterize the 1st order homo geneous Markov chain govering the \nevolution of the states. Also indic ate the means of the corresp onding Gaussian output \ndistributio ns (please use the boxes). \nBegin\nt=1 t=2321\n4\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "hmm", "section_heading": "ranking of features generated by the boosting algorithm likely to be more useful for a linear classiﬁer than the ranking", "source_title": "438614c2e5f2a2871b7ef89ffb506c45 final f03", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "83feddcd-b45f-402d-8bb8-86d1685562aa", "text": "should characterize the 1st order homo geneous Markov chain govering the evolution of the states. Also indic ate the means of the corresp onding Gaussian output distributio ns (please use the boxes). Begin t=1 t=2321 4 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n4.2. (4 points) In Figure 1a draw as ovals the clusters of outputs that would form if \nwe repeatedly generated samples from your HMM over time steps t =1,..., 5. The \nheigh t of the ovals should reﬂec t the variance of the clusters. \n4.3. (4 points) Supp ose at time t = 2 we observe x2 =1.5 but don’t see \nthe observations for other time points. What is the most likely state at \nt = 2 according to the marg inal posterior proba bility γ2(s) deﬁned as\nP (s2 = s|x2 =1.5).\n4.4. (2 points) What would be the most likely state at t = 2 if we also saw \nx3 =0 at t = 3? In this case γ2(s)= P (s2 = s|x2 =1.5,x3 = 0). \n4.5. (4 points) We can also try to model the data with conditio nal mixtures (mixtures \nof experts), where the conditioning is based on the time step. Supp ose we only use \ntwo experts which are linea r regression models with additiv e Gaussian noise, i.e., \n1 � 1 � \nP (xt,θi)= � exp |\n2πσi 2 − 2σi 2 (x − θi0 − θi1t)2 \nfor i =1, 2. The gating network is a logistic regression model from t to binary \nselection of the experts. Assuming your estima tion of the conditio nal mixture model \nis succe ssfully in the maximum likeliho od sense, draw the resulting mean predic tions \nof the two linear regression models as a function of time t in Figur e 1b). Also, with \na vertical line, indicat e where the gating network would chang e it’s preference from \none expert to the other. \n4.6. (T/F – 2 points) Claim: by repeatedly sampling from your con­\nditional mixture model at succes sive time points t =1, 2, 3, 4, 5, the \nresulting samples would resemble the data in Figure 1 \n. \n4.7. (4 points) Having two comp eting models for the same data, the HMM and the \nmixtur e of experts model, we’d like to select the better one. We think that any \nreaso nable model selection criterio n would be able to select the better model in this \ncase. Whic h model would we choose? Provide a brief justiﬁcatio n. \n5\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "logistic regression", "section_heading": "should characterize the 1st order homo geneous Markov chain govering the evolution of the states. Also indic ate the mea", "source_title": "438614c2e5f2a2871b7ef89ffb506c45 final f03", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "0d668a0c-65f9-4dd4-b5bc-0c2146093cf0", "text": "the better one. We think that any reaso nable model selection criterio n would be able to select the better model in this case. Whic h model would we choose? Provide a brief justiﬁcatio n. 5 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 5\nx1 x2 x3 \ny1 y2 y3 \nx1 x2 x3 \ny1 y2 y3 \na) Bayesian network (directed) b) Markov rando m ﬁeld (undirected) \nFigur e 2: Graphical models \n5.1. (2 points) List two diﬀerent types of indep endence properties satisﬁed by the Bayesian \nnetwork model in Figur e 2a. \n5.2. (2 points) Write the facto rizatio n of the joint distribut ion implied by the directed \ngraph in Figure 2a. \n5.3. (2 points) Provide an alternativ e facto rization of the joint distributi on, diﬀerent \nfrom the previous one. Your factorizat ion should be consisten t with all the properties \nof the directed graph in Figur e 2a. Consistency here means: what ever is implie d by \nthe graph should hold for the associated distributio n. \n6\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n5.4. (4 points) Provide an indep endenc e statemen t that holds for the undirected model \nin Figure 2b but does NOT hold for the Bayesian network. Whic h edge(s) should \nwe add to the undirected model so that it would be consisten t with (wouldn’t imply \nanything that is not true for) the Bayesian network? \n5.5. (2 points) Is your resulting undirected graph triangula ted (Y/N)? \n5.6. (4 points) Provide two directed graphs represen ting 1) a mixture of two experts \nmodel for classiﬁcat ion, and 2) a mixture of Gaussians classiﬁers with two mixture \ncomp onents per class. Please use the following notation: x for the input observation, \ny for the class, and i for any selection of comp onents. \n7\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nAddi tional set of ﬁgures\n0 1 2 3 4 5 600.511.522.533.544.55\ntime step tobservation x\n0 1 2 3 4 5 600.511.522.533.544.55\ntime step tobservation x\nFigur e 1a) Figure 1b)\nBegin\nt=1 t=2321\nx1 x2 x3 \ny1 y2 y3 \nx1 x2 x3 \ny1 y2 y3 \na) Bayesian network (directed) b) Markov rando m ﬁeld (undirected)\n8\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "bayesian network", "section_heading": "the better one. We think that any reaso nable model selection criterio n would be able to select the better model in thi", "source_title": "438614c2e5f2a2871b7ef89ffb506c45 final f03", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "6d8c4688-a776-4bea-a161-37d20d33e0e1", "text": "1Massa chusetts Institu te of Techn ology \n6.867 Machine Lear ning, Fall 2006 \nProblem Set 1 Solutions \nSecti on B \n1. (a) The functio ns are on the course website hw1/solutio ns. The major cause of variation \namong the solutio ns was the choice of the order in which points were picked for the update \nstep. This was also the cause of a few subtle mista kes as well. The most straightforw ard \nstrategy for picking points (in the order of their occurrence in the dataset), produces \nθ =2.3799 radians or θ = 136.3582◦. \nThe number of updates, as per the above strategy, was 10. The analy sis described in \nthe lectures indic ated that the number of perceptron updates (mista kes) was neces sarily � �2 bounded by R/γ∗ where γ∗ is the maxim um geometr ic marg in for this problem. geom geom \nA small number of updates therefor e suggests that γ∗ is reasona bly large in comparison geom \nto the radius R of the enclosing sphere. In other words, the two class popula tions appear \nto be well-separa ted. \n(b) The most straightforward strategy for picking points (in the order of their occurrence in \nthe dataset), produce s θ =2.3566 radia ns or θ = 136.3582◦. We will accept answers in the \nrange (2.3552, 2.3592) radians or (134.9454◦, 135.1730◦). The latter ranges corresponds \nto decision bounda ries going throug h points at the margins of the max-margin classiﬁer \n(see Prob 2). \nThe number of updates, as per the above strateg y, was 152. The bounding sphere is \nabout the same for these points, however. We would therefor e expect that the geometric \nmargin γ∗ is larger for this problem. geom \n(c) We can also evaluate γgeom, the margin actua lly achieved by the perceptro n algorithm. \nThis is not the maxim um margin but may neverthele ss be indica tive of how hard the \nproblem is. Given X and θ, γgeom can be calcula ted in MATLAB as follows: \ngamma_geom = min(abs(X*theta / norm(theta))) \nWe get γa =1.6405 and γb =0.0493, again with some variation due to the order geom geom \nin which one selects the training examples . These margins appear to be consisten t with \nour analysis, at least in terms of their relat ive magnitude. The bound on the number of \nupdates holds for any marg in, maxim um or not, but gives the tightest guarantee with \nthe maxim um margin. \n(d) Given X, R can be calcula ted in MATLAB as \nR = max(sqrt(sum(X.^2,2))) \nThen, Ra = 200.561 and Rb = 196.3826. Using these, and γ∗a =5.5731 and γ∗b = geom geom \n0.3267 evaluated below, the theoretical bounds on the number of perceptro n updates for \nthe two problems are \n� �2 ka ≤ Ra/γ∗\ngeom a ≈ 1295 (1) \n� �2 ka ≤ Rb/γ∗\ngeom b ≈ 361333 (2) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "1Massa chusetts Institu te of Techn ology ", "source_title": "84fa9697d4c11c5b511b28b0eb025987 hw1b soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "fb6aed85-57dc-49c5-bc72-729b9ebd70d2", "text": "below, the theoretical bounds on the number of perceptro n updates for the two problems are � �2 ka ≤ Ra/γ∗ geom a ≈ 1295 (1) � �2 ka ≤ Rb/γ∗ geom b ≈ 361333 (2) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2\n0 20 40 60 80 100 120 140 160 180020406080100120140160180200\n0 20 40 60 80 100 120 140 160 180020406080100120140160180(a) Datas et A (b) Datase t B \nFigur e 1: The decision boundary θT x for the two datasets is show in black. The two classe s are \nindicated by the colors red and blue, respectively. \nThe bounds are not very tight. This is in part because they must hold for any order \nin which you chose to go through the training examples (and in part because they are \nsimple theoretical bounds omitting a lot of speciﬁcs of the two problems). \n(e) The plots are shown in Fig 1. \n2. (a) The functions are on the course website. The distance must not be used, since it is \nmagnitude-de penden t. Measuring the angle between them is the most appro priat e way. \nThe angle will vary, depending upon the order ing strategy used in Prob 1 and, conse­\nquen tly, the θ found in Prob 1. However, the answer corresponding to the strategy where \npoints are picked in their order in the input , the answer for data set ‘A’ was 0.0221 radi­\nans or 1.2672◦. We will accept answers < 0.0351 radians (or 2.011◦). Simila rly, for data \nset ‘B’, the most straigthforward strategy produced the diﬀerence as 0.0019 radians or \n0.1114◦. We will accept answers < 0.002 radians (or 0.1146◦). These latter limit s are \nbased on the decision bounda ries going throug h points at the margins of the max-mar gin \nclassiﬁer. \n(b) γgeom a = geom 0.3267. Thes e are the maxim um margins achievable with 5.5731 and γb =\nany linea r classiﬁer through origin.\n3. (a) The correct invocation of SVM light in trainin g is had by setting C to inﬁnit y: \n% svm_learn -c +inf train-01-images.svm \nScanning examples...done \nReading examples into memory... <snip> ..OK. (12665 examples read) \nOptimizing... <snip> .done. (687 iterations) \nOptimization finished (0 misclassified, maxdiff=0.00100). \nRuntime in cpu-seconds: 1.30 \nNumber of SV: 84 (including 0 at upper bound) \nL1 loss: loss=0.00000 \nNorm of weight vector: |w|=0.00924 \nNorm of longest example vector: |x|=4380.65657 \nEstimated VCdim of classifier: VCdim<=890.06377 \nComputing XiAlpha-estimates...done \nRuntime for XiAlpha-estimates in cpu-seconds: 0.02 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "below, the theoretical bounds on the number of perceptro n updates for the two problems are � �2 ka ≤ Ra/γ∗ geom a ≈ 129", "source_title": "84fa9697d4c11c5b511b28b0eb025987 hw1b soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "dd940a93-b3bb-4995-ba0d-85cd4b8bfdf0", "text": "Number of SV: 84 (including 0 at upper bound) L1 loss: loss=0.00000 Norm of weight vector: |w|=0.00924 Norm of longest example vector: |x|=4380.65657 Estimated VCdim of classifier: VCdim<=890.06377 Computing XiAlpha-estimates...done Runtime for XiAlpha-estimates in cpu-seconds: 0.02 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3XiAlpha-estimate of the error: error<=0.65% (rho=1.00,depth=0)\nXiAlpha-estimate of the recall: recall=>99.48% (rho=1.00,depth=0)\nXiAlpha-estimate of the precision: precision=>99.30% (rho=1.00,depth=0)\nNumber of kernel evaluations: 164632\nWriting model file...done\nSince this is a hard-mar gin SVM, if a solutio n exists then the training error will be 0 (and \nyou get away witho ut submit ting an imag e). With the new model ﬁle, we can evaluate \nour model on the test set: \n$ svm_classify test-01-images.svm svm_model\nReading model...OK. (84 support vectors read)\nClassifying test examples..<snip>..done\nRuntime (without IO) in cpu-seconds: 0.00\nAccuracy on test set: 99.91% (2113 correct, 2 incorrect, 2115 total)\nPrecision/recall on test set: 99.91%/99.91%\nNow, for interest’s sake, we’d like to extra ct the two test imag es that were misclassiﬁed. \nThis is how it’s done using unix power tools: \n$ cat test-01-images.svm | awk ’{ print $1 }’ | paste -svm_predictions \\ \n| nl | awk ’{ if ($2 * $3 < 0) { print $1 } }’ \n1665 \n2032 \nIt can also be done somewhat less easily by using a spreadsheet utilit y. Here are the \nimag es: \ntest-001665.png test-002032.png \nWe also accepted training with the default value of C: \n$ svm_learn train-01-images.svm \nScanning examples...done \nReading examples into memory...<snip>..OK. (12665 examples read) \nSetting default regularization parameter C=0.0000 \nOptimizing...<snip>...done. (163 iterations) \nOptimization finished (9 misclassified, maxdiff=0.00089). \nRuntime in cpu-seconds: 0.60 \nNumber of SV: 141 (including 77 at upper bound) \nL1 loss: loss=35.80801 \nNorm of weight vector: |w|=0.00365 \nNorm of longest example vector: |x|=4380.65657 \nEstimated VCdim of classifier: VCdim<=142.17389 \nComputing XiAlpha-estimates...done \nRuntime for XiAlpha-estimates in cpu-seconds: 0.01 \nXiAlpha-estimate of the error: error<=0.92% (rho=1.00,depth=0) \nXiAlpha-estimate of the recall: recall=>99.12% (rho=1.00,depth=0) \nXiAlpha-estimate of the precision: precision=>99.15% (rho=1.00,depth=0) \nNumber of kernel evaluations: 135983 \nWriting model file...done \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "Number of SV: 84 (including 0 at upper bound) L1 loss: loss=0.00000 Norm of weight vector: |w|=0.00924 Norm of longest e", "source_title": "84fa9697d4c11c5b511b28b0eb025987 hw1b soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "c23f0266-87a4-45b4-a8b9-227505d2aac0", "text": "classifier: VCdim<=142.17389 Computing XiAlpha-estimates...done Runtime for XiAlpha-estimates in cpu-seconds: 0.01 XiAlpha-estimate of the error: error<=0.92% (rho=1.00,depth=0) XiAlpha-estimate of the recall: recall=>99.12% (rho=1.00,depth=0) XiAlpha-estimate of the precision: precision=>99.15% (rho=1.00,depth=0) Number of kernel evaluations: 135983 Writing model file...done Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n4In this case, nine training examples are misclassiﬁed. Again, there are many ways of \nextra cting the misclassiﬁed imag es. Here is how it is done with unix power tools: \n$ svm_classify train-01-images.svm svm_model \nReading model...OK. (141 support vectors read) \nClassifying test examples..<snip>..done \nRuntime (without IO) in cpu-seconds: 0.00 \nAccuracy on test set: 99.93% (12656 correct, 9 incorrect, 12665 total) \nPrecision/recall on test set: 99.93%/99.94% \n$ cat train-01-images.svm | awk ’{ print $1 }’ | paste -svm_predictions \\ \n| nl | awk ’{ if ($2 * $3 < 0) { print $1 } }’ \n315\n2824\n4496\n4612\n6031\n6627\n8185\n8547\n9327\nHere are the images: \ntrain-000315.pn g train-002824.pn g train -004496.pn g\ntrain-004612.pn g train-006031.pn g train -006627.pn g\ntrain-008185.pn g train-008547.p ng train-009327.p ng \n(b) I trained the SVM with 20 values of C. The training error and test error are depicted \nbelow: \n 1e-05 1e-04 0.001 0.01 0.1 1\n 1e-11 1e-10 1e-09 1e-08 1e-07 1e-06 1e-05error\nCtraining error\ntest error\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n5Notice that, except for exceptio nally small values of C in the ﬁgur e, the training error \nis 0 for this data set (it is easily separa ted by a hyper-plane). Observ e also that the \ntraining error simply decreases as C increases; consequen tly, the best training error is \nachieved for very large C (in this case, it is zero training error). In contrast, if we could \nchoose C with hindsigh t, i.e., measure the test error before having to select C, we would \nnot pick a very large value. \n(c) We produce an analogous graph for SVMs trained on the partially mislab eled data. \n 1e-04 0.001 0.01 0.1 1\n 1e-11 1e-10 1e-09 1e-08 1e-07 1e-06 1e-05error\nCtraining error\ntest error\nClearly now, having an intermediate value of C very slightly helps the training error and \nsigniﬁcan tly reduc es the test error. Again, though we could not legitima tely select C \nbased on the test error, we can expect similar results from cross-valida tion. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "classifier: VCdim<=142.17389 Computing XiAlpha-estimates...done Runtime for XiAlpha-estimates in cpu-seconds: 0.01 XiAlp", "source_title": "84fa9697d4c11c5b511b28b0eb025987 hw1b soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "0f17e9e8-3d6a-4944-b73d-110aa36341a1", "text": "Massa chusetts Institute of Technology \n6.867 Machine Learning, Fall 2006 \nProblem Set 1 \nDue Date: Thursday, Sep 28. (at 12:00 noon)\nYou may submit your solutions in class or in the box.\nSection A: Background (Answ ering this section is optional; if you do submit answers to these questions, \nthey will be graded but will be not scored i.e. your score for this assignmen t will not depend on this section.) \n1. Write a MATLAB function birthday prob(n) that computes the probabilit y that, in a group of n \npeople at least two share the same birthda y (assume that n � 2). Using this function, compute the \nminim um size of the group such that this probabilit y is atleast 0.5. Attach a printout of the code with \nyour answer. \n2. Let X1,X2,X3,...,Xn be independen t random variables, each with a Uniform distribution over (0, 1): \n⎥ 10 <x< 1 f(x)= 0 otherwise \nFind (a) E[Max( X1,X2,...,Xn)] and (b) E[Min(X1,X2,...,Xn)]. \n3. The Flushing Meado ws Open, a tennis tournamen t held every year, has a Men’s Doubles event with 16 \nseeded pairs. Unfortunately , it is unseasonably cold and wet, and many players are down with ﬂu. In \nparticular, 4 of the seeded players are sick so that they and their partner can not play. What is the ⎦ �32expected number of seeded pairs remaining in the tournamen t? Assume that any of the 4 possible \ncombinations of 4 seeded players is equally likely to be sick. \n4. You are a contestan t in the Monty Hall game show. In this game, there are three doors. Behind one \ndoor is a brand new Corvette; there are goats behind the other two doors. Monty Hall asks you to select \na door; after you have chosen one, he opens one of the other two doors to reveal a goat behind it. You \ncan now choose to stick with your original choice, or to switch to the remaining door. \n(a) What should you do? Will it make a diﬀerence if you switch? \n(b) Run a thousand trials of this experimen t in MATLAB. Can you explain the results? \n(c) Bonky Hall, Monty’s brother, starts his own game show in which there are four doors instead of \nthree, but with the same setup otherwise. When you choose a door initially , Bonky also opens \none door, revealing a goat behind it. In addition to your chosen door, there now remain two more \nclosed doors. Should you switch to one of these? Why? If you decide to switch, which of the two \ndoors should you choose? \n5. (a) Let X be a Gaussian vector with \n� ⎤ � ⎤ \nE[X] = 10 \n5 cov(X) = 2 \n1 1 1 \nWrite an expression for the pdf of X that does not use matrix notation, i.e., if X =[x1,x2] write \nthe joint pdf P (x1,x2). \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "Massa chusetts Institute of Technology ", "source_title": "9d7b38c2e21793c3351d2853077baea3 hw1", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "e7427242-7f58-4f7c-93b2-4a5bcd457919", "text": "� ⎤ E[X] = 10 5 cov(X) = 2 1 1 1 Write an expression for the pdf of X that does not use matrix notation, i.e., if X =[x1,x2] write the joint pdf P (x1,x2). Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n(b) Let A,B be p × q matrices and x be a random q × 1 vector. Prove that \ncov(Ax,Bx)= Acov(x) BT \nwhere cov(u,v)= E[(u − E[u])(v − E[v])T ] is the cross-co variance matrix between random vectors \nu and v, while cov(u)= E[(u − E[u])(u − E[u])T ] is the covariance matrix for u. \n6. From our Linear Algebra notes... \nTheorem 3.1.5: (Gram-Sc hmidt Process) Let v1,...,vk be an ordered, linearly indepen­\ndent set. Then, there exists an ordered set u1,...,uk of orthonormal vectors such that \n≤u1≥ = ≤v1≥ \n≤u1,u2≥ = ≤v1,v2≥ \n. . . \n≤u1,...,uk≥ = ≤v1,...,vk≥. \n(Here, ≤v1,v2,...,vk≥ denotes the span of the vectors v1, v2, ..., and vk; that is, the set of all \nvectors that can be expressed as a linear combination of the vi’s.) \nProof: Set u1 = v1 /∈v1∈. Then, ∈u1∈2 = 1 and ≤u1≥ = ≤v1≥. Set \nv2 − (v2 T u1)u1 u2 = . ∈v2 − (v2 T u1)u1∈ \nThen,\nT v2 T u1 − (v2 T u1)(uT \n1 u1)\nu2 u1 = ∈v2 − (v2 T u1)u1∈ =0, \nand so {u1,u2} is orthonormal, and ≤u1,u2≥ = ≤v1,v2≥. Next, set: \nv3 − (v3 T u2)u2 − (v3 T u2)u2 u3 = . ∈v3 − (v3 T u2)u2 − (v3 T u2)u2∈ \nAgain, u3 T u2 = u3 T u1 = 0 and ∈u3∈2 = 1. Continue in this way to obtain vectors u1,u2,...,uk \nwith the stated properties. � \nThe procedure describ ed in the proof is called the Gram-Sc hmidt Process. Using MATLAB, apply this \nprocedure to the following set of (indep enden t) vectors. \n�⎡ �⎡ � ⎡ �⎡ 0 1 1 1\n� 0 ⎢ � 2 ⎢ � 4 ⎢ � 0 ⎢\n�⎢ �⎢ � ⎢ �⎢ � 0 ⎢ � 3 ⎢ � 9 ⎢ � 0 ⎢ �⎢ �⎢ � ⎢ �⎢ v1 = ,v2 = ,v3 = ,v4 = . � 0 ⎢ � 4 ⎢ � 16 ⎢ � 0 ⎢ �⎢ �⎢ � ⎢ �⎢ � 0 ⎣ � 5 ⎣ � 25 ⎣ � 0 ⎣ \n1 6 36 0 \nWhat happens if you do it in a diﬀeren t order (say, when you move v4 to the front)? \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "� ⎤ E[X] = 10 5 cov(X) = 2 1 1 1 Write an expression for the pdf of X that does not use matrix notation, i.e., if X =[x1", "source_title": "9d7b38c2e21793c3351d2853077baea3 hw1", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "514a8eef-eae2-4327-bae5-6bc24df31895", "text": "�⎢ � 0 ⎣ � 5 ⎣ � 25 ⎣ � 0 ⎣ 1 6 36 0 What happens if you do it in a diﬀeren t order (say, when you move v4 to the front)? Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nSection B: Answering this section is compulsory; your score for this assignmen t will depend only on this \nsection. \nSome of the problems will require some MATLAB programming. On the course webpage, we have provided \nsome data and scripts to get you started. These are provided as part of a single .zip archive. Once you have \ndownloaded this archive, you can extract the individual ﬁles using the following command (on Linux): \n$unzip <FILE-NAME.zip> \nThe archive also contains load scripts for you to load data into MATLAB. These scripts are named by the \nproblem number. For example, to load the data for Problem 1(b), you should use the script load p1 b. \n1. Implemen t a Perceptron classiﬁer in MATLAB. Start by implemen ting the following functions (you \nshould attach a printout of your MATLAB code of these functions with your submission): \n• a function perceptron train(X, y) where X and y are n × d and n × 1 matrices respectively. \nThis function trains a Perceptron classiﬁer on a training set of n examples, each of which is a d-\ndimensional vector. The labels for the examples are in y and are 1 or -1. The function should return \n[theta, k], the ﬁnal classiﬁcation vector and the number of updates performed, respectively. You \nmay assume that the input data provided to your function is linearly separable. \n• a function perceptron test(theta, X test, y test) where theta is the classiﬁcation vector to \nbe used. X test and y test are m × d and m × 1 matrices respectively, corresp onding to m test \nexamples and their true labels. The function should return test err, the fraction of test examples \nwhich were misclassiﬁed. \nFor this problem, we have provided you two custom-created datasets. The dimension d of both the \ndatasets is 2, for ease of plotting and visualization. \n(a) Load data using the load p1 a script and train your Perceptron classiﬁer on it. Using the function \nperceptron test, ensure that your classiﬁer makes no errors on the training data. What is the \nangle between theta and the vector (1, 0)T ? What is the number of updates ka required before \nthe Perceptron algorithm converges? \n(b) Repeat the above steps for data loaded from script load p1 b. What is the angle between theta \nand the vector (1, 0)T now? What is the number of updates kb now? \n(c) For parts (a) and (b), compute the geometric margins, �a and �b , of your classiﬁers with geom geom\nrespect to their corresp onding training datasets. Recall that the distance of a point xt from the \nxtline �T x =0 is |�\n�T \n�� |. \n(d) For parts (a) and (b), compute Ra and Rb, respectively. Recall that for any dataset X , \nR = max{∈x∈ | x � X}. \n(e) Plot the data (as points in the X-Y plane) from part (a), along with decision boundary that your \nPerceptron classiﬁer computed. Create another plot, this time using data from part (b) and the \ncorresp onding decision boundary . Your plots should clearly indicate the class of each point (e.g., by \nchoosing diﬀeren t colors or symbols to mark the points from the two classes). We have a provided \na MATLAB function plot points and classifier which you may ﬁnd useful. \n2. Implemen t an SVM classiﬁer in MATLAB, arranged like the Perceptron in problem 1, with functions \nsvm train(X, y) and svm test(theta, X test, y test). Again, include a printout of your code for \nthese functions. \nHint: Use the built-in quadratic program solver quadprog(H, f, A, b) which solves the quadratic \nprogram: min 1\n2 xT Hx + fT x subject to the constrain t Ax � b. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "�⎢ � 0 ⎣ � 5 ⎣ � 25 ⎣ � 0 ⎣ 1 6 36 0 What happens if you do it in a diﬀeren t order (say, when you move v4 to the front)", "source_title": "9d7b38c2e21793c3351d2853077baea3 hw1", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "5e418ecd-11b6-451a-a0e4-1db6aaec8fe7", "text": "code for these functions. Hint: Use the built-in quadratic program solver quadprog(H, f, A, b) which solves the quadratic program: min 1 2 xT Hx + fT x subject to the constrain t Ax � b. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n(a) Try the SVM on the two datasets from Problem 1. How diﬀeren t are the values of theta from \nvalues the Perceptron achieved? To do this comparison, should you compute the diﬀerence between \ntwo vectors or something else? \n(b) For the decision boundaries computed by SVM, compute the corresp onding geometric margins (as \nin Problem 1(c)). How do the margins achieved using the SVM compare with those achieved by \nusing the Perceptron? \n3. Now that you are familiar with SVMs, download this highly optimized version called SVM light: \nhttp://svmlight.joachims.org/ \nWe will use this package to experimen t with classifying images of handwritten digits. To simplify things, \nwe will only be distinguishing between 0s and 1s. \n(a) Train a plain-v anilla SVM (i.e, no regularization) on train-01-images.svm (which is already in \nSVM light’s format). What is the training error, i.e., the fraction of examples in the training data \nthat are misclassiﬁed? From the set of misclassiﬁed images, pick one. Attach of plot of it with your \nsolution. Why do you think the SVM fails to classify it correctly during training? Now apply the \nSVM to the test set test-01-image.svm; what is the test error, i.e., the fraction of test data that \nis misclassiﬁed? \n(b) Experimen t with diﬀeren t values of the regularization term C (set using the -c ﬂag). Start by \nguessing/estimating a range in which you think C should lie. Then choose the values of C (within \nthat range) at which you will evaluate the performance of the SVM. You need not pick more than \n10 such values, though you should feel free to pick as many (or as few!) as you want. For these \nvalues of C, plot the corresp onding error. What value of C gives you the best training error on the \ndataset from part (a)? How does the test error for this choice of C compare with the test error you \ncomputed in part (a)? Could you optimize C so that it leads to the best test error, rather than the \ntraining error? Should you? \n(c) An adversary (w e’ll call him W) mislab eled 10% of the images in the original training set to produce \ntrain-01-images-W.svm. Using the same choices of C as in part (b), ﬁnd the C that results in \nthe best training error. What is the corresp onding test error on test-01-image.svm? \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "code for these functions. Hint: Use the built-in quadratic program solver quadprog(H, f, A, b) which solves the quadrati", "source_title": "9d7b38c2e21793c3351d2853077baea3 hw1", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "a43042bf-4edc-4d8b-9732-5a021ecbdcb7", "text": "Massa chusetts Institu te of Techn ology \n6.867 Machine Lear ning , Fall 2006 \nProbl em Set 2 \nDue Date: Thursda y, Oct 12, 12:00 noon \nYou may submit your solutions in class or in the box.\n1. The data and scripts for this problem are availab le in hw2/pr ob1. You can load the data using the \nMATLAB script load al data. This script shoul d load the matr ices y noisy, y true, X in. The y \nvectors are n × 1 while X in is a n × 3 matrix with each row corresponding to a point in R3 . \nThe ytrue vectors corres pond to the ideal y values, generated directly from the “true” model (whatev er it \nmay be) witho ut any noise. In contrast, the ynoisy vectors are the actual, noisy observations, generated \nby adding Gaussian noise to the ytrue vectors. You should use ynoisy for any estimation. ytrue is provided \nonly to make it easier to evaluate the error in your prediction s (simulate an inﬁnite test data) . You \nwould not have ytrue in any real task. \n(a) Write MATLAB functions theta = linear regress(y,X) and y hat = linear pred(theta,X test). \nNote that we are not expli citly including the oﬀset parameter but instead rely on the featur e vectors \nto provide a constan t component. See part (b). \n(b) The feature mappi ng can substan tially aﬀect the regression results. We will consider two possible \nfeatu re mapp ings: \nφ1(x1,x2,x3) = [1,x1,x2,x3]T \nφ2(x1,x2,x3) = [1, log x12 , log x22 , log x32]T \nUse the provided MATLAB function feature mapping to transform the input data matrix into a \nmatrix\n ⎡\n ⎤\nφ(x1)T \nφ(x2)T ⎢⎢⎣\n⎥⎥⎦\nX =\n··· \nφ(xn)T\nFor exampl e, X = feature mapping(X in,1) would get you the ﬁrst feature representation. Using\nyour complete d linear regres sion functions, compute the mean squared prediction error for each\nfeatu re mapp ing (2 numbers).\n(c) The selection of points to query in an activ e learni ng framework migh t depend on the feature rep­\nresentation. We will use the same selection criterion as in the lectures, the expected squar ed error in \nthe parameters, proportion al to Tr[(XT X)−1]. Write a MATLAB function idx = active learn(X,k1,k2). \nYour function should assume that the top k1 rows in X have been queried and your goal is to se­\nquentially ﬁnd the indices of the next k2 points to query . The ﬁnal set of k1 + k2 indices should be \nretur ned in idx. The latter may contain repeated entries. For each feature mapp ing, and k1 =5 \nand k2 = 10, compu te the set of points that should be queried (i.e., X(:,idx)). For each set of \npoints, use the featur e mappi ng φ2 to perform regression and compute the resulting mean squared \nprediction errors (MSE) over the entire data set (again, using φ2). \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "Massa chusetts Institu te of Techn ology ", "source_title": "6ec94b9fcb1e56af9ea87be7dbb9b62a hw2", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "7e4aec04-b1c7-40f1-84e0-4d9bffbbe670", "text": "be queried (i.e., X(:,idx)). For each set of points, use the featur e mappi ng φ2 to perform regression and compute the resulting mean squared prediction errors (MSE) over the entire data set (again, using φ2). Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� � (d) Let us repeat the steps of part (c) with randomly selected additional points to query. We have \nprovided a MATLAB function idx = randomly select(X,k1,k2) which is essentially the same \nas active learn except that it selects the k2 points uniformly at random from X. Repeat the \nregres sion s teps as in previou s part, and compu te the resulti ng mean squared prediction e rror again. \nTo get a reasonable comparison you should repeat this process 50 times, and use the median MSE. \nCom pare the resulting error s with the activ e learning strategie s. What conclusions can you draw? \n(e) Let us now compare the two sets of points chosen by activ e learn ing due to the diﬀerent feature \nrepresentations. We have provided a function plot points(X,idx r,idx b) which will plot each \nrow of X as a point in R3 . The points indexed by idx r will be circled in red and those mark ed \nby idx b will be circle d (larger) in blue (some of the points indexed by idx r and idx b migh t \nbe common). Plot the original data points using the indexes of the activ ely selected points based \non the two featur e repres entations. Also plot the same indexes using X from the second featur e \nrepresentation with its ﬁrst constan t column removed. In class, we saw an example where the \nactiv e learn ing strategy chose points at the extrem a of the availab le space . Can you see evidence \nof this in the two plots? \n2. We derived a kernel version of linear regression in class using regul arize d least squares criterion. The \nresulting prediction s were given by \nn\ny(x)= (ˆαt/λ)K (xt, x) \nt=1 \nwhere the optimal setting of the coeﬃcients (prediction diﬀerence s) ˆαt were given in a vector form by \na = λ(λI + K)−1y \nHere K is the Gram matrix based on the availab le data points x1,..., xn and y =[y1,...,yn]T . We are \nintereste d in exploring how the regular ization paramete r λ ∈ [0, ∞) aﬀects the solution when the kernel \nfunction is the radial basis kernel \nK(x, x�) = exp − β \n2 �x − x��2 , β> 0 \na) Let’s see ﬁrst that the limit λ 0 (no regularization ) makes sense. In this case we need the Gram →\nmatrix K to be invertible. Indicate why the Gram matrix corres ponding to the radial basis kernel \nis always invertible based on the following Michelli theorem: \nIf ρ(t) is a monot onic function of t ∈ [0, ∞) then the matrix ρij = ρ(�xi − xj �) is invertibl e for \nany distinct set of points x1,..., xn. \nb) How do we make prediction s when λ 0? In other words, what is the function y(x) in this limit? → \nb) Show that the corres ponding training error is exactly zero. \nc) Settin g λ = 0 (no regular ization) seems hardly optimal. Implement the kernel linear regression \nmethod above for λ> 0. We have provided trainin g and test data as well as helpf ul MATLAB \nscripts in hw2/pr ob2. You should only need to complete the relevant lines in run prob2 script. \nThe data pertains to the problem of predictin g Boston housing prices based on variou s indicators \n(normalize d). Evaluate and plot the trainin g and test errors (mean squared errors) as a function \nof λ in the range λ ∈ (0, 1). Use β =0.05. Explain the qualitati ve behavior of the two curves. \n3. Most linear classiﬁers can be turned into a kernel form. We will focus here on the simple perceptron \nalgor ithm and use the resulti ng kernel version to classify data that are not linearl y separab le. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "be queried (i.e., X(:,idx)). For each set of points, use the featur e mappi ng φ2 to perform regression and compute the ", "source_title": "6ec94b9fcb1e56af9ea87be7dbb9b62a hw2", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "d6266fdd-58f0-4d5b-8eb7-a2b58605ee0c", "text": "linear classiﬁers can be turned into a kernel form. We will focus here on the simple perceptron algor ithm and use the resulti ng kernel version to classify data that are not linearl y separab le. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� (a) First we need to turn the perceptron algori thm into a form that involves only inner products between \nthe featu re vectors. We will focus on hyper-planes throu gh origin in the featu re space (any oﬀset \ncomponent provided as part of the featu re vectors). The mistak e driven parameter updates are: \nθθ + ytφ(x t) if ytθT φ(x t) < 0, where θ = 0 initially . Show that we can rewrite the perceptron ←\nupdates in terms of simple additive updates on the discr iminant function f(x)= θT φ(x): \nf(x) f(x)+ ytK(xt, x) if ytf(xt) < 0 ← \nwhere K(xt, x)= φ(x t)T φ(x) is any kernel function and f(x) = 0 initially. \n(b) We can replace K(xt, x) with any kernel function of our choice such as the radial basis kernel \nwhere the corresponding featur e mapp ing is inﬁnite dimensional . Using the analysis in the previous \nquestion (2), show that there always is a separati ng hyperplane if we use the radial basis kernel. \n(c) With the radial basis kernel we can therefore conclude that the perceptron algorith m will converge \n(stop updating) after a ﬁnite number of steps for any dataset with distinct points. The resulting \nfunction can therefor e be written as \nn\nf(x)= wiyiK(xi, x) \ni=1 \nwhere wi is the number of times we made a mistak e on exam ple xi. Most of wi’s are exactly zero \nso our function won’t be diﬃcult to handle. The same form holds for any kernel except that we \ncan no longer tell whether the wi’s remain bounded (problem is separable with the chosen kernel). \nImplem ent the new kernel perceptron algor ithm in MATLAB using a radial basis and polynomial \nkernels. The data and helpf ul scripts are provided in hw2/pr ob3.\nDeﬁn e functions\nalpha = train kernel perceptron(X, y, kernel type) and\nf = discriminant function(alpha, X, kernel type, X test)\nto train the pereptron and to evaluate the resulting f(x) for test examples , respectively. \n(d) Load the data using the load p3 a script. When you use a polynomial kernel to separate the \nclasses, what degree polynomials do you need? Draw the decision boundary (see the provided scrip t \nplot dec boundary) for the lowest-degree polynomial kernel that separates the data. Repeat the \nprocess for the radial basis kernel. Brieﬂy discuss your observations. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "linear classiﬁers can be turned into a kernel form. We will focus here on the simple perceptron algor ithm and use the r", "source_title": "6ec94b9fcb1e56af9ea87be7dbb9b62a hw2", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "e9bec3c9-a556-4190-a4a5-ceccc56ec4af", "text": "1Massachusetts Institute of Technology \n6.867 Machine Learning, Fall 2006 \nProblem Set 1 Solutions \nSection A (background questions) \n1. Let’s begin with a little math. Let us denote by Pn the probability that n people with \nrandomly chosen birthdays (chosen uniformly from the year) have no common birthdays. \nClearly P1 = 1; \n365 − n Pn+1 = Pn;365 · \nafter n days have been taken, the chance that we will get one of the remaining free days is \n(365 − n)/365. If we expand this, we get the following formula: \nPn+1 = 365 − n \n365 · Pn \n= 365 − n \n365 · 365 − (n − 1) \n365 · Pn−1 \n. . . \n= 365 − n \n365 · 365 − (n − 1) \n365 · · · · · 365 − 1 \n365 · 365 \n365 \n= 365! /(365 − (n + 1))! . 365n+1 \nWe can easily write a MATLAB script to evaluate this formula (of course, we want 1 − Pn): \nfunction P =birthday prob (n) \nP = 1; · \nfor i = 2 : n · \nP = P ∗ (365 − i)/365; · · \nend· \nP = 1 − P · \nWe can also give an approximate solution by just sampling (and not doing any math). This \nis a much slower algorithm, but it works: \nfunction P =birthday prob (n) \nP = 0; · \nsamples = 1000; · \nfor i = 1 : samples · \ncounts = zeros(365, 1); · · \ncollision = 0; · · \nfor j = 1 : n · · \nbday = randint (1, 1, 365)+1; · · · \nif counts(bday) > 0 · · · \ncollision = 1; · · · · \nbreak · · · · \nend · · · \ncounts(bday) = 1; · · · \nend · · \nP = P + collision; · · \nend· \nP = P/samples; · \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "function", "section_heading": "1Massachusetts Institute of Technology ", "source_title": "2d12f0fc5d7554cd6d23586c3d163c53 hw1a soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 0}
{"id": "fba3b303-1156-4d29-b3c0-a8f1f75788f1", "text": "= 1; · · · · break · · · · end · · · counts(bday) = 1; · · · end · · P = P + collision; · · end· P = P/samples; · Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2� \n� With either script, we can quickly ﬁnd the smallest group that has a 50% chance of having a \ncommo n birthda y: \n� i = 2; \n� while birthday prob(i) < 0.5\ni = i + 1;\n · \n� end \n� i \ni = \n22 \n� birthday prob(22)\nans =\n0.5059 \n[Yes, we have assumed for simplicit y that no-one is born on a leap year and that, by extension, \nall people are spherical. The case of non-spherical people born in the real world is left as an \nexercise for the reader.] \n2. In order to solve these problems, we will consider the cumulativ e distributio n function (cdf) \nof the Xi and of their maxim um and minim um. \nFXi (x) probabilit y that X<x \n0 if x< 0 ⎧ \n⎪⎨ =\nx if 0 ≤ x ≤ 1 \nx> 1 =\n⎪⎩1 if\nFmax{Xi}(x) = probabilit y that max{Xi} <x \n= probabilit y that Xi <x for all i \nn� \n= FXi (x) [because the Xi are indep ende nt] \ni=1⎧ \n⎪⎨0 if x < 0 \nxn if 0 ≤ x ≤ 1 \nx> 1 =\n⎪⎩1 if\nn� 1 − Fmin{Xi}(x) = probabilit y that min{Xi}≥ x \nprobabilit y that Xi ≥ x for all i =\n⎧ \n⎪⎨ = (1 − FXi (x)) \ni=1 \n1 if x< 0\n(1 − x)n if 0 ≤ x ≤ 1 \n0 x> 1 =\n⎪⎩if\nBy taking a deriv ative, we can recover the densit y functions of all the variables:\nfmax{Xi}(x) = nxn−1 if 0 ≤ x ≤ 1 \n0 otherwise \nfmin{Xi}(x) = n(1 − x)n−1 if 0 ≤ x ≤ 1 \n0 otherwise \nWe can ﬁnally comput e the expectations! The mathematica lly agile will note that there are \nmany clever ways of evaluating this last integral, but we choose to remind you of the genera lly \napplica ble metho ds over demo nstrat ing cute mathematics. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "function", "section_heading": "= 1; · · · · break · · · · end · · · counts(bday) = 1; · · · end · · P = P + collision; · · end· P = P/samples; · Cite a", "source_title": "2d12f0fc5d7554cd6d23586c3d163c53 hw1a soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 1}
{"id": "00620b88-3348-4ad3-9cd2-d3ef4cd09281", "text": "mathematica lly agile will note that there are many clever ways of evaluating this last integral, but we choose to remind you of the genera lly applica ble metho ds over demo nstrat ing cute mathematics. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3� \n� � \n� � � \n� \n� \n� � � �� � � � � 1 \nE[max{Xi}]= xfmax{Xi} dx\n0\n� 1\n n = nx n dx = . n +1 0 \n� 1 \nE[min{Xi}]= xfmin{Xi} dx\n0\n� 1\n= nx(1 − x)n−1 dx \n0 � � �1 \n= −x(1 − x)n + (1 − x)n dx (by parts) \nx=0 � �1(1 − x)n+1 1 = −x(1 − x)n − n +1 = . n +1 x=0 \n3. We prese nt two ways of solving this problem: the right way and the better way. \nWe’ll start with the better way (the way we want you to do it). Deﬁne indic ator variables: \n0 if player i is sick Ii = 1 otherwise. \nThen, E[Ii]= P [Ii = 1] = 7/8, and: \n30 \n4 189 E[IiIj ]= P [Ii = 1 and Ij = 1] = �� = , for i = j. 32 248�\n4 \nWitho ut loss of genera lity, assume that the players are seeded so that player 2i − 1 is to play \nwith player 2i for i =1 ... 16. Then, the expected number of remaining pairs is: \n16\nE[number of health y pairs] = E I2i−1I2i\ni=1\n16\n= E[I2i−1I2i] by linearit y of expecatio n \ni=1 \n189 378 =16 = ≈ 12.2. · 248 31 \nNow for the right (and boring) way: \nn\nE[number of health y pairs] = iP [number of health y pairs = i] \ni=1 \n= 12 × P [number of health y pairs = 12] \n+ 13 × P [number of health y pairs = 13] \n+ 14 × P [number of health y pairs = 14] \n16 24 16 15 22 16 \n4 1 2 2 378 · \n= 12 × �� + 13 × �� + 14 × �� = . 32 32 32 31 4 4 4 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "if ", "section_heading": "mathematica lly agile will note that there are many clever ways of evaluating this last integral, but we choose to remin", "source_title": "2d12f0fc5d7554cd6d23586c3d163c53 hw1a soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 2}
{"id": "7c489050-337e-42a6-9ff5-8e96e07174ca", "text": "pairs = 14] 16 24 16 15 22 16 4 1 2 2 378 · = 12 × �� + 13 × �� + 14 × �� = . 32 32 32 31 4 4 4 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n4� � �� \n� � \n� � \n� � \n� � 4. (a) You should switch! This is equiv alent to changing your choice to both of the doors that \nyou did not choose. This is very coun terin tuitiv e, since it seems that switching to a single \nnew door is the same as choosing that door to begin with, but the door that Monty opens \ndepends on the door that you initia lly chose (since if we chose one of the goats to begin \nwith, we have forced Monty to open the door to the other goat, and the last door must \nconceal the Corvette). Thus, you should deﬁnitely switc h! \n(b) \n� switc hscore = 0; \n� stayscore = 0; \n� for i = 1 : 1000\ndoors = [1, 2, 3];\n · \ncorvette = randsampl e(do ors, 1); · \nmyguess = randsampl e(do ors, 1); · \ndoors = ﬁnd(do ors ∼= corvette); · \ndoors = ﬁnd(do ors ∼= myguess); · \nmontyopens = randsampl e(do ors, 1); · \nif myguess == corvette · \nstayscore = stayscore + 1; ·· \nelse· \nswitc hscore = switchscore + 1; ·· \nend· \n� end \n� [stayscore, switc hscore] \nans =\n335 665\n(c) Our previo us reasoning still holds; it doesn’t matter whic h door you switc h to; by sym­\nmetry , they both have the same probabilit y of hiding Corvettes. \n5. (a) \nP (x1,x2)= 21 \nπ exp − (x1 − \n210)2 \n− (x1 − 10)(x2 − 5) + (x2 − 5)2 \n(b) \ncov (Ax, Bx)= E (Ax − E [Ax]) (Bx − E [Bx])T \nT = EA (x − E [x])(B (x − E [x]))\n= EA (x − E [x])(x − E [x])T BT \n= AE (x − E [x])(x − E [x])T BT \n= A cov (x) BT . · · \n6. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "if ", "section_heading": "pairs = 14] 16 24 16 15 22 16 4 1 2 2 378 · = 12 × �� + 13 × �� + 14 × �� = . 32 32 32 31 4 4 4 Cite as: Tommi Jaakkola,", "source_title": "2d12f0fc5d7554cd6d23586c3d163c53 hw1a soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 3}
{"id": "acb2e534-e7c1-4e85-95bf-780455d8355c", "text": "− E [x])(B (x − E [x])) = EA (x − E [x])(x − E [x])T BT = AE (x − E [x])(x − E [x])T BT = A cov (x) BT . · · 6. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n5� v1 = [0, 0, 0, 0, 0, 1]�;\n� v2 = [1, 2, 3, 4, 5, 6]�;\n� v3 = [1, 4, 9, 16, 25, 36]�;\n� v4 = [1, 0, 0, 0, 0, 0]�;\n� u1= v1; \n� u1= u1/norm (u1);\n� u2= v2 − (v2� ∗ u1) ∗ u1;\n� u2= u2/norm (u2);\n� u3= v3 − (v3� ∗ u1) ∗ u1 − (v3� ∗ u2) ∗ u2;\n� u3= u3/norm (u3);\n� u4= v4 − (v4� ∗ u1) ∗ u1 − (v4� ∗ u2) ∗ u2 − (v4� ∗ u3) ∗ u3;\n� u4= u4/norm (u4); \n� [u1,u2,u3,u4]\nans =\n00.1348 −0.4040 0.9048\n00.2697 −0.5465 −0.2842\n00.4045 −0.4277 −0.2513\n00.5394 −0.0475 −0.1016\n00.6742 0.5941 0.1648\n1.0000 0 0 0 \nIf we start with vectors in a diﬀeren t order, we will get a diﬀeren t basis; for example , the ﬁrst \nvector of the new basis will always be a unit vector in the direction of the ﬁrst vector provided \nto us. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "− E [x])(B (x − E [x])) = EA (x − E [x])(x − E [x])T BT = AE (x − E [x])(x − E [x])T BT = A cov (x) BT . · · 6. Cite as:", "source_title": "2d12f0fc5d7554cd6d23586c3d163c53 hw1a soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "a9e630d8-0823-40f2-bc79-03f2cb2c0c30", "text": "6.867 Machine learning and neural networks \nFALL 2001 – Final exam \nDecem ber 11, 2001 \n(2 points) Your name and MIT ID #: \n(4 points) The grade you would give to yourself + brief justiﬁcat ion. If you \nfeel that there’s no question that your grade should be A (and you feel we agree with \nyou) then just write “A”. \nProblem 1\n1. (T/F – 2 points) The sequence of output symbols sampled from a \nhidden Markov model satisﬁes the ﬁrst order Markov property \n2. (T/F – 2 points) Increasing the number of values for the the hid­\nden states in an HMM has much great er eﬀect on the comput ational \ncost of forward-backward algorithm than increasing the length of the \nobserv ation sequence. \n1 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3. (T/F – 2 points) In HMMs , if there are at least two distinct most \nlikely hidden state sequences and the two state sequence s cross in the \nmiddle (share a single state at an intermediate time point), then there \nare at least four most likely state sequences . \n4. (T/F – 2 points) One advantage of Boosting is that it does not overﬁt. \n5. (T/F – 2 points) Supp ort vector machines are resistan t to outliers, \ni.e., very noisy examples drawn from a diﬀerent distribution. \n6. (T/F – 2 points) Activ e learning can substan tially reduce the number \nof training examples that we need. \nProblem 2 \nConsider two classiﬁe rs: 1) an SVM with a quadra tic (second order polynomial) kernel \nfunction and 2) an unconstra ined mixture of two Gaussians model, one Gaussian per class \nlabel. These classiﬁers try to map examples in R2 to binary labels. We assume that the \nproblem is separable, no slack penalties are added to the SVM classiﬁer, and that we have \nsuﬃcie ntly many training examples to estimate the covariance matrices of the two Gauss ian \ncomp onents. \n1. (T/F – 2 points) The two classiﬁe rs have the same VC-dimension. \n2. (4 points) Supp ose we evaluated the structural risk minimizatio n score for the two \nclassiﬁers. The score is the bound on the expected loss of the classiﬁer, when the \nclassiﬁer is estimated on the basis of n training examples . Whic h of the two classiﬁers \nmigh t yield the better (lower) score? Provide a brief justiﬁcat ion. \n2\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "6.867 Machine learning and neural networks ", "source_title": "41387b48c364b127843885db7cd56b01 final f01", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "f13f0f15-6f89-434b-8e87-a91289c1e7f0", "text": "loss of the classiﬁer, when the classiﬁer is estimated on the basis of n training examples . Whic h of the two classiﬁers migh t yield the better (lower) score? Provide a brief justiﬁcat ion. 2 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3. (4 points) Supp ose now that we regularize the estima tion of the covariance matr ices \nfor the mixture of two Gaussians. In other words, we would estimate each class \nconditio nal covariance matrix accor ding to \nn n�ˆ ˆ Σreg = Σ+ S (1) n + n� n + n� \nwhere n is the number of training examples , ˆΣ is the unregularized estima te of the \ncovariance matri x (sample covariance matrix of the examples in one class), S is our \nprior covariance matr ix (same for both classes ), and n� the equiv alent sample size \nthat we can use to balance between the prior and the data. \nIn computing the VC-dimension of a classiﬁer, we can choose the set of points that \nwe try to “shatter” . In particular, we can scale any k points by a large facto r and \nuse the resulting set of points for shatt ering. In light of this, would you expect our \nregula rization to chang e the VC-dimension? Why or why not? \n4. (T/F – 2 points) Regulariza tion in the above sense would impro ve \nthe structur al risk minimizatio n score for the mixtur e of two Gaussians. \n3\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "loss of the classiﬁer, when the classiﬁer is estimated on the basis of n training examples . Whic h of the two classiﬁer", "source_title": "41387b48c364b127843885db7cd56b01 final f01", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "bc4bdcce-9038-4070-9753-f98b29a62386", "text": "e the VC-dimension? Why or why not? 4. (T/F – 2 points) Regulariza tion in the above sense would impro ve the structur al risk minimizatio n score for the mixtur e of two Gaussians. 3 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 3 \nThe problem here is to predict the identity of a person based on a single handwritten \ncharacter. The observed characters (one of ’a’, ’b’, or ’c’) are transfor med into binary 8 \nby 8 pixel images. There are four diﬀerent people we need to identify on the basis of such \ncharacters. To do this, we have a training set of about 200 examples, where each example \nconsists of a binary 8x8 image and the identity of the person it belongs to. You can assume \nthat the overall number of occurences of each person and each character in the training set \nis roughly balanced. \nWe would like to use a mixture of experts architecture to solve this problem. \n1. (2 points) How might the experts be useful ? Suggest what task each expert might \nsolve. \n2. (4 points) Draw a graphical model that describ es the mixture of experts architecture \nin this context. Indicate what the variables are and the number of values that they \ncan take. Shade any nodes corresp onding to variables that are always observed. \n4\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3. (4 points) Before implemen ting the mixture of experts architecture, we need to know \nthe parametric form of the conditio nal proba bilities in your graphica l model. Provide \na reasonable speciﬁcat ion of the relevant conditiona l proba bilities to the exten t that \nyou could ask your class- mate to impleme nt the classiﬁer. \n4. (4 points) So we imple mented your metho d, ran the estima tion algorithm once, \nand measured the test performance. The method was unfortuna tely performing at \na chance level. Provide two possible expla natio ns for this. (there may be multiple \ncorrect answers here) \n5. (3 points) Would we get anything reaso nable out of the estimatio n if, initially , \nall the experts were identical while the parameters of the gating network would be \nchosen randomly? By reaso nable we mean training performance. Provide a brief \njustiﬁcatio n. \n5\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "for ", "section_heading": "e the VC-dimension? Why or why not? 4. (T/F – 2 points) Regulariza tion in the above sense would impro ve the structur a", "source_title": "41387b48c364b127843885db7cd56b01 final f01", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 2}
{"id": "82171327-b6d1-483b-a54e-3bb43507d0d7", "text": "of the estimatio n if, initially , all the experts were identical while the parameters of the gating network would be chosen randomly? By reaso nable we mean training performance. Provide a brief justiﬁcatio n. 5 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n6. (3 points) Would we get anything reasonable out the estimat ion if now the gating \nnetwork is initia lly set so that it assigns each training example uniformly to all experts \nbut the experts themselv es are initia lized with rando m parameter values? Again, \nreaso nable refers to the trainin g error. Provide a brief justiﬁcatio n. \nProblem 4 \nConsider the following pair of observed sequence s: \nSequence 1 (st): AATTGGCC AATTGGCC ... \nSequence 2 (xt): 11221122 11221122 ... \nPositio n t: 0 1 2 3 4 ... \nwhere we assume that the pattern (highligh ted with the spaces) will continue forever. Let \nst ∈{A,G,T,C}, t =0, 1, 2,... denote the variables associated with the ﬁrst sequence, \nand xt ∈{1, 2}, t =0, 1, 2,... the variables characterizing the second sequence . So, for \nexample, given the sequence s above, the o bserved values for t hese variables are s 0 = A,s1 = \nA,s2 = C,..., and, similarly, x0 =1,x1 =1,x2 =2,.... \n1. (4 points) If we use a simple ﬁrst order homogeneous mark ov model to predict the \nﬁrst sequence (values for st only), what is the maximum likelihood solut ion that we \nwould ﬁnd? In the transition diagr am below, please draw the relevant transitio ns and \nthe associated proba bilities (this should not requir e much calculat ion) \nstATC\nG\nATC\nG\nst−1. . . . . .\n6\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "if ", "section_heading": "of the estimatio n if, initially , all the experts were identical while the parameters of the gating network would be ch", "source_title": "41387b48c364b127843885db7cd56b01 final f01", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 3}
{"id": "fcd22590-0452-4d27-a919-1b5ab1a93b7b", "text": "In the transition diagr am below, please draw the relevant transitio ns and the associated proba bilities (this should not requir e much calculat ion) stATC G ATC G st−1. . . . . . 6 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2. (T/F – 2 points) The resulting ﬁrst order Markov model is ergodic \n3. (4 points) To impro ve the Mark ov model a bit we would like to deﬁne a graphica l \nmodel that predicts the value of st on the basis of the previo us observed values \nst−1,st−2,... (looking as far back as needed). The model parameters/structur e are \nassumed to rema in the same if we shift the model one step. In other words, it is the \nsame graphical model that predicts st on the basis of st−1,st−2,... as the model that \npredicts st−1 on the basis of st−2,st−3,.... In the graph below, draw the minimum \nnumb er of arrows that are needed to predict the ﬁrst observ ed sequence perfectly \n(disrega rding the ﬁrst few symbols in the sequenc e). Since we slide the model along \nthe sequence, you can draw the arrows only for st. \nstst−1st−2st−3st−4. . . . . .\n4. Now, to incorporate the second observ ation sequence, we will use a standard hidden \nMark ov model: \nstst−1st−2st−3st−4\n. . . . . .\n. . . . . .\nxtxt−1xt−2xt−3xt−4\nwhere again st ∈{A,G,T,C} and xt ∈{1, 2}. We will estimate the parameters of \nthis HMM in two diﬀerent ways. \n(I) Treat the pair of observ ed sequences (st,xt) (given above) as complete observa­\ntions of the variables in the model and estimate the parameters in the maximum \nlikeliho od sense. The initial state distribut ion P0(s0) is set according to the over­\nall frequency of symbols in the ﬁrst observed sequence (uniform). \n(II) Use only the second observ ed sequence (xt) in estimating the parameters, again \nin the maximum likeliho od sense. The initia l state distribut ion is again uniform \nacross the four symbols. \nWe assume that both estimatio n processe s will be success full relative to their criteria . \n7\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "hmm", "section_heading": "In the transition diagr am below, please draw the relevant transitio ns and the associated proba bilities (this should n", "source_title": "41387b48c364b127843885db7cd56b01 final f01", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "d52c72c7-a8d4-416f-afd9-d0f3ab99b284", "text": "the maximum likeliho od sense. The initia l state distribut ion is again uniform across the four symbols. We assume that both estimatio n processe s will be success full relative to their criteria . 7 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\na) (3 points) What are the observation probabilities P (x|s)(x ∈{1, 2}, s ∈ \n{A,G,T,C}) resulting from the ﬁrst estimatio n approa ch? (should not require \nmuch calculat ion) \nb) (3 points) Whic h estima tion approach is likely to yield a more accurate model \nover the second observ ed sequence (xt)? Brieﬂy expla in why. \n5. Consider now the two HMM s resulting from using each of the estimat ion appro aches \n(approa ches I and II above). These HMMs are estimated on the basis of the pair of \nobserv ed sequence s given above. We’d like to evaluate the proba bility that these two \nmodels assign to a new (diﬀeren t) observ ation sequenc e 1212, i.e., x0 =1,x1 = \n2,x2 =1,x3 = 2. For the ﬁrst model, for which we have some idea about what the \nst variables will capture, we also want to know the the associated most likely hidden \nstate sequenc e. (these should not require much calculatio n) \na) (2 points) What is the proba bility that the ﬁrst model (approach I) assigns to \nthis new sequence of observ ations? \nb) (2 points) What is the probabilit y that the second model (appr oach II) gives \nto the new sequence of observ ations? \n8\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "hmm", "section_heading": "the maximum likeliho od sense. The initia l state distribut ion is again uniform across the four symbols. We assume that", "source_title": "41387b48c364b127843885db7cd56b01 final f01", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "a7b88657-017d-44ee-b969-f1e3f43a477d", "text": "ﬁrst model (approach I) assigns to this new sequence of observ ations? b) (2 points) What is the probabilit y that the second model (appr oach II) gives to the new sequence of observ ations? 8 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nc) (2 points) What is the most likely hidde n state sequence in the ﬁrst model \n(from approa ch I) associated with the new observ ed sequence? \n6. (4 points) Finally , let’s assume that we observ e only the second sequenc e (xt) (the \nsame sequence as given above). In building a graphical model over this sequence we \nare no longer limiting ourselves to HMM s. However, we only consider models whose \nstructure/ parameters remain the same as we slide along the sequenc e. The variables \nst are included as before as they might come handy as hidden variables in predicting \nthe observ ed sequence. \na) In the ﬁgure below, draw the arrows that any reasonable model selection criterion \nwould ﬁnd given an unlimited supply of the observed sequence xt,xt+1,.... You \nonly need to draw the arrows for the last pair of variables in the graphs, i.e., \n(st, xt)). \nstst−1st−2st−3st−4\n. . . . . .\n. . . . . .\nxtxt−1xt−2xt−3xt−4\nb) Given only a small number of obervations, the model selection criterion migh t \nselect a diﬀeren t model. In the ﬁgure below, indicat e a possible alternat e model \nthat any reaso nable model selection criterion would ﬁnd given only a few ex­\namples. You only need to draw the arrows for the last pair of variables in the \ngraphs, i.e., (st,xt)). \nstst−1st−2st−3st−4\n. . . . . .\n. . . . . .\nxtxt−1xt−2xt−3xt−4\n9\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "hmm", "section_heading": "ﬁrst model (approach I) assigns to this new sequence of observ ations? b) (2 points) What is the probabilit y that the s", "source_title": "41387b48c364b127843885db7cd56b01 final f01", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "9e00b0f4-be50-4d91-b528-30d9691c0905", "text": "few ex­ amples. You only need to draw the arrows for the last pair of variables in the graphs, i.e., (st,xt)). stst−1st−2st−3st−4 . . . . . . . . . . . . xtxt−1xt−2xt−3xt−4 9 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nx1x2\nx3x4x5\nx6B AFigur e 1: Decision makers A and B and their “context” \nProblem 5 \nThe Bayesian network in ﬁgure 1 claims to model how two people, call them A and B, make \ndecis ions in diﬀeren t contexts. The context is speciﬁed by the setting of the binar y context \nvariables x1,x2,...,x6. The values of these variables are not known unless we speciﬁcally \nask for such values. \n1. (6 points) We are interested in ﬁnding out what informat ion we’d have to acquire \nto ensure that A and B will make their decisions indep enden tly from one another. \nSpecify the smal lest set of context variables whose insta ntiation would render A and \nB independen t. Brieﬂy expla in your reasoning (there may be more than one strateg y \nfor arriving at the same decision) \n2. (T/F – 2 points) We can in general achieve indep endence with less \ninforma tion, i.e., we don’t have to fully instan tiate the selected context \nvariables but provide some evidence about their values \n10 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3. (4 points) Could your choice of the minim al set of context variables chang e if we \nalso provided you with the actua l proba bility values associated with the dependencies \nin the graph? Provide a brief justiﬁca tion. \nAddi tional set of ﬁgures\nstATC\nG\nATC\nG\nst−1. . . . . .\nstst−1st−2st−3st−4. . . . . .\nstst−1st−2st−3st−4\n. . . . . .\n. . . . . .\nxtxt−1xt−2xt−3xt−4\n11\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nstst−1st−2st−3st−4\n. . . . . .\n. . . . . .\nxtxt−1xt−2xt−3xt−4\nstst−1st−2st−3st−4\n. . . . . .\n. . . . . .\nxtxt−1xt−2xt−3xt−4\nx1x2\nx3x4x5\nx6B A12\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "bayesian network", "section_heading": "few ex­ amples. You only need to draw the arrows for the last pair of variables in the graphs, i.e., (st,xt)). stst−1st−", "source_title": "41387b48c364b127843885db7cd56b01 final f01", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 7}
{"id": "7c472f4d-3746-49ed-ab2a-15d1d7c6cc0b", "text": "6.867 Machine learning \nMid-t erm exam \nOcto ber 13, 2004 \n(2 points) Your name and MIT ID: \nT. Assistant, 968672004\nProblem 1\nprediction error \n−1 0 1−101\n−1 0 1−101\n−1 0 1−101\nA B C \n1. (6 points) Each plot above claims to represent predictio n errors as a function of \nx for a trained regression model based on some dataset. Some of these plots could \npotentially be prediction errors for linear or quadra tic regression models, while oth­\ners couldn’t. The regression models are trained with the least squares estimatio n \ncriterio n. Please indicate compat ible models and plots. \nA B C \nlinear regression ( x ) ( x ) ( ) \nquadratic regression ( x ) ( ) ( ) \n1\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 2 \nHere we explore a regression model where the noise variance is a function of the input \n(variance increases as a function of input). Speciﬁcally \ny = wx + � \nwhere the noise � is normally distr ibuted with mean 0 and standard deviation σx. The \nvalue of σ is assumed known and the input x is restricted to the interval [1, 4]. We can \nwrite the model more compa ctly as y ∼ N(wx,σ2x2). \nIf we let x vary within [1, 4] and sample outputs y from this model with some w, the \nregression plot might look like \n1 2 3 40246810\nxy\n1. (2 points) How is the ratio y/x distributed for a ﬁxed (consta nt) x?\nSince y ∼ N(wx,σ2x2), for any constant x, y/x is also Gaussian with mean wx/x = \nw and varianc e σ2x2/x2 = σ2 . So, y/x ∼ N(w,σ2). \n2. Supp ose we now have n training points and targets {(x1,y1), (x2,y2),..., (xn,yn)}, \nwhere each xi is chosen at random f rom [1, 4] and the corresponding y i is subsequen tly \nsampled from yi ∼ N(w∗xi,σ2xi 2) with some true underlying parameter value w∗; the \nvalue of σ2 is the same as in our model. \n2\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "6.867 Machine learning ", "source_title": "ca7a3e3f6a26bddd927fab0ea5a2d861 midterm f04soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "78a88e53-6989-4e3a-ac33-b14e5c45a51e", "text": "rom [1, 4] and the corresponding y i is subsequen tly sampled from yi ∼ N(w∗xi,σ2xi 2) with some true underlying parameter value w∗; the value of σ2 is the same as in our model. 2 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n(a) (3 points) What is the maximum-lik eliho od estimate of w as a function of the \ntraining data? \nWe know that y/x ∼ N(w,σ2). We can therefore estimate w by inter preting yi/xi \nas observations. The maximum likeliho od estimate of w is simply the mean \n1 wˆn = yi/xi n \n(b) (3 points) What is the variance of this estimat or due to the noise in the target \noutputs as a function of n and σ2 for ﬁxed inputs x1,...,xn? For later utility \n(if you omit this answ er) you can deno te the answ er as V (n,σ2). \nThe varianc e of the estimator of a mean of a gaussian is σ2/n. \nSome potentially useful relations: if z ∼ N(µ,σ2), then az ∼ N(aµ,σ2a2) for a \nﬁxed a. If z1 ∼ N(µ1,σ2) and z2 ∼ N(µ2,σ2) and they are indep enden t, then1 2\nVar(z 1 + z2)= σ12 + σ22 .\n3. In sequen tial activ e learning we are free to choose the next training input xn+1, here \nwithin [1, 4], for which we will then receiv e the corresp onding noisy target yn+1, sam­\npled from the unde rlying model. Supp ose we already have {(x1,y1), (x2,y2),..., (xn,yn)}\nand are trying to ﬁgure out which xn+1 to select. The goal is to choose xn+1 so asto \nhelp minimize the variance of the predictions f(x;ˆwn)= wˆnx, where ˆwn is the maxi­\nmum likeliho od estimat e of the parameter w based on the ﬁrst n training examples. \n(a) (2 points) What is the variance of f(x;ˆwn) due to the noise in the training out­\nputs as a function of x, n, and σ2 given ﬁxed (alrea dy chosen) inputs x1,...,xn? \nf(x;ˆwn) is xw, and we know that the variance of w is σ2/n (from the previous part). \nThus the varianc e of f(x;ˆwn) is x2σ2/n. \n(b) (2 points) Whic h xn+1 would we choose (within [1, 4]) if we were to next select \nx with the maximum variance of f(x;ˆwn)? \nThe varianc e is maximize d when x2 is maximum, that is x =4. \n(c) (T/F – 2 points) Since the variance of f(x;ˆwn) only depends on x, \nn, and σ2, we could equally well select the next point at random from \n[1, 4] and obtain the same reductio n in the maximum variance. \nThe varianc e at x =4 is 16σ2/n. This does not depend on the actual \nchoic e of the querie d points, but only on the numb er of points queried. \n3 T\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "function", "section_heading": "rom [1, 4] and the corresponding y i is subsequen tly sampled from yi ∼ N(w∗xi,σ2xi 2) with some true underlying paramet", "source_title": "ca7a3e3f6a26bddd927fab0ea5a2d861 midterm f04soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 1}
{"id": "938fdca7-b421-4cc6-9db8-2f259a178d30", "text": "the maximum variance. The varianc e at x =4 is 16σ2/n. This does not depend on the actual choic e of the querie d points, but only on the numb er of points queried. 3 T Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n−2−1.5 −1−0.5 00.5 11.5 200.10.20.30.40.50.60.70.80.91(1) P (y = 1|x, ˆw) (2) P (y = 1|x, ˆw) \ny = 0 y = 1 y = 0 \nFigur e 1: Two possible logistic regression solutio ns for the three labeled points. \nProblem 3 \nConsider a simple one dimens ional logistic regression model \nP (y =1x, w)= g(w0 + w1x) |\nwhere g(z) = (1+exp(−z))−1 is the logistic functio n. \n1. Figur e 1 shows two possible conditio nal distributio ns P (y =1x, w), viewed as a |\nfunction of x, that we can get by changing the parameters w. \n(a) (2 points) Please indic ate the number of classiﬁcatio n errors for each condi­\ntional given the labeled example s in the same ﬁgure \nConditio nal (1) makes ( 1 ) classiﬁc ation errors \nConditio nal (2) makes ( 1 ) classiﬁc ation errors \n(b) (3 points) One of the conditiona ls in Figure 3 corresp onds to the 1 \nmaximum likeliho od setting of the parameters ˆw based on the labeled \ndata in the ﬁgur e. Whic h one is the ML solution (1 or 2)? \nThe likeliho od under model (2) is 0, because model (2) assigns 0 proba­\nbility to the sample at 1. The likeliho od under model (1) is 2/3·1/3·2/3. \n(c) (2 points) Would adding a regular ization penalt y |w1|2/2 to the log-\nlikeliho od estima tion criterio n aﬀect your choice of solutio n (Y/N)? N \nAt maximum likeliho od ˆw1 = 0 (cf. Conditional (1)), thus | ˆw1|2/2 is \nminimal even without a regular ization penalty. \n4\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "logistic regression", "section_heading": "the maximum variance. The varianc e at x =4 is 16σ2/n. This does not depend on the actual choic e of the querie d points", "source_title": "ca7a3e3f6a26bddd927fab0ea5a2d861 midterm f04soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "27a470b2-777f-413f-9050-5c699a3bdb15", "text": "likeliho od estima tion criterio n aﬀect your choice of solutio n (Y/N)? N At maximum likeliho od ˆw1 = 0 (cf. Conditional (1)), thus | ˆw1|2/2 is minimal even without a regular ization penalty. 4 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n0 50 100 150 200 250 300−1.5−1−0.500.51\nnumber of training examplesexpected log−likelihood of test labelsFigur e 2: The expected log-likeliho od of test labels as a functio n of the number of train ing \nexamples. \n2. (4 points) We can estima te the logistic regression parameters more accurately with \nmore training data. Figur e 2 shows the expected log-lik eliho od of test labels for a \nsimple logistic regression model as a function of the number of training example s and \nlabels. Mark in the ﬁgure the structural error (SE) and appr oximatio n error (AE), \nwhere “error” is measured in terms of log-likelihood. \nSE is the distanc e from the horizontal part of the graph to y =0. AE is everything \nbelow the horizontal part of the graph. \n3. (T/F – 2 points) In general for small training sets, we are likely \nto reduc e the appr oxima tion error by adding a regular ization penalt y \n|w1|2/2 to the log-likeliho od criterion. \nRegularization prevents over-ﬁtting by constr aining the input to the lo­\ngistic function to be a smooth function of input . Such functions can be \nestimate d with fewer samples. Put another way, you are reducing the \nvarianc e of the estimator in favor of introducing some bias. Varianc e \ndominates when we have only few training samples. T\n5\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "regularization", "section_heading": "likeliho od estima tion criterio n aﬀect your choice of solutio n (Y/N)? N At maximum likeliho od ˆw1 = 0 (cf. Condition", "source_title": "ca7a3e3f6a26bddd927fab0ea5a2d861 midterm f04soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "842a0ab4-7737-4861-a7f0-291a21f72a88", "text": "be estimate d with fewer samples. Put another way, you are reducing the varianc e of the estimator in favor of introducing some bias. Varianc e dominates when we have only few training samples. T 5 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nx\n2 \n(0,1) (1,1) o x \n(0,0) (1,0) x o x1 \nFigur e 3: Equa lly likely input conﬁgura tions in the training set \nProblem 4 \nHere we will look at metho ds for selecting input featur es for a logistic regression model \nP (y =1x, w)= g(w0 + w1x1 + w2x2) |\nThe available training examples are very simple, involving only binary valued inputs: \nNum ber of copies x1 x2 y\n10 1 11\n10 0 10\n10 1 00\n10 0 01\nSo, for example, there are 10 copies of x = [1, 1]T in the training set, all labeled y = 1. \nThe correct label is actually a deterministic functio n of the two features: y =1 if x1 = x2 \nand zero otherwise. \nWe deﬁne greedy selection in this context as follows: we start with no featur es (train only \nwith w0) and success ively try to add new featur es provided that each addition strictly \nimpro ves the training log-likeliho od. We use no other stopping criterio n. \n1. (2 points) Could greedy selection add either x1 or x2 in this case? N \nAnsw er Y or N. \nWe have equally many y =1 and y =0 examples under x1 =1, and\nthe same is true for x1 =0. Thus we cannot bias the probabilities in\nany way based on the presenc e of x1 alone. The same is true for x2.\n6 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "be estimate d with fewer samples. Put another way, you are reducing the varianc e of the estimator in favor of introduci", "source_title": "ca7a3e3f6a26bddd927fab0ea5a2d861 midterm f04soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "0afc7594-61fe-45d1-94ff-95808169267c", "text": "examples under x1 =1, and the same is true for x1 =0. Thus we cannot bias the probabilities in any way based on the presenc e of x1 alone. The same is true for x2. 6 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2. (2 points) What is the classiﬁc ation error of the training examples that 0.25 \nwe could achieve by including both x1 and x2 in the logistic regression \nmodel? \n3. (3 points) Supp ose we deﬁne another possible feature to include, a function of x1 \nand x2. Whic h of the following features, if any, would permit us to correctly classify \nall the training example s when used in combinat ion with x1 and x2 in the logistic \nregression model: \n() x1 − x2 \n(x) x1x2 \n() x 2\n2 \nx1 − x2 is a linear combinat ion of existing features, thus adding it does\nnot change the model in any way. The same is true for x2\n2, because\nx22 = x2 for x2 ∈{0, 1}.\nNow suppose we model the conditional by P (y =1x, w)= g(w0 +\n |\nw1x1 + w2x2 + w3x1x2). Let w1 and w2 be such that when w3 =0 only\n(1, 1) is misclassiﬁe d. Since x1x2 =0 except at (1, 1), w3 does not aﬀect\nthe classiﬁc ation of the correctly classiﬁe d training points. We can than\nchoose w3 such that (1, 1) is also correctly classiﬁe d.\n4. (2 points) Could the greedy selection metho d choose this feature as Y \nthe ﬁrst feature to add when the available featur es are x1, x2 and your \nchoice of the new feature? Answ er Y or N. \n7\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "examples under x1 =1, and the same is true for x1 =0. Thus we cannot bias the probabilities in any way based on the pres", "source_title": "ca7a3e3f6a26bddd927fab0ea5a2d861 midterm f04soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "23b26b35-fa3f-4f6b-bd0b-624cdffcb4ff", "text": "the greedy selection metho d choose this feature as Y the ﬁrst feature to add when the available featur es are x1, x2 and your choice of the new feature? Answ er Y or N. 7 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 5\n00.5 11.5 22.5 300.511.522.53\n(0,0)(2,2)(0,3)\n(h,1)\nFigur e 4: Labeled trainin g exampl es \nSupp ose we only have four training examples in two dimensions (see Figure 4): \npositiv e exam ples at x1 = [0, 0]T , x2 = [2, 2]T and \nnega tive examples at x3 = [h, 1]T , x4 = [0, 3]T . \nwhere we treat 0 ≤ h ≤ 3 as a parameter. \n1. (2 points) How large can h ≥ 0 be so that the training points are still \nlinearly separ able? h ≤ 1 \n2. (2 points) Does the orientation of the maxim um margin decision \nbounda ry change as a functio n of h when the points are separ able? \nAnsw er Y or N. N \n8\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3. (4 points)What is the margin achieved by the maximum margin bounda ry as a \nfunction of h? \nThe margin is (1 − h)/√\n2, for 0 ≤ h ≤ 1, and 0 for h> 1.\nOne way to calculate the margin is as follows: since the orientation of the boundar y\ndoes not change as a function h, the margin is a linear function of h. The margin\nis zero at h =1 and 1/√\n2 at h =0.\n4. (3 points) Assume that h =1/2 (as in the ﬁgur e) and that we can \nonly observ e the x2-componen t of the input vector s. Witho ut the other \ncomp onen t, the labeled training points reduce to (0,y = 1), (2,y = 1), \n(1,y = −1), and (3,y = −1). What is the lowest order p of polyno mial \nkernel that would allow us to correctly classif y these points? \n9\n3 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "the greedy selection metho d choose this feature as Y the ﬁrst feature to add when the available featur es are x1, x2 an", "source_title": "ca7a3e3f6a26bddd927fab0ea5a2d861 midterm f04soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "5428783e-76ff-4549-96c9-adf8541d04df", "text": "to (0,y = 1), (2,y = 1), (1,y = −1), and (3,y = −1). What is the lowest order p of polyno mial kernel that would allow us to correctly classif y these points? 9 3 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nAddi tional set of ﬁgures\n−1 0 1−101\n−1 0 1−101\n−1 0 1−101\nA B C\n1 2 3 40246810\nxy\n−2−1.5 −1−0.5 00.5 11.5 200.10.20.30.40.50.60.70.80.91\n(1) P (y = 1|x, ˆw) (2) P (y = 1|x, ˆw) \ny = 0 y = 1 y = 0 \n10\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n0 50 100 150 200 250 300−1.5−1−0.500.51\nnumber of training examplesexpected log−likelihood of test labels\nx1 x2 \nx \nx (1,1) (0,1) \n(1,0) (0,0) o \no \n00.5 11.5 22.5 300.511.522.53\n(0,0)(2,2)(0,3)\n(h,1)\n11\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "kernel", "section_heading": "to (0,y = 1), (2,y = 1), (1,y = −1), and (3,y = −1). What is the lowest order p of polyno mial kernel that would allow u", "source_title": "ca7a3e3f6a26bddd927fab0ea5a2d861 midterm f04soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 7}
{"id": "d3959b49-ac0a-4b2a-acf2-b6c6f394428b", "text": "� \n� Massa chusetts Institu te of Techn ology \n6.867 Machine Lear ning , Fall 2006 \nProbl em Set 5 \nDue Date: Thursda y, Nov 30, 12:00 noon\nYou may submit your solutions in class or in the box. \n1. Wilhelm and Klaus are friends, but Wilhelm only speaks English and Klau s only speaks German. Klaus \nhas sent Wilh elm an urgent message, and Wilh elm is locked in a room with only the proceedings of \nthe European Parliament (in both German and English) to keep him company. Luckily, Wilh elm is a \nmachine learning expert. \nHe decides to treat Klau s as a “noisy channel”: assume that thoughts in Klau s’s head start out in \nEnglish, but before they are emitted, they pass through a medium that replaces the English words with \nnew (German) words and rearranges them from the English word-orderin g to a new (German) orderin g \nall accordi ng to some unkown distributions. Next, he plans to come up with parame trizations for the \ndistributions and then to optimize the parameters over the European Parliame nt data he is convenien tly \nlocked up with. \nWilhelm thus achieves the following optimization problem: \nP (e)argmaxP (e | g) = arg maxP (g | e)P (g) e e \n= arg maxP (ge)P (e), e | \nwhere e and g denot e English and German sentence s, respectively. This corresponds to Wilh elm’s \nnoisy-channel characte rization of Klaus’s head. \nNext, Wilh elm decides on parameterization s for the two terms in his optimiz ation criterion. To estimate \nprobab ilities of English sentence s, he will pretend that English is a Markov process (that is, that English \nspeakers choose each word in a sentence based only on the last two words they have chosen): \n|e|\nP (e)= LM(ei | ei−1, ei−2), (1) \ni=1 \nwhere |e| denotes the length of the English sentence e, ei denot es the ith word of e, and e0 and \ne−1 corres pond to start symbols S0 and S−1, respectively, which are before every English sentence for \nsimplicity. \nFor the other term, Wilh elm is aware that both the words and their order may change, so he introduces \na hidden variab le: the align ment between the words. \n|g|\nP (g,a | e)= \nj=1 T (gj | eaj )D (aj | j, |e|, |g|) (2) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "� ", "source_title": "4a4488557e4ecc288639a79dfbf2e3ff hw5", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "955e247e-c1c0-4059-8bbb-0c561fbfc84c", "text": "words and their order may change, so he introduces a hidden variab le: the align ment between the words. |g| P (g,a | e)= j=1 T (gj | eaj )D (aj | j, |e|, |g|) (2) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� � \n� � \n| ||| � � � � Thus, \nP (g | e)= P (g,a | e) \na \n|g|\n= T (gj | eaj )D (aj | j, |e|, |g|) \na j=1 \n|g| |e|\n= T (gj | eaj )D (aj | j, |e|, |g|) . \nj=1 aj =1 \nThis set-up is known as IBM Model 2. \n(a) To perform the optimization, Wilhelm decides to use the EM algori thm. Derive the updates of the \nparameters LM, T , and D. We place no constraints on these distrib utions other than that they \nare distrib utions (cf. regul arization below). \n(b) The initial parame ter settin gs are importan t. Give an example of a bad choice. What should you \nchoose? \n(c) Insert the updates and initial parame ter settings in the provided templates ibm2 train lm.m and \nibm2 train.m and train the syste m using the provided data europarl.m. Use the provided decoder \nibm2 beam decoder.m to use the trained system to decode Klau s’s message klaus.m. The system \nis able to achieve surprisingl y good results with a very simple probabi lity model! (Alth ough the \ntraining is easy, the decoding is non-trivi al; check out the source of the decoder.) \n(d) Suppose Wilhelm believes that his model is reordering the German too aggre ssively. He is con­\nsiderin g regul arizing the alignme nts. He has several ideas; what eﬀect do you think each of these \nregul arization techniques will have? \ni. Regularize D (aj j, e|, g) toward a uniform distrib ution. \nii. Regularize D (aj | j, |e|, |g|) toward the distrib ution Daj = j · ||\nge|\n| | j, |e|, |g| =1. \niii. Estimate ﬁrst a simpler distrib ution D (aj ||e|, |g|) and regularize D (aj | j, |e|, |g|) toward it. \n2. In this problem, we will explore the combined use of regres sion and Expectation Maximization. The \nmotiv ating application is as follows: we’d like to understan d how the expression levels of genes (in a \ncell) change with time. The expression level of a gene indicates, rough ly, the amoun t of gene-pro duct in \nthe cell. The experime ntal data we have available are noisy measurements of this level (for each gene) \nat certain time points. More precisely, we have m genes (g1,...,gm) and meas urements are availabl e \nfor r timep oints (t1,...,tr). The data matrix is Y =[y1,..., ym]T where each row yiT =[yi1,...,yir] \ncorres ponds to the expression levels of gene gi acros s the r timep oints. \nOur goal is to estimate continuous functions, each fˆi(t) capturing the expression level of one gene gi. \nWe’ll call these expression curves. Clearly , if we treat each gene’s expression as indep endent from others, \nwe would need to perform m unrelated regres sion tasks. We can do better, however, since we expect \nthat there are groups of genes with similar expression curves. \nWe hypothesize that the expression curves can be grouped into k classes and that, within each group , \nthe functions fi(t) look very similar. When estimating fˆi(t) we can pool together data from all the \ngenes in the same class. We have two problems to solve: (1) ﬁgure out which class each gene belongs \nto, and (2) given class assignmen ts, estimate the expression curves. As you migh t guess, we will use the \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "words and their order may change, so he introduces a hidden variab le: the align ment between the words. |g| P (g,a | e)", "source_title": "4a4488557e4ecc288639a79dfbf2e3ff hw5", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "c70a9ba5-dbdf-49ad-9798-a59739297e66", "text": "same class. We have two problems to solve: (1) ﬁgure out which class each gene belongs to, and (2) given class assignmen ts, estimate the expression curves. As you migh t guess, we will use the Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� EM algor ithm. However, we still need to understand how to estimate continuous curves from discrete \ndata-points. \nTo perform regres sion, we will use an idea similar to kernel regre ssion. Recall that even if f(t) is some \nnon-linear function of t, it is possible to ﬁnd a feature mapp ing φ(t)=[φ1(t),...,φp(t)]T such that f(t) \nis a linear function of φ’s: f(t)= wj φj (t). In class, we had discussed how to implem ent this idea to \nperform kernel regres sion without explicitly constru cting the feature mapping. In the current setting, \nwe extend this to include a prior over the parame ters w =[w1,...,wp]T . We set this to be simpl y a \nspheri cal multivariate Gaussian N(w; 0,σw2 I). Now, given any set of time points t1,...,tr, we imagine \ngeneratin g function values at those time points accordi ng to the following procedure \nStep 1: draw one sample w ∼ N(0,σ2 I)w\nStep 2: generate function values for the time points of interes t \np\nf(tk)= wj φj (tk),k =1,...,r (3) \nj=1 \nSince wj are normally distributed, so is f =[f(t1),...,f(tr)]T . The distri bution of this vector is \nN(f; 0, K) where Kkl = K(tk,tl)= σ2 φ(tk)T φ(tl). Looks familiar ? Let’s try to understand it a bit w\nbetter. We have a distribution over function values at any set of time points. In other words, we \nhave deﬁn ed a prior distrib ution over functions! The kernel function K(t,t�), where the inputs are the \ntime points, gives rise to a Gram matri x Kkl = K(tk,tl) over any ﬁnite set of inputs. This probab ility \ndistribution over functions is known as a Gaussian Process (GP for short). \nThe choice of the kernel K(t,t�) controls the kind of functions that are sampled. Consider the following \nkernel, similar to the radial basis function kernel seen before \nσ2 (t − t�)2 \nK(t,t�)= f exp(−2ρ2 ) (4) \nThe hyperparame ter ρ controls the time span at which the function values are stron gly coupled. A large \nρ implies, for example, that the sampled functions would appear roughly constant across a long time \nrange. σf scales this correlation by a constan t factor. \n(a) Matc h each of the following four ﬁgures, with the four kernels deﬁned below. Each ﬁgure describes \nsome function s sampled using a particular kernel. Note that these functions are drawn by selecting \nﬁnely space d time points and drawing a sampl e from the corresponding function values at these \npoints from N(f; 0, K) where K is the Gram matrix over the same points. \nHere are the kernels: \ni. 3 exp(−(t−t�)2 )2(2.1)2 \n(t−t�)2 ii. 2 exp(−2(0.7)2 ) \n4iii. 2 exp(−sin2( π(t−t�)2 \n) )2(0.3)2 \n(t−t�)2iv. 2 exp(−2(0.05)2 )+0.1(tt�)2 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "same class. We have two problems to solve: (1) ﬁgure out which class each gene belongs to, and (2) given class assignmen", "source_title": "4a4488557e4ecc288639a79dfbf2e3ff hw5", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "1aaebac1-8e03-48d3-8acb-fdec77c562a8", "text": "N(f; 0, K) where K is the Gram matrix over the same points. Here are the kernels: i. 3 exp(−(t−t�)2 )2(2.1)2 (t−t�)2 ii. 2 exp(−2(0.7)2 ) 4iii. 2 exp(−sin2( π(t−t�)2 ) )2(0.3)2 (t−t�)2iv. 2 exp(−2(0.05)2 )+0.1(tt�)2 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \nFigure 1: Each subﬁ gure corresponds to one kernel \nLet us now consider how we will use GP’s to simultaneously perform regression across genes belongin g \nto the same class. Since we assume they have similar expression curves we will model them as sampl es \nfrom the same Gauss ian process. Our model for the data is as follows. There are k classes in the data \nand each gene belongs to a single class. Let the genes in class l be gl1,gl2,...,glml where ml is the \nnumber of genes in that class. The expression curves for these genes are assumed to be samples from \nthe same GP. The kernels of the k GP’s (one for each class) all have the same parametric form, but \ndiﬀerent hyperparamete rs θ =(σf ,ρ,σn): \nK(t,t�)= σ2 exp(−(t − t�)2 \n)+ σ2 δ(t,t�), where (5)f 2ρ2 n\n1 if t = t�\nδ(t,t�) = (6)0 otherwise \nThis is essentially the same as before except for the second term that accounts for meas urement noise . \nIn other words, the responses y are modeled as y(t)= f(t)+ �t, �t ∼ N(0,σ2 ) and f(t) is a Gaussian n\nprocess with kernel given by Egn 4. Thus, for any time points t1,...,tr, the response vector y has a \ndistribution N(0, K), where K is the Gram matrix from the kernel in Eqn 5. \n(b) We have provided for you a function log likelihood gp(params, t, Yobs), where params specify \nthe hyperparame ters of the kernel, t is a vector of r timep oints and Yobs is a q×r matrix of observed \ngene expression values (for q genes ) at those timepoints. For each of the q sets of observations, \nthe function computes the log-lik elihood that observation s were generated from the GP speciﬁed \nby params. Fill in the missing lines of the code. Attach a printout of your code (you only need to \nshow the parts you changed). \nWe now go back to our original problem and set up an Expectation Maximization framework to solve it. \nSuppose we know that the genes in the data cluster into k classes (i.e., k is a user-speciﬁed parameter). \nWe start oﬀ by guessing some initial values for the hyperparame ters of the k GPs. In each E-step, we \ncompute the posterior assignmen t probabi lity P (k|i) i.e., the probab ility that gene gi is in class k. To \nsimplify the M-step we will turn these probabi lities into hard assignments. In each M-ste p, we compute \nthe MLE estimates for the hyperparame ters of the GPs associated with the classes. \n(c) To perform EM, we have provided you a function [W,V,L] = em gp(t, Yobs, k, init class). \nThe code is missing some steps in the Expectation sub-f unction . Fill them in. Attach a printout \nof your code for the Expectation function . \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "N(f; 0, K) where K is the Gram matrix over the same points. Here are the kernels: i. 3 exp(−(t−t�)2 )2(2.1)2 (t−t�)2 ii.", "source_title": "4a4488557e4ecc288639a79dfbf2e3ff hw5", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "04f84b20-ff73-475f-a32c-ff6e59893426", "text": "a function [W,V,L] = em gp(t, Yobs, k, init class). The code is missing some steps in the Expectation sub-f unction . Fill them in. Attach a printout of your code for the Expectation function . Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n(d) Using the datase t provided with this problem, run em gp with diﬀerent choices of k (k =2, 3, 4, 5). \nWe have provided a function plot results(W,V,t,Yobs,k,nn) which you may ﬁnd useful. You \ncan use it to plot nn randomly chosen curves from each of the k classes. What choice of k seems \nmost appropriate? \n(e) Gene expression data often has missing values, i.e., you may not know the value yij , the expression \nof gene gi at time tj . With some machine learn ing methods, handling these missing values can be \na big hassle . But with our model, the missing values problem can be hand led very simply. How? \n(f) (Opti onal: Extra Credit) Our initial choice of hyperparame ters is constru cted as follows: we \nassign genes to speciﬁc classes, either randomly or based upon some insigh ts. Using this class \nassignment, the initial hyperpar ameters are compu ted by an MLE approach. Describe a method \nthat could be used to generate a good initial class assignment. \n(g) (Opti onal: Extra Credit) Since we have deﬁn ed a prior over function s we could turn the clus­\nterin g problem into a Bayesian model selection problem. We assume that each cluster has a single \nunderly ing expression curve f(t), sampled from a ﬁxed GP model, and the responses for each gene \nin the cluste r are subsequently sampled from y(t)= f(t)+ �t, �t ∼ N(0,σ2 ), using the same f(t).n\nWrite an expression for the margin al likelihood of the data corresponding to a ﬁxed assignment of \ngenes into cluste rs. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "margin", "section_heading": "a function [W,V,L] = em gp(t, Yobs, k, init class). The code is missing some steps in the Expectation sub-f unction . Fi", "source_title": "4a4488557e4ecc288639a79dfbf2e3ff hw5", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "fa56a9a1-9727-431f-bf69-1733ece18ac5", "text": "� 1 6.867 Mac hine learning , lectur e 18 (Jaak kola) \nLecture topics: \n• Spectral clustering, rando m walks and Markov chains \nSpectral clustering \nSpectral clustering refers to a class of clustering metho ds that approximate the problem \nof partitioning nodes in a weighted graph as eigen value problems. The weighted graph \nrepresen ts a simila rity matrix between the objects associated with the nodes in the graph. \nA large positiv e weight connecting any two nodes (high similar ity) biases the clusteri ng \nalgorithm to place the nodes in the same cluster. The graph repres entation is relationa l in \nthe sense that it only holds infor matio n about the compar ison of objects associated with \nthe nodes. \nGraph construction \nA relati onal representation can be advantageous even in cases where a vector space repre­\nsentation is readily available. Consider, for example, the set of points in Figure 1a. There \nappears to be two clusters but neither cluster is well-captur ed by a small number of spheri­\ncal Gaussians. By connecting each point to their two nearest neighbors (two closes t points) \nyields a graph in Figure 1b that places the two clusters in diﬀeren t connected components. \nWhile typically the weighted graph representation would have edges spanning across the \nclusters, the example nevertheless highligh ts the fact that the relational repres entation can \npotentially be used to identify clusters whose form would make them otherwise diﬃcult \nto ﬁnd. This is particularly the case when the points lie on a lower dimens ional surface \n(manifold). The weighted graph represe ntation can essentially perfor m the clustering along \nthe surface rather than in the enclosing space. \nHow exactly do we construct the weighted graph? The problem is analo gous to the choice \nof dista nce functio n for hiera rchical clustering and there are many possible ways to do this. \nA typical way, alluded to above, starts with a k−nearest neigh bor graph, i.e., we construct \nan undirec ted graph over the n points such that i and j are connected if either i is among \nthe k nearest neigh bors of j or vice versa (near est neighbor relations are not symmetric). \nGiven the graph, we can then set \n= exp(−β�xi − xj �) if i and j connected (1) Wij 0, otherwise \nThe resulting weights (similarit ies) are symmetric in the sense that Wij = Wji. All the \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "� 1 6.867 Mac hine learning , lectur e 18 (Jaak kola) ", "source_title": "0871ac04afeb2df4e9b69738a6d0d38b lec18", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "35ef0567-3184-41d0-9b48-03410eb08906", "text": "the graph, we can then set = exp(−β�xi − xj �) if i and j connected (1) Wij 0, otherwise The resulting weights (similarit ies) are symmetric in the sense that Wij = Wji. All the Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2 6.867 Mac hine learning , lectur e 18 (Jaak kola) \na) \n−6−4−2024681012−6−4−20246 b) \n−6−4−2024681012−6−4−20246\nFigur e 1: a) a set of points and b) the corresp onding 2-nearest neigh bor graph. \ndiagonal entries are set to zero: Wii = 0 for i =1,...,n. The n × n matrix W now \nrepresen ts the weighted graph. \nThere are two parameters to set: k and β. The choice of k is tied to the dimensiona lity \nof the clusters we are trying to ﬁnd. For example, if we believe that the clusters look like \nd−dimensional surfaces , then k should be at least d. A small k leads to a sparse graph and \nserves to limit the comparisons between points to those that are close. This is advantageous \nsince the Euclidean distance is unlik ely to be reasona ble for points far away. For example, \nconsider points on the surfa ce of a unit sphere, and a context where their distance really \nshould be measured along the surface. The simple Euclidean dista nce nevertheles s provides \na reaso nable approximatio n for points that are close on the surfa ce. β serves a similar role \nbut, unlik e k, is tied to the actual scale of the points (their dista nces). \nGraph parti tioning and criteria \nLet’s now deﬁne the clustering problem more formally . Supp ose we have n objects to \nbe clustered into two groups (binary parti tion). A multi-w ay partition can be obtained \nthrough a recursive applicat ion of binary partition ing. The objects are represented by a \nweighted graph with symmetric positiv e weights Wij = Wji ≥ 0, Wij is zero when no edge \nis presen t between i and j, and Wii = 0. The goal is to use the weighted graph as a \nsimilarit y measure to partition the nodes into two disjoin t groups C+ and C− such that \nC+ ∪ C− = {1,...,n}. Any such partitio n corresp onds to a labeling of the nodes in the \ngraph with binar y labels yi ∈ {−1, 1} such that, e.g., yi = 1 when i ∈ C+ . The clusters can \nbe therefore equivalently speciﬁe d by the sets (C+,C−) or the labeling y (a binar y vector \nof length n). \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "the graph, we can then set = exp(−β�xi − xj �) if i and j connected (1) Wij 0, otherwise The resulting weights (similari", "source_title": "0871ac04afeb2df4e9b69738a6d0d38b lec18", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "c8760952-85cb-4489-8a21-c3e274cd143a", "text": "{−1, 1} such that, e.g., yi = 1 when i ∈ C+ . The clusters can be therefore equivalently speciﬁe d by the sets (C+,C−) or the labeling y (a binar y vector of length n). Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� 3 6.867 Mac hine learning , lectur e 18 (Jaak kola) \nIt remains to specify an objectiv e function for ﬁnding the clusters. Each binary cluster ing \nis associated with a cut in the graph. The weight of the cut is given by \n� 1 � \ns(C+,C−)= Wij = Wij (yi − yj )2 = J(y) (2) 4 \ni∈C+,j∈C− i,j \nwhere (yi − yj )2 = 4 when yi and yj diﬀer and zero otherwise. The cut simply corresponds \nto adding the weights of the edges conn ecting nodes that are in diﬀerent clusters (labeled \ndiﬀeren tly). The value of the cut is obviously zero if all the nodes are labeled the same. If we \nrequire both labels to be presen t we arrive at a minimum cut criterio n. It is actua lly eﬃcient \nto ﬁnd the labeling or, equiv alently, sets C+ and C− that minimiz e the value of the cut \nunder this constra int. The approach does not work well as a clustering algorithm, however, \nas it tends to simply identify outliers as clusters (individual points weakly connected to \nothers). We will have to modify the objectiv e to ﬁnd more balanced clusters. \nA better criter ion is given by so called normalize d cut (see Shi and Malik 2000): \nNorm-cut(C+,C−)= s(C+,C−)+ s(C+,C−) (3) s(C+,C+)+ s(C+,C−) s(C−,C−)+ s(C+,C−) \nwhere, e.g., s(C+,C+)= i∈C+,j∈C+ Wij , the sum of weights between nodes in cluster C+ . \nEach term in the criterio n is a ratio of the weight of the cut to the total weight associated \nwith the nodes in the cluster. In other words, it is the fraction of weight tied to the cut. \nThis norma lization clear ly prohib its us from separating outliers from other nodes. For \nexample, an outlier connected to only one other node with a small weight cannot form \na single cluster as the fraction of weight associated with the cut would be 1, the highest \npossible value of the ratio. So we can expect the criterion to yield more balanced parti tions. \nUnfort unately , we can no longer ﬁnd the solutio n eﬃcien tly (it is an integer programmin g \nproblem). An approximat e solutio n can be found by relaxing the optimizat ion problem \ninto an eigenvalue problem. \nSpectral clustering, the eigenvalue problem \nWe begin by extending the “labeling ” over the reals zi We will still interpret the ∈R. \nsign of the real number zi as the cluster label. This is a relax ation of the binary labeling \nproblem but one that we need in order to arriv e at an eigen value problem. First, let’s \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "{−1, 1} such that, e.g., yi = 1 when i ∈ C+ . The clusters can be therefore equivalently speciﬁe d by the sets (C+,C−) o", "source_title": "0871ac04afeb2df4e9b69738a6d0d38b lec18", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "56a99b70-f299-4566-bf8b-914112d4f3ce", "text": "of the real number zi as the cluster label. This is a relax ation of the binary labeling problem but one that we need in order to arriv e at an eigen value problem. First, let’s Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� 4 6.867 Mac hine learning , lectur e 18 (Jaak kola) \nrewrite the cut as \nJ(z) = 1 \n4 � \nWij (zi − zj )2 = 1 4 � \nW\nij(z 2 \ni − 2zizj + z 2 \nj ) = 1 4 � \nW\nij (2z 2 \ni − 2zizj ) (4) \ni,j i,j i,j \n�� Dii�� �� \n= 1 \n2 � � \nWij z 2 \ni + 1 2 � \nW\nij zizj = 1 2 z \nT (D − W )z (5) \ni j i,j \nwhere we have used the symme try of the weights. D is a diagonal matr ix with elemen ts \nDii = j Wij . The matrix L = D − W is known as the graph Laplacian and is guaranteed \nto be positiv e semi-de ﬁnite (all the eigen values are non-nega tive). The smallest eigen value \nof the Laplacian is always exactly zero and corresp onds to a constan t eigenvector z = 1. \nWe will also have to take into account the normalizat ion terms in the normalized cut \nobjective. A complete deriv ation is a bit lengthy (describ ed in the Shi and Malik paper \navailable on the website). So we will just motivate here how to get to the relax ed version \nof the problem. Now, the norma lized cut objective tries to balance the overall weight \nassociated with the nodes in the two clusters, i.e., s(C+,C+)+ s(C+,C−) ≈ s(C−,C−)+ \ns(C+,C−). In terms of the labels, the conditio n for exactly balancing the weights would be \nyT D1 = 0. We will instea d use a relax ed criterio n zT D1 = 0. Moreo ver, now that zi’s are \nreal numbers and not binary labels, we will have to norma lize them so as to avoid zi → 0. \nWe can do this by requiring that zT Dz = 1.�As a result, small changes in zi for nodes \nthat are strongly connected to others (Dii = j Wij is large) requir e larger comp ensating \nchang es at nodes that are only weakly coupled to others. This helps ensure that isolated \nnodes become “follo wers”. \nThe resulting relaxed optimizat ion problem is given by \n1 minimize z T (D − W )z subject to z T Dz =1,z T D1 = 0 (6) 2 \nThe solutio n can be found easily via Lagrange multipliers and reduces to ﬁnding the eigen­\nvector z2 (comp onents z2i) corresp onding to the second smallest eigenvalue from \n(D − W )z = λDz or, equiv alently, (I − D−1W )z = λz (7) \nThe eigenvector with the smallest (λ = 0) eigen value is always the consta nt vector z = 1. \nThis would not satisfy zT D1 = 0 but the second smallest eigen vector does. Note that \nsince the goal is to minimize zT (D − W )z we are interested in only the eigenvectors with \nsmall eigen values. The clusters can now be found by labeling the nodes according to \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "of the real number zi as the cluster label. This is a relax ation of the binary labeling problem but one that we need in", "source_title": "0871ac04afeb2df4e9b69738a6d0d38b lec18", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "a5229162-0b83-41a8-827c-e289ff63bb53", "text": "Note that since the goal is to minimize zT (D − W )z we are interested in only the eigenvectors with small eigen values. The clusters can now be found by labeling the nodes according to Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� 5 6.867 Mac hine learning , lectur e 18 (Jaak kola) \nyˆi = sign(z2i). If we wish to further balance the number of nodes in each cluster, we could \nsort the comp onen ts of z2 in the ascending order and label nodes as negative in this order. \nFigur e 2 illustra tes a possible solution and the corresp onding values of the eigen vector. \na) \n−4 −2 0 2 4 6−2−10123456 b) \n0510152025303540−0.5−0.4−0.3−0.2−0.100.10.20.30.40.5\nFigur e 2: a) spectral clustering solut ion and b) the values of the second largest eigen vector. \nSpectral clustering, rando m walk \nThe relaxed optimizatio n problem is an approximat e solutio n to the normalized cut prob­\nlem. It is theref ore not immediat ely clear that this approximate solut ion behaves appropri­\nately. We can try t o justify it from a very diﬀeren t perspective, that of ra ndom w alks on the \nweighted graph. To this end, note that the eigen vectors we get by solving (I−D−1W )z = λz \nare exactly the same as those obtained from D−1Wz = λ�z. The resulting eigenvalues are \nalso in one to one corresp ondence : λ� =1 − λ. Thus the constan t eigen vector z = 1 with \nλ = 0 should have λ� = 1 and satisfy D−1Wz = z. Let’s understa nd this further. Deﬁne \nWij WijPij = � = (8) \nj� Wij� Dii \nso that P = D−1W . Clearly , j Pij = 1 for all i so that P 1 = 1. We can therefor e \ninterpr et P as a transition probability matr ix associated with the nodes in the weighted \ngraph. In other words, Pij deﬁnes a random walk where we hop from node i to node j with \nprobabilit y Pij . If X(t) denotes the node we happen to be at time t, then \nP (X(t +1) = j|X(t)= i)= Pij (9) \nOur rando m walk corresponds to a homo gene ous Markov chain since the transition proba­\nbilities remain the same e very time w e come back to a node (i.e., the tra nsition pro babilities \nare not time dependen t). Markov chains are typica lly deﬁned in terms of states and tran­\nsitions between them. The states in our case are the nodes in the graph. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "Note that since the goal is to minimize zT (D − W )z we are interested in only the eigenvectors with small eigen values.", "source_title": "0871ac04afeb2df4e9b69738a6d0d38b lec18", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "97668efc-3070-4f02-a243-794fe4a7d130", "text": "the tra nsition pro babilities are not time dependen t). Markov chains are typica lly deﬁned in terms of states and tran­ sitions between them. The states in our case are the nodes in the graph. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n6 6.867 Mac hine learning , lectur e 18 (Jaak kola) \nIn a Mark ov chain two states i and j are said to be communic ating if you can get from i to j \nand from j to i with ﬁnite probabilit y. If all the pairs of states (nodes) are comm unicating, \nthen the Mark ov chain is irreducible. Note that a rando m walk deﬁned on the basis of \nthe graph in Figure 1b would not be irreducible since the nodes across the two connected \ncomp onen ts are not communicating . \nIt is often useful to write a transition diagr am that speciﬁes all the permissible one-step \ntransitions i j, those corresponding to Pij > 0. This is usually a directed graph. →\nHowever, in our case, because the weights are symmetric, if you can directly transition \nfrom i to j then you can also go directly from j to i. The transition diagram therefor e \nreduces to the undirected graph (or directed graph where each undirected edge is directed \nboth ways). Note that the transitio n probabilit ies themselv es are not symmetric as the \nnormalizat ion terms Dii vary from node to node. On the other hand, the zeros (prohibited \none-step transitio ns) do appear in symmetric places in the matrix Pij . \nWe need to understa nd one additiona l property of (some) Mark ov chains – ergodicity. To \nthis end, let us consider one-s tep, two-step, and m−step transitio n probabilit ies: \nP (X(t +1) = j|X(t)= i)= � (10) Pij \nP (X(t +2) = j|X(t)= i)= PikPkj =[PP ]ij =[P 2]ij (11) \nk \n(12) ··· \nP (X(t + m)= j|X(t)= i)=[P m]ij (13) \nwhere [P m]ij is the i,j eleme nt of the matrix PP P (m multiplica tions). A Mark ov ··· \nchain is ergodic if there is a ﬁnite m such that for this m (and all larger values of m) \nP (X(t + m)= j|X(t)= i) > 0, for all i and j (14) \nIn other words, we have to be able to get form any state to any other state with ﬁnite \nprobabilit y after m transitions. Note that this has to hold for the same m. For example, a \nMark ov chain with three states and possible transitions 1 2 3 1 is not ergodic even →→→\nthough we can get from any state to any other state. However, for this Markov chain, any \nm step transitio n probabilit y matrix would still have prohibited transitions. For example, \nstarting from 1, after three steps we can only be back in 1. \nNow, what will happen if we let m →∞, i.e., follow the rando m walk for a long time? If \nthe Markov chain is ergodic then \nlim P (X(t + m)= j|X(t)= i)= πj (15) \nm→∞ \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "the tra nsition pro babilities are not time dependen t). Markov chains are typica lly deﬁned in terms of states and tran", "source_title": "0871ac04afeb2df4e9b69738a6d0d38b lec18", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "85e17f8a-c4dd-4df9-af44-650ca70af812", "text": "Now, what will happen if we let m →∞, i.e., follow the rando m walk for a long time? If the Markov chain is ergodic then lim P (X(t + m)= j|X(t)= i)= πj (15) m→∞ Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � 7 6.867 Mac hine learning , lectur e 18 (Jaak kola) \nfor some stationary distr ibution π. Note that πj does not depend on i at all. In other \nwords, the random walk will forget where it started from. Ergo dic Markov chains ensure \nthat there’s enough “mixing ” so that the infor matio n about the initial state is lost. In our \ncase, roughly speaking , any connected graph gives rise to an ergodic Mark ov chain. \nBack to clustering . The fact that a rando m walk on the graph forgets where it started \nfrom is very useful to us in terms of identifying clusters. Consider, for example, two tightly \nconnected clusters that are only weakly coupled across. The random walk started at a node \nin one of the clusters quickly forgets which state within the cluster it begun. However, the \ninforma tion about which cluster the sta rting node was in lingers much longer. It is precisely \nthis lingering infor matio n about clusters in rando m walks that helps us identify them. This \nis also something we can understa nd based on eigen values and eigenvector s. \nSo, let’s try to identify clusters by seeing what informati on we have about the rando m walk \nafter a large number of steps. To make our analysis a bit easier, we will rewrite \nP m = D−1/2(D−1/2WD−1/2)mD1/2 (16) \nYou can easily verify this for m =1, 2. The symme tric matrix D−1/2WD−1/2 can be written \nin terms of its eigen values λ�\n1 ≥ λ�\n2 ≥ ... and eigen vectors ˜z1,z˜2,... \n(D−1/2WD−1/2)m =(λ�\n1)m z˜1z˜1 T +(λ�\n2)m z˜2z˜2 T + ... +(λn�)m z˜nz˜nT (17) \nThe eigen values are the same as those of P and any eigen vector ˜z of D−1/2WD−1/2 corre­\nsponds to an eigenvector z2 = D−1/2z˜2 of P . As m →∞, clearly \nP ∞ = D−1/2 z˜1z˜1 T D1/2 (18) \nsince λ�\n1 = 1 as before and λ�\n2 < 1 (ergo dicity). The goal is to unde rstand which transitions \nremain strong even for large m. These should be transitions within clusters. So, since the \neigen values are ordered, for large m \nP m ≈ D−1/2 z˜1z˜1 T +(λ�\n2)m z˜2z˜2 T D1/2 (19) \nwhere ˜z2 is the eigenvector with the second largest eigenvalue. Note that its comp onents \nhave the same signs as the comp onents of z2, the second largest eigen vector of P . Let’s \nlook at the “correction term” (˜z2z˜2 T )ij = z˜2iz˜2j . In other words, we get lingering strong er \ntransitions between i and j corresponding to nodes where ˜z2i and ˜z2j have the same sign, \nand decreased transitio n across. Thes e are the clusters and indeed obtained by reading the \ncluster assignmen ts from the signs of the components of the relev ant eigen vector. \n Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "Now, what will happen if we let m →∞, i.e., follow the rando m walk for a long time? If the Markov chain is ergodic then", "source_title": "0871ac04afeb2df4e9b69738a6d0d38b lec18", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "06f4927d-a21d-4001-869a-7c914df1d699", "text": "6.867 Machine learning \nMid-t erm exam \nOcto ber 13, 2006 \n(2 points) Your name and MIT ID: \nProblem 1 \nSupp ose we are trying to solve an activ e learning problem, where the possible inputs you \ncan select form a discrete set. Speciﬁca lly, we have a set of N unlab eled documen ts, \nΦ1,..., ΦN , where each documen t is represen ted as a binary feature fector \nΦ=[φ1,...,φm]T \nand φi = 1 if word i appears in the docume nt and zero otherwise. Our goal is to quickly label \nthese N documen ts with 0/1 labels. We can request a label for any of the N documen ts, \npreferably as few as possible . We also have a small set of n already labeled documen ts to \nget us started. \nWe use a logistic regression model to solve the classiﬁcatio n task: \nP (y =1|Φ, w)= g( w T Φ) \nwhere g() is the logistic function. Note that we do not include the bias term . ·\n1. (T/F – 2 points) Any word that appears in all the N documen ts \nwould eﬀectively provide a bias term for the logistic regression model. \n2. (T/F – 2 points) Any word that appears only in the available n la­\nbeled documen ts used for initia lly training the logistic regression model, \nwould serve equally well as a bias term. \n1 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "6.867 Machine learning ", "source_title": "e422ff477d12ba1bf2544e6293535c51 midterm f03", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "53a5e016-e2fc-42e6-b251-0fcbd27a08fa", "text": "2. (T/F – 2 points) Any word that appears only in the available n la­ beled documen ts used for initia lly training the logistic regression model, would serve equally well as a bias term. 1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3. Having trained the logistic regression model on the basis of the n labeled documen ts, \nobtaining wˆn, we’d like to request additiona l labeled documen ts. For this, we will \nuse the following measure of uncerta inty in our predictions: \nEy∼pt |y − pt| = pt|1 − pt| + (1 − pt)|0 − pt| =2pt(1 − pt) \nwhere pt = P (y =1|Φt, wˆn), our current predic tion of the proba bility that y = 1 for \nthe tth unlab eled docume nt Φt. \na) (4 points) We would request the label for the docume nt/query point Φt that \nhas \n( ) the smallest value of 2pt(1 − pt) \n( ) the largest value of 2pt(1 − pt) \n( ) an intermedia te value of 2pt(1 − pt) \nBrieﬂy expla in the rationa le behind the selection criterion that you chose. \nb) (2 points) Sketch wˆn in Figur e 1.1. Write down the equatio n, express ed solely \nin terms of Φ and wˆn, that Φ has to satisfy for it to lie exactly on the decision \nbounda ry: \n2\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "2. (T/F – 2 points) Any word that appears only in the available n la­ beled documen ts used for initia lly training the ", "source_title": "e422ff477d12ba1bf2544e6293535c51 midterm f03", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "68b31155-f560-4600-a4b6-150cf8328072", "text": "Sketch wˆn in Figur e 1.1. Write down the equatio n, express ed solely in terms of Φ and wˆn, that Φ has to satisfy for it to lie exactly on the decision bounda ry: 2 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nc) (4 points) In ﬁgure 1.2, circle the next point we would select according to the \ncriterio n. Draw two decision boundari es that would result from incor porating \nthe new point in the training set, labeling the boundaries as y = 1 and y = 0, \ndepending on the outcome of the query . \n+ o Φt =[φt1,...,φtm]T \n+ o \nFigur e 1.1. Two labeled points, unlabeled Figur e 1.2. Two labeled points, unlabeled \npoints, and the decis ion boundar y. The points, and the decis ion boundar y. The \npoint “+” corresponds to y = 1. point “+” corresponds to y = 1. \n4. (T/F – 2 points) The criterio n we have used here for activ e learning \nguarantees that the measure of uncertainty about the labels of the \nunlab eled points will decrease mono tonica lly for each point after each\nquery .\nProblem 2 \nConsider a regression problem where the two dimensiona l input points x =[x1,x2]T are \nconstra ined to lie within the unit square: xi ∈ [−1, 1], i =1, 2. The training and test input \npoints x are sampled uniformly at rando m within the unit squar e. The target output s y \nare governed by the following model \ny ∼ N(x13 x25 − 10x1x2 +7x12 +5x2 − 3, 1) \nIn other words, the outputs are norma lly distributed with mean given by \nx13 x25 − 10x1x2 +7x12 +5x2 − 3 \nand variance 1.\nWe learn to predict y given x using linear regression models with 1st through 10th order\npolyno mial features. The models are nested in the sense that the higher order models will\ninclude all the lower order features. The estimation criterio n is the mean squared error.\n3\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "Sketch wˆn in Figur e 1.1. Write down the equatio n, express ed solely in terms of Φ and wˆn, that Φ has to satisfy for ", "source_title": "e422ff477d12ba1bf2544e6293535c51 midterm f03", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "e726fabe-e8de-457d-8fb2-2562b4d5655d", "text": "1st through 10th order polyno mial features. The models are nested in the sense that the higher order models will include all the lower order features. The estimation criterio n is the mean squared error. 3 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nWe ﬁrst train a 1st, 2nd, 8th, and 10th order model using n = 20 training points, and then \ntest the predictio ns on a large number of indep enden tly sampled points. \n1. (6 points) Selec t all the appro priat e model(s) for each column. If you think the \nhighest, or lowest, error would be shared among several models, be sure to list all \nmodels. \nLowest test error \nLowest training error Highest training error (typically) \n1st order ( ) ( ) \n2nd order ( ) ( ) ( ) \n8th order ( ) ( ) \n10th order ( ) ( ) ( ) \nBrieﬂy explain your selection in the last column, i.e., the model you would expect to \nhave the lowest test error : \n2. (6 points) We now train the polyno mial regression models using n = 106 (one \nmillion) training points. Again select the appropria te model(s) for each column. If \nyou think the highest, or lowest, error would be shared among several models, be sure \nto list all models. \nLowest structur al error Highest approx. error Lowest test error \n1st order ( ) ( ) ( ) \n2nd order ( ) ( ) ( ) \n8th order ( ) ( ) ( ) \n10th order ( ) ( ) ( ) \n3. (T/F – 2 points) The approximation error of a polynomia l regression \nmodel depends on the number of training points. \n4. (T/F – 2 points) The structural error of a polynomia l regression \nmodel depends on the number of training points. \n4\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "regression", "section_heading": "1st through 10th order polyno mial features. The models are nested in the sense that the higher order models will includ", "source_title": "e422ff477d12ba1bf2544e6293535c51 midterm f03", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "1c09b817-dd70-4f85-9719-412623dfb0e6", "text": "error of a polynomia l regression model depends on the number of training points. 4. (T/F – 2 points) The structural error of a polynomia l regression model depends on the number of training points. 4 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 3 \nWe consider here linear and non-linea r supp ort vector machines (SVM) of the form: \nmin w12/2 subject to yi(w1xi + w0) − 1 ≥ 0,i =1,...,n, or \nmin w T w/2 subject to yi(w T Φi + w0) − 1 ≥ 0,i =1,...,n \nwhere Φi is a feature vector constructed from the corresp onding real valued input xi. We \nwish to compa re the simple linear SVM classiﬁer (w1x + w0) and the non-linear classiﬁer \n(wT Φ+ w0), where Φ=[x,x2]T . \n1. (3 points) Provide three input points x1, x2, and x3 and their associated ±1 labels \nsuch that they cannot be separ ated with the simple linear classiﬁer, but are separ able \nby the non-linea r classif er with Φ = [x,x2]T . You may ﬁnd Figure 3.1. helpf ul in \nansw ering this question. \n2. (3 points) In the ﬁgure below (Figure 3.1), mark your three points x1, x2, and x3 as \npoints in the featur e space with their associated labels. Draw the decision boundary \nof the non-line ar SVM classiﬁer with Φ = [x,x2]T that separ ates the points. \n0 1 2 3 40246810121416\nf1=xf2=x2\n(x,x2) \nFigur e 3.1. Feature space.\n5\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3. (3 points) Consider two labeled points (x =1,y = 1) and (x =3,y = −1). Is the \nmargin we attain using feature vector s Φ = [x,x2]T \n( ) greater \n( ) equal \n( ) smaller\nthan the margin resulting from using the input x directly?\n4. (2 points) In general, is the margin we would attain using scaled feature vectors \nΦ = [2x, 2x2]T \n( ) greater \n( ) equal \n( ) smaller \n( ) any of the above\nin compar ison to the margin resulting from using Φ = [x,x2]T ?\n5. (T/F – 2 points) The values of the marg ins obtained by two diﬀeren t \nkernels K(x,x�) and K˜(x,x�) on the same training set do not tells us \nwhich classiﬁer will perfor m better on the test set.\n6\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "error of a polynomia l regression model depends on the number of training points. 4. (T/F – 2 points) The structural err", "source_title": "e422ff477d12ba1bf2544e6293535c51 midterm f03", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "d907c393-4377-436d-bc2f-a53e1b6b415c", "text": "points) The values of the marg ins obtained by two diﬀeren t kernels K(x,x�) and K˜(x,x�) on the same training set do not tells us which classiﬁer will perfor m better on the test set. 6 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 4 \nWe consider here generativ e and discriminativ e approa ches for solving the classiﬁcat ion \nproblem illustra ted in Figur e 4.1. Speciﬁca lly, we will use a mixture of Gaussians model \nand regula rized logistic regression models. \n0 0.5 1 1.5 200.511.52\nx1x2\nFigur e 4.1. Labeled training set, where “+” corresponds to class y = 1. \n1. We will ﬁrst estimate a mixture of Gaussians model, one Gaussian per class, with the \nconstra int that the co variance matrices are iden tity matr ices. The mixing proportions \n(class frequencies) and the means of the two Gaussians are free parameters. \na) (3 points) Plot the maximum likeliho od estimates of the means of the two class \nconditio nal Gaussians in Figure 4.1. Mark the means as points “x” and label \nthem “0” and “1” according to the class. \nb) (2 points) Draw the decision boundary in the same ﬁgure. \n7\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2. We have also trained regula rized linea r logistic regression models \nP (y =1|x, w)= g(w0 + w1x1 + w2x2) \nfor the same data. The regularizat ion penalties, used in penalized conditio nal log-\nlikeliho od estimatio n, were −Cw2, where i =0, 1, 2. In other words, only one of the i \nparameters were regular ized in each case. Based on the data in Figure 4.1, we gen­\nerated three plots, one for each regularized parameter, of the number of misclassiﬁed \ntraining points as a function of C (Figure 4.2). The three plots are not identiﬁed \nwith the corresponding parameters, however. Please assign the “top”, “middle”, and \n“bottom” plots to the correct parameter, w0, w1, or w2, the parameter that was \nregula rized in the plot. Provide a brief justiﬁcatio n for each assignm ent. \n• (3 points) “top” =( ) \n• (3 points) “middle” = ( )\n• (3 points) “bottom” = ( )\n8\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "logistic regression", "section_heading": "points) The values of the marg ins obtained by two diﬀeren t kernels K(x,x�) and K˜(x,x�) on the same training set do no", "source_title": "e422ff477d12ba1bf2544e6293535c51 midterm f03", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "47d47569-b57e-49af-b90a-5b328deb50fa", "text": "was regula rized in the plot. Provide a brief justiﬁcatio n for each assignm ent. • (3 points) “top” =( ) • (3 points) “middle” = ( ) • (3 points) “bottom” = ( ) 8 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n0 0.5 1 1.5 200.511.52\nx1x2\n0 0.5 1 1.5 2 2.5 3024top\n0 0.5 1 1.5 2 2.5 3024middle\n0 0.5 1 1.5 2 2.5 3024bottom\nregularization parameter CFigur e 4.1 Labeled training set Figure 4.2. Training errors as a functio n \n(repro duced here for clarity) of regula rizatio n penalty \n9\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nAddi tional set of ﬁgures\n+ o Φt =[φt1,...,φtm]T \n+ o \nFigur e 1.1. Two labeled points, unlabeled Figur e 1.2. Two labeled points, unlabeled \npoints, and the decis ion boundar y. The points, and the decis ion boundary . The \npoint “+” corresponds to y = 1. point “+” corresponds to y = 1. \n0 1 2 3 40246810121416\nf1=xf2=x2\n(x,x2) \nFigur e 3.1. Feature space.\n10\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n0 0.5 1 1.5 200.511.52\nx1x2Figur e 4.1. Labeled training set, where “+” corresponds to class y = 1.\n0 0.5 1 1.5 200.511.52\nx1x2\n0 0.5 1 1.5 2 2.5 3024top\n0 0.5 1 1.5 2 2.5 3024middle\n0 0.5 1 1.5 2 2.5 3024bottom\nregularization parameter C\nFigur e 4.1 Labeled training set Figure 4.2. Training errors as a functio n \n(repr oduced here for clarity) of regula rization penalty \n11\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "regularization", "section_heading": "was regula rized in the plot. Provide a brief justiﬁcatio n for each assignm ent. • (3 points) “top” =( ) • (3 points) “", "source_title": "e422ff477d12ba1bf2544e6293535c51 midterm f03", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "f961f9b8-f182-4746-a50a-ee1af432f099", "text": "� \n� 6.867 Machine learning \nMid-t erm exam \nOcto ber 8, 2003 \n(2 points) Your name and MIT ID: \nProblem 1 \nWe are interested here in a particula r 1-dimensiona l linear regression problem. The dataset \ncorrespondin g to this problem has n examples (x1,y1),..., (xn,yn), where xi and yi are real \nnumbers for all i. Part of the diﬃcult y here is that we don’t have acces s to the inputs or \noutputs directly. We don’t even know the number of example s in the dataset. We are, \nhowever, able to get a few numbers computed from the data. \nLet w∗ =[w0∗,w1∗]T be the least squares solutio n we are after. In other words, w∗ minimizes \nn1 � \nJ(w)= (yi − w0 − w1xi)2 \nn i=1 \nYou can assume for our purp oses here that the solut ion is unique. \n1. (4 points) Chec k each statemen t that must be true if w∗ =[w0∗,w1∗]T is indeed the \nleast squares solution \n( ) (1/n) in \n=1(yi − w0 ∗ − w1∗xi)yi =0 \n( ) (1/n) �in \n=1(yi − w0 ∗ − w1∗xi)(yi − y¯) = 0 \n( ) (1/n) �in \n=1(yi − w0 ∗ − w1∗xi)(x i − x¯) = 0 \n( ) (1/n) n (yi − w0 ∗ − w1∗xi)(w0 ∗ + w1∗xi)=0 i=1\nwhere ¯x and ¯y are the sample means based on the same dataset. \n1 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "� ", "source_title": "7b98f36e1b7c80851e19b32d31dc4101 midterm f02", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "bafdfaf1-4312-4532-a767-7a43e32f07de", "text": "∗ − w1∗xi)(x i − x¯) = 0 ( ) (1/n) n (yi − w0 ∗ − w1∗xi)(w0 ∗ + w1∗xi)=0 i=1 where ¯x and ¯y are the sample means based on the same dataset. 1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2. (4 points) There are several numbers (statistics) comput ed from the data that we \ncan use to infer w∗. These are \n¯x = 1 \nn n� \nxi, ¯y = 1 \nn n� \nyi, Cxx = 1 \nn n� \n(xi − ¯x)2 \ni=1 i=1 i=1 \nn n1 � 1 � \nCxy = (xi − x¯)(yi − y¯),Cyy = (yi − y¯)2 \nn n i=1 i=1 \nSupp ose we only care about the value of w1∗. We’d like to determine w1 ∗ on the basis \nof only two numbers (stat istics) listed above. Whic h two numbers do we need for \nthis? \n3. Here we chang e the rules governing our access to the data. Instea d of simply get­\nting the statistics we want, we have to reconstruct these from examples that we \nquery . There are two types of queries we can make. We can either request additiona l \nrandomly chosen examples from the training set, or we can query the output corre­\nsponding to a speciﬁc input that we specify. (We assume that the dataset is large \nenoug h that there is always an example whose input x is close enoug h to our query). \nThe activ e learning scenar io here is somewhat diﬀerent from the typical one. Norma lly \nwe would assume that the data is governed by a linear model and choose the input \npoints so as to best recover this assumed model. Here the task is to recover the best \nﬁtting linear model to the data but we make no assumptions about whether the linea r \nmodel is appropria te in the ﬁrst place. \n(2 points) Supp ose in our case the input points are constr ained to lie in the interval \n[0, 1]. If we followed the typical activ e learning approach, where we assume that the \ntrue model is linear, what are the input points we would query? \n(3 points) In the new setting, where we try to recover the best ﬁtting linea r model \nor parameters w∗, we should (choose only one): \n( ) Query inputs as you have answered above \n( ) Draw inputs and corresponding outputs at random from the dataset \n( ) Use another strat egy since neither of the above choices would yield satisfactor y \nresults \n2\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "∗ − w1∗xi)(x i − x¯) = 0 ( ) (1/n) n (yi − w0 ∗ − w1∗xi)(w0 ∗ + w1∗xi)=0 i=1 where ¯x and ¯y are the sample means based ", "source_title": "7b98f36e1b7c80851e19b32d31dc4101 midterm f02", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "20a9ef92-f24f-4b01-bfac-a978df61055c", "text": "inputs as you have answered above ( ) Draw inputs and corresponding outputs at random from the dataset ( ) Use another strat egy since neither of the above choices would yield satisfactor y results 2 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n(4 points) Brieﬂy justify your answ er to the previous question\nProblem 2 \nIn this problem we will refer to the binar y classiﬁcatio n task depicted in Figure 1(a), which \nwe attempt to solve with the simple linear logistic regression model \n1 Pˆ(y =1x,w1,w2)= g(w1x1 + w2x2)= |1 + exp( −w1x1 − w2x2) \n(for simplicit y we do not use the bias parameter w0). The training data can be separ ated \nwith zero training error -see line L1 in Figure 1(b) for insta nce. \n0\n0xx\n12\n(a) The 2-dimensional data set used in Prob­\nlem 1 \n4L\n1L\n3L2L\n00\nxx2\n1(b) The points can be separ ated by L1 (solid \nline). Possibl e other decision boundaries are \nshown by L2, L3, L4. \n3 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n1. (6 points) Consider a regula rizatio n approa ch where we try to maximize \nn� C log p(y i|xi,w1,w2) − 2 w 2 \n2 \ni=1 \nfor large C. Note that only w2 is penalized. We’d like to know which of the four \nlines in Figure 1(b) could arise as a result of such regularizat ion. For each potential \nline L2, L3 or L4 determine whether it can result from regularizing w2. If not, explain \nvery brieﬂy why not. \nL2• \nL3• \nL4• \n2. (4 points)If we chang e the form of regula rizatio n to one-norm (absolute value) and \nalso regularize w1 we get the following penalized log-likeliho od \nn� C log p(y i|xi,w1,w2) − 2(|w1| + |w2|) . \ni=1 \nConsider again the problem in Figure 1(a) and the same linear logistic regression \nmodel Pˆ(y =1|x,w1,w2)= g(w1x1 + w2x2). As we increase the regula rization \nparameter C which of the following scenarios do you expect to observ e (choose only \none): \n( ) First w1 will become 0, then w2. \n( ) w1 and w2 will become zero simultaneo usly \n( ) First w2 will become 0, then w1. \n( ) None of the weights will become exactly zero, only smaller as C increases \n4\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "inputs as you have answered above ( ) Draw inputs and corresponding outputs at random from the dataset ( ) Use another s", "source_title": "7b98f36e1b7c80851e19b32d31dc4101 midterm f02", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "a766d53a-ea0b-43d4-b0b1-2f1569cb5e3b", "text": "w2. ( ) w1 and w2 will become zero simultaneo usly ( ) First w2 will become 0, then w1. ( ) None of the weights will become exactly zero, only smaller as C increases 4 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n−3 −2 −1 0 1 2 3 4 5−3−2−1012345\n0.350.670.66\n0.673.781.26\n0.415.751.24 0.180.77\n8.810.670.440.740.71Figur e 1: A 2-dim classiﬁc ation problem, the resulting SVM decision boundary with a \nradial basis kernel, as well as the supp ort vectors (indicat ed by larger circles around them). \nThe numbers next to the support vector s are the corresp onding coeﬃcien ts ˆα. \nProblem 3 \nFigur e 1 illustra tes a binar y classiﬁcat ion problem along with our solut ion using supp ort \nvector machines (SVMs). We have used a radia l basis kernel function given by \nK(x, x�) = exp{−�x − x��2/2 } \nwhere �·� is a Euclide an dista nce and x =[x1,x2]T . The classiﬁcatio n decision for any x \nis made on the basis of the sign of \nwˆT φ(x)+ ˆw0 = yjαˆj K(xj , x)+ ˆw0 = f(x;ˆα, wˆ0) \nj∈SV \nwhere wˆ, wˆ0,ˆαi are all coeﬃcie nts estimated from the available data displa yed in the ﬁgure \nand SV is the set of supp ort vectors. φ(x) is the feature vector deriv ed from x corresponding \nto the radial basis kernel. In other words, K(x, x�)= φ(x)T φ(x�). While technically φ(x) \nis an inﬁnite dimensiona l vector in this case, this fact plays no role in the questions below. \nYou can assume and treat it as a ﬁnite dimens ional vector if you like. \nThe supp ort vectors we obtain for this classiﬁc ation problem (indicated with larger circles \nin the ﬁgure) seem a bit curio us. Some of the support vectors appear to be far away from \nthe decis ion boundary and yet be support vectors. Some of our questions below try to \nresolv e this issue. \n5 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "w2. ( ) w1 and w2 will become zero simultaneo usly ( ) First w2 will become 0, then w1. ( ) None of the weights will bec", "source_title": "7b98f36e1b7c80851e19b32d31dc4101 midterm f02", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "0c89d5e6-0bbc-463e-8a14-842d634fa04e", "text": "a bit curio us. Some of the support vectors appear to be far away from the decis ion boundary and yet be support vectors. Some of our questions below try to resolv e this issue. 5 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n1. (3 points) What happ ens to our SVM predictio ns f(x;ˆα, wˆ0) with the radial basis \nkernel if we choose a test point xfar far away from any of the training points xj \n(dista nces here measured in the space of the origina l points)? \n2. (3 points) Let’s assume for simplicit y that ˆw0 = 0. What equat ion do all the training \npoints xj have to satisfy? Would xfar satisfy the same equation? \n3. (4 points) If we included xfar in the training set, would it become a supp ort vector? \nBrieﬂy justify your answ er. \n4. (T/F – 2 points) Leave-one-out cross-v alidatio n error is always small \nfor supp ort vector machines. \n5. (T/F – 2 points) The maximum marg in decision boundar ies that \nsupp ort vector machines construct have the lowest generalizatio n error \namong all linear classiﬁers \n6. (T/F – 2 points) Any decision bounda ry that we get from a generat ive \nmodel with class- conditiona l Gaussian distributions could in principle \nbe repro duced with an SVM and a polynomial kernel of degree less \nthan or equal to three .\n7. (T/F – 2 points) The decis ion boundary implied by a genera tive \nmodel (with parameterized class-conditio nal densities ) can be optimal \nonly if the assumed class-conditio nal densities are correct for the prob­\nlem at hand \n6\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "a bit curio us. Some of the support vectors appear to be far away from the decis ion boundary and yet be support vectors", "source_title": "7b98f36e1b7c80851e19b32d31dc4101 midterm f02", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "08c68262-7c3e-45c3-847f-9e4a94eceb7b", "text": "points) The decis ion boundary implied by a genera tive model (with parameterized class-conditio nal densities ) can be optimal only if the assumed class-conditio nal densities are correct for the prob­ lem at hand 6 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 4 \nConsider the following set of 3-dimensiona l points, sampled from two classes: \nx1 x2 x3 x1 x2 x3 \n1, 1, −1 1, 1, 2 \nlabeled ’1’: 0, 2, −2 labeled ’0’: 0, 2, 1 \n0, −1, 1 1, −1, −1 \n0, −2, 2 1, −2, −2 \nWe have included 2-dime nsional plots of pairs of features in the “Addit ional set of ﬁgures” \nsection (ﬁgure 3). \n1. (4 points) Explain brieﬂy why features with higher mutual informa tion with the \nlabel are likely to be more useful for classiﬁca tion task (in general, not necessarily in \nthe given example). \n2. (3 points) In the example above, which feature (x1, x2 or x3) has the \nhighest mutual informa tion with the class label, based on the training \nset?\n3. (4 points) Assume that the learning is done with quadra tic logistic \nregression, where \nP (y =1|x, w)= g(w0 + w1xi + w2xj + w3xixj + w4xi 2 + w5xj 2) \nfor some pair of features (xi,xj ). Based on the training set given above, \nwhich pair of features would result in the lowest training error for the \nlogistic regression model? \n4. (T/F – 2 points) From the point of view of classiﬁc ation it is always \nbeneﬁcial to remo ve featur es that have very high variance in the data \n5. (T/F – 2 points) A feature which has zero mutual informatio n with \nthe class label migh t be selected by a greedy selectio n metho d, if it \nhapp ens to impro ve classiﬁer’s performance on the training set \n7 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "logistic regression", "section_heading": "points) The decis ion boundary implied by a genera tive model (with parameterized class-conditio nal densities ) can be ", "source_title": "7b98f36e1b7c80851e19b32d31dc4101 midterm f02", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "f2563bc8-fdbd-41c0-99e9-2deafd9df1bb", "text": "feature which has zero mutual informatio n with the class label migh t be selected by a greedy selectio n metho d, if it happ ens to impro ve classiﬁer’s performance on the training set 7 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 5\n+−h 1\n+\n+++−−−−\nFigur e 2: h1 is chosen at the ﬁrst iteration of boosting; what is the weight α1 assigned to \nit? \n1. (3 points) Figur e 2 shows a dataset of 8 points, equally divided among \nthe two classes (positive and negative). The ﬁgure also shows a particu­\nlar choice of decis ion stump h1 picked by AdaBo ost in the ﬁrst iteratio n. \nWhat is the weight α1 that will be assigned to h1 by AdaBoost? (Initial \nweights of all the data points are equal, or 1/8.) \n2. (T/F – 2 points) AdaBo ost will eventually reach zero training error , \nregardless of the type of weak classiﬁe r it uses, provided enoug h weak \nclassiﬁers have been combined.\n3. (T/F – 2 points) The votes αi assigned to the weak classiﬁers in \nboosting generally go down as the algorithm proceeds, because the \nweighted training error of the weak classiﬁers tends to go up\n4. (T/F – 2 points) The votes α assigned to the classiﬁers assem bled \nby AdaBo ost are always non-nega tive \n8\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nAddi tional set of ﬁgures\n0\n0xx\n12\n4L\n1L\n3L2L\n00\nxx2\n1\n−3 −2 −1 0 1 2 3 4 5−3−2−1012345\n0.350.670.66\n0.673.781.26\n0.415.751.24 0.180.77\n8.810.670.440.740.71\n+−h 1\n+\n+++−−−−\nthere’s more ...\n9\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n−3 −2 −1 0 1 2 3−3−2−10123\nx1x2\n−3 −2 −1 0 1 2 3−3−2−10123\nx1x3\n−3 −2 −1 0 1 2 3−3−2−10123\nx2x3Figur e 3: 2-dimens ional plots of pairs of features for problem 4. Here ’+’ corresponds to \nclass label ’1’ and ’o’ to class label ’0’. \n. \n10\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "if ", "section_heading": "feature which has zero mutual informatio n with the class label migh t be selected by a greedy selectio n metho d, if it", "source_title": "7b98f36e1b7c80851e19b32d31dc4101 midterm f02", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 6}
{"id": "93efad4a-a9ea-4dca-9203-9ee367e7ce0b", "text": "� \n� � 6.867 Mac hine learning , lectur e 8 (Jaakkola) 1 \nLecture topics: \n• Supp ort vector machine and kernels \n• Kernel optimizatio n, selection \nSupp ort vector machine revisited \nOur task here is to ﬁrst turn the supp ort vector machine into its dual form where the exam­\nples only appear in inner products. To this end, assume we have mapped the examples into \nfeature vector s φ(x) of dimension d and that the resulting training set (φ(x1),y1),..., (φ(xn),yn) \nis linearly separable. Finding the maxim um marg in linear separa tor in the feature space \nnow corresponds to solving \nminimize �θ�2/2 subject to yt(θT φ(xt)+ θ0) ≥ 1,t =1,...,n (1) \nWe will discuss later on how slack variables aﬀect the resulting kernel (dual) form. They \nmerely complicate the deriv ation witho ut changin g the procedure. Optimizat ion problems \nof the above type (convex, linea r constraints) can be turned into their dual form by means of \nLagrange multipl iers. Speciﬁcally , we introduce a non-nega tive scala r parameter αt for each \ninequalit y constra int and cast the estima tion problem in terms of θ and α = {α1,...,αn}: \nn� � \nJ(θ,θ0; α)= �θ�2/2 − αt yt(θT φ(xt)+ θ0) − 1 (2) \nt=1 \nThe original minimizatio n problem for θ and θ0 is recovered by maximizing J(θ,θ0; α) with \nrespect to α. In other words, \nJ(θ,θ0) = max J(θ,θ0; α) (3) \nα≥0 \nwhere α ≥ 0 means that all the comp onents αt are non-negative. Let’s try to see ﬁrst that \nJ(θ,θ0) really is equiv alent to the original problem. Supp ose we set θ and θ0 such that at \nleast one of the constr aints, say the one corresp onding to (xi,yi), is violated. In that case \n− αi yi(θT φ(x i)+ θ0) − 1 > 0 (4) \nfor any αi > 0. We can then set αi = ∞ to obtain J(θ,θ0)= ∞. You can think of the \nLagrange multipliers playing an adversar ial role to enfor ce the margin constrain ts. More \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "convex", "section_heading": "� ", "source_title": "33a6c8e66c62602f9f03ab6a2c632eed lec8", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "7e4a6a78-370c-433f-9c03-b183d751a065", "text": "for any αi > 0. We can then set αi = ∞ to obtain J(θ,θ0)= ∞. You can think of the Lagrange multipliers playing an adversar ial role to enfor ce the margin constrain ts. More Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� � � \n� 6.867 Mac hine learning , lectur e 8 (Jaakkola) 2 \nforma lly, \nJ(θ,θ0)= �θ�2/2 if yt(θT φ(xt)+ θ0) ≥ 1,t =1,...,n (5) ∞, otherwise \nSo the minimizing θ and θ0 are therefor e those that satisfy the constra ints. On the basis of \na genera l set of criteria governing the optimalit y when dealing with Lagrange multipliers, \ncriteria known as Slater conditions, we can actua lly switc h the maximizing over α and the \nminimizat ion over {θ,θ0} and get the same answer: \nminmax J(θ,θ0; α) = maxmin J(θ,θ0; α) (6) \nθ,θ0 α≥0 α≥0 θ,θ0 \nThe left hand side, equiv alent to minimizing Eq.(5), is known as the primal form, while the \nright hand side is the dual form. Let’s solve the right hand side by ﬁrst obtaining θ and θ0 \nas a function of the Lagrange multipliers (and the data). To this end \nnd � \ndθ0 J(θ,θ0; α)= − αtyt = 0 (7) \nt=1 \nnd � \ndθJ(θ,θ0; α)= θ − αtytφ(xt)=0 (8) \nt=1 \nSo, again the solution for θ is in the span of the feature vectors corresp onding to the training \nexamples. Substituting this form of the solution for θ back into the objectiv e, and taking \ninto account the constrain t corresp onding to the optimal θ0, we get \nJ(α) = min J(θ,θ0; α) (9) \nθ,θ0�� �� � n n n n \n= t=1 αt − (1/2) i=1 j=1 αiαj yiyj[φ(xi)T φ(xj )], if t=1 αtyt = 0 (10) −∞, otherwise \nThe dual form of the solutio n is therefore obtained by maximizing \nn n n\nαt − (1/2) αiαj yiyj [φ(x i)T φ(xj )], (11) \nt=1 i=1 j=1 \nn\nsubject to αt ≥ 0, αtyt = 0 (12) \nt=1 \nThis is the dual or kernel form of the supp ort vector machine, and is also a quadr atic \noptimization problem. The constra ints are simpler, however. Moreo ver, the dimension of \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "for any αi > 0. We can then set αi = ∞ to obtain J(θ,θ0)= ∞. You can think of the Lagrange multipliers playing an advers", "source_title": "33a6c8e66c62602f9f03ab6a2c632eed lec8", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "e56aee02-cbe5-4a4e-ad01-5dc88de9f3a4", "text": "= 0 (12) t=1 This is the dual or kernel form of the supp ort vector machine, and is also a quadr atic optimization problem. The constra ints are simpler, however. Moreo ver, the dimension of Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� \n� 6.867 Mac hine learning , lectur e 8 (Jaakkola) 3 \nthe input vectors does not appear explicitly as part of the optimizatio n problem. It is \nformulated solely on the basis of the Gram matrix: \n⎡ ⎤ φ(x1)T φ(x1) · · · φ(x 1)T φ(x n) \nK = ⎣ · · · · · · · · · ⎦ (13) \nφ(xn)T φ(x1) . . . φ(xn)T φ(x n) \nWe have already seen that the maxim um marg in hyperplane can be constructed on the \nbasis of only a subse t of the training examples. This should also also in terms of the \nfeature vectors. How will this be manifested in the ˆαt’s? Many of them will be exactly \nzero due to the optimizat ion. In fact, they are non-zer o only for examples (feature vectors) \nthat are supp ort vector s. \nOnce we have solved for ˆαt, we can classify any new example according to the discrimina nt \nfunction \nyˆ(x) = θˆT φ(x) + θˆ0 (14) \nn\n= αˆtyt[φ(xt)T φ(x)] + θˆ0 (15) \nt=1 \n= αˆtyt[φ(xt)T φ(x)] + θˆ0 (16) \nt∈SV \nwhere SV is the set of support vector s corresponding to non-zero values of αt. We don’t \nknow which examples (feature vectors) become as supp ort vectors until we have solved the \noptimization problem. Moreo ver, the identity of the supp ort vector s will depend on the \nfeature mapping or the kernel function. \nBut what is θˆ0? It appeared to drop out of the optimization problem. We can set θ0 after \nsolving for ˆαt by looking at the supp ort vectors. Indeed, for all i ∈ SV we should have \nyi(θˆT φ(xi)+ θˆ0)= yi αˆt[φ(xt)T φ(xi)] + yiθˆ0 = 1 (17) \nt∈SV \nfrom which we can easily solve for θˆ0. In principle , selecting any supp ort vector would suﬃce \nbut since we typically solve the quadratic program over αt’s only up to some resolutio n, \nthese constr aints may not be satisﬁed with equalit y. It is therefor e advisable to construct \nθˆ0 as the media n value of the solutions implie d by the supp ort vectors. \nWhat is the geometric margin we attain with some kernel function K(x, x�)= φ(x)T φ(x�)?", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "= 0 (12) t=1 This is the dual or kernel form of the supp ort vector machine, and is also a quadr atic optimization probl", "source_title": "33a6c8e66c62602f9f03ab6a2c632eed lec8", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "829cb581-0399-4c7d-baf1-fd4019b632c5", "text": "solve the quadratic program over αt’s only up to some resolutio n, these constr aints may not be satisﬁed with equalit y. It is therefor e advisable to construct θˆ0 as the media n value of the solutions implie d by the supp ort vectors. What is the geometric margin we attain with some kernel function K(x, x�)= φ(x)T φ(x�)?\n\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "solve the quadratic program over αt’s only up to some resolutio n, these constr aints may not be satisﬁed with equalit y", "source_title": "33a6c8e66c62602f9f03ab6a2c632eed lec8", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "3a3390c1-6db3-4b06-b190-da819b81fd62", "text": "therefor e advisable to construct θˆ0 as the media n value of the solutions implie d by the supp ort vectors. What is the geometric margin we attain with some kernel function K(x, x�)= φ(x)T φ(x�)? Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� �� � \n� \n� � � \n� 6.867 Mac hine learning , lectur e 8 (Jaakkola) 4 \nIt is still 1/�θˆ�. In a kernel form \nn n−1/2 \nγˆgeom = αˆiαˆjyiyj K(xi, xj ) (18) \ni=1 j=1 \nWould it make sense to compare geometric marg ins we attain with diﬀerent kernels? We \ncould perhaps use it as a criterio n for selecting the best kernel functio n. Unfortuna tely \nthis won’t work without some care. For example, if we multiply all the feature vectors by \n2, then the resulting geometr ic margin will also be twice as large (we just expanded the \nspace; the relations between the points remain the same). It is necessary to perform some \nnormalizat ion before any compariso n makes sense . \nWe have so far assumed that the examples in their feature represe ntatio ns are linea rly \nsepara ble. We’d also like to have the kernel form of the relaxed support vector machine \nformulatio n \nn\nminimize �θ�2/2+ C ξt (19) \nt=1 \nsubject to yt(θT φ(xt)+ θ0) ≥ 1 − ξt,t =1,...,n (20) \nThe resulting dual form is very simila r to the simple one we derived above. In fact, the \nonly diﬀerence is that the Lagrange multipliers αt are now also bounded from above by \nC (the same C as in the above primal formulatio n). Intuitively, the Lagrange multipliers \nαt serve to enforce the classiﬁcation constra ints and adopt larger values for constrai nts \nthat are harder to satisfy. Without any upper limit, they would simply reach ∞ for any \nconstra int that canno t be satisﬁed. The limit C speciﬁes the point when we should stop \nfrom trying to satisfy such constr aints. More forma lly, the dual form is \nn n n\nαt − (1/2) αiαj yiyj [φ(x i)T φ(xj )], (21) \nt=1 i=1 j=1 \nn\nsubject to 0 ≤ αt ≤ C, αtyt = 0 (22) \nt=1 \nThe resulting discrimina nt function has the same form except that the ˆαt values can be \ndiﬀeren t. What about θˆ0? To solve for θˆ0 we need to identify classiﬁc ation constrain ts that \nare satisﬁed with equalit y. Thes e are no longer simply the ones for which ˆαt > 0 but those \ncorrespondin g to 0 <αˆt <C. In other words, we have to exclude points that violate the \nmargin constra ints. These are the ones for which ˆαt = C. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "therefor e advisable to construct θˆ0 as the media n value of the solutions implie d by the supp ort vectors. What is th", "source_title": "33a6c8e66c62602f9f03ab6a2c632eed lec8", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "ba5f6782-064b-4612-a433-47d7589544b5", "text": "for which ˆαt > 0 but those correspondin g to 0 <αˆt <C. In other words, we have to exclude points that violate the margin constra ints. These are the ones for which ˆαt = C. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n6.867 Mac hine learning , lectur e 8 (Jaakkola) 5 \nKernel optimization \nWhether we are interes ted in (linea r) classiﬁcat ion or regression we are faced with the \nproblem of selecting an appro priate kernel function. A step in this direction migh t be to \ntailor a particular kernel a bit better to the available data. We could, for example, introduce \naddit ional parameters in the kernel and optimize those parameters so as to impro ve the \nperforma nce. These parameters could be simple as the β parameter in the radia l basis \nkernel, weight each dimension of the input vectors, or more ﬂexible as ﬁnding the best \nconvex combinatio n of basic (ﬁxed) kernels . Key to such an approach is the measure we \nwould optimize . Ideally , this measure would be the genera lization error but we obviously \nhave to settle for a surro gate measure. The surro gate measure could be cross-v alidatio n or \nan alternativ e criterio n related to the generalizat ion error (e.g., margin) . \nKernel selection \nWe can also explicitly select among possible kernels and cast the problem as a model \nselection problem. By choosing a kernel we specify the feature vectors on the basis of \nwhich linear predictions are made. Each model1 (class) refers to a set of linear functions \n(classiﬁers) based on the chosen feature represen tation. In many cases the models are \nnested in the sense that the more “complex” model contains the “simpler” one. We will \ncontinue from this further at the next lecture. \n1In statistics , a model is a family /set of distr ibutions or a family/se t of linear separators. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "convex", "section_heading": "for which ˆαt > 0 but those correspondin g to 0 <αˆt <C. In other words, we have to exclude points that violate the marg", "source_title": "33a6c8e66c62602f9f03ab6a2c632eed lec8", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "0ef3dd2c-7046-4fda-a122-dddcb819ffe8", "text": "1 6.867 Mac hine learning , lectur e 10 (Jaak kola) \nLecture topics: model selection criteria \n• Structur al risk minimiz ation, example deriv ation \n• Bayesian score, Bayesian Informa tion Criterio n (BIC) \nModel selection criteria: structural risk minimization \nOne perspectiv e to model selection is to ﬁnd the model (set of discriminant functio ns) that \nhas the best guar ante e of gener alization. To obtain such guarantees we have to relate the \nempiric al risk Rn(fˆ i) \nn\nRn(fˆ i)= 1 � \nLoss∗ � \nyt,fˆ i(xt) � \n(1) n t=1 \nthat we can compute to the (expected) risk R(fˆ i) \n�� �� \nR(fˆ i)= E(x,y)∼P Loss∗ y, fˆ i(x) (2) \nthat we would like to have. In fact, we would like to keep these somewhat close so that the \nempirical risk (training error ) still reﬂects how well the metho d will generalize. The empir­\nical risk is computed on the basis of the available training set Sn = {(x1,y1),..., (xn,yn)}\nand the loss functio n Loss∗(·, ·) rather than say the hinge loss. For our purp oses fˆ i ∈Fi \ncould be any estima te deriv ed from the training set that appro xima tely tries to minimi z­\ning the empirica l risk. In our analysis we will assume that Loss∗(·, ) is the zero-one loss ·\n(classiﬁcatio n error). \nWe’d like to quantify how much R(fˆ i) can deviate from Rn(fˆ i). The more powerful our \nset of classiﬁe rs is the more we would expect them to devia te from one another. In other \nwords, the more choices we have in terms of discrimina nt functions, the less represen tative \nthe training error of the minimizing classiﬁer is about its generaliza tion error. So, our goal \nis to show that \nR(fˆ i) ≤ Rn(fˆ i)+ C(n, Fi,δ) (3) \nwhere the complexity penalty C(n, Fi) only depends on the model Fi, the number of training \ninstances, and a parameter δ. The peanalty does not depend on the actual training data. \nWe will discuss the parameter δ below in more detail. For now, it suﬃces to say that 1 − δ \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "1 6.867 Mac hine learning , lectur e 10 (Jaak kola) ", "source_title": "d5a30e80d0ae7a6796186f40640d3b7d lec10", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "f96790bd-5f4f-447a-a99d-d94fa4165ced", "text": "training instances, and a parameter δ. The peanalty does not depend on the actual training data. We will discuss the parameter δ below in more detail. For now, it suﬃces to say that 1 − δ Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � 2 6.867 Mac hine learning , lectur e 10 (Jaak kola) \nspeciﬁes the probability that the bound holds. We can only give a proba bilistic guarantee \nin this sense since the empirica l risk (training error) is a random quan tity that depends on \nthe speciﬁc insta ntiation of the data. \nFor nested models, F1 ⊆F2 ⊆ ..., the penalt y is necessarily an increasing function of i, \nthe model order (e.g., the degree of polynomia l kernel). Moreover, the penalt y should go \ndown as a function n. In other words, the more data we have, the more complex models \nwe expect to be able to ﬁt and still have the training error close to the generaliza tion error. \nThe type of result in Eq.(3) gives us an upper bound guar ante e of gener alization error. \nWe can then select the model with the best guarantee, i.e., the one with the lowest bound. \nFigur e 1 shows how we would expect the upper bound to behave as a function of increa singly \ncomplex models in our nested “hierar chy” of models. \n0 10 20 30 40 5000.10.20.30.40.50.60.70.80.91\nVC dimensionBound \nComplexity penalty \nTraining error \nFigur e 1: Bound on the generalizatio n error as a functio n of model order (e.g., degree of \npolyno mial kernel). \nLet’s deriv e a result of this type in the simple context where Fi only contains a ﬁnite \nnumber of classiﬁers |Fi| < ∞. We will get to the genera l theory later on but this simple \nsetting is helpful in understa nding how such results come about. To avoid the question of \nhow exactly we estima te fˆ i, we will require a stronger guarantee: the bound should hold \nfor all the classiﬁers in our set. Speciﬁcally , we try to ﬁnd a tight upper bound on \nP max |R(f) − Rn(f)| >� ≤ δ (4) \nf∈F i \nThis is the probabilit y that at least one classiﬁer in our set devia tes by more than � from \nits training error. The proba bility is taken over the choice of the training data. So, if we \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "kernel", "section_heading": "training instances, and a parameter δ. The peanalty does not depend on the actual training data. We will discuss the par", "source_title": "d5a30e80d0ae7a6796186f40640d3b7d lec10", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "4df39d03-3d8b-4c86-97ed-cc7c14ef112b", "text": "the probabilit y that at least one classiﬁer in our set devia tes by more than � from its training error. The proba bility is taken over the choice of the training data. So, if we Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � 3 6.867 Mac hine learning , lectur e 10 (Jaak kola) \nused � to claim that \nR(f) ≤ Rn(f)+ � for all f ∈Fi (5) \nthen this expression would fail with proba bility \nδ = P max R(f) − Rn(f)>� (6) \nf∈F i | | \nor, put another way, it would hold with proba bility 1 − δ over the choice of the training \ndata. If we ﬁx δ, then the smallest � = �(n, Fi,δ) that satisﬁes Eq.(6) is the complexit y \npenalt y we are after. Note that since the express ion holds for all f ∈Fi it necess arily also \nholds for fˆ i. \nIn most cases we canno t compute δ exactly from Eq.(6) but we can deriv e an upper bound. \nThis upper bound will lead to a larger than necessary complexit y penalt y but at least we \nwill get a closed form expression (the utilit y of the model selection criterio n will indeed \ndepend on how tight a bound we can obtain). We will proceed as follows: \n� � \nP max \nf∈F i |R(f) − Rn(f)| > � = P (∃f ∈ Fi s.t. \n� |R(f) − Rn(f)| > �) (7) \n≤ \nf ∈F i P (|R(f) − Rn(f)| > �) (8) \nwhere we have used the union bound P (A1 ∪ A2 ∪ ...) ≤ P (A1)+ P (A2)+ ... for any set \nof events A1,A2,... (not necessarily disjoin t). In other words, we bound the probabilit y \nthat there are functions in our set with larger than � deviat ion by a sum that each function \nindividua lly has more than � deviat ion between training and genera lization errors. \nNow, the discriminant function is ﬁxed in any individua l term in the sum \nP (|R(f) − Rn(f)| >�) (9) \nIt won’t chang e as a function of the training data. We can then associate with each i.i.d. \ntraining sample (xt,yt), an indic ator st ∈{0, 1} of whether the sample disagr ees with f: \nst = 1 iﬀ ytf(xt) ≤ 0. The empirical error Rn(f) is therefo re just an average of indep enden t \nrandom variables (indicato rs) st: \nn1 � \nRn(f)= st (10) n t=1 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "the probabilit y that at least one classiﬁer in our set devia tes by more than � from its training error. The proba bili", "source_title": "d5a30e80d0ae7a6796186f40640d3b7d lec10", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "25fd1082-54d4-4f5f-894f-fcf797152d03", "text": "ees with f: st = 1 iﬀ ytf(xt) ≤ 0. The empirical error Rn(f) is therefo re just an average of indep enden t random variables (indicato rs) st: n1 � Rn(f)= st (10) n t=1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � \n� � \n� \n� 4 6.867 Mac hine learning , lectur e 10 (Jaak kola) \nWhat is the expected value of each st when the expectation is taken over the choice of the \ntraining data? It’s just R(f), the expected risk. So, we can rewrite \nP (|R(f) − Rn(f)| > �) (11) \nas \n� � \n1 n� \nP |q − n t=1 st| > � (12) \nwhere q equals R (f) and the pro babilit y is no w over n indep endent binar y random v ariables \ns1,...,sn for which P (st = 1) = q. There are now standa rd results for evaluatin g a \nbound on how much an average of binar y rando m variables deviat es from its expectati on \n(Hoeﬀding’s inequalit y): \nn1 � \nP |q − nst| >� ≤ 2exp(−2n�2) (13) \nt=1 \nNote that the bound does not depend on q (or R(f)) and therefore not on which f we \nchose. Using this result in Eq.(8), gives \nP max |R(f) − Rn(f)| >� ≤ 2|Fi| exp(−2n�2)= δ (14) \nf ∈F i \nThe last equa lity relates δ, |Fi|, n, and �, as desired. By solving for � we get \n� = �(n, Fi,δ) = log |Fi| + log(2/δ) (15) 2n \nThis is the complexit y penalty we were after in this simple case with only a ﬁnite number \nof classiﬁe rs in our set. \nWe have now showed that with proba bility at least 1 − δ over the choice of the training set, \nR(f) ≤ Rn(f) + log |Fi| + log(2/δ) , uniform ly for all f ∈Fi (16) 2n \nSo, for model selectio n, we would then estimat e fˆ i ∈Fi for each model, plug the resulting \nfˆ i and |Fi| on the right hand side of the above equat ion, and choose the model with the \nlowest bound. n and δ would be the same for all models under considerat ion. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "for ", "section_heading": "ees with f: st = 1 iﬀ ytf(xt) ≤ 0. The empirical error Rn(f) is therefo re just an average of indep enden t random varia", "source_title": "d5a30e80d0ae7a6796186f40640d3b7d lec10", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 3}
{"id": "fafdf44a-17ba-4fdf-b9c4-0ab359dcfacc", "text": "resulting fˆ i and |Fi| on the right hand side of the above equat ion, and choose the model with the lowest bound. n and δ would be the same for all models under considerat ion. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� 5 6.867 Mac hine learning , lectur e 10 (Jaak kola) \nAs an example of another way of using the result, supp ose we set δ =0.05 and would like \nany classiﬁer that achieves zero training error to have at most 10% genera lizatio n error. \nLet’s solve for the number of training example s we would need for such a guarantee within \nmodel Fi. We want \nR(f) ≤ 0 + log |Fi| + log(2/0.05) ≤ 0.10 (17) 2n \nSolving for n gives \nn = log |Fi| + log(2/0.05) (18) 2(0.10)2 \ntraining examples. \nModel selection criteria: Bayesian score, Bayesian informa tion criterio n \nIt is perhaps the easies t to explain the Bayesian score with an example. We will start by \nproviding a Bayesian analysis of a simple linea r regression problem. So, supp ose our model \nF takes a d−dimensiona l input x and maps it to a real valued output y (a distribut ion \nover y) accor ding to: \nP (y|x,θ,σ2)= N(y; θT x,σ2) (19) \nwhere N(y; θT x,σ2) is a norma l distribution with mean θT x and variance σ2 . To keep our \ncalculat ions simpler, we will keep σ2 ﬁxed and only try to estimate θ. Now, given any set \nof observ ed data D = {(x1,y1),..., (xn,yn)}, we can deﬁne the likeliho od function \nn n � �� � 1 1 L(D; θ)= N(yt; θT xt,σ2)= √\n2πσ2 exp −2σ2 (yt − θT xt)2 (20) \nt=1 t=1 \nWe have previo usly used only the maximizing parameters θˆas estima tes of the underlying \nparameter value (if any). In Bayesian analysis we are no longer satisﬁed with selecting a \nsingle linear regression functio n but would like to keep all of them, just weighted by their \nabilit y to expla in the data, i.e., weighted by the corresp onding likeliho od L(D; θ). From \nthis perspective, our knowledge about the parameter θ after seeing the data is deﬁned by \nthe posterior distribution P (θ|D) prop ortional to the likeliho od \nP (θ|D) ∝ L(D; θ) (21) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "resulting fˆ i and |Fi| on the right hand side of the above equat ion, and choose the model with the lowest bound. n and", "source_title": "d5a30e80d0ae7a6796186f40640d3b7d lec10", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "bef95a6f-b64a-449f-8b32-7877859a25eb", "text": "od L(D; θ). From this perspective, our knowledge about the parameter θ after seeing the data is deﬁned by the posterior distribution P (θ|D) prop ortional to the likeliho od P (θ|D) ∝ L(D; θ) (21) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� 6 6.867 Mac hine learning , lectur e 10 (Jaak kola) \nIn many cases we cannot normalize this distributio n, however. Supp ose, as an extreme \nexample, that we have no data. The likeliho od function in this case is just one for all \nthe paramet er values. As a result the “posterio r” after seeing no data is not well deﬁned \nas a distribution (we cannot normalize the distributio n by 1 dθ = ∞). To correct this \nproblem it is advantageous to also put our prior belief about the parameter values in a form \nof a distributio n, the prior distr ibution P (θ). This distribution captur es what we believe \nabout the parameter values before seeing any data. Similarly to the regularizat ion penalty, \nwe will typically choose the prior to prefe r small parameter values, e.g., \nP (θ)= N(θ;0,σp 2 I) (22) · \nwhich is a zero mean spherica l Gaussian (same variance in all directions). The smaller σp 2 \nis, the smaller values of θ we prefer prior to seeing the data. The posterio r distribution, \nnow well-deﬁne d as a distributio n regardles s of how much data we see, is proportional to \nthe prior distribut ion P (θ) times the likeliho od: \nP (θ|D) ∝ L(D; θ)P (θ) (23) \nThe normalizat ion constant for the posterio r, also known as the marginal likeliho od, is given \nby \nP (D|F)= L(D; θ)P (θ)dθ (24) \nand depends on the model F and the data but not speciﬁc parameter values. In our \nregression context, we can actua lly evaluate this margina l likelihood in closed form: \nn d 1 log P (D|F)= − 2 log(2πσ2)+ 2log λ − 2 log |XT X + λI| (25) \n1 � � \n−2σ2 �y�2 − y T X(XT X + λI)−1XT y (26) \nwhere λ = σ2/σp 2 (ratio of noise to prior variance), X =[x1,..., xn]T , and y =[y1,...,yn]T . \nThes e deﬁnitions are identical to the regula rized least squar es regression discussed earlier. \nThe posterio r distributi on over the parameters is simply normalized by the margina l like­\nlihood: \nL(D; θ)P (θ)P (θD) = (27) |P (D|F) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "od L(D; θ). From this perspective, our knowledge about the parameter θ after seeing the data is deﬁned by the posterior ", "source_title": "d5a30e80d0ae7a6796186f40640d3b7d lec10", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "01727b0a-c8fc-4932-8fe1-58004a4004d3", "text": "identical to the regula rized least squar es regression discussed earlier. The posterio r distributi on over the parameters is simply normalized by the margina l like­ lihood: L(D; θ)P (θ)P (θD) = (27) |P (D|F) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� 7 6.867 Mac hine learning , lectur e 10 (Jaak kola) \nIn our context the posterior is also Gaussian P (θ|D)= N(θ; µ, Σ) with mean µ and co­\nvariance Σ given by \nµ =(XT X + λI)−1XT y (28) \nΣ= σ2(XT X + λI)−1 (29) \nNote that the posterior mean of the parameters is exactly the parameter estimat e we \nderiv ed earlier using penalized log-lik eliho od with the same prior. This is not an acciden t \nwhen all the distribut ions involved are indeed Gaussians. It is also worth pointing out \nthat P (θ|D) is very diﬀeren t from the normal distribut ion over θˆwe deriv ed earlier when \nassuming that the responses y came from a linear model of the same type. We have made \nno such assumpt ion here and the distributio n P (θ|D) is deﬁned on the basis of the single \nobserv ed y. \nIn Bayesian analysis the prediction of y in response to a new x would be given by weighting \nprediction s based on individual θ’s by the posterio r distribution: \nP (y|x,D)= P (y|x,θ)P (θ|D)dθ (30) \nSo what is the model selection problem in this context? A true Bayesian would refra in from \nselecting a single model but include all of them in proportio n to their ability to expla in \nthe data (just as with parameters). We will not go that far, however, but instead try to \nselect diﬀeren t regression models, speciﬁed by diﬀerent featur e mapping s x φ(x). Let’s \nconsider then two regression models speciﬁe d by linear φ(1)(x) and quadra tic φ→\n(2)(x) feature \nmapping s. The models we compar e are therefor e \nF1 : P (y|x,θ,σ2)= N(y; θT φ(1)(x),σ2),θ ∈Rd1 ,P (θ|F1) (31) \nF2 : P (y|x,θ,σ2)= N(y; θT φ(2)(x),σ2),θ ∈Rd2 ,P (θ|F2) (32) \nNote that θ is of diﬀeren t dime nsion in the two models and thus the prior distributio ns over \nthe parameters, P (θ|F1) and P (θ|F2), will have to be diﬀeren t. You might be wondering \nthat since we are including the speciﬁc ation of the prior distribut ion as part of the model, \nthe result will depend on how we selected the priors. Indee d, but not strongly so. This \ndependence on the prior is both an advantage and a disadv antage from the model selection \npoint of view. We will discuss this further later on. \nSo, how do we select between the two comp eting models? We simply select the one whose \nmarginal likeliho od (Bayesian score1) is larger. In other words, after seeing data D we \n1The deﬁn ition of the Bayesian score often includes a prior over the models as well, e.g., how much we \n Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "identical to the regula rized least squar es regression discussed earlier. The posterio r distributi on over the paramet", "source_title": "d5a30e80d0ae7a6796186f40640d3b7d lec10", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "ecb6910f-0659-407c-b0a7-d8cd85f349f1", "text": "whose marginal likeliho od (Bayesian score1) is larger. In other words, after seeing data D we 1The deﬁn ition of the Bayesian score often includes a prior over the models as well, e.g., how much we Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n8 6.867 Mac hine learning , lectur e 10 (Jaak kola) \nwould select model F1 if \nP (D|F1) >P (D|F2) (33) \nModel selection criteria: Bayesian informa tion criterio n \nBayesian Inform ation Criteri on or BIC for short is an asympto tic approximat ion to the \nBayesian score. It is frequen tly used for its simplicit y. The criterio n is simply \nBIC = l(D; θˆ) − d log(n) (34) 2 \nwhere l(D; θ) is the log-likeliho od of the data, θˆis the maxi mum likeliho od estima te of the \nparameters, and d is the number of indep endent parameters in the model; n is the number \nof training example s as before. BIC is what the Bayesian score will converge to in the limit \nof large n. The Bayesian score is typically diﬃcult to evaluat e in practice and BIC serves \nas a simple tractable alternat ive. Similar ly to the Bayesian score (mar ginal likeliho od), we \nwould select the model with the largest BIC score. \nwould prefer the simpler model before seeing any data. We have no reason to prefer one over another and \ntherefore has used the same prior probabi lity for both. As a result, the selection is carried out entirely on \nthe basis of the margi nal likelihood. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "margin", "section_heading": "whose marginal likeliho od (Bayesian score1) is larger. In other words, after seeing data D we 1The deﬁn ition of the Ba", "source_title": "d5a30e80d0ae7a6796186f40640d3b7d lec10", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 7}
{"id": "cf500c5f-7fa1-4c6f-a027-e3e6cf47a6e9", "text": "6.867 Machine learning \nMid-term exam \nOctob er 22, 2002 \n(2 points) Your name and MIT ID: \nProblem 1 \nWe are interested here in a particular 1-dimensional linear regression problem. The dataset \ncorresp onding to this problem has n examples (x1,y1),..., (xn,yn), where xi and yi are real \nnumbers for all i. Part of the diﬃcult y here is that we don’t have access to the inputs or \noutputs directly . We don’t even know the number of examples in the dataset. We are, \nhowever, able to get a few numbers computed from the data. \nLet w∗ =[w0∗,w1∗]T be the least squares solution we are after. In other words, w∗ minimizes \nn\nJ(w)= 1 �\n(yi − w0 − w1xi)2 \nn i=1 \nYou can assume for our purposes here that the solution is unique. \n1. (4 points) Check each statemen t that must be true if w∗ =[w0∗,w1∗]T is indeed the \nleast squares solution \n( ) (1/n) �n\ni=1(yi − w0∗− w1∗xi)yi =0 \n( ) (1/n) �n\ni=1(yi − w0∗− w1∗xi)(yi − y¯) = 0 \n(x) (1/n) �\nin \n=1(yi − w0∗− w1∗xi)(xi − x¯) = 0 \n(x) (1/n) �n\ni=1(yi − w0∗− w1∗xi)(w0∗+ w1∗xi)=0 \nwhere ¯x and ¯y are the sample means based on the same dataset. \n1 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "6.867 Machine learning ", "source_title": "54259bd801ab4823d969bb836624cf59 midterm f02soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "7f9329d1-18c5-415a-9c0e-e07ddbe08dee", "text": "= 0 (x) (1/n) � in =1(yi − w0∗− w1∗xi)(xi − x¯) = 0 (x) (1/n) �n i=1(yi − w0∗− w1∗xi)(w0∗+ w1∗xi)=0 where ¯x and ¯y are the sample means based on the same dataset. 1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� \n� � \n� \n� �\n� � � � \n� \n�\n� \n� Taking the derivative with respect to w1 and w0 gives us the following conditions of \noptimality (as in the lectures) \nn∂ 2 J(w)= (yi − w0 − w1xi)=0 ∂w0 n i=1 \nn∂ 2 J(w)= (yi − w0 − w1xi)xi =0 ∂w1 n i=1 \nThis means that the prediction error (yi − w0 − w1xi) does not co-vary with any linear \nfunction of the inputs (has a zero mean and does not co-vary with the inputs). (xi −x¯) \nand (w0∗+ w1∗xi) are both linear functions of inputs. \n2. (4 points) There are several numbers (statistics) computed from the data that we \ncan use to infer w∗. These are \n¯x n�1 = ¯n�1 n�1 = (xi − x¯)2 y =\n Cxx xi, yi, n n n i=1 i=1 i=1 \nn n1\n 1\n(yi − y¯)2Cxy (xi − x¯)(yi − y¯),Cyy =\n =\nn\n n\ni=1 i=1 \nSupp ose we only care about the value of w1∗. We’d like to determine w1∗on the basis \nof only two numbers (statistics) listed above. Whic h two numbers do we need for \nthis? \nWe need Cxx (spread of x) and Cxy (linear dependenc e between x and y). No justi­\nﬁcation was necessary as these basic points have appeared repeatedly in the course. \nIf we want to derive these more mathematic ally, we can, for example, look at one of \nthe answers to the previous question: \nn1 (yi − w0 ∗− w1∗xi)(xi − x¯) = 0, which we can rewrite as n i=1 \nn n n� \n1 � \n1 � \n1 yi(xi − x¯) − w0∗(xi − x¯) − w1∗xi(xi − x¯) =0 n n n i=1 i=1 i=1 \nBy using the fact that (1/n) �\ni(xi − x¯) = 0 we see that \nn n1 1 yi(xi − x¯)= (yi − y¯)(xi − x¯) = Cxyn n i=1 i=1 \nn n1 1 xi(xi − x¯)= (xi − x¯)(xi − x¯) = Cxx n n i=1 i=1 \nSubstituting these back into our equation above gives Cxy − w1∗Cxx =0. \n2\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "= 0 (x) (1/n) � in =1(yi − w0∗− w1∗xi)(xi − x¯) = 0 (x) (1/n) �n i=1(yi − w0∗− w1∗xi)(w0∗+ w1∗xi)=0 where ¯x and ¯y are ", "source_title": "54259bd801ab4823d969bb836624cf59 midterm f02soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "a346bf1c-b5e4-4cd9-b244-7b01056a31c9", "text": "x¯) = Cxyn n i=1 i=1 n n1 1 xi(xi − x¯)= (xi − x¯)(xi − x¯) = Cxx n n i=1 i=1 Substituting these back into our equation above gives Cxy − w1∗Cxx =0. 2 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3. Here we change the rules governing our access to the data. Instead of simply get­\nting the statistics we want, we have to reconstruct these from examples that we \nquery . There are two types of queries we can make. We can either request additional \nrandomly chosen examples from the training set, or we can query the output corre­\nsponding to a speciﬁc input that we specify. (We assume that the dataset is large \nenough that there is always an example whose input x is close enough to our query). \nThe active learning scenario here is somewhat diﬀeren t from the typical one. Normally \nwe would assume that the data is governed by a linear model and choose the input \npoints so as to best recover this assumed model. Here the task is to recover the best \nﬁtting linear model to the data but we make no assumptions about whether the linear \nmodel is appropriate in the ﬁrst place. \n(2 points) Supp ose in our case the input points are constrained to lie in the interval \n[0, 1]. If we followed the typical active learning approac h, where we assume that the \ntrue model is linear, what are the input points we would query? \nWe would query the extreme points x =0 and x =1 as they constr ain the linear \nfunction the most. \n(3 points) In the new setting, where we try to recover the best ﬁtting linear model \nor parameters w∗, we should (choose only one): \n( ) Query inputs as you have answ ered above \n( x ) Draw inputs and corresp onding outputs at random from the dataset \n( ) Use another strategy since neither of the above choices would yield satisfactory \nresults \n3\n \n Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "x¯) = Cxyn n i=1 i=1 n n1 1 xi(xi − x¯)= (xi − x¯)(xi − x¯) = Cxx n n i=1 i=1 Substituting these back into our equation ", "source_title": "54259bd801ab4823d969bb836624cf59 midterm f02soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "cbf1c952-525b-401c-a36b-2af6be60cf03", "text": "as you have answ ered above ( x ) Draw inputs and corresp onding outputs at random from the dataset ( ) Use another strategy since neither of the above choices would yield satisfactory results 3 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n0\n0xx\n12\n4L\n1L\n3L2L\n00xx2\n1(4 points) Brieﬂy justify your answ er to the previous question\nThe objective is to recover the least squar es solution. This solution depends on the \nfrequency of inputs and outputs in the dataset. Without additional assumptions, the \nbest thing to do is to draw a representative set of examples from the dataset so that \nthe resulting least squar es solution would approximate the solution based on the full \ndataset. \nProblem 2 \nIn this problem we will refer to the binary classiﬁcation task depicted in Figure 1(a), which \nwe attempt to solve with the simple linear logistic regression model \n1 Pˆ(y =1x,w1,w2)= g(w1x1 + w2x2)= | 1 + exp(−w1x1 − w2x2) \n(for simplicit y we do not use the bias parameter w0). The training data can be separated \nwith zero training error -see line L1 in Figure 1(b) for instance. \n(a) The 2-dimensional data set used in Prob- (b) The points can be separated by L1 (solid \nlem 1 line). Possible other decision boundaries are \nshown by L2,L3,L4. \n4 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "logistic regression", "section_heading": "as you have answ ered above ( x ) Draw inputs and corresp onding outputs at random from the dataset ( ) Use another stra", "source_title": "54259bd801ab4823d969bb836624cf59 midterm f02soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "94367e4a-225c-4b28-bace-ce0d5f4ffdfc", "text": "line L1 in Figure 1(b) for instance. (a) The 2-dimensional data set used in Prob- (b) The points can be separated by L1 (solid lem 1 line). Possible other decision boundaries are shown by L2,L3,L4. 4 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n1. (6 points) Consider a regularization approac h where we try to maximize \nn\n2� \nlog p(yi|xi,w1,w2) − C \n2 w2 \ni=1 \nfor large C. Note that only w2 is penalized. We’d like to know which of the four \nlines in Figure 1(b) could arise as a result of such regularization. For each potential \nline L2, L3 or L4 determine whether it can result from regularizing w2. If not, explain \nvery brieﬂy why not. \nL2 • \nNo. When we regularize w2, the resulting boundary can rely less on the value of x2 \nand therefore becomes more vertic al. L2 here seems to be more horizontal than the . \nunregularize d solution so it cannot come as a result of penalizing w2 \nL3 • \nYes. Here w22 is small relative to w12 (as evidenc ed by high slope), and even though \nit would assign a rather low log-probability to the observe d labels, it could be forced \nby a large regularization parameter C. \nL4 • \nNo. For very large C, we get a boundary that is entirely vertic al (line x1 =0 or \nthe x2 axis). L4 here is reﬂected across the x2 axis and represents a poorer solution \nthan it’s counter part on the other side. For moderate regularization we have to get \nthe best solution that we can construct while keeping w2 small. L4 is not the best \nand thus cannot come as a result of regularizing w2. \n2. (4 points)If we change the form of regularization to one-norm (absolute value) and \nalso regularize w1 we get the following penalized log-lik elihood \nn� \nlog p(yi|xi,w1,w2) − C \n2(|w1| + |w2|) . \ni=1 \nConsider again the problem in Figure 1(a) and the same linear logistic regression \nmodel Pˆ(y =1x,w1,w2)= g(w1x1 + w2x2). As we increase the regularization |\nparameter C which of the following scenarios do you expect to observ e (choose only \none): \n( x ) First w1 will become 0, then w2. \n( ) w1 and w2 will become zero simultaneously \n5 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "line L1 in Figure 1(b) for instance. (a) The 2-dimensional data set used in Prob- (b) The points can be separated by L1 ", "source_title": "54259bd801ab4823d969bb836624cf59 midterm f02soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "e2dfd0f8-3d82-4955-97bb-8a9585c3edfd", "text": "parameter C which of the following scenarios do you expect to observ e (choose only one): ( x ) First w1 will become 0, then w2. ( ) w1 and w2 will become zero simultaneously 5 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n( ) First w2 will become 0, then w1. \n( ) None of the weights will become exactly zero, only smaller as C increases \nThe data can be classiﬁe d with zero training error and therefore also \nwith high log-probability by looking at the value of x2 alone, i.e. making \nw1 =0. Initial ly we might prefer to have a non-zer o value for w1 but it \nwill go to zero rather quickly as we increase regularization. Note that we \npay a regularization penalty for a non-zer o value of w1 and if it doesn’t \nhelp classiﬁc ation why would we pay the penalty? The absolute value \nregularization ensur es that w1 will indeed go to exactly zero. \nAs C increases further, even w2 will eventual ly become zero. We pay \nhigher and higher cost for setting w2 to a non-zer o value. Eventual ly \nthis cost overwhelms the gain from the log-probability of labels that we \ncan achieve with a non-zer o w2. Note that when w1 = w2 =0, the \nlog-probability of labels is a ﬁnite value n log(0.5). · \n6\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "regularization", "section_heading": "parameter C which of the following scenarios do you expect to observ e (choose only one): ( x ) First w1 will become 0, ", "source_title": "54259bd801ab4823d969bb836624cf59 midterm f02soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "77689af2-cacd-44dd-be2b-eff6d1e04bc5", "text": "overwhelms the gain from the log-probability of labels that we can achieve with a non-zer o w2. Note that when w1 = w2 =0, the log-probability of labels is a ﬁnite value n log(0.5). · 6 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n−3−2−1 012345−3−2−1012345\n0.350.670.66\n0.673.781.26\n0.415.751.24 0.180.77\n8.810.670.440.740.71Figure 1: A 2-dim classiﬁcation problem, the resulting SVM decision boundary with a \nradial basis kernel, as well as the support vectors (indicated by larger circles around them). \nThe numbers next to the support vectors are the corresp onding coeﬃcien ts ˆα. \nProblem 3 \nFigure 1 illustrates a binary classiﬁcation problem along with our solution using support \nvector machines (SVMs). We have used a radial basis kernel function given by \nK(x, x�) = exp{−�x − x�� 2/2 } \nwhere �·� is a Euclidean distance and x =[x1,x2]T . The classiﬁcation decision for any x \nis made on the basis of the sign of \nwˆT φ(x)+ ˆw0 = � \nyjαˆj K(xj , x)+ ˆw0 = f(x;ˆα, wˆ0) \nj∈SV \nwhere wˆ, wˆ0,ˆαi are all coeﬃcien ts estimated from the available data displa yed in the ﬁgure \nand SV is the set of support vectors. φ(x) is the feature vector deriv ed from x corresp onding \nto the radial basis kernel. In other words, K(x, x�)= φ(x)T φ(x�). While technically φ(x) \nis an inﬁnite dimensional vector in this case, this fact plays no role in the questions below. \nYou can assume and treat it as a ﬁnite dimensional vector if you like. \nThe support vectors we obtain for this classiﬁcation problem (indicated with larger circles \nin the ﬁgure) seem a bit curious. Some of the support vectors appear to be far away from \nthe decision boundary and yet be support vectors. Some of our questions below try to \nresolv e this issue. \n7 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "overwhelms the gain from the log-probability of labels that we can achieve with a non-zer o w2. Note that when w1 = w2 =", "source_title": "54259bd801ab4823d969bb836624cf59 midterm f02soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "5aa93820-8eb9-4693-ae5f-ad5d42e54c88", "text": "ﬁgure) seem a bit curious. Some of the support vectors appear to be far away from the decision boundary and yet be support vectors. Some of our questions below try to resolv e this issue. 7 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n1. (3 points) What happ ens to our SVM predictions f(x;ˆα, wˆ0) with the radial basis \nkernel if we choose a test point xfar far away from any of the training points xj \n(distances here measured in the space of the original points)? \nThe radial basis kernel K(xfar, xj ) vanishes as the distanc e �xfar − xj � to the train­\ning point increases. The value of f(x;ˆα, wˆ0) therefore approaches wˆ0 for any point \nxfar suﬃciently far from any of the training points. \n2. (3 points) Let’s assume for simplicit y that ˆw0 = 0. What equation do all the training \npoints xj have to satisfy? Would xfar satisfy the same equation? \nIf wˆ0 =0, then all the training points will satisfy \nyiwˆT φ(xi) − 1 ≥ 0 \nsince the problem is separable. xfar cannot satisfy this equation regardless of the \nlabel associated with this point since wˆT φ(x far)= f(xfar;ˆα, 0) ≈ 0. \n3. (4 points) If we included xfar in the training set, would it become a support vector? \nBrieﬂy justify your answ er. \nxfar would have to become a support vector. Our answers to the above questions \nindicate that this point could not satisfy the margin constr aints without being include d \nin the solution f(x;ˆα, wˆ0). \n4. (T/F – 2 points) Leave-one-out cross-v alidation error is always small F \nfor support vector machines. \nThe claim is roughly the same as saying that the SVM always has a low gener alization \nerror – which is false. The question is admitte dly a little ambiguous since you could \nhave been thinking about a diﬀer ent notion of “smal l”. \nNote that the numb er of support vectors is only partial ly related to the cross-validation \nerror. We know that cross-validation error has to be smaller than the relative numb er \nof support vectors. However, we can have a large numb er of support vectors and yet \nvery small cross-validation error. This happens when many of the support vectors are \nnot “essential”. \n8 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "ﬁgure) seem a bit curious. Some of the support vectors appear to be far away from the decision boundary and yet be suppo", "source_title": "54259bd801ab4823d969bb836624cf59 midterm f02soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 7}
{"id": "b92ec5e4-87a8-40eb-93c0-f14879c819c7", "text": "the relative numb er of support vectors. However, we can have a large numb er of support vectors and yet very small cross-validation error. This happens when many of the support vectors are not “essential”. 8 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n5. (T/F – 2 points) The maxim um margin decision boundaries that F \nsupport vector machines construct have the lowest generalization error \namong all linear classiﬁers \nThe maximum margin hyperplane is often a reasonable choic e but it is by no means \noptimal in all cases. \n6. (T/F – 2 points) Any decision boundary that we get from a generativ e T \nmodel with class-conditional Gaussian distributions could in principle \nbe repro duced with an SVM and a polynomial kernel of degree less \nthan or equal to three \nA polynomial kernel of degree two suﬃc es to represent any quadr atic decision bound­\nary such as the one from the gener ative model in question. \n7. (T/F – 2 points) The decision boundary implied by a generativ e F \nmodel (with parameterized class-conditional densities) can be optimal \nonly if the class-conditional densities are correct for the problem at \nhand \nThe decision boundary may not depend on all aspects of the class-c onditional den­\nsities. For example, in the trivial case where the class-c onditional densities are the \nsame for the two classes, the optimal decision boundary is based only on the prior \nclass frequencies. We can easily reproduce this with any identic al class-c onditional \ndensities. \nProblem 4 \nConsider the following set of 3-dimensional points, sampled from two classes: \nx1 x2 x3 x1 x2 x3 \nlabeled ’1’: 1, \n0, 1, 2, −1 −2 \nlabeled ’0’: 1, \n0, 1, \n2, 2 1 \n0, −1, 1 1, −1, −1 \n0, −2, 2 1, −2, −2 \nWe have included 2-dimensional plots of pairs of features in the “Additional set of ﬁgures” \nsection (ﬁgure 3). \n1. (4 points) Explain brieﬂy why features with higher mutual information with the \nlabel are likely to be more useful for classiﬁcation task (in general, not necessarily in \nthe given example). \n9\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "the relative numb er of support vectors. However, we can have a large numb er of support vectors and yet very small cros", "source_title": "54259bd801ab4823d969bb836624cf59 midterm f02soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 8}
{"id": "c55f84ac-93dc-4263-9027-287cc2aad878", "text": "ﬁgures” section (ﬁgure 3). 1. (4 points) Explain brieﬂy why features with higher mutual information with the label are likely to be more useful for classiﬁcation task (in general, not necessarily in the given example). 9 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nA high value of I(feature;label) means that we can substantial ly reduce our uncer­\ntainty about the label by knowing the value of the feature. In other words, we have \na good idea about the label if we know the value of the feature. For classiﬁc ation we \nwant features that tell us most about the label. \n2. (3 points) In the example above, which feature (x1, x2 or x3) has the x1 \nhighest mutual information with the class label, based on the training\nset?\nClearly, both x2 and x3 have zero mutual information with the label ­\ntheir values do not appear to depend on the label. The value of x1 does\nseem to provide some information about the label since, for example,\nclass 0 has a higher chanc e of x1 =1 than class 1.\n3. (4 points) Assume that the learning is done with quadratic logistic x2,x3 \nregression, where \nP (y =1x, w)= g(w0 + w1xi + w2xj + w3xixj + w4x 2 + w5xj 2)i |\nfor some pair of features (xi,xj ). Based on the training set given above,\nwhich pair of features would result in the lowest training error for the\nlogistic regression model?\nOne could refer to plots in Figur e 3, or simply analyze the values of the\nfeatures. The values of x1,x2 are the same for two pairs examples that\nbelong to diﬀer ent classes: (1,1,-1)/(1,1,2) and (0,2,-2)/(0,2,1). We\ncannot classify all of these correctly no matter what kind of decision\nboundary we would have. The same is true for x1,x3 -consider, (1,1,­ \n1)/(1,-1,-1) and (0,-1,1)/(0,2,1). However, including x2,x3 as features,\nall the training points appear distinct (see the plot). They can also be\nseparated with a quadr atic decision boundary. This is clear from the\nﬁgure but you can also check that thresholding x2 x3 is suﬃcient for\n · \ncorrect classiﬁc ation (set all the weights to zero except w3). \n4. (T/F – 2 points) From the point of view of classiﬁcation it is always F \nbeneﬁcial to remove features that have very high variance in the data \n10\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "logistic regression", "section_heading": "ﬁgures” section (ﬁgure 3). 1. (4 points) Explain brieﬂy why features with higher mutual information with the label are l", "source_title": "54259bd801ab4823d969bb836624cf59 midterm f02soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 9}
{"id": "06e3d529-65b9-4842-9978-90ac2a461a36", "text": "all the weights to zero except w3). 4. (T/F – 2 points) From the point of view of classiﬁcation it is always F beneﬁcial to remove features that have very high variance in the data 10 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n−8 −6 −4 −2 0 2 4 6 8−6−4−20246\n+−h 1\n+\n+++−−−−The feature with the highest varianc e can still have the high­\nest mutual information with the label (as in the ﬁgure below).\n5. (T/F – 2 points) A feature which has zero mutual information with \nthe class label might be selected by a greedy selection metho d, if it \nhapp ens to impro ve classiﬁer’s performance on the training set \nThe mutual information of a single feature and the label measures how \ngood the feature is alone. Greedy selection picks features that are useful \nin conjunction with those already selected. It is possible that a feature \nwhich would be useless alone proves to be useful when combine d with \nanother. \nProblem 5 T\nFigure 2: h1 is chosen at the ﬁrst iteration of boosting; what is the weight α1 assigned to \nit? \n11\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "if ", "section_heading": "all the weights to zero except w3). 4. (T/F – 2 points) From the point of view of classiﬁcation it is always F beneﬁcial", "source_title": "54259bd801ab4823d969bb836624cf59 midterm f02soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 10}
{"id": "16b405ac-01a9-4455-8e90-6e62e51d1507", "text": "would be useless alone proves to be useful when combine d with another. Problem 5 T Figure 2: h1 is chosen at the ﬁrst iteration of boosting; what is the weight α1 assigned to it? 11 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n1. (3 points) Figure 2 shows a dataset of 8 points, equally divided among \nthe two classes (positiv e and negativ e). The ﬁgure also shows a particu­\nlar choice of decision stump h1 picked by AdaBo ost in the ﬁrst iteration. \nWhat is the weight α1 that will be assigned to h1 by AdaBo ost? (Initial \nweights of all the data points are equal, or 1/8.) \nThe weighte d training error � is 1/8 -thus α = 21 log21−\n�� = 21 log2 17\n//\n88 . \n2. (T/F – 2 points) AdaBo ost will eventually reach zero training error, \nregardless of the type of weak classiﬁer it uses, provided enough weak \nclassiﬁers have been combined. \nNot if the data in the training set cannot be separated by a linear com­\nbination of the speciﬁc type of weak classiﬁers we are using. \n3. (T/F – 2 points) The votes αi assigned to the weak classiﬁers in \nboosting generally go down as the algorithm proceeds, because the \nweighted training error of the weak classiﬁers tends to go up \nIn the course of boosting iterations the weak classiﬁers are forced to try \nto classify more diﬃcult examples. The weights will increase for exam­\nples that are repeatedly misclassiﬁe d by the weak component classiﬁers. \nThe weighte d training error of the components therefore tends to go up \nand, as a result, their votes go down. \n4. (T/F – 2 points) The votes α assigned to the classiﬁers assem bled \nby AdaBo ost are always non-negativ e \nAs deﬁne d in class, AdaBo ost will choose classiﬁers with training error \nabove 1/2. This will ensur e that log2(1−�/�), and therefore the vote, is \npositive. Note that if the classiﬁer does worse than 1/2 we can always \n“ﬂip” the sign of its predictions and therefore get a classiﬁer that does \nslightly better than 1/2. The vote assigne d to the “ﬂipp ed” classiﬁer \nwould be non-ne gative. log2 √\n7 \nF\nT\nT\n12\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "if ", "section_heading": "would be useless alone proves to be useful when combine d with another. Problem 5 T Figure 2: h1 is chosen at the ﬁrst i", "source_title": "54259bd801ab4823d969bb836624cf59 midterm f02soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 11}
{"id": "5c91fd49-d74b-46c8-831c-65dce6dbb2a0", "text": "the sign of its predictions and therefore get a classiﬁer that does slightly better than 1/2. The vote assigne d to the “ﬂipp ed” classiﬁer would be non-ne gative. log2 √ 7 F T T 12 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n−3−2−1 012345−3−2−1012345\n0.350.670.66\n0.673.781.26\n0.415.751.24 0.180.77\n8.810.670.440.740.71Additional set of ﬁgures \n0\n0xx\n12\n4L\n1L\n3L2L\n00xx2\n1\n+−h 1\n+\n+++−−−−\nthere’s more ...\n13\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n−3−2−1 0123−3−2−10123\nx1x2\n−3−2−1 0123−3−2−10123\nx1x3\n−3−2−1 0123−3−2−10123\nx2x3Figure 3: 2-dimensional plots of pairs of features for problem 4. Here ’+’ corresp onds to \nclass label ’1’ and ’o’ to class label ’0’. \n. \n14\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "for ", "section_heading": "the sign of its predictions and therefore get a classiﬁer that does slightly better than 1/2. The vote assigne d to the ", "source_title": "54259bd801ab4823d969bb836624cf59 midterm f02soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 12}
{"id": "24632e5a-70cd-43e7-a59f-0f901114aa99", "text": "6.867 Machine learning, lecture 21 (Jaakkola) \nLecture topics: \nBayesian networks \nBayesian networks \nBayesian networks are useful for representing and using probabilistic information. There \nare two parts to any Bayesian network model: 1) directed graph over the variables and 2) \nthe associated probability distribution. The graph represents qualitative information about \nthe random variables (conditional independence properties), while the associated proba-\nbility distribution, consistent with such properties, provides a quantitative description of \nhow the variables relate to each other. If we already have the distribution, why consider \nthe graph? The graph structure serves two important functions. First, it explicates the \nproperties about the underlying distribution that would be otherwise hard to extract from \na given distribution. It is therefore useful to maintain the consistency between the graph \nand the distribution. The graph structure can also be learned from available data, i.e., we \ncan explicitly learn qualitative properties from data. Second, since the graph pertains to \nindependence properties about the random variables, it is very useful for understanding \nhow we can use the probability model efficiently to evaluate various marginal and condi-\ntional properties. This is exactly why we were able to carry out efficient computations in \nHMMs. The forward-backward algorithms relied on simple Markov properties which are \nindependence properties, and these are generalized in Bayesian networks. We can make \nuse of independence properties whenever they are explicit in the model (graph). \nFigure 1: A simple Bayesian network over two independent coin flips x1 and x2 and a \nvariable x3checking whether the resulting values are the same. All the variables are binary. \nLet's start with a simple example Bayesian network over three binary variables illustrated \nin Figure 1. We imagine that two people are flipping coins independently from each other. \nThe resulting values of their unbiased coin flips are stored in binary (011) variables x1 \nand x2.Another person checks whether the coin flips resulted in the same value and the \nCite as: Tommi laakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare \n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month W]. \n2 6.867 Machine learning, lecture 21 (Jaakkola) \noutcome of the comparison is a binary (011) variable x3 =6(x1, x2). Based on the problem \ndescription we can easily write down a joint probability distribution over the three variables \nwhere P(xl) = 0.5, x1 t (0, I), P(x2) = 0.5, x2 t (0, I), and P(x3 = llxlrx2) = 1 if \nx1 =x2 and zero otherwise. \nWe could have read the structure of the joint distribution from the graph as well. We need \na bit of terminology to do so. In the graph, x1 is a parent of x3 since there's a directed edge \nfrom x1 to x3 (the value of x3 depends on xl). Analogously, we can say that x3 is a child of \nxl. Now, x2 is also a parent of x3 so that the value of x3 depends on both x1 and x2. We \nwill discuss later what the graph means more formally. For now, we just note that Bayesian \nnetworks always define acyclic graphs (no directed cycles) and represent how values of the \nvariables depend on their parents. As a result, any joint distribution consistent with the \ngraph, i.e., any distribution we could imagine associating with the graph, has to be able to \nbe written as a product of conditional probabilities of each variable given its parents. If a \nvariable has no parents (as is the case with xl) then we just write P(xl). Eq.(l) is exactly \na product of conditional probabilities of variables given their parents. \nMarginal independence and induced dependence \nLet's analyze the properties of the simple model a bit. For example, what is the marginal \nprobability over x1 and x2? This is obtained from the joint simply by summing over the \nvalues of x3 \nThus x1 and x2 are marginally independent of each other. In other words, if we don't know \nthe value of x3 then there's nothing that ties the coin flips together (they were, after all, \nflipped independently in the description). This is also a property we could have extracted \ndirectly from the graph. We will provide shortly a formal way of deriving this type of \nindependence properties from the Bayesian network. \nAnother typical property of probabilistic models is induced dependence. Suppose now that \nthe coins x1 and x2 were flipped independently but we don't know their outcomes. All \nwe know is the value of x3, i.e., whether the outcomes where identical or not (say they \nwere identical). What do we know about x1 and x2 in this case? We know that either \nx1 = x1 = 0 or x1 = x2 = 1. SO their values are clearly dependent. The dependence was \ninduced by additional knowledge, in this case the value of x3. This is again a property we \nCite as: Tommi laakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare \n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month W]. \n3 6.867 Machine learning, lecture 21 (Jaakkola) \ncould have read off directly from the graph (explained below). Note that the dependence \npertains to our beliefs about the values of x1 and x2. The coins were physically flipped \nindependently of each other and our knowledge of the value of x3 doesn't change this. \nHowever, the value of x3 narrows down the set of possible outcomes of the two coin flips \nfor this particular sample of x1 and x2. \nBoth marginal independence and induced dependence are typical properties of realistic \nmodels. Consider, for example, a factorial Hidden Markov Model in Figure 2c). In this \nmodel you have two marginally independent Markov models that conspire to generate the \nobserved output. In other words, the two Markov models are tied only through observations \n(induced dependence). To sample values for the variables in the model, we would be \nsampling from the two Markov models independently and just using the two states at each \ntime point to sample a value for the output variables. The joint distribution over the \nvariables for the model in Figure 2c) is again obtained by writing a product of conditional \nprobabilities of each variable given its parents: \nwhere, e.g., P(yl xi, xl) could be defined as N(y; p(xi) + p(xl), 0'). Such a model could, \nfor example, capture how two independent subprocesses in speech production generate the \nobserved acoustic signal, model two speakers observed through a common microphone, \nor with a different output model, capture how haplotypes generate observed genotypes. \nGiven the model and say an observed speech signal, we would be interested in inferring \nlikely sequences of states for the subprocesses. \nFigure 2: Different models represented as Bayesian networks: a) mixture model, b) HMM, \nc) factorial HMM. \nExplaining away \nCite as: Tommi laakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare \n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month W]. \n6.867 Machine learning, lecture 21 (Jaakkola) \nE=\"earthquake\" B = \"bmglr~y\" E=\"earthquake\" B = \"bmglr~y\" \na) R = \"radio report\" A=\"alrn\" b) R = 'i-adio report\" A=\"alrn\" \nFigure 3: Network structure exhibiting explaining away. a) basic model, b) alarm went off, \nc) we have also heard a radio report about an earthquake \nAnother typical phenomenon that probabilistic models can capture is ercplaining away. \nConsider the following typical example (Pearl 1988) in Figure 3. We have four variables A, \nB, E, and R capturing possible causes for why a burglary alarm went off. All the variables \nare binary (011) and, for example, A = 1 means that the alarm went off (Figure 3b). \nShaded nodes indicate that we know something about the values of these variables. In our \nexample here all the observed values are one (property is true). We assume that earthquakes \n(E = 1) and burglaries (B = 1) are equally unlikely events P(E = 1) = P(B = 1) zz 0. \nAlarm is likely to go off only if either E = 1 or B = 1 or both. Both events are equally \nlikely to trigger the alarm so that P(A = 1E,B) zz A or B. An earthquate (E = 1) is \nlikely to be followed by a radio report (R = I), P(R = 1E = 1) zz 1, and we assume that \nthe report never occurs unless an earthquake actually took place: P(R = 1E = 0) = 0. \nWhat do we believe about the values of the variables if we only observe that the alarm \nwent off (A = I)? At least one of the potential causes E = 1 or B = 1 should have \noccured. However, since both are unlikely to occur by themselves, we are basically left \nwith either E = 1 or B = 1 but (most likely) not both. We therefore have two alternative \nor competing explanations for the observation and both explanations are equally likely. If \nwe know hear, in addition, that there was a radio report about an earthquake, we believe \nthat E = 1. This makes B = 1 unnecessary for explaining the alarm. In other words, \nthe additional observation about the radio report ercplained away the evidence for B = 1. \nThus, P(E = 1lA = 1,R = 1) zz 1 whereas P(B = 1lA = 1,R = 1) zz 0. \nNote that we have implicitly captured in our calculation here that R and B are dependent \nCite as: Tommi laakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare \n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month W]. \n5 6.867 Machine learning, lecture 21 (Jaakkola) \ngiven A = 1. If they were not, we would not be able to learn anything about the value \nof B as a result of also observing R = 1. Here the effect is drastic and the variables are \nstrongly dependent. We could have, again, derived this property from the graph. \nBayesian networks and conditional independence \nWe have claimed in several occasions that we could have derived useful properties about \nthe probability model directly from the graph. How is this done exactly? Since the graph \nencodes independence properties about the variables, we have to define a criterion for \nextracting independence properties between the variables directly from the graph. For \nBayesian networks (acyclic graphs) this is given by so called D-separation criterion. \nAs an example, consider a slightly extended version of the previous model in Figure 4a, \nwhere we have added a binary variable L (whether we \"leave work\" as a result of hear- \ningllearning about the alarm). We will define a procedure for answering questions such as: \nare R and B independent given A? \nThe general procedure involves three graph transformation steps that we will illustrate in \nrelation to the graph in Figure 4a. \n1. Construct ancestral graph of the variables of interest. The variables we care about \nhere are R, B, and A. The ancestral graph includes these variables as well as all \nthe variables (ancestors) you can get to by starting from one of these variables and \nfollowing the arrows in the reverse direction (their parents, their parents' parents, \nand so on). The ancestral graph in our case is given in Figure 4b. \nThe motivation for this step is that unobserved effects of random variables cannot \nlead to dependence and can be therefore removed. \n2. Moralize the resulting ancestral graph. This operation simply adds an undirected edge \nbetween any two variables in the ancestral graph that have a common child (\"marry \nthe parents\"). In case of multiple parents, they are connected pairwise, i.e., by adding \nan edge between any two parents. See Figure 4c. \nMoralization is needed to take into account induced dependences discussed earlier \n3. Change all the direct edges into undirected edges. This gives the resulting undirected \ngraph in Figure 4d. \nWe can now read off the answer to the original question from the resulting undirected graph. \nR and B are independent given A (they are D-separated given A) if they are separated by \nA in the undirected graph. In other words, if they become disconnected in the undirected \nCite as: Tommi laakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare \n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month W]. \n6 6.867 Machine learning, lecture 21 (Jaakkola) \ngraph by removing the conditioning variable A and its associated edges. They clearly \nremain connected in our example and thus, from the point of view of the Bayesian network \nmodel, would have to be assumed to be dependent. \nLet's go back to the previous examples to make sure we can read off the properties we \nclaimed from the graphs. For example, if we are interested in asking whether x1and x2 \nare marginally independent (i.e., given nothing) in the model in Figure 1,we would create \nthe graph transformations shown in Figure 5. The nodes are clearly separated. Similarly, \nto establish that x1 and x2 become dependent with the observation of x3,we would ask \nwhether x1and x2are independent given x3and get the transformations in Figure 6. The \nnodes are not separated by x3and therefore not independent. \nE=\"earthquake\" B = \"b~rglary\" n n \nE=\"earthquake\" B = \"b~rglary\" E=\"earthquake\" B = \"b~rglary\" \nC) R = \"radio report\" A=\"alarm\" d) R = \"radio report\" A=\"alarm\" \nFigure 4: a) Burglary model, extended, b) ancestral graph of R, B, and A, c) moralized \nancestral graph, d) resulting undirected graph. \nGraph and the probability distribution \nThe graph and the independence properties we can derive from it are useful to us only if \nthe probability distribution we associate with the graph is consistent with the graph. By \nconsistency we meant that all the independence properties we can derive from the graph \nshould hold for the associated distribution. In other words, if the graph is an explicit \nrepresentation of such properties, then clearly whatever we can infer from it, should be \ntrue. There are actually a large number of possible independence properties that we can \nderive from any typical graph, even in the context of HMMs. How is it that we can ever \nhope to find and deal with distributions that are consistent with all such properties? While \nCite as: Tommi laakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare \n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month W]. \n6.867 Machine learning, lecture 21 (Jaakkola) \nFigure 5: a) Bayesian network model, b) ancestral graph of x1 and x2, already moralized \nand undirected. \nFigure 6: a) Bayesian network model, b) ancestral graph of x1 and x2 given x3, C) moralized \nancestral graph, d) resulting undirected graph. \nthe task is hard, the answer is simple. In fact, given an acyclic graph G over d variables, \nthe most general form of the joint distribution consistent with all the properties we can \nderive from the graph is given by \nwhere xPairefers to the set of variables that are the parents of variable xi (e.g., x,,, = \n{xlrx2) for x3 in the above models). So, we can just read off the answer from the graph: \nlook at each variable and include a term in the joint distribution of that variable given its \nparents (those that directly influence it). \nNote that some distributions may satisfy more independence properties that are represented \nin the graph. For example, a distribution where all the variables are independent of each \nother is consistent with every acyclic graph. It clearly satisfies all the possible independence \nproperties (edges in the graph only indicate possible dependences; they may actually be \nCite as: Tommi laakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare \n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month W]. \n8 6.867 Machine learning, lecture 21 (Jaakkola) \nweak or non-existent). We typically would use a graph representation that tries to capture \nmost if not all of the independence properties that hold for the associated distribution. \nNot all independence properties can be captured (are representable) by our D-separation \ncriterion. \nCite as: Tommi laakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare \n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month W].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "6.867 Machine learning, lecture 21 (Jaakkola) ", "source_title": "c0becfe9e6d659575a8c9e30b90f55dd lec21", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "6d8495f1-0a58-4892-90c3-920bac29c839", "text": "� � 1 6.867 Mac hine learning , lectur e 12 (Jaak kola) \nLecture topics: model selection criteria \n• Feature subset selectio n (cont’d) \n– informa tion criterio n \n– conditio nal log-likeliho od and descript ion length \n– regula rization \n• Com bination of classiﬁers, boosting \nFeature subset selecti on (cont’d) \nWe have already discussed how to carry out feature selection with the Naive Bayes model. \nThis model is easy to specify and estima te when the input vectors x are discrete (here \nvectors with binary {−1, 1} comp onents). When we elect to use only a subset J of features \nfor classiﬁcatio n, our model can be written as \n� �� � \nP (x,y)= P (xi|y) P (xi) P (y) (1) \ni∈J i�∈J \nNote that the features not used for classiﬁcatio n, indexed by j �∈ J , are assumed indep en­\ndent of the label. This way we still have a distribut ion over the origina l input vector s x and \nlabels but assert that only some of the comp onen ts of x are relevant for classiﬁcatio n. The \nprobabilities involved such as P (xi|y) are obtained directly from empirical counts involving \nxi and y (see previo us lecture). \nThe selection criterion we arriv ed at indic ated that, as far as the log-likeliho od of all the \ndata is concerned, we should include features (replace P (xi) with P (xi|y) in the above \nmodel) in the decreasing order of \nIˆ(Xi; Y )= Hˆ(Xi) − Hˆ(Xi|Y ) (2) \nwhere the entropies Hˆ(Xi) and Hˆ(Xi|Y ) are evaluated on the basis of the estimat ed prob­\nabilit ies for the Naiv e Bayes model. The disadv antage of this criterio n is that the features \nare selected individually , i.e., witho ut regard to how eﬀectiv e they migh t be in speciﬁc \ncombinations with each other. This is a direct result of the Naiv e Bayes model as well as \nthe fact that we opted to ﬁnd features that maximally help increase the log-lik elihood of \nall the data. This is clearly slightly diﬀeren t from trying to classify examples accurately . \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "� � 1 6.867 Mac hine learning , lectur e 12 (Jaak kola) ", "source_title": "fa94ca034b2700e553744c689fb9360e lec12", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "cc2b0471-9695-4e71-97d0-c30f458c1957", "text": "model as well as the fact that we opted to ﬁnd features that maximally help increase the log-lik elihood of all the data. This is clearly slightly diﬀeren t from trying to classify examples accurately . Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2 6.867 Mac hine learning , lectur e 12 (Jaak kola) \nFor example, from the point of view of classiﬁcation, it is not neces sary to model the distri­\nbutio n over the featur e vectors x. All we care about is the condit ional probabilit y P (y|x). \nWhy should our estimat ion and the feature selection criterion then aim to increase the \nlog-likeliho od of genera ting the featur e vector s as well? We will now turn to impro ving the \nfeature selection part with MDL while still estima ting the proba bilities P (xi|y) and P (y) \nin closed form as before. \nFeature subset selecti on with MDL \nLet’s assume we have estimated the parameters in the Naiv e Bayes model, i.e., Pˆ(xi|y) and \nPˆ(y), as before by maximizing the log-lik eliho od of all the data. Let’s see what this model \nimplies in terms of classiﬁcat ion: \nPˆ(y =1x)= Pˆ(x,y =1) = ˆ1 (3) |\nPˆ(x,y = 1)+ Pˆ(x,y = −1) P (x,y=−1)1+ Pˆ(x,y=1) \n1 1 = = (4) Pˆ(x,y=1) 1 + exp(− log Pˆ(x,y=−1) ) 1 + exp(−fˆ(x)) \nwhere \nfˆ(x) = log Pˆ(x,y = 1) (5) \nPˆ(x,y = −1) \nis the discrimina nt function arising from the Naive Bayes model. So, for examp le, when \nfˆ(x) > 0, the logistic function implies that Pˆ(y =1|x) > 0.5 and we would classify \nthe example as positive. If fˆ(x) is a linear function of the input s x, then the form of \nthe conditiona l probabilit y from the Naive Bayes model would be exactly as in a logistic \nregression model. Let’s see if this is indeed so. \nfˆ(x) = log Pˆ(x,y =1) = � \nlog Pˆ(xi|y = 1) + log Pˆ(y = 1) (6) \nPˆ(x,y = −1) i∈J Pˆ(xi|y = −1) Pˆ(y = −1) \nNote that only terms tried to the labels remain. Is this a linear function of the binar y \nfeatures xi? Yes, it is. We can write each term as a linear function of xi as follows: \nlog Pˆ(xi|y =1) = δ(xi, 1)log Pˆ(xi =1|y =1) + δ(xi, −1)log Pˆ(xi = −1|y = 1) (7) \nPˆ(xi|y = −1) Pˆ(xi =1|y = −1) Pˆ(xi = −1|y = −1) \n= xi + 1 log Pˆ(xi =1|y =1) +1 − xi log Pˆ(xi = −1|y = 1) (8) 2 Pˆ(xi =1|y = −1) 2 Pˆ(xi = −1|y = −1) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "model as well as the fact that we opted to ﬁnd features that maximally help increase the log-lik elihood of all the data", "source_title": "fa94ca034b2700e553744c689fb9360e lec12", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "9aada6cc-6d13-4f92-af9a-ab7e7b95a3f4", "text": "= −1) Pˆ(xi = −1|y = −1) = xi + 1 log Pˆ(xi =1|y =1) +1 − xi log Pˆ(xi = −1|y = 1) (8) 2 Pˆ(xi =1|y = −1) 2 Pˆ(xi = −1|y = −1) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� \n� � 3 6.867 Mac hine learning , lectur e 12 (Jaak kola) \nBy combining these terms we can write the discrimina nt function in the usual linear form \nfˆ(x)= wˆixi +ˆw0 (9) \ni∈J \nwhere the parameters ˆwi and ˆw0 are functio ns of the Naiv e Bayes conditio nal probabilit ies. \nNote that while we ended up with a logistic regression model, the parameters of this \nconditio nal distributio n, ˆwi and ˆw0, have been estima ted quite diﬀeren tly from the logistic \nmodel. \nWe can now evaluate the conditiona l proba bility Pˆ(y|x, J ) from the Naive Bayes model \ncorrespondin g to any subset J of relev ant features. Let’s go back to the featur e selection \nproblem. The subset J should be optimize d to maximize the conditio nal log-likeliho od of \nlabels given examples. Equivalently, we can minimize the descriptio n length \nn\nDL-dat a(J )= − log Pˆ(yt|xt, J ) (10) \nt=1 \nwith respect to J . This is a criterion that evaluates how useful the features are for classiﬁ­\ncatio n and it can be no longer reduced to evaluating features indep enden tly of others. This \nalso means that the optimizat ion problem for ﬁnding J is a diﬃcult one. The simplest \nway of appro xima tely minimizing this criterion would be to start with no features, then \ninclude the single best feature, followed by the second featur e that works best with the ﬁrst \none, and so on. Note that in this simple setting the classiﬁer parameters associated with \ndiﬀeren t subsets of features are ﬁxed by the Naiv e Bayes model; we only optimize over the \nsubse t of features. \nThe above sequential optimizatio n of the criterio n would yield features that are more useful \nfor classiﬁcation than ranking them by mutual informat ion (the ﬁrst feature to include \nwould be the same, however). But we do not yet have a criterio n for deciding how many \nfeatures to include. In the MDL termino logy, we need to describ e the model as well. The \nmore featur es we include the more bits we need to describ e both the set and the parameters \nassociated with using that set (the Naive Bayes conditiona l probabilit ies). \nSo, ﬁrst we need to communicate the set (size and the elements). The (any) integer |J|\ncan be comm unicated with the cost of \nlog∗ |J | = log |J| + log log |J| + ... (11) \nnats. The eleme nts in the set, assuming (a priori) that they are drawn uniformly at rando m \nfrom d possible featur es, require \nd log (12) |J| \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "= −1) Pˆ(xi = −1|y = −1) = xi + 1 log Pˆ(xi =1|y =1) +1 − xi log Pˆ(xi = −1|y = 1) (8) 2 Pˆ(xi =1|y = −1) 2 Pˆ(xi = −1|y", "source_title": "fa94ca034b2700e553744c689fb9360e lec12", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "47deb58f-43cb-4a95-bb53-3c9db6e59831", "text": "|J| + log log |J| + ... (11) nats. The eleme nts in the set, assuming (a priori) that they are drawn uniformly at rando m from d possible featur es, require d log (12) |J| Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n| \n� � \n� \n� � 4 6.867 Mac hine learning , lectur e 12 (Jaak kola) \nnats. Finally , we need to communicate the Naive Bayes parameters associated with the \neleme nts in the feature set. Pˆ(y) can only take n + 1 possible values and thus requires \nlog(n + 1) nats. Once we have sent this distributio n, the receiv er will have access to the \ncoun ts ny(1) and ny(−1) in additio n to n that they already knew . Pˆ(xi|y = 1) can only \ntake ny(1)+1 possible v alues with the cost of log(ny(1)+1), and simila rly for Pˆ(xi|y = −1). \nGiven that there are |J of these , the total communicat ion cost for the model is then \nd DL-mo del(|J |) = log∗ |J | + log + (13) \n�|J | � \nlog(n + 1) + |J| log(ny(1) + 1) + log(ny(−1)+1) (14) \nNote that the model cost only depends on the size of the feature set, not the elemen ts in \nthe set. We would ﬁnd the subset of features by minimizing \nDL(J ) = DL-dat a(J ) + DL-model(|J|) (15) \nsequen tially or otherwise. \nFeature select ion via regul arization \nAn alterna tive to explicitly searching for the right featur e subse t is to try to formulate the \nselection problem as a regula rizatio n problem. We will no longer use parameters from a \nNaiv e Bayes model but instead work directly with a logistic regression model \n� d � \nP (y|x,θ)= g θixi + θ0 (16) \ni=1 \nThe typical regular ization problem for estimating the parameters θ would be penalized \nconditio nal log-likeliho od with the squar ed norm regulariza tion \nn d\nJ(θ; Sn) = log P (yt|xt,θ) − λ θ2 (17) i \nt=1 i=1 \nThe squared norm regularizat ion won’t work for our purp oses, however. The reason is that \nnone of the parameters would be set exactly to zero as a result of solving the optimizati on \nproblem (none of the features would be clearly selected or excluded). We will instead use \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "|J| + log log |J| + ... (11) nats. The eleme nts in the set, assuming (a priori) that they are drawn uniformly at rando ", "source_title": "fa94ca034b2700e553744c689fb9360e lec12", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "d17a3c77-1b8a-4884-8b06-1b33db534da2", "text": "reason is that none of the parameters would be set exactly to zero as a result of solving the optimizati on problem (none of the features would be clearly selected or excluded). We will instead use Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� �� � \n� � 5 6.867 Mac hine learning , lectur e 12 (Jaak kola) \na one-norm regula rization penalty: \nl(θ;Sn) \nn d\nJ(θ; Sn) = log P (yt|xt,θ) −λ |θi| (18) \nt=1 i=1 \nIf we let λ →∞, then all θi (except for θ0) would go to zero. On the other hand, when \nλ = 0, none of θi are zero. These are identical to the squared penalt y. However, the \nintermedia te values of λ produce more interesting results: some of θi will be set exactly to \nzero while this would never happen with the squared penalt y. Let’s try to understa nd why. \nSupp ose we ﬁx all but a single parameter θk to their optimized values (zero or not) and \nview l(θ; Sn) as a functio n of this parameter. Supp ose the optimal value of θk > 0, then \nd l(θ; Sn) − λ = 0 (19) dθk \nat the optimal θk. In other words, the slope of the conditiona l log-likelihood function \nrelative to this parameter has to be exactly λ. If parameter θk is irrelevant then the \nconditio nal log-lik eliho od is unlik ely to be aﬀected much by the parameter, keeping the \nslope very small. In this case, θk would be set exactly to zero. The larger the value of λ, \nthe more of the parameters would end up going to zero. Another nice prop erty is that if \nthe optim um value of θk is zero, then we would get the same regression model whether or \nnot the corresp onding feature were included in the model to begin with. A good value of \nλ, and therefore a good number of features, can be found via cross- valida tion. \nSo why don’t we get the same “zero ing” of the paramet ers with the squar ed penalty? \nConsider the same situat ion, ﬁx all but θk. Then at the optimum \nd l(θ; Sn) − 2λθ k = 0 (20) dθk \nHowever small the slope of the log-likeliho od is at θk = 0, we can move the parameter just \nabove zero so as to satisfy the above optimalit y condition. There’s no ﬁxed lower bound \non how useful the parameter has to be for it to take non-zero values. \nCombining classiﬁers and boosting \nWe have so far discusse d feature selection as a problem of ﬁnding a subset of features out \nof d possible ones. In many cases the possible feature set available to us may not even \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "reason is that none of the parameters would be set exactly to zero as a result of solving the optimizati on problem (non", "source_title": "fa94ca034b2700e553744c689fb9360e lec12", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "0db4d912-da8a-421d-90e1-058fad7d1b9f", "text": "boosting We have so far discusse d feature selection as a problem of ﬁnding a subset of features out of d possible ones. In many cases the possible feature set available to us may not even Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � \n� \n� 6 6.867 Mac hine learning , lectur e 12 (Jaak kola) \nbe enumerable. As an example , consider real valued input vectors x =[x1,...,xd]d, still \nd−dimensional. We can turn this real valued vector into a featur e vector of binary {−1, 1}\nfeatures in a number of diﬀerent ways. One possibilit y is to run simple classiﬁers on the \ninput vector and collect the predicted class labels into a feature vector. One of the simplest \nclassiﬁers is the decision stump: \nh(x; θ) = sign s(xk − θ0) (21) \nwhere θ = {s,k,θ0} and s ∈ {−1, 1} speciﬁes the label to predic t on the positiv e side of \nxk − θ0. In other words, we simply select a speciﬁc comp onen t and threshold its value. \nThere are still an uncoun table number of such classiﬁers, even based on the same input \ncomp onen t since the threshold θ0 is real valued. Our “feature selection” problem here is to \nﬁnd a ﬁnite set of such stumps. \nIn order to determine which stumps (“featur es”) would be useful we have to decide how \nthey will be exploited. There are many ways to do this. For example , we could run a linea r \nclassiﬁer based on the resulting binar y feature vectors \nφ(x; θ)=[h(x; θ1),...,h(x; θm)]T (22) \nWe will instea d collect the output s of the simple stumps into an ensemble: \nm\nhm(x)= αj h(x; θj ) (23) \nj=1 \nwhere αj ≥ 0 and m αj = 1. We can view the ensem ble as a voting combination. j=1 \nGiven x, each stump votes for a label and it has αj votes. The ensemble then classiﬁes \nthe example according to which label received the most votes. Note that hm(x) ∈ [−1, 1]. \nhm(x) = 1 only if all the stumps agree that the label should be y = 1. The ensemble is a \nlinear classiﬁer based on φ(x; θ), for a ﬁxed θ, but with constr ained parameters. However, \nour goal to learn both which features to include, i.e., h(x; θj )’s, as well as how they are \ncombined (the α’s). This is a diﬃcult problem solve. \nWe can combine any set of classiﬁers into an ensem ble, not just stumps. For this reason, \nwe will refer to the simple classiﬁers we are combining as base learner s or base classiﬁers \n(also called weak learners or comp onen t classiﬁers). Note that the process of combining \nsimple “weak” classiﬁers into one “strong” classiﬁer is analog ous to the use of kernels to go \nfrom a simple linear classiﬁer to a non-linear classiﬁer. The diﬀerenc e is that here we are \nlearning a small number of highly non-line ar features from the inputs rather than using a \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "boosting We have so far discusse d feature selection as a problem of ﬁnding a subset of features out of d possible ones.", "source_title": "fa94ca034b2700e553744c689fb9360e lec12", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "816a7e15-3328-44d8-a7fc-4994f87d141d", "text": "kernels to go from a simple linear classiﬁer to a non-linear classiﬁer. The diﬀerenc e is that here we are learning a small number of highly non-line ar features from the inputs rather than using a Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� 7 6.867 Mac hine learning , lectur e 12 (Jaak kola) \nﬁxed process of genera ting a large number of featur es from the inputs as in the polynom ial \nkernel. \nThe ensembles can be useful even if generated through rando mization. For example , we can \ngenera te rando m subsets of sma ller training sets from the origina l one, and train a classiﬁer, \ne.g., an SVM, based on each such set. The outputs of the SVM classiﬁers, trained with \nslightly diﬀerent training sets, can be combined into an ensem ble with unifo rm α’s. This \nprocess, known as bagging, is a metho d of reducing the varianc e of the resulting classiﬁer. \nThe unifo rm weighting will not help with the bias. \nNow, let’s ﬁgure out how to train ensem bles. We will need a loss function and there many \npossibilities, including the logistic loss. For simplicity, we will use the exponential loss: \nLoss(y,h(x)) = exp(−yh(x)) (24) \nThe loss is small if the ensem ble classiﬁe r agrees with the label y (the smaller the stronger \nthe agreement). It is large if the ensem ble strongly disagrees with the label. This is the \nbasic loss function is in an ensem ble metho d called Boosting. \nThe simplest way to optimize the ensemble is to do it in stages. In other words, we will \nﬁrst ﬁnd a single stump (an ensem ble of one), then add another while keeping the ﬁrst one \nﬁxed, and so on, never retra ining those already added into the ensem ble. To facilita te this \ntype of estimat ion, we will assume that αj ≥ 0 but won’t require that they will sum to one \n(we can always renormalize the votes after having trained the ensem ble). \nSupp ose now that we have already added m − 1 base learners into the ensem ble and call \nthis ensemble hm−1(x). This part will be ﬁxed for the purp ose of adding αmh(x; θm). We \ncan then try to minimize the training loss corresp onding to the ensemble \nm−1\nhm(x) = αˆj h(x; θˆj )+ αmh(x; θm) (25) \nj=1 \n= hm−1(x)+ αmh(x; θm) (26) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "kernels to go from a simple linear classiﬁer to a non-linear classiﬁer. The diﬀerenc e is that here we are learning a sm", "source_title": "fa94ca034b2700e553744c689fb9360e lec12", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "be6ed4db-c208-47fb-b584-5e0b79286a15", "text": "purp ose of adding αmh(x; θm). We can then try to minimize the training loss corresp onding to the ensemble m−1 hm(x) = αˆj h(x; θˆj )+ αmh(x; θm) (25) j=1 = hm−1(x)+ αmh(x; θm) (26) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� \n� \n� 8 6.867 Mac hine learning , lectur e 12 (Jaak kola) \nTo this end, \nn\nJ(αm,θm) = Loss(yt,hm(xt)) (27) \nt=1 \nn � � \n= exp − ythm−1(xt) − ytαmh(x t; θm) (28) \nt=1 \nWm−1(t) \nn���� ��� � \n= exp − ythm−1(xt) exp − ytαmh(x t; θm) (29) \nt=1 \nn � � \n= Wm−1(t)exp − ytαmh(x t; θm) (30) \nt=1 \nIn other words, for the purp ose of estimat ing the new base learner, all we need to know \nfrom the previous ensem ble are the weights Wm−1(t) associated with the training examples. \nThes e weights are exactly the losses of the m − 1 ensemble on each of the training example. \nThus, the new base learner will be “directed” towards examples that were misclassiﬁed by \nthe m − 1 ensemble hm−1(x). \nThe estimatio n probl em that couples αm and θm is still a bit diﬃcult. We will simplify \nthis further by ﬁguring out how to estimat e θm ﬁrst and then decide the votes αm that we \nshould assign to the new base learner (i.e., how much we should rely on its predictions). \nBut what is the criterion for θm indep endent of αm? Consider as a thoug ht experimen t \nthat we calculat e for all possible θm the deriv ative \n� md �� \nJ(αm,θm)� = − Wm−1(t)y th(x t; θm) (31) dαm αm=0 t=1 \nThis derivative tells us how much we can reduce the overall loss by increasing the vote \n(from zero) of the new base learner with parameters θm. This deriv ative is expected to be \nnegative so that the training loss is decreased by addin g the new base learner. It makes \nsense then to ﬁnd a base learner h(x; θm), parameters θm, so as to minimiz e this derivative \n(making it as negative as possible). Such a base learner would be expected to lead to a \nlarge reductio n of the training loss. Once we have this θˆm we can subsequen tly optimize \nthe trainin g loss J(αm,θˆm) with respect to αm for a ﬁxed θˆm. \nWe have now essentially all the components to deﬁne the Adaboost algorithm. We will \nmake one modiﬁcatio n which is that the weights will be normalized to sum to one. This is \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "purp ose of adding αmh(x; θm). We can then try to minimize the training loss corresp onding to the ensemble m−1 hm(x) = ", "source_title": "fa94ca034b2700e553744c689fb9360e lec12", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 7}
{"id": "0460c7e7-9048-452d-8d72-3091f8a2ebe6", "text": "for a ﬁxed θˆm. We have now essentially all the components to deﬁne the Adaboost algorithm. We will make one modiﬁcatio n which is that the weights will be normalized to sum to one. This is Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� � \n� � 9 6.867 Mac hine learning , lectur e 12 (Jaak kola) \nadvantageous as they can become rather small in the course of adding the base learners. \nThe normalizat ion won’t chang e the optimiza tion of θm nor αm. We denot e the normalized \nweights with W˜m−1(t). The boosting algorith m is deﬁned as \n(0) Set W0(t)=1/n for t =1,...,n. \n(1) At stage m, ﬁnd a base learner h(x; θˆm) that approximately minimizes \nm\n− W˜m−1(t)y th(x t; θm)=2�m − 1 (32) \nt=1 \nwhere �m is the weighted classiﬁcatio n error on the training examples, weighted by \nthe norma lized weights W˜m−1(t). \n(2) Set \nαˆm =0.5 log 1 − ˆ�m (33) �ˆm \nwhere ˆ�m is the weighted error corresponding to θˆm chosen in step (1). The value ˆαm \nis the closed form solution for αm that minim izes J(αm,θˆm) for ﬁxed θˆm. \n(3) Update the weights on the training example s \nW˜m(t)= cm · W˜m−1(t)exp − ytαˆmh(x t; θˆm) (34) \nwhere cm is the normaliza tion consta nt to ensure that W˜m(t) sum to one. The \nnew weights can be interpreted as norma lized losses for the new ensem ble hm(xt)= \nhm−1(x)+ˆαmh(x; θˆm). \nThe Adaboost algorithm sequen tially adds base learners to the ensemble so as to decrease \nthe trainin g loss. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "for a ﬁxed θˆm. We have now essentially all the components to deﬁne the Adaboost algorithm. We will make one modiﬁcatio ", "source_title": "fa94ca034b2700e553744c689fb9360e lec12", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 8}
{"id": "6c8ebcfe-99c5-4938-940e-6457f7842a76", "text": "1 6.867 Mac hine learning , lectur e 23 (Jaak kola) \nLecture topics: \nMark ov Random Fields • \nProba bilistic inference • \nMarkov Random Fields \nWe will brieﬂy go over undir ected graphic al models or Markov Random Fields (MRFs) as \nthey will be needed in the context of probabilistic inference discussed below (using the \nmodel to calcula te various probabilit ies over the variables). The origin of these models is \nphysics (e.g., spin glass) and they retain some of the termino logy from the physics litera ture. \nThe semantics of MRFs is similar but simpler than Bayesian networks. The graph again \nrepresen ts indep endenc e properties between the variables but the properties can be read \noﬀ from the graph through simple graph separa tion rather than D-separation criterio n. So, \nfor examp le, \nx\n1 x2 \nx3 x4 \nencodes two indep endence properties. First, x1 is indep enden t of x4 given x2 and x3. In \nother words, if we remove x2 and x3 from the graph then x1 and x4 are no longer connected. \nThe second property is that x2 is indep enden t of x3 given x1 and x4. Incide ntally, we \ncouldn’t deﬁne a Bayesian network over the same four variables that would explicate both \nof these properties (you can capture one while failing the other). So, in terms of their \nabilit y to explicate indep endence properties, MRFs and Bayesian networks are not strict \nsubse ts of each other. \nBy Hammer sley-Cliﬀ ord theorem we can specify the form that any joint distribution con­\nsisten t with an undirected graph has to take. A distribution is consisten t with the graph \n Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "1 6.867 Mac hine learning , lectur e 23 (Jaak kola) ", "source_title": "b1139f1648df9d4cfe7fd4cc09c3b259 lec23", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "794459a0-79df-49c3-a964-dfb3ad2cd7f8", "text": "ts of each other. By Hammer sley-Cliﬀ ord theorem we can specify the form that any joint distribution con­ sisten t with an undirected graph has to take. A distribution is consisten t with the graph Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2 6.867 Mac hine learning , lectur e 23 (Jaak kola) \nif it satisﬁe s all the conditio nal indep endence properties we can read from the graph. The \nresult is again in terms of how the distribution has to factor. For the above example, we \ncan write the distribution as a product of (non-negative) potential functions over pairs of \nvariables that specify how the variables depend on each other \n1 P (x1,x2,x3,x4)= ψ12(x1,x2)ψ13(x1,x3)ψ24(x2,x4)ψ34(x3,x4) (1) Z \nwhere Z is a norma lizatio n consta nt (required since the potential functions can be any non­\nnegative functio ns). The distributio n is therefore globally normalize d. More generally , an \nundirected graph places no constr aints on how any fully connected subset of the variables, \nvariables in a clique, depend on each other. In other words, we are free to associate any \npotential function with such variables. Witho ut loss of genera lity we can restrict ourselv es \nto maximal cliques, i.e., not consider separa tely cliques that are subsets of other cliques. \nIn the above example, the maxima l cliques where the pairs of connected variables. Now, in \ngenera l, the Hammersley-Cliﬀ ord theor em states that the joint distribution has to facto r \naccor ding to (maximal) cliques in the graph: \n1 � \nP (x1,...,xn)= ψc(xc) (2) Z c∈C \nwhere c ∈C is a (max imal) clique in the graph and xc = {xi}i∈c denotes the set of variables \nin the clique. The normalization constan t Z could be easily absorb ed into one of the \npotential functions but we will write it explicitly here as a reminder that the model has \nto be norma lized globa lly (is not automa tically norma lized as Bayesian networks). Figure \nbelow provides an example of a graph with three maxima l cliques. \nc1 \nx1 x2 c2 \nx3 x4 \nx5 c3 \nC = {c1,c2,c3} \n Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "ts of each other. By Hammer sley-Cliﬀ ord theorem we can specify the form that any joint distribution con­ sisten t with", "source_title": "b1139f1648df9d4cfe7fd4cc09c3b259 lec23", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "5c3db012-371b-4442-9a59-95f4de6c813b", "text": "lized globa lly (is not automa tically norma lized as Bayesian networks). Figure below provides an example of a graph with three maxima l cliques. c1 x1 x2 c2 x3 x4 x5 c3 C = {c1,c2,c3} Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3 6.867 Mac hine learning , lectur e 23 (Jaak kola) \nBayesian networks as undire cted models \nWe can always turn a Bayesian network into a MRF via moralization, i.e., connecting \nall the parents of a commo n child and dropping the directions on the edges. After the \ntransformat ion we natur ally still have the same proba bility distribut ion but may no longer \ncaptur e all the indep endence properties explicitly in the graph. For example, in \nP (x1, x2, x3) = P (x1)P (x2)P (x3|x1, x2) P (x1, x2, x3) = ψ(x1, x2, x3) x1 x2 x1 x2 \nx3 x3 \nwhere, clearly , ψ(x1,x2,x3)= P (x1)P (x2)P (x3|x1,x2) so that the underlying distributions \nare the same (only the repres entation changed). The undirec ted graph is fully connected, \nhowever, and the marg inal indep endence of x1 and x2 is no longer visible in the graph. In \nterms of probabilistic inferenc e, i.e., calculat ing various proba bilities, little is typica lly lost \nby turning a Bayesian network ﬁrst into an undirected model. For example, we would often \nhave some evidence pertaining to the variables, something would be known about x3 and, \nas a result, x1 and x2 would become dependen t. The advantage from the transfor matio n \nis that the inference algorithms will run uniformly on both types of models. \nLet’s consider one more example of turning Bayesian networks into MRFs. The ﬁgure \nbelow gives a simple HMM with the associated probabilit y model and the same for the \nundirected version after moralizatio n. \nEach conditi onal probabilit y on the left can be assigned to any potential function that \ncontains the same set of variables. For example, P (x1) could be included in ψ12(x1,x2) or \nin φ1(x1,y1). The objectiv e is merely to maintain the same distribu tion when we take the \n×P (y1|x1)P (y2|x2)P (y3|x3) x1 \ny1 x2 \ny2 x3 \ny3 \nP (x1)P (x2|x1)P (x3|x2)× \n×φ1(x1, y1)φ2(x2, y2)φ3(x3, y3) x1 \ny1 x2 \ny2 x3 \ny3 \n1 \nZ ψ12(x1, x2)ψ23(x2, x3)× \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "lized globa lly (is not automa tically norma lized as Bayesian networks). Figure below provides an example of a graph wi", "source_title": "b1139f1648df9d4cfe7fd4cc09c3b259 lec23", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "ef904817-e263-46a1-8090-eae1c9e8bbd7", "text": "same distribu tion when we take the ×P (y1|x1)P (y2|x2)P (y3|x3) x1 y1 x2 y2 x3 y3 P (x1)P (x2|x1)P (x3|x2)× ×φ1(x1, y1)φ2(x2, y2)φ3(x3, y3) x1 y1 x2 y2 x3 y3 1 Z ψ12(x1, x2)ψ23(x2, x3)× Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n4 6.867 Mac hine learning , lectur e 23 (Jaak kola) \nproduct (it doesn’t matter how we reorder terms in a product) . Here’s a possible complete \nsetting of the potential functio ns: \nZ = 1 (already norma lized as we start with a Bayesian network) (3) \nψ12(x1,x2)= P (x1)P (x2|x1) (4) \nψ23(x2,x3)= P (x3|x2) (5) \nφ1(x1,y1)= P (y1|x1) (6) \nφ1(x2,y2)= P (y2|x2) (7) \nφ1(x3,y3)= P (y3|x3) (8) \nProba bilistic inference \nOnce we have the graph and the associated distri bution (either learned from data or given \nto us), we would like to make use of this distribution. For example, in HMM s discus sed \nabove, we could try to compute P (y3|y1,y2), i.e., predict what we expect to see as the \nnext observation having already seen y1 and y2. Note that the sequence of observ ations \nin an HMM does not satisfy the Mark ov property, i.e., y3 is not indep enden t of y1 given \ny1. This is easy to see from either Bayesian network or the undirected version via the \nsepara tion criter ia. You can also unde rstand it by noting that y1 may inﬂuence the state \nsequenc e, and therefor e which value x3 takes provided that y2 does not fully constra int x2 \nto take a speciﬁc value. We are also often interested in diagnostic probabilities such as \nP (x2|y1,y2,y3), the posterior distributio n over the states x2 at t = 2 when we have already \nobserv ed y1,y2,y3. Or we may be interested in the most likely hidden state sequence and \nneed to evaluate max-proba bilities. All of these are probabilistic inference calculatio ns. \nIf we can compute basic conditiona l probabilit ies such as P (x2|y2), we can also evaluate \nvarious other quantities. For example, supp ose we have an HMM \nx1 x2 x3 \ny1 y2 y3 \nand we are interes ting known which y’s we should observ e (query ) so as to obtain the most \ninforma tion about x2. To this end we have to deﬁne a value of new informat ion. Consider, \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "same distribu tion when we take the ×P (y1|x1)P (y2|x2)P (y3|x3) x1 y1 x2 y2 x3 y3 P (x1)P (x2|x1)P (x3|x2)× ×φ1(x1, y1)", "source_title": "b1139f1648df9d4cfe7fd4cc09c3b259 lec23", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "a0192cb0-12b0-40c9-8ed1-c37deea1e092", "text": "are interes ting known which y’s we should observ e (query ) so as to obtain the most informa tion about x2. To this end we have to deﬁne a value of new informat ion. Consider, Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n|� |\n� � � � 5 6.867 Mac hine learning , lectur e 23 (Jaak kola) \nfor examp le, deﬁning \nValue({P (x2yi)}x2=1,...,m)= −Entropy(x2yi) (9) \n= P (x2|yi)log P (x2|yi) (10) \nx2 \nIn other words, if given a speciﬁc observ ation yi, we can evaluate the value of the resulting \nconditio nal distribut ion P (x2|yi) where yi is known. There are many ways to deﬁne the \nvalue and, for simplicit y, we deﬁned it in terms of the uncerta inty about x2. The value \nis zero if we know x2 perfectly and negative otherwise. Since we cannot know yi prior \nto querying its value, we will have to evaluat e its expected value assuming our HMM is \ncorrect: the expected value of information in response to querying a value for yi is given by \nP (yi) P (x2|yi)log P (x2|yi) (11) \nyi x2 \nwhere P (yi) is a marginal proba bility computed from the same HMM. We could now use \nthe above criterio n to ﬁnd the observation most helpful in determining the value of x2. Note \nthat all the proba bilities we needed were simple marg inal and cond itiona l probabi lities . We \nstill need to discuss how to evaluate such proba bilities eﬃciently from a given distributi on. \nBelief propagat ion \nBelief propagation is a simple message passing algorithm that genera lizes the forward-\nbackward algorithm. It is exact on undirected graphs that are trees (a graph is a tree if \nany pair of nodes have a unique path connecting them, i.e., has no loops). In case of more \ngenera l graphs, we can cluster variables together so as to obtain a tree of clusters, and \napply the same algorithm again, now on the level of clusters. \nWe will begin by demo nstrating how messages are comput ed in the belief propag ation algo­\nrithm. Conside r the problem of evaluating the marginal probabilit y of x3 in an undirec ted \nHMM \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "are interes ting known which y’s we should observ e (query ) so as to obtain the most informa tion about x2. To this end", "source_title": "b1139f1648df9d4cfe7fd4cc09c3b259 lec23", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "32751cc0-5a91-4509-81a3-453de04ee2c6", "text": "clusters. We will begin by demo nstrating how messages are comput ed in the belief propag ation algo­ rithm. Conside r the problem of evaluating the marginal probabilit y of x3 in an undirec ted HMM Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� � \n� 6 6.867 Mac hine learning , lectur e 23 (Jaak kola) \nmy2→x2 (x2) x1 \ny1 x2 \ny2 x3 \ny3 \n1 \nZ ψ12(x1, x2)ψ23(x2, x3)× \n×φ 1(x1, y1)φ2(x2, y2)φ3(x3, y3) mx1→x2 (x2) mx2→x3 (x3) \nmy1→x1 (x1) my3→x3 (x3) \nWe can perform the required marg inalizat ions (summing over the other variables) in order: \nﬁrst y1, then x1, then y2, and so on. Each of such operatio ns will aﬀect the variables they \ninteract with. This eﬀect is captured in terms of messag es that are shown in red in the \nabove ﬁgure. For example, my1→(x1) is a message, a function of x1, that summarizes the x1 \neﬀect of marginalizing over y1. It is computed as \nmy1→x1 (x1)= φ1(x1,y1) (12) \ny1 \nNote that in the absence of any evidence about y1, φ(x1,y1)= P (y1|x1) and we would \nsimply get a consta nt functio n of x1 as the message. Supp ose, instead, that we had already \nobserv ed the value of y1 and denot e this value as ˆy1. We can incorporate this observation \n(evidence about y1) into the potential function φ1(x1,y1) by redeﬁning it as \nφ1(x1,y1)= δ(y1,yˆ1)P (y1|x1) (13) \nThis way the message would be calculat ed as before but the value of the message as a \nfunction of x1 would certa inly change: \nmy1→x1 (x1)= φ1(x1,y1)= δ(y1,yˆ1)P (y1|x1)= P (ˆy1|x1) (14) \ny1 y1 \nAfter comple ting the marg inaliza tion over y1, we turn to x1. This marginaliza tion results \nin a message mx1→(x2) as x2 relates to x1. In calcula ting this mess age, we will have to x2 \ntake into accoun t the mess age from y1 so that \nmx1→x2 (x2)= my1→x1 (x1)ψ12(x1,x2) (15) \nx1 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "clusters. We will begin by demo nstrating how messages are comput ed in the belief propag ation algo­ rithm. Conside r t", "source_title": "b1139f1648df9d4cfe7fd4cc09c3b259 lec23", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "51b7489c-00a3-49d0-bf26-4c0c61e3e6f2", "text": "a message mx1→(x2) as x2 relates to x1. In calcula ting this mess age, we will have to x2 take into accoun t the mess age from y1 so that mx1→x2 (x2)= my1→x1 (x1)ψ12(x1,x2) (15) x1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� 7 6.867 Mac hine learning , lectur e 23 (Jaak kola) \nMore generally, in evaluating such messages, we incorporate (take a product of) all the \nincoming mess ages except the one coming from the variable we are marg inalizing towards. \nThus, x2 will send the following mess age to x3 \nmx2→x3 (x3)= mx1→x2 (x2)my2→x2 (x2)ψ23(x2,x3) (16) \nx2 \nFinally , the proba bility of x3 is obtained by collecting all the messages into x3 \nP (x3,D)= mx2→x3 (x3)my3→x3 (x3) (17) \nwhere D refers to any data or observ ations incorporated into the potential functions (as \nillustra ted above). The distributio n we were after is then \nP (x3,D)P (x3|D)= � P (x�\n3,D) (18) \nx�\n3 \nIt might seem that to evaluate similar distri butions for all the other variables we would \nhave to perform the message passing operatio ns (marg inalizati ons) in a particula r order in \neach case. This is not necess ary, actually . We can simply initia lize all the messag es to all \none functions, and carry out the mess age passing operations async hronously . For example, \nwe can pick x2 and calculate its message to x1 based on the other available incoming \nmess ages (that may be wrong at this time). The messag es will converge to the correct ones \nprovided that the undirected model is a tree, and we repeatedly update each message (i.e., \nsend mess ages from each variable in all directions). The async hronous messag e updates \nwill propaga te the necess ary informatio n across the graph. This exchange of informati on \nbetween the variables is a bit more eﬃcient to carry out synchrono usly, however. In other \nwords, we designa te a root variable and imagine the edges orien ted outwards from this \nvariable (this orientation has nothing to do with Bayesian networks). Then “collect” all \nthe messages toward the root, start ing from the leaves. Once the root has all its incom ing \nmess ages, the direction is switched and we send (or “distribute” ) the messages outwards \nfrom the root, starting with the root. These two passes suﬃce to get the correct incom ing \nmess ages for all the variables. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "bayesian network", "section_heading": "a message mx1→(x2) as x2 relates to x1. In calcula ting this mess age, we will have to x2 take into accoun t the mess ag", "source_title": "b1139f1648df9d4cfe7fd4cc09c3b259 lec23", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "09292e87-ec68-438a-bb68-a946fe9ebf83", "text": "1 6.867 Mac hine learning \nLagrang e multipliers and optimization problem s \nWe’ll present here a very simple tutorial example of using and unde rstanding Lagrange multipliers. Let \nw be a scala r parameter we wish to estimate and x a ﬁxed scalar . We wish to solve the following (tiny) \nSVM like optimizatio n problem: \n1 2minimize w subject to wx − 1 ≥ 0 (1) 2 \nThis is diﬃcult only because of the constr aint. We’d rather solve an unconstra ined version of the prob lem \nbut, someho w, we have to take into accoun t the constrain t. We can do this by including the constr aint \nitself in the minimizat ion objectiv e as it allows us to twist the solutio n towards satisfying the constra int. \nWe need to know how much to emphasize the constrain t and this is what the Lagrange multiplier is \ndoing . We will deno te the Lagrange multiplier by α to be consisten t with the SVM problem. So we \nhave now constru cted a new minimizatio n problem (still minimizing with respect to w) that includes the \nconstra int as an additio nal linear term: \n1 2J(w; α)= w − α(wx − 1) (2) 2 \nThe Lagrange multiplier α appears here as a parameter. You might view this new objectiv e a bit \nsuspic iously since we appear to have lost the inform ation about what type of constra int we had, i.e., \nwhether the constra int was wx − 1 ≥ 0, wx − 1 ≤ 0, or wx − 1 = 0. How is this informat ion encoded? \nWe can encode this by constra ining the values of the Lagrange multipliers: \nwx − 1 ≥ 0 α ≥ 0 ⇒ \nwx − 1 ≤ 0 ⇒ α ≤ 0 \nwx − 1=0 α is unco nstrained ⇒ \nNote, for example , that when the constr aint is wx − 1 ≥ 0, as we have above, large positive values of α \nwill encourage choices of w that result in large positiv e values for wx − 1. This is because in the above \nobjective, J(w; α), we try to minimize −α(wx − 1) in additio n to w2/2; minimizing −α(wx − 1) is the \nsame as maximizing α(wx − 1) or wx − 1 since α is positiv e. Figur e 1 tries to illustr ate this eﬀect. \nAssuming x = 1 we can plot the new objectiv e function as a function of w for diﬀeren t values of α. \nLarger values of α clearly move the solution (minimizing w) towards satisfying w − 1 ≥ 0 (we assume \nhere that x = 1). Based on the ﬁgure we can see that setting α = 1 produces just the right solut ion, i.e., \nw∗ = 1, which satisﬁes the constrain t wx − 1 ≥ 0 (when x = 1) with minima l distort ion of the origina l \nobjective. There’s no reaso n to consider negative values for α since they would push the solut ion away \nfrom satisfying our inequality constraint. \nEﬀectively what we are doing here is solving a large number of optimizatio n probl ems, once for each \nsetting of the Lagrange multiplier α. Indeed, we can express the solutio n (the minimizing w) as a \nparametric function of α: \n∂ J(w; α)= w − αx = 0 (3) ∂w \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "1 6.867 Mac hine learning ", "source_title": "47e2537db9248268c9e20bbaa98f0c4b lagrange", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "25604eec-a7c5-437b-b98e-b00f4a8623fe", "text": "ems, once for each setting of the Lagrange multiplier α. Indeed, we can express the solutio n (the minimizing w) as a parametric function of α: ∂ J(w; α)= w − αx = 0 (3) ∂w Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2 6.867 Mac hine learning \n−3 −2 −1 0 1 2 3−20246810\nwJ(w;α)α = 2 \nα = 1\nα = 0 \nFigur e 1: J(w; α) as a function of w for diﬀeren t values of α. The minimizing w values are indica ted with \ndashed line segmen ts. x was set to 1. \n∗ ∗meaning that Wecould nowﬁnd thesetting of α suchthatthecontraint w wα\nsatisﬁed. There aremultiple answerstothissince largervalues of α would bettersatisfy theconstr aint. \nFinding thesmallest(non-negative) α forwhichtheconstr aintissatisﬁed would inthiscase produce the \nrightsolution (onecorresponding totheminimal chang eoftheoriginalproblem). \nWecanproceedabitmoregenerally ,however,thewaywehandled thequadraticoptimizatio nprob lem α αx.\n x − 1 ≥ 0 is\n =\n∗forSVMs. Let’sinsert oursolutio n wαback into the objectiv e function:\n1\n 1 1\n)2 (αx)2 − α(αx2 (αx)2 ∗ ∗J( α(− w wα\nThe result, which we denote as J(α), is a function of the Lagrange multipler α only. Let’s understand \nthis function a bit better . In Figur e 1, the values of the objective at the dashed lines corresp ond exactly α∗(wα ; α)\n x − 1) =\n − 1) = α −\n (4)\n =\n2\n 2\n 2\n2 ∗1 αx=1 − − wato J(w∗\nyield the maximum of J(α)? This is a very useful property. Let’s verify this by ﬁnding the maxim um of\n1 J(α)= α − (αx)2 (5) α\nJ(α)abitmore forma lly: \n2\n∂ ; α) or J(α), evaluated at α =0, 1, 2. Isn’t it strange that the right solutio n (α = 1) appears to\nJ(α)\n x = 0 (6)\n =\n∂α\n∗where wehaveused ourprevious result wα \na\n∗α vanishes ,i.e., (wamaximum of J(α). More rigorously, since α ≥ 0 in our setting, the maxim um is obtained either at α =0 \nor at the point where 1 − w∗ 0. We can express this more concisely by saying that their product = αx. So, the constraint is satisﬁed with equality at the\nx =\nx − 1) = 0 at the optim um. This is generally true, i.e., either the Lagrange multiplier \nis not used and α = 0 (the constra int is satisﬁed witho ut any modiﬁcatio n) or the Lagrange multiplier is \npositiv e and the constra int is satisﬁed with equa lity. \nThe rema ining question for us here is why \n1 maximize α − 2(αx)2 subject to α ≥ 0 (7) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "svm", "section_heading": "ems, once for each setting of the Lagrange multiplier α. Indeed, we can express the solutio n (the minimizing w) as a pa", "source_title": "47e2537db9248268c9e20bbaa98f0c4b lagrange", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "0638ca85-6da5-4dd1-9ad1-1bedbb3ba7bf", "text": "or the Lagrange multiplier is positiv e and the constra int is satisﬁed with equa lity. The rema ining question for us here is why 1 maximize α − 2(αx)2 subject to α ≥ 0 (7) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3 6.867 Mac hine learning \nis any better than the problem we started with. The short answer is that the constrain ts here are very \nsimple non-negat ivity constraints that are easy to deal with in the optimizatio n. In the SVM context, \nwe have another reason to prefer this formulation. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "svm", "section_heading": "or the Lagrange multiplier is positiv e and the constra int is satisﬁed with equa lity. The rema ining question for us h", "source_title": "47e2537db9248268c9e20bbaa98f0c4b lagrange", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "6c569427-a4a3-4f2f-8000-151341993f71", "text": "�\n����\n�\n�\n�\n�\n����\n�\n� � \n� � 1 6.867 Mac hine learning , lectur e 6 (Jaakkola) \nLecture topics: \n• Activ e learning \n• Non-linear predictio ns, kernels \nActive learning \nWe can use the expressions for the mean squar ed error to activ ely select input points \nx1,..., xn, when possible, so as to reduce the resulting estimatio n error . This is an active \nlearning (exp eriment design ) problem. By letting the metho d guide the selec tion of the \ntraining examples (input s), we will generally need far fewer examples in compar ison to \nselecting them at rando m from some underlying distr ibution, database, or trying available \nexperimen ts at rando m. \nTo develop this further, recall that we continue to assume that the responses y come from \nsome linear model y = θ∗T x + θ0 ∗ + � where � ∼ N(0,σ∗2). Nothi ng is assumed about the \ndistributio n of x as the choice of the inputs is in our control. For any given set of inputs, \nx1,..., xn, we deriv ed last time an expression for the mean squared error of the maximum \nlikeliho od parameter estimat es θˆand θˆ0: \nE\n2 \n|\nX\nθˆ\n θ∗ \n= σ∗2Tr\n(XT X)−1 (1)\n −\nθˆ0 θ0 ∗ \nwhere the expectation is relativ e to the responses genera ted from the underlying linea r \nmodel, i.e., over diﬀerent training sets generat ed from the linear model. We do not know \nthe noise variance σ∗2 for the correct model but it only appears as a multipli cative constant \nin the above express ion and therefore won’t aﬀect how we should choose the inputs. When \nthe choice of inputs is indeed up to us (e.g., whic h experimen ts to carry out) we can select \nthem so as to minimize Tr (XT X)−1 . One caveat of this appr oach is that it relies on the \nunderlying relationship between the inputs and the responses to be linear. Whe n this is no \nlonger the case we may end up with clearly suboptimal selection s. \nGiven the selection criterion, how should we ﬁnd say n input examples x1,..., xn that \nminimize it? One simple approa ch is to select them one after the other, merely optimizing \nthe selectio n of the next one in light of what we already have. Let’s assume then that we \nalready have X and thus have A =(XT X)−1 (assuming it is already invertible). We are \ntrying to select another input example x that adds a row [xT , 1] to X. In an applied context \nwe are typica lly constrained by what x can be (e.g., due to the experimen tal setup). We \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "kernel", "section_heading": "�", "source_title": "1eedc5b3427ca3eef198d707f016f295 lec6", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "5d90ef3b-eada-4729-9dbd-0dec94e01622", "text": "select another input example x that adds a row [xT , 1] to X. In an applied context we are typica lly constrained by what x can be (e.g., due to the experimen tal setup). We Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � 2 6.867 Mac hine learning , lectur e 6 (Jaakkola) \nwill discus s simple constr aints below. Let’s now evaluat e the eﬀect of adding a new (valid) \nrow: \n� �T � � ��� �T X X =(XT X)+ x x = A−1 + vv T (2) xT 1 xT 1 1 1 \nwhere v =[xT , 1]T . We would like to ﬁnd a valid v that minimiz es \nTr (A−1 + vv T )−1 (3) \nThe matrix inverse can actua lly be carried out in closed form (easy enoug h to check) \n1 (A−1 + vv T )−1 = A − (1 + vT Av) AvvT A (4) \nso that the trace becomes \n� � 1 � � \nTr (A−1 + vv T )−1 = Tr [A] − (1 + vT Av) Tr AvvT A (5) \n1 � � \n= Tr [A] − (1 + vT Av) Tr v T AAv (6) \nvT AAv = Tr [A] − (1 + vT Av) (7) \nNote that since Tr[A]= Tr[(XT X)−1] is the mean squa red error before adding the new \nexample, any choice of v, i.e., any addit ional example x will reduce the mean squared error. \nWe are interested in ﬁnding the one that reduces the error the most. This is the example \nthat maximizes \nvT AAv (8) (1 + vT Av) \nHow much can we possibly reduce the squa red error? The above term is bounde d by the \nlargest eigenvalue of A. In other words, with each new example we can at most remo ve \none degree of freedom from the parameter space. If we assume no constrain ts on the choice \nof v, the maxi mizing vector would be of inﬁnit e length and proportio nal to the eigen vector \nof A with the largest eigen value (all the eigenvalues of A are positive as it is an inverse \nof a positive deﬁnite matrix XT X). It is indee d advantageous in linear regression to have \nthe input points as far from each other as possible (see Figure 1). If we constra in �v�≤ c, \nthen the maxim izing v is the norma lized eigen vector of A with the largest eigen value, \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "select another input example x that adds a row [xT , 1] to X. In an applied context we are typica lly constrained by wha", "source_title": "1eedc5b3427ca3eef198d707f016f295 lec6", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "30c9e3ec-9a0d-4704-b86a-2b65cd4ca707", "text": "input points as far from each other as possible (see Figure 1). If we constra in �v�≤ c, then the maxim izing v is the norma lized eigen vector of A with the largest eigen value, Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � � � � � 3 6.867 Mac hine learning , lectur e 6 (Jaakkola) \ny\n y \ny1 y2 y1 y2 \ny1 y2 y1 y2 \nxa) x1 x2 b) x1 x2 x \nFigur e 1: a) The eﬀect of noise in the responses has a large eﬀect on the parameters of the \nlinear model when the corresp onding input s are close to each other; b) the eﬀect is smaller \nwhen the inputs are further away. \nnormalized such that �v� = c. Note that it may not be possible to select this eigenvector \nsince v =[xT , 1]T . Other constr aints on x will further restrict v. \nLet’s take a simple example to illustrate the criterio n. Supp ose we have a 1-dimension al \nregression model y = θx + θ0 + � where x is constr ained to lie within [−1, 1]. Assume we \nhave already observed responses for x1 = 1 and x2 = −1. Thus \nX = 11 , XT X = 20 , A =(XT X)−1 =1 10 (9) −11 02 201 \nv =[x, 1]T and therefore vT Av =(x2 + 1)/2 and vT AAv =(x2 + 1)/4. The criterio n to \nbe maximized becomes \nvT AAv (x2 + 1)/4 = (10) (1 + vT Av) 1+(x2 + 1)/2 \nSince z/(1 + z) is an increasing function of z, it follows that the criterio n is maximized \nwhen (x2 + 1)/2 is maximize d. Given the constra ints, the maximizing point is x = 1 or \nx = −1. Either choice would do but, after selecting one, the other one would be preferred \nat the next step. The result is consisten t with the intuition that for linear models the \ninputs should be as far from each other as possible (cf. Figure 1). \nWe have so far used the mean squa red error in the paramet ers as the selec tion criter ion. \nWhat about the variance in the predictio ns? Let’s try to ﬁnd the point x whose response Cite as: Edward Flemming, course materials for 24.910 Topics in Linguistic Theory: Laboratory Phonology, Spring 2007.\n \nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "input points as far from each other as possible (see Figure 1). If we constra in �v�≤ c, then the maxim izing v is the n", "source_title": "1eedc5b3427ca3eef198d707f016f295 lec6", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "5e7710e5-6712-4d6b-b12b-9754e7c50f3c", "text": "try to ﬁnd the point x whose response Cite as: Edward Flemming, course materials for 24.910 Topics in Linguistic Theory: Laboratory Phonology, Spring 2007. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n4 6.867 Mac hine learning , lectur e 6 (Jaakkola) \nwe are the most uncerta in about. We again write v =[xT , 1]T so that \n�� �2 � \nV ar{y|x, X} = \n= E ˆθT x + ˆθ0 − θ∗T x − θ∗ \n0 |x, X \nE �� x \n1 �T �� ˆθ \nˆθ0 � \n− � θ∗ \nθ∗ \n0 �� �� ˆθ \nˆθ0 � \n− � θ∗ \nθ∗ \n0 ��T � x \n1 (11) \n� \n|x, X � \n(12) \n� �T � � \n= x \n1 σ∗2(XT X)−1 x \n1 (13) \n= σ∗2 · v T Av (14) \nwhere the expectation is over responses for existing training examples, again assuming that \nthere is a correct underlying linear model. So the largest variance corresponds to the input \nx that maximizes vT Av where v =[xT , 1]T . In the unconstr ained case where there are few \nor no restrictio ns on v, this maximizing point is exactly the one we would query according \nthe previous selectio n criterio n. \nNon-linear predictions, kerne ls \nEssentially all of what w e have disc ussed can be extended to no n-linea r models, models that \nremain linear in the parameters as before but perform non-line ar operatio ns in the origin al \ninput space. This is achieved by mapping the input examples to a higher dimension al \nfeature space where the dimensions in the new feature vectors include non-linear functions of \nthe inputs. The simplest setting for demo nstrat ing this is linear regression in one dimens ion. \nConsider therefore the linear model y = θx + θ0 + � where � ∼ N(0,σ2). We can obtain a \nquadratic model by simply mapping the input x to a longer feature vector that includes a \nterm quadra tic in x. A third order model can be constructed by including all term s up to \ndegree three, and so on. Explicitly , we would make linear predictio ns using feature vectors \nφ x → [1, √\n2x,x 2]T = φ(x) (15) \nφ 23]T x → [1, √\n3x, √\n3x ,x (16) \n(17) ··· \nThe role of √\n2 and other constan ts will become clear shortly. The new polynomia l regres­\nsion model is then given by \ny = θT φ(x)+ θ0 + �, � ∼ N(0,σ2) (18) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "try to ﬁnd the point x whose response Cite as: Edward Flemming, course materials for 24.910 Topics in Linguistic Theory:", "source_title": "1eedc5b3427ca3eef198d707f016f295 lec6", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "a4da2a68-6027-4feb-a6c2-156adaf6d075", "text": "··· The role of √ 2 and other constan ts will become clear shortly. The new polynomia l regres­ sion model is then given by y = θT φ(x)+ θ0 + �, � ∼ N(0,σ2) (18) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n5 6.867 Mac hine learning , lectur e 6 (Jaakkola) \na) \n−2 −1 0 1 2−505\nxy b) \n−2 −1 0 1 2−505\nxy\nc) \n−2 −1 0 1 2−505\nxy d) \n−2 −1 0 1 2−505\nxy\nFigur e 2: a) a linear model ﬁt; b) a third order polynomia l model ﬁt to the same data; c) \na ﬁfth order polyno mial model; d) a seventh order polynom ial model. \nwhere the dimensiona lity of φ(x) (and therefore also θ) depends on the order of the poly­\nnomia l expan sion. Regulariza tion of the parameters is almost always used in conjunctio n \nwith higher dimens ional feature vectors. This is neces sary since otherwise the higher order \nmodels could seriously overﬁt to the data. Examples of ﬁtted polynomia l regression mod­\nels without regular ization are shown in Figure 2a-d (the data were generated from a linea r \nmodel). Note that all these models are linear in the parameters but non-linear in x, save \nthe standar d linea r regression model in Figure 2a. \nThe polynomia l expa nsion of input vectors works the same in higher dimens ions, e.g., \nφ 22 x =[x1,x2]T → [1,x1,x2, √\n2x1x2,x 1,x 2]T = φ(x) (19) \nOne limitatio n of explicating the featur e vector s is that the dimensiona lity can increase \nrapidly with the degree of polyno mial expansion, especially when the dimension of the \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "··· The role of √ 2 and other constan ts will become clear shortly. The new polynomia l regres­ sion model is then given", "source_title": "1eedc5b3427ca3eef198d707f016f295 lec6", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "488a9a22-139d-463a-96e3-3fac62998223", "text": "1,x 2]T = φ(x) (19) One limitatio n of explicating the featur e vector s is that the dimensiona lity can increase rapidly with the degree of polyno mial expansion, especially when the dimension of the Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n6 6.867 Mac hine learning , lectur e 6 (Jaakkola) \ninput vector is already high. The table below gives some indicatin g of this though the \neﬀect is more drama tic with higher input dimensions. Can we someho w avoid explicati ng \nthe dimensions of the feature vector s? In the context of models we have discussed thus \nfar, yes, we can. We can deﬁne the feature vectors implicitly by focusing on specifying \nthe values of their inner products or kernel instead. To get a sense of the power of this \nappr oach, let’s evaluate the inner product between two feature vectors corresp onding to \nthe speciﬁc cubic expansions of 1-dime nsional inputs shown before: \n23]Tφ(x) = [1, √\n3x, √\n3x,x , (20) \nφ(x�) = [1, √\n3x�, √\n3x�2 ,x�3]T , (21) \nφ(x)T φ(x�) = 1+3xx� + 3(xx�)2 +(xx�)3 = (1+ xx�)3 (22) \nSo it seems we can compactly evaluate the inner products between polynomia l feature \nvectors. The eﬀect is more strik ing with higher dimensiona l inputs and higher polynom ial \ndegrees. (We did have to specify the consta nts appr opria tely in the feature vectors to make \nthis work). To shift the modeling from explicit feature vectors to inner products (kernels) \nwe obviously have to ﬁrst turn the estimatio n problem into a form that involves only inner \nproducts between featur e vectors. \ndim(x) =2 dim(x) =3 \ndegree p # of features degree p # of features \n2 6 2 10 \n3 10 3 20 \n4 15 4 35 \n5 21 5 56 \nLinear regression and kernels \nLet’s simplify the model slightly by omitt ing the oﬀset parameter θ0, reducing the model \nto y = θT φ(x)+ � where φ(x) is a particular featur e expansio n (e.g., polynomia l). Our goal \nhere is to turn both the estimat ion problem and the subsequen t predictio n task into forms \nthat involve only inner products between the featur e vectors. \nWe have already emphasized that regula rizat ion is neces sary in conjunctio n with mapp ing \nexamples to higher dimens ional feature vector s. The regular ized least squares objectiv e to \nbe minimized, with parameter λ, is given by \nn\nJ(θ)= �� \nyt − θT φ(xt) �2 + λ�θ�2 (23) \nt=1 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "1,x 2]T = φ(x) (19) One limitatio n of explicating the featur e vector s is that the dimensiona lity can increase rapidl", "source_title": "1eedc5b3427ca3eef198d707f016f295 lec6", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "5fd3faa4-90a7-442b-9e24-3e86b8a137e5", "text": "examples to higher dimens ional feature vector s. The regular ized least squares objectiv e to be minimized, with parameter λ, is given by n J(θ)= �� yt − θT φ(xt) �2 + λ�θ�2 (23) t=1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n7 6.867 Mac hine learning , lectur e 6 (Jaakkola) \nThis form can be deriv ed from penalized log-likeliho od estima tion (see previo us lecture \nnotes). The eﬀect of the regulariza tion penalty is to pull all the parameters towards zero. \nSo any linear dime nsions in the parameters that the training feature vectors do not pertain \nto are set explicitly to zero. We would therefore expect the optima l parameters to lie in \nthe span of the featur e vector s corresp onding to the training example s. This is indee d the \ncase. \nWe will continue with the derivation of the kernel (inner product) form of the linea r re­\ngression model next time. Cite as: Edward Flemming, course materials for 24.910 Topics in Linguistic Theory: Laboratory Phonology, Spring 2007.\n \nMIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "examples to higher dimens ional feature vector s. The regular ized least squares objectiv e to be minimized, with parame", "source_title": "1eedc5b3427ca3eef198d707f016f295 lec6", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "9236117d-deff-4c6f-8319-0435bdc08c58", "text": "1 6.867 Mac hine learning , lectur e 13 (Jaak kola) \nLecture topics: \n• Boosting , margin, and gradien t descent \n• complexit y of classiﬁers, generaliza tion \nBoosting \nLast time we arrived at a boosting algorithm for sequen tially creat ing an ensem ble of \nbase classiﬁers. Our base classiﬁers were decision stumps that are simple linear classiﬁers \nrelying on a single comp onen t of the input vector. The stump classiﬁers can be written \nas h(x; θ) = sign( s(xk − θ0) ) where θ = {s,k,θ0} and s ∈ {−1, 1} speciﬁes the label to \npredict on the positive side of xk − θ0. Figure 1 below shows a possible decision boundar y \nfrom a stump when the input vectors x are only two dimens ional. \n−0.6 −0.4 −0.2 00.2 0.4 0.6 0.8 11.2 1.4−1−0.500.511.5\nFigur e 1: A possible decis ion boundary from a trained decision stump. The stump in the \nﬁgure depends only on the vertical x2-axis. \nThe boosting algorithm combines the stumps (as base learners) into an ensemble that, after \nensemble by αˆj after the fact. The simple Adaboost algorithm can be written in the m rounds of boosting , takes the following form \nm� \nhm(x) = αˆj h(x; θˆj ) (1) \nj=1 \nwhere αˆj ≥ 0 but they do not neces sarily � m sum to one. We can always normalize the \nj=1 \nfollowing modular form: \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "1 6.867 Mac hine learning , lectur e 13 (Jaak kola) ", "source_title": "616f1abe4c4fec09907aa27e5a81f18d lec13", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "16345120-9a95-441e-b155-6d7014b452ef", "text": "the following form m� hm(x) = αˆj h(x; θˆj ) (1) j=1 where αˆj ≥ 0 but they do not neces sarily � m sum to one. We can always normalize the j=1 following modular form: Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� � \n� � � \n� � 2 6.867 Mac hine learning , lectur e 13 (Jaak kola) \n(0) Set W0(t)=1/n for t =1,...,n. \n(1) At stage m, ﬁnd a base learner h(x; θˆm) that approximately minimizes \nm\n− W˜m−1(t)y th(x t; θm)=2�m − 1 (2) \nt=1 \nwhere �m is the weighted classiﬁcatio n error (zero-one loss) on the training examples, \nweighted by the norma lized weights W˜m−1(t). \n(2) Given θˆm, set \nαˆm =0.5 log 1 − ˆ�m (3) �ˆm \nwhere ˆ�m is the weighted error corresp onding to θˆm chosen in step (1). For binary \n{−1, 1} base learners, ˆαm exactly minimiz es the weighted training loss (loss of the \nensemble): \nn\nJ(αm,θˆm)= W˜m−1(t)exp −ytαmh(x t; θˆm) (4) \nt=1 \nIn cases where the base learners are not binary (e.g., return values in the interval \n[−1, 1]), we would have to minimiz e Eq.(4) directly. \n(3) Update the weights on the training example s based on the new base learner: \nW˜m(t)= cm · W˜m−1(t)exp −ytαˆmh(x t; θˆm) (5) \nwhere cm is the normalizat ion constan t to ensure that W˜m(t) sum to one after the \nupdate. The new weights can be again interpreted as norma lized losses for the new \nensemble hm(xt)= hm−1(x)+ ˆαmh(x; θˆm). \nLet’s try to understand the boosting algorithm from several diﬀeren t perspectiv es. First \nof all, there are several diﬀeren t types of errors (erro rs here refer to zero-one classiﬁcati on \nlosses, not the surrogate exponen tial losse s). We can talk about the weighted error of base \nlearner m, introduced at the mth boosting iteratio n, relative to the weights W˜m−1(t) on \nthe training examples. This is the weighted training error ˆ�m in the algorithm. We can \nalso measure the weighted error of the same base classiﬁer relative to the updated weights, \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "the following form m� hm(x) = αˆj h(x; θˆj ) (1) j=1 where αˆj ≥ 0 but they do not neces sarily � m sum to one. We can a", "source_title": "616f1abe4c4fec09907aa27e5a81f18d lec13", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "49e3ae8c-499d-4d34-ba28-b151fafaf9a1", "text": "relative to the weights W˜m−1(t) on the training examples. This is the weighted training error ˆ�m in the algorithm. We can also measure the weighted error of the same base classiﬁer relative to the updated weights, Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� 3 6.867 Mac hine learning , lectur e 13 (Jaak kola) \ni.e., relative to W˜m(t). In other words, we can measure how well the current base learn er \nwould do at the next iterat ion. Finally, in terms of the ensemble, we have the unweighted \ntraining error and the corresp onding generalizat ion (test) error, as a function of boosting \niterations. We will discuss each of these in turn. \nWeighted error. The weighted error achieved by a new base learner h(x; θˆm) relativ e to \nW˜m−1(t) tends to increase with m, i.e., with each boosting iteration (thoug h not monoton­\nically) . Figure 2 below shows this weighted error ˆ�m as a functio n of boosting iteratio ns. \nThe reason for this is that since the weights concen trate on examples that are diﬃc ult to \nclassify correctly , subsequen t base learners face harder classiﬁcatio n tasks. \n0 10 20 30 40 500.050.10.150.20.250.30.350.4weighted training error\nnumber of iterations\nFigur e 2: Weighted error ˆ�m as a function of m. \nWeighted error relative to updated weights. We claim that the weighted error of the \nbase learner h(x; θˆm) relativ e to the updated weights W˜m(t) is exactly 0.5. This means that \nthe base learner introduced at the mth boosting iteration will be useles s (at chance level) \nfor the next boosting iteration. So the boosting algorithm would never introduce the same \nbase learner twice in a row. The same learner might, however, reappear later on (relative \nto a diﬀerent set of weights). One reason for this is that we don’t go back and update \nαˆj ’s for base learners alrea dy introduced into the ensemble. So the only way to change the \nprevio usly set coeﬃcients is to reintroduce the base learners. Let’s now see that the claim \nis indeed true. We can equiv alently show that the weighte d agreement relative to W˜m(t) is \nexactly zero: \nm\nW˜m(t)yth(x t; θˆm)=0 (6) \nt=1 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "relative to the weights W˜m−1(t) on the training examples. This is the weighted training error ˆ�m in the algorithm. We ", "source_title": "616f1abe4c4fec09907aa27e5a81f18d lec13", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "88fb3d01-8295-4cf2-8d54-800049d53cb4", "text": "to reintroduce the base learners. Let’s now see that the claim is indeed true. We can equiv alently show that the weighte d agreement relative to W˜m(t) is exactly zero: m W˜m(t)yth(x t; θˆm)=0 (6) t=1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� \n� \n� � \n� � \n� 4 6.867 Mac hine learning , lectur e 13 (Jaak kola) \nConsider the optimizatio n problem for αm after we have already found θˆm: \nn � � \nJ(αm,θˆm)= W˜m−1(t)exp −ytαmh(x t; θˆm) (7) \nt=1 \nThe derivative of J(αm,θˆm) with respect to αm has to be zero at the optimal value ˆαm so \nthat \nn\ndαd \nm J(αm,θˆm)��\nαm =αˆm = − � \nW˜m−1(t)exp � \n−ytαˆmh(x t; θˆm) � \nyth(x t; θˆm) (8) \nt=1 \nn\n= −cm W˜m(t)yth(x t; θˆm)=0 (9) \nt=1 \nwhere we have used Eq.(5) to move from W˜m−1(t) to W˜m(t). So the result is an optima lity \nconditio n for αm. \nEnse mble traini ng error. The training error of the ensem ble does not neces sarily de­\ncrease monotonically with each boosting iterat ion. The exponen tial loss of the ensem ble \ndoes, however, decrease monotonically . This should be eviden t since it is exactly the loss \nwe are sequentially minimizing by adding the base learners. We can also quantify, based on \nthe weighted error achieved by each base learner, how much the exponential loss decreases \nafter each iteration. We will need this to relate the training loss to the classiﬁcatio n error. \nIn fact, the amount that the training loss decreases after iteration m is exactly cm, the nor­\nmaliza tion consta nt for the updated weights (we have to norma lize the weights precisely \nbecause the exponen tial loss over the training examples decreases). Note also that cm is \nexactly J(ˆαm,θˆm). Now, \nn � � \nJ(ˆαm,θˆm)= W˜m−1(t)exp −ytαˆmh(x t; θˆm) (10) \nt=1 \n= W˜m−1(t)exp( −αˆm)+ W˜m−1(t)exp( ˆαm) (11) \nt: yt =h(x t;θˆm) t: yt�=h(x t;θˆm) \n= (1 − �ˆm)exp( −αˆm)+ˆ�m exp( ˆαm) (12) \n= (1 − �ˆm)1 − ˆ�m \n�ˆm +ˆ�m 1 − \n�ˆm �ˆm (13) \n=2 ˆ�m(1 − �ˆm) (14) \nNote that this is always less than one for any ˆ�m < 1/2. The training loss of the ensem ble \nafter m boosting iterati ons is exactly the product of these terms (reno rmaliza tions). In \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "to reintroduce the base learners. Let’s now see that the claim is indeed true. We can equiv alently show that the weight", "source_title": "616f1abe4c4fec09907aa27e5a81f18d lec13", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "12526898-f18b-43c2-a58b-13f2aef070a6", "text": "Note that this is always less than one for any ˆ�m < 1/2. The training loss of the ensem ble after m boosting iterati ons is exactly the product of these terms (reno rmaliza tions). In Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � � 5 6.867 Mac hine learning , lectur e 13 (Jaak kola) \nother words, \nm m\nexp( −ythm(xt))= 2 �ˆk(1 − �ˆk) (15) \nt=1 k=1 \nThis and the observ ation that \nstep(z ) ≤ exp(z ) (16) \nfor all z, where the step function step(z ) = 1 if z> 0 and zero otherwise , suﬃces for our \npurp oses. A simple upper bound on the training error of the ensem ble, errn(hm), follows \nfrom \n1 n� \nerrn(hm) = n step ( −ythm(xt) ) (17) \nt=1 \n1 n� \n≤ n exp ( −ythm(xt) ) (18) \nt=1 \n1 m� � \n= n 2 ˆ�k(1 − ˆ�k) (19) \nk=1 \nThus the exponen tial loss over the training examples is an upper bound on the training error \nand this upper bound goes down monotonically with m provided that the base learners are \nlearning something at each iteratio n (their weighted errors less than half). Figure 3 shows \nthe trainin g error as well as the upper bound as a function of the boosting iterat ions. \nEnse mble test error. We have so far discussed only training errors. The goal, of course, is \nto genera lize well. What can we say about the generaliza tion error of ensem ble generated by \nthe boosting algorithm? We have repeatedly tied the generalizatio n error to some notion \nof margin. The same is true here. Consider ﬁgure 5 below. It shows a typical plot of \nthe ensem ble training error and the corresp onding generaliza tion error as a function of \nboosting iterations. Two things are notable in the plot. First, the genera lizatio n error \nseems to decrease (slightly) even after the ensemble has reached zero training error . Why \nshould this be? The second surprising thing seems to be the fact that the generalizat ion \nerror does not increase even after a large number of boosting iterations. In other words, the \nboosting algorithm appears to be somewhat resistant to overﬁtting . Let’s try to expla in \nthese two (relat ed) observations. \nThe votes {αˆj } genera ted by the boosting algorithm won’t sum to one. We will therefore \nrenor malize ensem ble \nh˜m(x)= αˆ1h(x; θˆ1)+ ... αˆmh(x; θˆm) (20) αˆ1 + ... +ˆαm \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "Note that this is always less than one for any ˆ�m < 1/2. The training loss of the ensem ble after m boosting iterati on", "source_title": "616f1abe4c4fec09907aa27e5a81f18d lec13", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "f100fa60-16ef-4bb9-a9dd-6afc73b013fa", "text": "two (relat ed) observations. The votes {αˆj } genera ted by the boosting algorithm won’t sum to one. We will therefore renor malize ensem ble h˜m(x)= αˆ1h(x; θˆ1)+ ... αˆmh(x; θˆm) (20) αˆ1 + ... +ˆαm Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n6 6.867 Mac hine learning , lectur e 13 (Jaak kola) \n0 10 20 30 40 5000.020.040.060.080.10.120.140.16\nnumber of iterationstraining error\nFigur e 3: The training error of the ensem ble as well as the corresponding exponential loss \n(upp er bound) as a functio n of the boosting iteratio ns. \nso that h˜m(x) ∈ [−1, 1]. As a result, we can deﬁne a “voting margin” for training example s \nas marg in(t) = yth˜m(xt). The margin is positiv e if the example is classiﬁed correctly by the \nensemble. It represen ts the degree to which the base classiﬁers agree with the correct clas­\nsiﬁcatio n decision (negat ive value indicates disag reemen t). Note that margin(t) ∈ [−1, 1]. \nIt is a very diﬀerent type of margin (voting marg in) than the geometric margin we have \ndiscus sed in the context linear classiﬁers. Now, in addition to the training error errn(hm) \nwe can deﬁne a margin error errn(hm; ρ) that is the fractio n of example marg ins that are at \nor below the threshold ρ. Clearly, errn(hm) = errn(hm; 0). We now claim that the boosting \nalgorithm, even after errn(hm; 0) = 0 will decrease errn(hm; ρ) for larger values of ρ> 0. \nFigur e 4a-b provide an empir ical illustra tion that this is indeed happ ening. This is perhaps \neasy to understand as a consequence of the fact that exponen tial loss, exp(−margin(t)), \ndecreases as a function of the margin, even after the margin is positive. \nThe second issue to expla in is the appa rent resistance to overﬁtt ing. One reason is that the \ncomplexit y of the ensemble does not increase very quickly as a function of the number of \nbase learners. We will make this statemen t more precise later on. Moreover, the boosting \niterations modify the ensem ble in sensible ways (increa sing the margin) even after the \ntraining error is zero. We can also relate the margin, or the margin error errn(hm; ρ) directly \nto generaliza tion error. Anot her reason for resistance to overﬁtting is that the sequen tial \nprocedure for optimizing the exponen tial loss is not very eﬀective. We would overﬁt much \nmore quickly if we reoptimized {αj }’s jointly rather than through the sequen tial procedure \n(see the discussion of boosting as gradien t desce nt below). \nBoosting as gradient descen t. We can also view the boosting algorithm as a simple \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "margin", "section_heading": "two (relat ed) observations. The votes {αˆj } genera ted by the boosting algorithm won’t sum to one. We will therefore r", "source_title": "616f1abe4c4fec09907aa27e5a81f18d lec13", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "0c0580ab-da6d-4b1d-82f9-49013323e728", "text": "{αj }’s jointly rather than through the sequen tial procedure (see the discussion of boosting as gradien t desce nt below). Boosting as gradient descen t. We can also view the boosting algorithm as a simple Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� 7 6.867 Mac hine learning , lectur e 13 (Jaak kola) \na) \n−1 −0.5 0 0.5 100.10.20.30.40.50.60.70.80.91 c) \n−1 −0.5 0 0.5 100.10.20.30.40.50.60.70.80.91\nFigur e 4: The marg in errors errn(hm; ρ) as a function of ρ when a) m = 10 b) m = 50. \ngradien t desce nt procedure (with line searc h) in the space of discrimina nt functions. To \nunderstand this we can view each base learner h(x; θ) as a vector based on evaluating it \non each of the training examples : \n⎡ ⎤ h(x 1; θ) \n�h(θ)= ⎣ ⎦ (21) ··· \nh(x n; θ) \nThe ensem ble vector �hm, obtained by evaluating hm(x) at each of the training examples , \nis a positive combinat ion of the base learner vectors: \nm\n�hm = αˆm �h(θˆm) (22) \nj=1 \nThe exponen tial loss objective we are trying to minimize is now a functio n of the ensem ble \nvector �hm and the training labels. Supp ose we have �hm−1. To minimize the objectiv e, we \ncan select a useful direc tion, �h(θˆm), along which the objective seems to decrease. This is \nexactly how we deriv ed the base learners. We can then ﬁnd the minim um of the objective \nby moving in this direction, i.e., evaluating vector s of the form �hm−1 + αm �h(θˆm). This isa \nline search operation. The minimum is attained at ˆαm, we obtain �hm = �hm−1 +ˆαm �h(θˆm), \nand the procedure can be repeated. \nViewing the boosting algorithm as a simple gradien t descent procedure also helps us un­\nderstand why it can overﬁt if we continue with the boosting iteratio ns. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "{αj }’s jointly rather than through the sequen tial procedure (see the discussion of boosting as gradien t desce nt belo", "source_title": "616f1abe4c4fec09907aa27e5a81f18d lec13", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "9c83ccdc-57ed-4e61-ad00-635d95fd1701", "text": "+ˆαm �h(θˆm), and the procedure can be repeated. Viewing the boosting algorithm as a simple gradien t descent procedure also helps us un­ derstand why it can overﬁt if we continue with the boosting iteratio ns. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n8 6.867 Mac hine learning , lectur e 13 (Jaak kola) \n0 10 20 30 40 5000.020.040.060.080.10.120.140.16\nnumber of iterationstraining/test error\nFigur e 5: The training error of the ensem ble as well as the corresponding generalizati on \nerror as a functio n of boosting iteratio ns. \nComplexity and generalization \nWe have approa ched classiﬁc ation problems using linear classiﬁe rs, probabilistic classiﬁers, \nas well as ensemble metho ds. Our goal is to unde rstand what type of performance guaran­\ntees we can give for such metho ds based on ﬁnite training data. This is a core theor etical \nquestion in machine learning. For this purp ose we will need to understand in detail what \n“comple xity” means in terms of classiﬁers. A single classiﬁer is never complex or simple; \ncomplexit y is a prop erty of t he set of classiﬁers or the mo del. Each mo del selec tion criter ion \nwe have encoun tered provided a slightly diﬀeren t deﬁnition of “model complexit y”. \nOur focus here is on performance guarantees that will eventually relate the margin we can \nattain to the general ization error, especially for linear classiﬁe rs (geometric marg in) and \nensembles (voting margin) . Let’s start by motiv ating the comple xity measure we need for \nthis purp ose with an example. \nConsider a simple decis ion stump classiﬁer restricted to x1 coordinate of 2−dimensional \ninput vectors x =[x1,x2]T . In other words, we consider stumps of the form \nh(x; θ) = sign ( s(x 1 − θ0) ) (23) \nwhere s ∈ {−1, 1} and θ0 ∈R and call this set of classiﬁers F1. Example decision bound­\naries are displa yed in Figure 6. \nSupp ose we are getting the data points in a sequence and we are interested in seeing when \nour predictions for future points become constrained by the labeled points we have alrea dy \n Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "+ˆαm �h(θˆm), and the procedure can be repeated. Viewing the boosting algorithm as a simple gradien t descent procedure ", "source_title": "616f1abe4c4fec09907aa27e5a81f18d lec13", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 7}
{"id": "d471eed2-049d-4721-a7bb-93ed709bc346", "text": "in Figure 6. Supp ose we are getting the data points in a sequence and we are interested in seeing when our predictions for future points become constrained by the labeled points we have alrea dy Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n9 6.867 Mac hine learning , lectur e 13 (Jaak kola) \na) \n+1 \nx1 x2 \nx \nx +1 \n-1 \nx1 x2 \nx \nx +1 b) \n+1 \nx1 x2 \nx \nx -1 \n? \nx1 x2 \nx \nx -1 \n+1 x c) \nd) e) f) \n-1 \nx1 x2 \nx \nx -1 \nh2 \nx1 x2 \nx \nx -1 \n-1 x ? h1 \nFigur e 6: Possible decision boundaries corresponding to decis ion stumps that rely only on \nx1 coordinate. The arrow (normal) to the bounda ry speciﬁes the positiv e side. \nseen. Such constra ints pertain to both the data and the set of classiﬁers F1. See Figur e \n6e. Having seen the labels for the ﬁrst two points, −1 and +1, all classiﬁers h ∈F1 that \nare consisten t with these two labeled points have to predict +1 for the next point. Since \nthe labels we have seen force us to classify the new point in only one way, we can claim \nto have learned something. We can also understand this as a limit ation of (the complexit y \nof) our set of classiﬁers. Figure 6f illustra tes an alternat ive scenario where we can ﬁnd two \nclassiﬁers h1 ∈F1 and h2 ∈F1, both consisten t with the ﬁrst two labels in the ﬁgure, but \nmake diﬀeren t predictio ns for the new point. We could therefore classif y the new point \neither way. Recall that this freedom is not available for all label assignments to the ﬁrst \ntwo points. So, the stumps in F1 can classif y any two points (in general positio n) in all \npossible ways (Figures 6a-d) but are already partially constra ined in how they assign labels \nto three points (Figur e 6e). In more technica l terms we say that F1 shatters (can generate \nall possible labels over) two points but not three. \nSimilar ly, for linear classiﬁers in 2−dimensions, all the eight possible labelings of three \npoints can be obtained with linear classiﬁers (Figure 7a). Thus linear classiﬁers in two \ndimensions shatter three points. However, there are labels over four points that no linear \nclassiﬁer can produce (Figure 7b). \nVC-dimensi on. As we increa se the number of data points, the set of classiﬁers we are \nconsidering may no longer be able to label the points in all possible ways. Such emerg ing \nconstra ints are critical to be able to predic t labels for new points. This motivates a key \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "in Figure 6. Supp ose we are getting the data points in a sequence and we are interested in seeing when our predictions ", "source_title": "616f1abe4c4fec09907aa27e5a81f18d lec13", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 8}
{"id": "23343c3d-f92b-4178-8b72-f520a60dcdd7", "text": "are considering may no longer be able to label the points in all possible ways. Such emerg ing constra ints are critical to be able to predic t labels for new points. This motivates a key Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n10 6.867 Mac hine learning , lectur e 13 (Jaak kola) \na) \nxx\nxxx\nxxx\nxxx\nx\nxx\nxxx\nxxx\nxxx\nx+-\n+-+-+-\n+ -\n+- +- +-\nb) \nx\nxx\nx+ -\n+ -\nFigur e 7: Linea r classiﬁers on the plane can shatter three points a) but not four b). \nmeasure of comple xity of the set of classiﬁers, the Vapnik-Cher vonen kis dimension. The \nVC-dime nsion is deﬁned as the maxim um number of points that a classiﬁer can shatter. \nSo, the VC-dimension of F1 is two, denoted as dVC (F1), and the VC-dime nsion of linear \nclassiﬁers on the plane is three. Note that the deﬁnition involves a maximum over the \npossible points. In other words, we may ﬁnd less than dVC points that the set of classiﬁers \ncanno t shatt er (e.g., linear classiﬁers with points exactly on a line in 2−d) but there canno t \nbe any set of more than dVC points that the classiﬁer can shatter. \nThe VC-dimension of the set of linear classiﬁers in d−dimensions is d + 1, i.e., the number \nof parameter s. This is not a useful result for unde rstanding how kernel metho ds work. \nFor example, the VC-dimension of linear classiﬁers using the radial basis kernel is ∞. We \ncan incor porate the notion of marg in in VC-dimension, however. This is known as the Vγ \ndimension. The Vγ dimension of a set of linear classiﬁers that attain geometric margin γ \nwhen examples lie within an enclosing sphere of radius R is bounded by R2/γ2 . In other \nwords there are not that many points we can label in all possible ways if any valid labeling \nhas to be with marg in γ. This result is independen t of the dimension of the classiﬁer, and \nis exactly the mista ke bound for the perceptron algorithm! \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "perceptron", "section_heading": "are considering may no longer be able to label the points in all possible ways. Such emerg ing constra ints are critical", "source_title": "616f1abe4c4fec09907aa27e5a81f18d lec13", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 9}
{"id": "022624b2-73c1-4333-9520-bc5af6784036", "text": "6.867 Mac hine learning , lectur e 1 (Jaakkola) 1 \nExampl e \nLet’s start with an example. Suppose we are charged with providing automa ted access \ncontrol to a building. Before entering the building each person has to look into a camera so \nwe can take a still image of their face. For our purp oses it suﬃces just to decide based on \nthe imag e whether the person can enter the building . It migh t be helpful to (try to) also \nidentify each person but this might require type of informat ion we do not have (e.g., names \nor whether any two face images corresp ond to the same person). We only have face images \nof people recorded while access control was still provided manually . As a result of this \nexperience we have labeled imag es. An image is labeled positive if the person in question \nshould gain entry and negative otherwise. To supplemen t the set of negatively labeled \nimag es (as we would expect only few cases of refuse d entries under norma l circumstances) \nwe can use any other face imag es of people who we do not expect to be permitted to \nenter the building. Images taken with simila r camera-face orien tation (e.g., from systems \noperationa l in other buildings) would be preferred. Our task then is to come up with a \nfunction – a classiﬁer – that maps pixel images to binar y (±1) labels. And we only have \nthe small set of labeled images (the training set) to constra in the function . \nLet’s make the task a bit more formal. We assume that each imag e (grayscale) is represen ted \nas a column vector x of dimension d. So, the pixel intensit y values in the imag e, column by \ncolumn, are concatena ted into a single column vector . If the imag e has 100 by 100 pixels, \nthen d = 10000. We assume that all the imag es are of the same size. Our classiﬁer is a \nbinar y valued functio n f : Rd → {−1, 1} chosen on the basis of the training set alone. \nFor our task here we assume that the classiﬁer knows nothing about images (or faces for \nthat matter) beyond the labeled training set. So, for example, from the point of view \nof the classiﬁer, the images could have been measuremen ts of weight, height, etc. rather \nthan pixel intensities. The classiﬁer only has a set of n training vectors x1,..., xn with \nbinar y ±1 labels y1,...,yn. This is the only informa tion about the task that we can use to \nconstra int what the functio n f should be. \nWhat kind of solution would suﬃce? \nSupp ose now that we have n = 50 labeled pixel imag es that are 128 by 128, and the pixel \nintensities range from 0 to 255. It is therefor e possible that we can ﬁnd a single pixel, \nsay pixel i, such that each of our n imag es have a distinct value for that pixel. We could \nthen construct a simple binary functio n based on this single pixel that perfec tly maps the \ntraining imag es to their labels. In other words, if xti refers to pixel i in the tth training \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "6.867 Mac hine learning , lectur e 1 (Jaakkola) 1 ", "source_title": "d26f49e758fa83b40c8f22496c857f14 lec1", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "fcb85887-5f71-40d1-b1ef-9c5c0e0195d1", "text": "then construct a simple binary functio n based on this single pixel that perfec tly maps the training imag es to their labels. In other words, if xti refers to pixel i in the tth training Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� � � � 6.867 Mac hine learning , lectur e 1 (Jaakkola) 2 \nimag e, and x�\ni is the ith pixel in any imag e x�, then \nfi(x�)= yt, if xti = x�\ni for some t =1,...,n (in this order) (1) −1, otherwise \nwould appear to solve the task. In fact, it is always possible to come up with such a \n“perfect” binary functio n if the training images are distinct (no two imag es have identical \npixel intensities for all pixels). But do we expect such rules to be useful for images not \nin the training set? Even an imag e of the same person varies somewhat each time the \nimag e is taken (orientation is slightly diﬀeren t, lighting conditions may have chang ed, etc). \nThes e rules provide no sensible predic tions for images that are not identical to those in the \ntraining set. The primary reaso n for why such trivia l rules do not suﬃce is that our task is \nnot to correctly classify the training imag es. Our task is to ﬁnd a rule that works well for \nall new images we would encounter in the acces s control setting; the trainin g set is merely \na helpful source of informat ion to ﬁnd such a functio n. To put it a bit more formally , we \nwould like to ﬁnd classiﬁers that gener alize well, i.e., classiﬁers whose performa nce on the \ntraining set is represe ntativ e of how well it works for yet unseen imag es. \nModel selection \nSo how can we ﬁnd classiﬁers that generalize well? The key is to constra in the set of \npossible binary functio ns we can entertain. In other words, we would like to ﬁnd a class of \nbinar y functio ns such that if a functio n in this class works well on the training set, it is also \nlikely to work well on the unseen images. The “right” class of functio ns to consider canno t \nbe too large in the sense of containing too many clearly diﬀerent functions. Otherwise we \nare likely to ﬁnd rules similar to the trivial ones that are close to perfect on the traini ng \nset but do not generalize well. The class of function should not be too small either or we \nrun the risk of not ﬁnding any functions in the class that work well even on the traini ng \nset. If they don’t work well on the training set, how could we expect them to work well on \nthe new images? Finding the class of functio ns is a key problem in machine learning , also \nknown as the model selection problem. \nLinear classiﬁers throug h origin \nLet’s just ﬁx the function class for now. Speciﬁcally , we will consider only a type of linear \nclassiﬁers. These are thresholded linear mapping s from images to labels. More forma lly, \nwe only consider functio ns of the form \nf(x; θ) = sign θ1x1 + ... + θdxd = sign θT x (2) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "function", "section_heading": "then construct a simple binary functio n based on this single pixel that perfec tly maps the training imag es to their l", "source_title": "d26f49e758fa83b40c8f22496c857f14 lec1", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 1}
{"id": "cf91753c-6f31-4ce0-96a6-2fa6e3cb678e", "text": "classiﬁers. These are thresholded linear mapping s from images to labels. More forma lly, we only consider functio ns of the form f(x; θ) = sign θ1x1 + ... + θdxd = sign θT x (2) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n6.867 Mac hine learning , lectur e 1 (Jaakkola) 3 \nwhere θ =[θ1,...,θd]T is a column vector of real valued parameters. Diﬀeren t settings of \nthe parameters give diﬀeren t functio ns in this class, i.e., functions whose value or output \nin {−1, 1} could be diﬀeren t for some input imag es x. Put another way, the functions in \nour class are parameterize d by θ ∈Rd . \nWe can also understand these linea r classiﬁe rs geometrically . The classiﬁer chang es its \nprediction only when the argumen t to the sign functio n changes from positiv e to negat ive \n(or vice versa). Geometrically , in the space of imag e vector s, this transitio n corresp onds to \ncrossing the decision boundary where the argumen t is exactly zero: all x such that θT x = 0. \nThe equa tion deﬁnes a plane in d-dimens ions, a plane that goes throug h the origin since \nx = 0 satisﬁes the equatio n. The parameter vector θ is normal (ortho gonal) to this plane; \nthis is clear since the plane is deﬁned as all x for which θT x = 0. The θ vector as the \nnormal to the plane also speciﬁes the direc tion in the image space along which the value of \nθT x would increase the most. Figure 1 below tries to illustra te these concepts. \nimag es labeled +1 \nθT x > 0 \nx x decis ion boundary \nθ θT x =0 \nx \nx x x \nx x imag es labeled -1 \nx θT x < 0 \nx x \nFigur e 1: A linear classiﬁer throug h origin. \nBefore moving on let’s ﬁgure out whether we lost some useful properties of imag es as a \nresult of restricting ourselves to linear classiﬁers? In fact, we did. Consider, for example, \nhow nearby pixels in face images relate to each other (e.g., continuity of skin). This \ninforma tion is completely lost. The linear classiﬁer is perfectly happ y (i.e., its abilit y \nto classify images remains unchang ed) if we get imag es where the pixel positions have \nbeen reordered provided that we apply the same transformatio n to all the images. This \npermutation of pixels merely reorders the terms in the argumen t to the sign function in Eq. \n(2). A linear classiﬁer therefore does not have access to infor matio n about which pixels are \nclose to each other in the image. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "classiﬁers. These are thresholded linear mapping s from images to labels. More forma lly, we only consider functio ns of", "source_title": "d26f49e758fa83b40c8f22496c857f14 lec1", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "2c0cd9c0-46de-4c1f-b130-fefce545c889", "text": "the terms in the argumen t to the sign function in Eq. (2). A linear classiﬁer therefore does not have access to infor matio n about which pixels are close to each other in the image. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n6.867 Mac hine learning , lectur e 1 (Jaakkola) 4 \nLearning algorithm: the perceptron \nNow that we have chosen a function class (perhaps suboptimally) we still have to ﬁnd a \nspeciﬁc function in this class that works well on the training set. This is often referred to \nas the estimation problem. Let’s be a bit more precis e. We’d like to ﬁnd a linea r classiﬁe r \nthat makes the fewest mista kes on the training set. In other words, we’d like to ﬁnd θ that \nminimizes the training error \nˆE(θ) = 1 \nn n� � \n1 − δ(yt, f(xt; θ)) � \n= 1 \nn n� \nLoss(yt, f(xt; θ)) (3) \nt=1 t=1 \nwhere δ(y,y�)=1 if y = y� and 0 otherwise. The training error merely coun ts the average \nnumber of training imag es where the function predicts a label diﬀeren t from the label \nprovided for that image. More generally , we could compare our predictions to labels in \nterms of a loss function Loss(yt,f(xt; θ)). This is useful if error s of a particular kind are \nmore costly than others (e.g., letting a person enter the building when they shouldn’t) . For \nsimplicit y, we use the zero-one loss that is 1 for mistak es and 0 otherwis e. \nWhat would be a reasona ble algorithm for setting the parameters θ? Perhaps we can just \nincremen tally adjust the parameters so as to correct any mistakes that the corresponding \nclassiﬁer makes. Such an algorithm would seem to reduce the training error that counts \nthe mista kes. Perhaps the simplest algorithm of this type is the perceptron update rule. \nWe consider each training imag e one by one, cycling through all the imag es, and adjust \nthe parameters according to \nθ� ← θ + ytxt if yt =�f(xt; θ) (4) \nIn other words, the parameters (classiﬁer) is changed only if we make a mistak e. These \nupdates tend to correct mistakes. To see this, note that when we make a mista ke the sign \nof θT xt disag rees with yt and the product ytθT xt is negative; the product is positive for \ncorrectly class iﬁed images. Supp ose we make a mistak e on xt. Then the updated parameters \nare given by θ� = θ + ytxt, written here in a vector form. If we consider classif ying the \nsame image xt after the update, then \nytθ�T x t = yt(θ + ytxt)T xt = ytθT xt + yt 2 x T\nt xt = ytθT xt + �xt�2 (5) \nIn other words, the value of ytθT xt increases as a result of the update (becomes more \npositiv e). If we consider the same imag e repeatedly , then we will neces sarily chang e the \nparameters such that the image is classiﬁed correctly, i.e., the value of ytθT xt becomes \npositiv e. Mistakes on other imag es may steer the parameters in diﬀerent directions so it \nmay not be clear that the algorithm converges to something useful if we repeatedly cycle \nthrough the training images. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "the terms in the argumen t to the sign function in Eq. (2). A linear classiﬁer therefore does not have access to infor m", "source_title": "d26f49e758fa83b40c8f22496c857f14 lec1", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "9b28d974-18f5-4a66-9ba0-42cf95d339d6", "text": "becomes positiv e. Mistakes on other imag es may steer the parameters in diﬀerent directions so it may not be clear that the algorithm converges to something useful if we repeatedly cycle through the training images. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n6.867 Mac hine learning , lectur e 1 (Jaakkola) 5 \nAnalysis of the perceptron algorithm \nThe perceptro n algorithm ceases to update the parameters only when all the traini ng \nimag es are classiﬁe d correctly (no mista kes, no updates). So, if the training images are \npossible to classify correctly with a linear classiﬁer, will the perceptr on algorithm ﬁnd such \na classiﬁer? Yes, it does, and it will converge to such a classiﬁe r in a ﬁnite number of \nupdates (mistak es). We’ll show this in lectur e 2. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\nhttp://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n(", "topic": "Supervised Learning", "subtopic": "perceptron", "section_heading": "becomes positiv e. Mistakes on other imag es may steer the parameters in diﬀerent directions so it may not be clear that", "source_title": "d26f49e758fa83b40c8f22496c857f14 lec1", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "6fc45766-76d2-4d8f-a975-b852f2eccc3c", "text": "� 1 6.867 Mac hine learning , lectur e 14 (Jaak kola) \nLecture topics: \n• margin and generaliza tion \n– linear classiﬁers \n– ensembles \nmixtur e models • \nMargin and gene ralization: linear classiﬁers \nAs we increase the number of data points, any set of classiﬁers we are conside ring may no \nlonger be able to label the points in all possible ways. Such emerging constr aints are critical \nto be able to predict labels for new points. This motiv ates a key measure of complexit y of \nthe set of classiﬁers, the Vapnik-Chervonenkis dimension. The VC-dimension is deﬁned as \nthe maxim um number of points that a classiﬁer can shatter. The VC-dimension of linea r \nclassiﬁers on the plane is three (see previous lecture). Note that the deﬁnition involves a \nmaximum over the possible points. In other words, we may ﬁnd less than dVC points that \nthe set of classiﬁers canno t shatter (e.g., linear classiﬁers with points exactly on a line in \n2 − d) but there canno t be any set of more than dVC points that the classiﬁe r can shatter. \nThe VC-dimension of the set of linea r classiﬁers in d−dimensions is d+1, i.e., the number of \nparameters. This relat ion to the number of parameters is typical albeit certainly not always \ntrue (e.g., the VC-dimens ion may be inﬁnite for a classiﬁer with a single real parameter!). \nThe VC-dimension immedia tely genera lizes our previous results for bounding the expected \nerror from a ﬁnite number of classiﬁe rs. There are a number of technical steps involved that \nwe won’t get into, however. Loosely speaking, dVC takes the place of the logarithm of the \nnumber of classiﬁers in our set. In other words, we are coun ting the number of classiﬁers \non the basis of how they can label points, not based on their identities in the set. More \nprecise ly, we have for any set of classiﬁers F: with probabilit y at least 1 − δ over the choice \nof the training set, \nR(f) ≤ Rn(f)+ �(n,dVC ,δ), uniformly for all f ∈F (1) \nwhere the complexit y penalty is now a functio n of dVC = dVC (F): \ndVC (log(2n/d VC ) + 1) + log(1/(4δ))�(n,dVC ,δ) = (2) n \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "margin", "section_heading": "� 1 6.867 Mac hine learning , lectur e 14 (Jaak kola) ", "source_title": "44b0ad92b2f9d58e590783fa97b5edf3 lec14", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "9fe1df5a-f9f8-4399-bf5e-137cd2adfbb1", "text": "Rn(f)+ �(n,dVC ,δ), uniformly for all f ∈F (1) where the complexit y penalty is now a functio n of dVC = dVC (F): dVC (log(2n/d VC ) + 1) + log(1/(4δ))�(n,dVC ,δ) = (2) n Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� 2 6.867 Mac hine learning , lectur e 14 (Jaak kola) \nThe result is problematic for kernel methods. For example, the VC-dimens ion of kernel \nclassiﬁers with the radia l basis kernel is ∞. We can, however, incorp orate the notion of \nmargin in the classiﬁer “dimension” . One such deﬁnition is Vγ dimension that measures \nthe VC-dimension with the constra int that distinct labeling s have to be obtained with \na ﬁxed marg in γ. Supp ose all the examples fall within an enclosing sphere of radius R. \nThen, as we increase γ, there will be very few examples we can classify in all possible ways \nwith this constr aint (especially when γ R; cf. Figure 1). Put another way, the VC­ →\ndimension of a set of linea r classiﬁers required to attain a presc ribed margin can be much \nlower (decreasing as a functio n of the marg in). In fact, Vγ dimension for linear classiﬁers \nis bounded by R2/γ2, i.e., inversely prop ortiona l to the squared margin. Note that this \nresult is indep endent of the dimension of input examples, and is exactly the mistak e bound \nfor the perceptron algorithm! \nxo\noooo\noo\noo xx\nx\nx\nxx\nx\nFigur e 1: The set of linear classiﬁers required to obtain a speciﬁc geometr ic margin has a \nlower VC-dimension when the examples rema in within an enclosing sphere. \nThe previous generalizatio n guarantees can be used with Vγ dimension as well so long as \nwe replace the training error with marg in violations, i.e., we coun t the fraction of examples \nthat cannot be separa ted with margin at least γ. \nMargin and gene ralization: ensem bles \nAn ensem ble classiﬁer can be writt en as a convex combinat ion of simpler base classiﬁers \nm\nhm(x)= αj h(x; θj ) (3) \nj=1 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "convex", "section_heading": "Rn(f)+ �(n,dVC ,δ), uniformly for all f ∈F (1) where the complexit y penalty is now a functio n of dVC = dVC (F): dVC (l", "source_title": "44b0ad92b2f9d58e590783fa97b5edf3 lec14", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "c24ab19c-3b07-4413-8ce8-970e8283742b", "text": "with margin at least γ. Margin and gene ralization: ensem bles An ensem ble classiﬁer can be writt en as a convex combinat ion of simpler base classiﬁers m hm(x)= αj h(x; θj ) (3) j=1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� 3 6.867 Mac hine learning , lectur e 14 (Jaak kola) \nwhere αj ≥ 0 and m αj = 1. Boosting generates such ensem bles but does not normalized j=1 \nthe coeﬃcien ts αj to sum to one. The norma lizatio n can be easily perfor med after the fact. \nWe are interested in understa nding the complexit y of such ensem bles and how generalizat ion \nguarantees depends on the voting margin achieved, e.g., through boosting algorithm . Note \nthat our discussion here will not refer to how such ensem bles are generated. \nLet’s start by deﬁning what the ensem bles are not. They are not decision trees . A decision \n(classiﬁcatio n) tree is a metho d of recursiv ely partitioning the set of examples into regions \nsuch that within each regio n the examples would have as uniform labels as possible. The \npartitionin g in a decision tree could be based on the same type of decision stumps as we \nhave used for the ensem ble. In the ensem ble, however, the doma in for all the stumps is \nthe whole space. In other words, you canno t restrict the applica tion of the stump within \na speciﬁc partitio n. In the ensem ble, each stum p contribut es to the class iﬁcation of all the \nexamples. \nHow powerful are ensembles based on the decis ion stumps? To unde rstand this further let’s \nshow how we can shatter any n points with 2n stumps even in one dimensions. It suﬃces \nto show that we can ﬁnd an ensem ble with 2n stumps that repro duces any speciﬁc labeling \ny1,...,yn of n points x1,...,xn (now real numbers). To do so, we will construct an ensem ble \nof two stumps to repro duce the label yt for xt but without aﬀecting the classiﬁc ation of \nother training examples. If � is less than the smallest dista nce between any two training \nexamples, then \n1 1 hpair(x; xt,yt) = sign (yt(x − xt + �))+ sign(−yt(x − xt − �)) (4) 2 2\nhas value yt within interval [xt − �,xt + �] is zero everywhe re else. Thus, setting αt =1/n, \nh2n(x)= αthpair(x; xt,yt) (5) \nt=1 \nhas the correct sign for all the training example s. The ensem ble of 2n comp onents therefore \nhas VC-dimension at least n. Ensem bles are powerful as classiﬁe rs in this sense and their \nVC-dime nsion poorly explains their success in practice. \nEach example in the above construction only has a very low voting marg in 1/n, however. \nPerhaps we can similar ly reﬁne the analysis to incorporate the voting margin as we did \nabove with linear classiﬁers and the geometric marg in. The key idea is to reduce an \nensemble with many componen ts to a coarse ensemble with few comp onents but one that \nnevertheless classiﬁe s the examples in the same way. When the original ensem ble achieves \na large voting marg in this is indeed possible, and the size of the coarse appr oxima tion that \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "convex", "section_heading": "with margin at least γ. Margin and gene ralization: ensem bles An ensem ble classiﬁer can be writt en as a convex combin", "source_title": "44b0ad92b2f9d58e590783fa97b5edf3 lec14", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "f0647655-e037-4fa7-b095-bc63d1dbb053", "text": "one that nevertheless classiﬁe s the examples in the same way. When the original ensem ble achieves a large voting marg in this is indeed possible, and the size of the coarse appr oxima tion that Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n�� � 4 6.867 Mac hine learning , lectur e 14 (Jaak kola) \nwe need decreases with increasing voting marg in. In other words, if we achieve a large \nvoting margin, we could have solved the same classiﬁc ation problem with much smaller \nensemble insofar as we only pay attention to the classiﬁcat ion error. \nBased on this and other more technica l ideas we can show that with probability at least \n1 − δ over the choice of the training data, \nRn(hˆ) ≤ Rn(hˆ; ρ)+ O˜dVC \nn/ρ2 \n, (6) \nwhere the O˜() notation hides consta nts and logarithmic terms, Rn(hˆ; ρ) coun ts the number ·\nof training examples with voting marg in less than ρ, and dVC is the VC-dimens ion of the \nbase classiﬁers. Note that the result does not depend on the number of base classiﬁers in \nthe ensem ble hˆ. Note also that the eﬀective dimension dVC /ρ2 that the number of training \nexamples is compa red to has a similar form as before, decreasing with the marg in ρ. See \nSchapir e et al. (1998) for details. The paper is available from the course website as optional \nreading . \nMixture models \nThere are many problems in machine learning that are not simple classiﬁcat ion problems \nbut rather modeling problems (e.g., clustering, diagnosis, combining multiple informati on \nsources for sequence annota tion, and so on). Moreo ver, even within class iﬁcation problems, \nwe often have unobserv ed variables that would make a diﬀerence in terms of classiﬁcati on. \nFor example, if we are interested in classifying tissue samples into speciﬁc catego ries (e.g., \ntumo r type), it would be useful to know the composition of the tissue sampl e in terms \nof cells that are presen t and in what proportio ns. While such variables are not typica lly \nobserv ed, we can still model them and make use of their presence in predictio n. \nMixture models are simple probabilit y models that try to capture ambiguit ies in the avail­\nable data. They are simple , widely used and useful. As the name suggests, a mixtur e model \n“mix es” diﬀeren t predictions on the probabilit y scale. The mixing is based on alternat ive \nways of gener ating the observ ed data. Let x be a vector of observations. A mixture model \nover vector s x is deﬁned as follows. We assume each x could be of m possible types. If we \nknew the type, j, we would model x with a conditiona l distributio n P (x|j) (e.g., Gaussian \nwith a speciﬁc mean). If the overall frequency of type j in the data is P (j), then the \n Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "one that nevertheless classiﬁe s the examples in the same way. When the original ensem ble achieves a large voting marg ", "source_title": "44b0ad92b2f9d58e590783fa97b5edf3 lec14", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "206ee140-af69-4d32-9435-28b6a2143a12", "text": "the type, j, we would model x with a conditiona l distributio n P (x|j) (e.g., Gaussian with a speciﬁc mean). If the overall frequency of type j in the data is P (j), then the Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� � � � � 5 6.867 Mac hine learning , lectur e 14 (Jaak kola) \n“mix ture distribution” over x is given by \nm\nP (x)= P (x|j)P (j) (7) \nj=1 \nIn other words, x could be generated in m possible ways. We imag ine the genera tive \nprocess to be as follows: sample j from the frequenc ies P (j), then x from the corresp onding \nconditio nal distributio n P (x|j). Since we do not observ e the particular way the example \nwas generated (assuming the model is correct), we sum over the possibilities, weighted by \nthe overall frequenc ies. \nWe have already encoun tered mixt ure models. Take, for example, the Naive Bayes model \nP (x|y)P (y) over the feature vector x and label y. If we pool together exampl es labeled +1 \nand those labeled −1, and throw away the label informa tion, then the Naive Bayes model \npredicts feature vectors x accor ding to \nd\nP (x)= P (x|y)P (y)= P (xi|y) P (y) (8) \ny=±1 y=±1 i=1 \nIn other words, the distribut ion P (x) assumes that the examples come in two diﬀerent \nvarieties corresp onding to their label. This type of unobserved label informa tion is precisely \nwhat the mixtures aim to captur e. \nLet’s start with a simple two component mixture of Gaussians model (in two dimensions): \nP (x|θ)= P (1)N(x; µ1,σ12 I)+ P (2)N(x; µ2,σ22 I) (9) \nThe parameters θ deﬁning the mixtur e model are P (1), P (2), µ1, µ2, σ12, and σ22 . Figure \n2 shows data genera ted from such a model. Note that the frequencies P (j) (a.k.a . mixing \nproportions) control the size of the resulting clusters in the data in terms of how many \nexamples they involve, µj ’s specify the location of cluster centers, and σj 2’s control how \nspread out the clusters are. Note that each example in the ﬁgur e could in principle have \nbeen genera ted in two possible ways (whic h mixture comp onen t it was sampled from). \nThere are many ways of using mixtures. Conside r, for example, the problem of predicting \nﬁnal exam score vectors for studen ts in machine learning . Each observation is a vector \nx with components that specify the points the studen t received in a particula r questio n. \nWe would expect that diﬀeren t types of studen ts succee d in diﬀeren t types of questions. \nThis “student type” informa tion is not available in the exam score vectors, however, but \nwe can model it. Supp ose there are n studen ts taking the course so that we have n vector \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "the type, j, we would model x with a conditiona l distributio n P (x|j) (e.g., Gaussian with a speciﬁc mean). If the ove", "source_title": "44b0ad92b2f9d58e590783fa97b5edf3 lec14", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "f85c05ed-05b1-4350-9bcf-fac52ff5f719", "text": "questions. This “student type” informa tion is not available in the exam score vectors, however, but we can model it. Supp ose there are n studen ts taking the course so that we have n vector Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� \n� � � � � � � � 6 6.867 Mac hine learning , lectur e 14 (Jaak kola) \nFigur e 2: Samples from a mixt ure of two Gaussians \nobserv ations x1,..., xn. Suppose there are m underlying types of studen ts (the number \nof types can be inferred from the data; this is a model selectio n problem). We also don’t \nknow how many studen ts taking the course are of particular type, i.e., we have to estima te \nthe mixing prop ortions P (j) as well. The mixture distr ibution over a single example score \nvector is now given by \nm\nP (x|θ)= P (x|j)P (j) (10) \nj=1 \nWe won’t concern ourselves at this point with the problem of deciding how to parameterize \nthe conditio nal distribut ions P (xj). Suﬃce it to say that it wouldn’t be unreasona ble to �d |\nassume that P (x|j)= P (xi|j) as in the Naive Bayes model but each xi would take i=1 \nvalues in the range of scores for the corresp onding exam questio n. Now, our mixture model \nassumes that each student is of particula r type. If someo ne gave us this informat ion, i.e., \njt for xt, then we would model the observations with the conditio nal distributio n \nn\nP (x1,..., xnj1,...,jn,θ)= P (xtjt) (11) |\nt=1 |\nassuming each studen t obtains their score indep enden tly from others. But the type infor­\nmation is not present in the data so we will have to sum over the possible values of jt for \neach studen t, weighted by the prior probabilities of types, P (jt) (same for all studen ts): \nn m n m\nP (x1,..., xnθ)= P (xtjt)P (jt)= P (xtj)P (j) (12) |\nt=1 jt=1 |\nt=1 j=1 |\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "questions. This “student type” informa tion is not available in the exam score vectors, however, but we can model it. Su", "source_title": "44b0ad92b2f9d58e590783fa97b5edf3 lec14", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "9a8008c6-44d6-4a16-92f0-93cb0a3a3337", "text": "t, weighted by the prior probabilities of types, P (jt) (same for all studen ts): n m n m P (x1,..., xnθ)= P (xtjt)P (jt)= P (xtj)P (j) (12) | t=1 jt=1 | t=1 j=1 | Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � � � 7 6.867 Mac hine learning , lectur e 14 (Jaak kola) \nThis is the likeliho od of the observ ed data according to our mixtu re model. It is important \nto understa nd that the model would be very diﬀeren t if we exchang ed the product and the \nsum in the above expression, i.e., deﬁne the model as \nm n\nP (x1,..., xnθ)= j) P (j) (13) |\nj=1 t=1 P (xt|\nThis is also a mixt ure model but one that assumes that all studen ts in the class are of speciﬁc \nsingle type, we just don’t know which one, and are summing over the m possibilities (in \nthe previous model there were mn possible assignments of types over n studen ts). \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "if ", "section_heading": "t, weighted by the prior probabilities of types, P (jt) (same for all studen ts): n m n m P (x1,..., xnθ)= P (xtjt)P (jt", "source_title": "44b0ad92b2f9d58e590783fa97b5edf3 lec14", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 6}
{"id": "e9d929eb-4013-445f-ae76-dd68195fd6bd", "text": "6.867 Machine learning \nFinal exam \nDecem ber 5, 2002 \n(2 points) Your name and MIT ID: \n(4 points) The grade you would give to yourself + a brief justiﬁcat ion:\nProblem 1 \nWe wish to estima te a mixture of two experts model for the data displa yed in Figure 1. \nThe experts we can use here are linear regression models of the form \np(y|x, w)= N( y; w1x + w0,σ2 ) \nwhere N(y; µ,σ2) denotes a Gaussian distr ibution over y with mean µ and variance σ2 . \nEach expert i can choose its parameter s wi =[wi0,wi1]T and σi 2 indep endently from other \nexperts. Note that the ﬁrst subindex i in wij refers to the expert. \nThe gating network in the case of two experts is given by a logistic regression model \nP (expert = 1|x, v)= g( v1x + v0 ) \nwhere g(z) = (1+exp(−z))−1 and v =[v0,v1]T . \n1 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "logistic regression", "section_heading": "6.867 Machine learning ", "source_title": "1aaaf3360297b91047ad2120d92e5843 final f02", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "7393d5b7-2c27-4006-917d-efe6609c1ac0", "text": "expert. The gating network in the case of two experts is given by a logistic regression model P (expert = 1|x, v)= g( v1x + v0 ) where g(z) = (1+exp(−z))−1 and v =[v0,v1]T . 1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n0 1 2 3 4 5 6 7−1.5−1−0.500.511.522.533.5\nxy\n0 1 2 3 4 5 6 7−1.5−1−0.500.511.522.533.5\nxya) unreg ularized case b) regula rized case \nFigur e 1: Data for mixt ures of experts \n1. (4 points) Supp ose we estimat e a mixt ure of two experts model based on the data in \nFigur e 1. You can assume that the estima tion is succes sful in the sense that we will \nﬁnd a setting of the parameters that maximizes the log-likeliho od of the data. Please \nindicate (appro xima tely) in Figure 1a) the mean predictio ns from the two experts as \nwell as the decision bounda ry for the gating network. Label the mean predic tions \n– functio ns of x – with “(1)” and “(2)” corresponding to the two experts, and the \ndecis ion boundary with “gate”. \n2. (4 points) We now switc h to a regula rized maximum likeliho od objective by incor­\nporating the following regular ization penalt y \nc 2 2−2(w11 + w21) \ninto the log-likeliho od objectiv e. Note that the penalty includes only one parameter \nfrom each of the experts. By increasing c, we impose a stronger penalty. Simila rly to \nthe previo us questio n, please indicate in Figure 1b) the optima l regularized solutio n \nfor the mixture of two experts model when the regulariza tion parameter c isset to a \nvery large value. \n3. (3 points) Are the variances in the predictive Gaussian distributio ns of the experts \n( ) larger, \n( ) smaller, \n( ) about the same \nafter the regula rizatio n? \n2 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n012345678−2−101234\nxyFigur e 2: Diﬀerent data for mixtur es of experts \n4. (4 points) Consider again the unreg ularized mixture of two experts. If we tried to \nestimate this model on the basis of the data in ﬁgure 2, what would the solut ion look \nlike in this case? As before, indicat e the solutio n in the ﬁgur e. Brieﬂy justify your \nansw er. \n3\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "logistic regression", "section_heading": "expert. The gating network in the case of two experts is given by a logistic regression model P (expert = 1|x, v)= g( v1", "source_title": "1aaaf3360297b91047ad2120d92e5843 final f02", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "263cd051-83d0-4cb1-beac-281542832816", "text": "on the basis of the data in ﬁgure 2, what would the solut ion look like in this case? As before, indicat e the solutio n in the ﬁgur e. Brieﬂy justify your answ er. 3 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n1 2P(x=1)\nP(x=2)\nP(x=3)0s12\n0\n0.8\nP(x=4)0.1\n0.199\n0.001.01\n1 .990.20.7Figur e 3: A two-state HMM for Problem 2\n1 2 3 4\n.5.5\n.5.5 .5.5\n.5.01.49\nFigur e 4: An alter nativ e, four-stat e HMM for Problem 2 \nProblem 2 \nFigur e 3 shows a two-state HMM. The transition proba bilities of the Markov chain are \ngiven in the transition diagram. The output distributio n corresponding to each state is \ndeﬁned over {1, 2, 3, 4} and is given in the table next to the diagram. The HMM is equally \nlikely to start from either of the two states. \n1. (3 points) Give an example of an output sequence of length 2 which \ncan not be generated by the HMM in Figure 3. \n2. (2 points) We generat ed a sequence of 6, 8672002 observ ations from the \nHMM, and found that the last observation in the sequence was 3. Wha t \nis the most likely hidden state corresp onding to that last observ ation?\n3. (2 points) Consider an output sequence 3 3. What is the most likely \nsequenc e of hidden states corresponding to these observ ations? \n4. (2 points) Now, consider an output sequence 3 3 4. What are the ﬁrst \ntwo states of the most likely hidde n state sequence ? \n4\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n5. (4 points) We can try to increase the modeling capacit y of the HMM a bit by \nbreak ing each state into two states. Following this idea, we created the diagr am in \nFigur e 4. Can we set the initia l state distribut ion and the output distribut ions so \nthat this 4-state model, with the transition probabilities indic ated in the diagram, \nwould be equiv alent to the origina l 2-state model? If yes, how? If no, why not? \n6. (T/F – 2 points) The Markov chain in Figure 4 is ergodic\n5\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "hmm", "section_heading": "on the basis of the data in ﬁgure 2, what would the solut ion look like in this case? As before, indicat e the solutio n", "source_title": "1aaaf3360297b91047ad2120d92e5843 final f02", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "22bb570c-f996-4e46-b1d8-d0ec57d8c1a0", "text": "indic ated in the diagram, would be equiv alent to the origina l 2-state model? If yes, how? If no, why not? 6. (T/F – 2 points) The Markov chain in Figure 4 is ergodic 5 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 3 \nFigur e 5 shows a graphica l model over four binary valued variables, x1,...,x4. We do not \nknow the parameters of the probabilit y distribu tion associated with the graph. \nx1 x2 \nx3 x4 \nFigur e 5: A graphical model \n1. (2 points) Would it typically help to know the value of x3 so as to \ngain more informat ion about x2? (please answer yes or no) \n2. (2 points) Assume we already know the value of x4. Would it help in \nthis case to know the value of x3 to gain more information about x2? \n(please answ er yes or no) \n3. (3 points) List three diﬀerent conditio nal indep endence statemen ts between the four \nvariables that can be inferred from the graph. You can include margina l indep endence \nby saying “given nothing” . \n(a) ( ) is indep enden t of ( ) given ( ) \n(b) ( ) is indep enden t of ( ) given ( ) \n(c) ( ) is indep enden t of ( ) given ( ) \n4. (2 points) The following table gives a possible partial speciﬁc ation of the conditiona l \nprobabilit y P (x4x1,x2) associated with the graph. Fill in the missing values so that |\nwe could omit the arrow x1 → x4 in the graph and the graph would still adequately \nrepresen t the probabilit y distributio n. \nP (x4 =1x1 =0,x2 = 0) 0.8 |\nP (x4 =1x1 =0,x2 = 1) 0.4 |\nP (x4 =1x1 =1,x2 = 0) |\nP (x4 =1x1 =1,x2 = 1) |\n6\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "margin", "section_heading": "indic ated in the diagram, would be equiv alent to the origina l 2-state model? If yes, how? If no, why not? 6. (T/F – 2", "source_title": "1aaaf3360297b91047ad2120d92e5843 final f02", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "20fbce1d-f00f-4594-b12f-2358c5556fcf", "text": "the probabilit y distributio n. P (x4 =1x1 =0,x2 = 0) 0.8 | P (x4 =1x1 =0,x2 = 1) 0.4 | P (x4 =1x1 =1,x2 = 0) | P (x4 =1x1 =1,x2 = 1) | 6 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n5. (4 points) Let’s again focus on the original graph in Figure 5. Since we don’t know \nthe unde rlying proba bility distributio n, we need to estima te it from observ ed data. \nUnfort unately , the dataset we have is incomplet e and contains only observ ations for \nx2 and x3. In other words, the dataset is D = {(x2,xt t \n3),t =1,...,n}. In the joint\ndistributio n\nP (x1,x2,x3,x4)= P (x1)P (x2)P (x3|x1)P (x4|x1,x2) \nwe have a number of comp onents (smaller probabilit y tables) that need to be esti­\nmated. Please indicate which components we can hope to estimate (adjust) on the \nbasis of the available data? \n() P (x1)\n() P (x2)\n() P (x3|x1)\n() P (x4|x1,x2)\n6. (4 points) If we use the EM algorithm to carry out the estima tion task, what \nposterio r probabilities do we have to evaluat e in the E-step? Please provide the \nnecessary posterior probabilities in the form P (\n···\nt|x1,xt \n2).\n7\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "if ", "section_heading": "the probabilit y distributio n. P (x4 =1x1 =0,x2 = 0) 0.8 | P (x4 =1x1 =0,x2 = 1) 0.4 | P (x4 =1x1 =1,x2 = 0) | P (x4 =1", "source_title": "1aaaf3360297b91047ad2120d92e5843 final f02", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 4}
{"id": "42255793-77f2-4b5b-a6ab-c581d8941923", "text": "algorithm to carry out the estima tion task, what posterio r probabilities do we have to evaluat e in the E-step? Please provide the necessary posterior probabilities in the form P ( ··· t|x1,xt 2). 7 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 4 \nWe try to select here between two models. Both models are logistic regression models \nbut diﬀer in terms of the type of featur es that are used in making the predictions. More \nspeciﬁcally , the models have the commo n squashed additiv e form \nP (y =1|x, w)= g( w0 + w1φ1(x)+ ... + φm(x)) \nwhere the input is a real number x ∈R. The models diﬀer in terms of the number and the \ntype of basis functio ns used: \nmodel 1 : m =1,φ1(x)= x\nmodel 2 : m =2,φ1(x)= x, φ2(x) = sin(x)\n1. (1 points) The VC-dimension of the set of classiﬁers corresp onding to \nmodel 1 is \n2. (1 points) The VC-dimension of the set of classiﬁers corresp onding to \nmodel 2 is \n3. (4 points) Supp ose we have n training examples (xt,yt), t =1,...,n, and we eval­\nuate structur al risk minimizatio n scores (bounds on the generalizat ion error ) for the \ntwo classiﬁers. Whic h of the following statements are valid in genera l for our two \nmodels: \n( ) Score for model 1 ≥ Score for model 2 \n( ) Score for model 1 ≤ Score for model 2 \n( ) Score for model 1 = Score for model 2 \n( ) Each of the above three cases may be correct depending on the data \n( ) None of the above \n8\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "algorithm to carry out the estima tion task, what posterio r probabilities do we have to evaluat e in the E-step? Please", "source_title": "1aaaf3360297b91047ad2120d92e5843 final f02", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "ae80ff22-85dc-46c1-968a-c1a6ef164dc4", "text": "for model 2 ( ) Score for model 1 = Score for model 2 ( ) Each of the above three cases may be correct depending on the data ( ) None of the above 8 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n4. (4 points) We will now switch to the Bayesian infor matio n criterion (BIC) for select­\ning amon g the two models. Let L1(n) be the log-pro babilit y of the labels that model \n1 assigns to n training labels, where the probabilities are evaluat ed at the maxim um \nlikeliho od setting of the parameters. Let L2(n) be the corresp onding log-probabilit y \nfor model 2. We imagine here that L1(n) and L2(n) are evaluated on the basis of the \nﬁrst n training examples from a much larger set. \nNow, in our empirica l studies , we found that these log-probabilit ies are related in a \nsimple way: \nL2(n) − L1(n) ≈ 0.01 n· \nHow will we end up selecting between the two models as a function of the number of \ntraining examples? Please choose one of the following cases. \n( ) Always selec t 1 \n( ) Always selec t 2 \n( ) First select 1, then 2 for larger n \n( ) First select 2, then 1 for larger n \n5. (4 points) Provide a brief justiﬁcation for your answ er to the previous question.\n9\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "function", "section_heading": "for model 2 ( ) Score for model 1 = Score for model 2 ( ) Each of the above three cases may be correct depending on the ", "source_title": "1aaaf3360297b91047ad2120d92e5843 final f02", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 6}
{"id": "5ed30dff-ac8e-4f66-9f99-ce63d40afc93", "text": "( ) First select 1, then 2 for larger n ( ) First select 2, then 1 for larger n 5. (4 points) Provide a brief justiﬁcation for your answ er to the previous question. 9 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� Problem 5 \nConsider a simple two-class documen t (text) classiﬁca tion problem. Each documen t is \nrepresen ted by a binary feature vector [φi \n1,...,φi ], where φi = 1 if keyword k is present in N k \nthe documen t, and zero otherwise. N is the number of keywords we have chosen to include. \nWe use a Naiv e Bayes model for this classiﬁcatio n task. The joint distributio n of the \nfeatures and the binary labels y ∈{0, 1} is in this case given by \nN\nP (φ1,...,φN ,y)= P (y) P (φk|y) \nk=1 \nwhere, for example, \nP (φk =1|y = 0) = θk,0,P (φk =1|y = 1) = θk,1 \n1. (2 points) In the space below, draw the graphical model corresponding to Naiv e \nBayes generat ive model describ ed above. Assume that N = 3 (three keywords). \n2. (4 points) To be able to make use of training examples with possibly missing labels, \nwe will have to resort to the EM algorithm. In the EM algorithm we need to evaluate \nthe posterior proba bility of the label y given the docume nt. We will use a message \npassing algorithm (belief propaga tion) to get this poster ior probabilit y. The problem \nhere is that we relied on a rather careless friend to evaluat e whether a docume nt \ncontains any of the keywords. In other words, we do not fully trust the “observ ed” \nvalues of the features. Let φˆk be the “observed” value for the kth feature in a given \ndocumen t. The evidence we now have about the actual value of φk is given by \nP (φˆk|φk), which is a table that models how we expect the friend to respond. \nGiven that we observe φˆk = 1, what is the message that φk needs to send to y? \n10\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "( ) First select 1, then 2 for larger n ( ) First select 2, then 1 for larger n 5. (4 points) Provide a brief justiﬁcati", "source_title": "1aaaf3360297b91047ad2120d92e5843 final f02", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 7}
{"id": "083adc9d-1e17-4995-ba21-7462fb63d875", "text": "given by P (φˆk|φk), which is a table that models how we expect the friend to respond. Given that we observe φˆk = 1, what is the message that φk needs to send to y? 10 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n(t)3. (3 points) We know that the EM algorithm is in some sense monotonic. Let θˆ\nk,y be \nthe estima te of the parameters θk,y in the beginning of iteration t of the EM algorithm, \nand θ(t) be the vector of all parameter estima tes in that itera tion, [θˆ(t) θˆ1(t\n,1) ,..., θˆ(t) ].1,0, N,y=1\nWhic h of the following quan tities increa ses monot onically with t? \n(t)() P (φk =1y,θk,y) for all k �N |\n( ) �N P (φ1i ,...,φi |θ(t))i=1 N \n() P (yi =1φ1i ,...,φi ,θ(t))i=1 |N \n4. (4 points) The class labels actua lly correspond to “relev ant” and “irrelevant” doc­\numen ts. In classifying any documen t as relevant or irrelevant, we have to take into \naccoun t that we might prefer to miss a few relev ant documen ts if we can avoid mis­\nclassifying a large number of irrelev ant documen ts as relevant. To express such a \npreference we deﬁne a utilit y U(y, yˆ), where y is the correct label and ˆy is how we \nclassify the documen t. Draw an inﬂuence diagr am that incorp orates the Naiv e Bayes \nmodel, our decisions, and the utilit y. Mark each node in the graph according to the \nvariables (or utility) that they represent. \n5. (2 points) Let’s modify the Naiv e Bayes model a bit, to account for some of the \npossible dependencies between the keywords. For example, supp ose we order the \nkeywords so that it would be useful to model the depende ncy of φk on φk−1, k = \n2,...,N (the keywords may, for example, repres ent nested categories). We expect \nthese dependencies to be the same for each class, but the parameters can be aﬀected \nby the class label. Draw the graphica l model for this – call it Sophistic ated Bayes – \nmodel. Please assume again that N = 3. \n11\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "given by P (φˆk|φk), which is a table that models how we expect the friend to respond. Given that we observe φˆk = 1, wh", "source_title": "1aaaf3360297b91047ad2120d92e5843 final f02", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 8}
{"id": "72b250ad-5e94-4736-962d-0364babc530a", "text": "for each class, but the parameters can be aﬀected by the class label. Draw the graphica l model for this – call it Sophistic ated Bayes – model. Please assume again that N = 3. 11 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nAddi tional set of ﬁgures \nx2 x1 \nx3 x4 \n0 1 2 3 4 5 6 7−1.5−1−0.500.511.522.533.5\nxy\n012345678−2−101234\nxy\n1 2P(x=1)\nP(x=2)\nP(x=3)0s12\n0\n0.8\nP(x=4)0.1\n0.199\n0.001.01\n1 .990.20.7\n1 2 3 4\n.5.5\n.5.5 .5.5\n.5.01.49\n12\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "for ", "section_heading": "for each class, but the parameters can be aﬀected by the class label. Draw the graphica l model for this – call it Sophi", "source_title": "1aaaf3360297b91047ad2120d92e5843 final f02", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 9}
{"id": "54ebb694-e071-4d31-9c08-0613346318b9", "text": "Massachusetts Institute of Technology \n6.867 Machine Learning, Fall 2006 \nProblem Set 3: Solutions \n1. (a) (5 points) If A and B are random variables (RVs) with the same probability distribution, then \nE[f(A)] = E[f(B)]. This may be clearer when we write out the corresponding integral: \n[f(A)] � \nE = f(x)pA(x)dx = �\nf(x)p B (x)dx = E[f(B)] \nThis hold also when A and B are sets of RVs. In particular, A = {S−1\nn , (x1, y1)}, where we \nwould train on 1 Sn−\n and test on (x1, y1), has the same distribution as B = {Sn1, (x, y) − } where we \nwould train on another set of n − 1 samples Sn−1 and test on (x, y), also sampled from the same \ndistribution. \n(b) (4 points)This essentially follows from part (a) when we invoke linearity of expectation. First of \nall, we observe that the argument in part (a) generalizes to: \nerror 1(Sn) = error 2(Sn) = . . . = error i(Sn) = . . . error n(Sn) \nSo that \n1 �n 1 �n \nE[ ( error ˆ 2LOOCV Sn)] = E[(y i − fi(x− i)) ] = error i(S n) = error 1(Sn) n ni=1 i=1 \nIn the ﬁrst equality of the second equation, we have invoked the Linearity of Expectation Rule to \nmove “E” inside the summation. \n(c) (3 points) The variances will not be equal: error LOOCV is an estimate based on averaging the \nerror over n trials while error 1 is based on a single trial. Recall that if r numbers v1, v2, . . . , v r are \ndistributed i.i.d., the variance of the sample mean v¯ is 1/r times the variance of the vi’s (for large \nr). While the trials in error LOOCV are not independent, error LOOCV will nevertheless have lower \nvariance. \n(d) (4 points) There are two possible sets of assignments to xr’s such that the training error will be \nzero: xk = yk and xk = −yk. The former corresponds to fkeep and the latter to fflip. In total, \nthere are 2n possible assignments to xk’s. Thus, the probability of training error being exactly zero \nis 2/2n or 1 /2n−1. \n(e) (4 points) Supppose that the classiﬁer chosen after training is fˆ= fkeep (the argument for the case \nwhen fflip is chosen is essentially the same). Given a training error of �, the number of examples \nwith xk = yk is m1 = n(1 − � ) and number of examples with xk = −yk is m2 = n�/4 where n 4 \nis the number of examples (note that the previous computation holds for any �). The factor of \n4 comes about because each erroneous prediction incurs a penalty of 4 (= (1−(−1))2 or (−1−(1))2) \nWe claim that if � � 1/2, then in any cross-validation step, the chosen classiﬁer fˆ i will be fkeep. −\nHere’s why: in any cross-validation step, the number of examples of the above categories will \nchange by at most 1, i.e. |m1, i − m m − 1| ≤ 1 and similarly |2, i − m − 2| ≤ 1. Clearly, since m1 � m2, \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "if ", "section_heading": "Massachusetts Institute of Technology ", "source_title": "62aabb6f7aad6ddbb77d01581a0c3afa hw3 soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 0}
{"id": "fc5bc408-d938-4859-aafd-3e56e8f0ce5d", "text": "examples of the above categories will change by at most 1, i.e. |m1, i − m m − 1| ≤ 1 and similarly |2, i − m − 2| ≤ 1. Clearly, since m1 � m2, Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n�then m1,−i >m2,−i and so that fkeep will be chosen in the i-th cross-validation step. In other \nwords, each of the cross- validation classiﬁers will be the same as the trainin g class iﬁer. Clearly, \nthen trainin g error = cross-validation error. \n(f) (5 points) When � � 1/2, the result of the previous part implies that we can compute the desired \nquantity solely in terms of the training error. If training error = δ then the classiﬁer is maki ng \nnδ/4 mistakes (each mistak e costs 22 = 4). Suppose that along some dimension i, the training \nerror is ≤ �, i.e., the number of mistakes made is at most �n� If the number of mistakes made is �� 4 �. \nk, there are n\nk possible ways to arrange them over n exam ples, each such way having a probab ility \nof 21−n (analogous to results of part (d)). Then, the prob ability of error being less than � is: \nn� �� \n4��� n 1 p = k 2n−1 \nk=0 \nWe require that the prob ability that at least one dime nsion lead to error <� to be at least 1/2, \ni.e., the probabi lity that no dimension leads to error <� to be at most 1/2 i.e. \n(1 − p)d ≤ 1/2 \nThis does not include the one dimension that may be relev ant. Simp lifying the expression, we have \n1 d ≥ log21−1 \np \n2. (a) \nn n1 � 1 � \nP (Sn |J = {�})= 2 (1+ ytxt�)/2+ 2 (1 − ytxt�)/2 \nt=1 t=1 \n(b) The problem is that this assigns zero probabi lities to gettin g anything wron g (it doesn’t give partial \ncredit). Any technique that assigns a nonzero score to things that are close is ﬁne. The solution \nwe chose for part (c) with J = {�} is: \nn1 �� \nP (Sn |J = {�})= 2 f(ytθ,xt�), \nθ∈{+1,−1} t=1 \nwhere �\n1 − ε if y = x\nf(y,x)= ε if y = x. \nThat is, we assign positive probabi lity to gettin g it wrong. In general, the marginal likelihood \nbecomes: ⎡ ⎤ �� n\n⎣ ⎦ P (Sn |J )= � \n21 |J| � 1 � \nf(ytθj ,xtj ) . \nt=1 θ∈{+1,−1}|J | |J| j∈J \n(c) As the set becomes larger, the prob ability will increase (rapidly, at ﬁrst, and then it will level oﬀ). \nThis is because having more features gives us a better chance of choosing a featu re that matc hes \n(or closely matc hes) the labels. But the margi nal likeliho od is a model selection criterion. How is \nit that we prefer to include more randomly chosen featur es when the correct model (no dependence \nbetween x’s and y) has no features at all? The answ er lies in the amoun t of label noise . The correct \nvalue of label noise is 0.5 (labels chosen at random withou t regard to the inputs). Indeed, as you \nincrease the label noise paramete r, the margi nal likeliho od prefers fewer features. Setting the noise \nlevel correc tly is critical in guiding what aspects of the data the model shou ld try to capture. The \nnoise level would typically be estimate d in conjunction with the other parameters . \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "examples of the above categories will change by at most 1, i.e. |m1, i − m m − 1| ≤ 1 and similarly |2, i − m − 2| ≤ 1. ", "source_title": "62aabb6f7aad6ddbb77d01581a0c3afa hw3 soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "4f0bd652-1265-4928-9538-37e0cf1f1ceb", "text": "the noise level correc tly is critical in guiding what aspects of the data the model shou ld try to capture. The noise level would typically be estimate d in conjunction with the other parameters . Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n(d) We would stop adding featur es when the value of the margi nal likelihood starts decreasing. \n(e) Now, the likelihood will be virtuall y unity when the second featu re is inclu ded and will decrease \nas more featur es are includ ed. This is exactly how the model selection criterion should behave. \nAssuming a low level of label noise is correc t in this case. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "for ", "section_heading": "the noise level correc tly is critical in guiding what aspects of the data the model shou ld try to capture. The noise l", "source_title": "62aabb6f7aad6ddbb77d01581a0c3afa hw3 soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 2}
{"id": "d8815c3c-5636-421a-9a01-b1305885a798", "text": "� � � \n� � � \n� � Massa chusetts Institu te of Techn ology \n6.867 Machine Lear ning , Fall 2006 \nProbl em Set 4 \nDue Date: Thursda y, Nov 9, 12:00 noon\nYou may submit your solutions in class or in the box.\n1. We will explore here the use of the Bayesian Information Criterion (BIC) for model selection, speciﬁcally \nto select features for a Naive Bayes model. The input consists of d-dimensional binary featur e vectors \nx =[x1,...,xd]T where xi ∈ {−1, 1} and binary class labels y ∈ {−1, 1}. \nA Naive Bayes model may be used, for example, to classify emails as spam (y = 1) or not-s pam (y = −1). \nThe body of each email can be represented as a bag of words (i.e., we ignor e frequency, placement, and \ngrammar etc.). We could restrict ourselves to a dictionary of d words {w1,...,wd}. and coordinate xi \ncould serve as an indicator of word wi: xi =1 if wi is present in the email and xi = −1 if it is absent. \nRecall that the Naive Bayes model assume s that the featur es are conditionally independen t given the \nlabel so that \nd\nP (x,y)= P (xi|y) P (y) (1) \ni=1 \nWe parameterize our model by introducing separate parameters for each component: \nd\nP (x|y,θ)= P (xi|y,θi) where (2) \ni=1 \nxi+1 1−xi \n2 2P (xi|y,θi)= θi|y (1 − θi|y ) (3) \n(4) \nHere θ =[θ1,...,θd]T and θi =[θi|1,θi|−1]T so that \nθi|1 = P (xi =1|y = 1) \nθi|−1 = P (xi =1|y = −1) \nIn class, we have already discussed how to compu te the Maxi mum Likelihood estimates of these param­\neters. We will take a Bayesian approach, however, and introduce a prior probab ility over the parame ters \nθ as: \nd\nP (θ)= P (θi|y) (5) \ni=1 y∈{1,−1} \nΓ(r+ + r− + 2) + P (θi|y)= Γ(r+ + 1)Γ(r− + 1) · θir\n|y (1 − θi|y)r− (6) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "� � � ", "source_title": "f11ea4792f43a775187d45477bfd7952 hw4", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "6f7b69cb-03aa-4dda-92b0-2fdc9d1a0e89", "text": "probab ility over the parame ters θ as: d P (θ)= P (θi|y) (5) i=1 y∈{1,−1} Γ(r+ + r− + 2) + P (θi|y)= Γ(r+ + 1)Γ(r− + 1) · θir |y (1 − θi|y)r− (6) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� where r+ and r− are hyper-parameters , common acros s all the θi|y’s, and, for integer k, Γ(k +1) = k!. \nThe hyper-par ameters characte rize our beliefs about the paramete rs prior to seeing any data. You may \nassume here that r+ and r− are non-n egativ e integers. \n(a) Eqn 5 speciﬁes the prior P (θi|y) as a Beta distribution. This choice of the prior is particul arly \nconvenient, as we will see now. Show that the posterior distribution P (θ|D), given n training \nexam ples {(xj ,yj ) | j =1,...,n}, has the following form: \n+ �d� mi|y )m−\nP (θ|D) ∝ L(D; θ)P (θ)= θi|y (1 − θi|yi|y \ni=1 y∈{−1,1} \nwhere L(D; θ) is the likelihood function . What are m + \ni|y and m−\ni|y ? Your answ er should use the \nnˆy and ˆniy(xi,y) notation used in the lectures; e.g., ˆnky(1, −1) is the number of examples where \ny = −1 and xk = 1. \nYou have just shown that the Beta distrib ution is a conjugate prior to the Binomial distribution. \nIn other words, if the prior probab ility P (θ) has the form of a Beta distribution, and the likelihood \nP (D|θ) has the form of a Binomial distrib ution , then the posterior probability P (θ|D) will be a \nBeta distribution. When the class labels and features are not binary but take on k values, the same \nrelation ship holds between the Dirichlet and Multinomial distrib ution s. \n(b) Recall that, in the Bayesian scheme for model selection, our aim is to ﬁnd the model that maximize s \nthe marginal likeli hood, P (D|Fd), which is the normalization constant for the posterior: \nP (D|Fd)= L(D; θ)P (θ)dθ \nwhere Fd denot es the model. Com pute a close d-form expression for P (D|Fd). (Hint: use the form \nof the normalization constant for the beta distrib ution). \n(c) Let us now use our results to choose between two models F1 and F2. The only diﬀerence between \nthem is in how they use feature i: F2 involves P (xi|y) term that connects the feature to the \nlabel y whereas F1 only has P (xi), assuming that feature xi is independent of the label. In F2 \nthe distrib ution of xi’s is paramete rized by two paramete rs [θi|1,θi|−1]T as described before. In \ncontrast, F1 only requires a single paramete r θi�, \nxi+1 1−xi2P (xi|θ�)= θ� (1 − θ�) 2 (7)i i i\nThe prior probability over θi�is the same as Eqn 6: \nP (θi�) ∝ θi�r+ (1 − θi�)r− (8) \nThe expressions of the margin al likeliho od from the two models will diﬀer only in terms of featur e i. \nTo compare the two models we can ignore all the other terms . From your results in part (b), extract \nfrom P (D|F2) the factors involving feature i. Use a similar analysis to evaluate the terms involving \nfeatu re i in P (D|F1). Using these, evaluate the “decision rule”, log[P (D|F2)/P (D|F1)] > 0 to \ninclude feature xi. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "probab ility over the parame ters θ as: d P (θ)= P (θi|y) (5) i=1 y∈{1,−1} Γ(r+ + r− + 2) + P (θi|y)= Γ(r+ + 1)Γ(r− + 1)", "source_title": "f11ea4792f43a775187d45477bfd7952 hw4", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "782d6d01-f59f-48c0-bd68-d654b9b76740", "text": "(D|F2) the factors involving feature i. Use a similar analysis to evaluate the terms involving featu re i in P (D|F1). Using these, evaluate the “decision rule”, log[P (D|F2)/P (D|F1)] > 0 to include feature xi. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n�����\n� \n� �(d) Optional (Extra Credit): For many models the marginal likelihood is diﬃcult to compu te \nexactly and we have to an asymptotic approximation: \nBIC(D, Fr) = log L(D; θˆML) − r log n2 \nWe will now see (rough ly speakin g) how to relate BIC and P (D|F) in the general case. Our \nintuition is as follows: as the number of examples n increases the posterior distribution P (θ|D) \nbecomes more and more peaked aroun d the maximum likelihood estimate θˆML. In the limit n →∞, \nthe posterior distribution become s a point estimate at θˆML. We then approximate the integral by \nevaluatin g it only in the neigh bourhood of θˆML. We start by writing: \nP (D|Fr) = exp(l og L(D; θ))P (θ)dθ \nand perform a Taylor series expansion of log L(D; θ) around θˆML: \n1 θML)T A1 + ˆ log ( )+( ;L θ θ D − ML\n����ˆ\n θˆML)T A2(θ − θˆML), log L(D; θ)\n (θ −\n where\n =\n2\n∂ log L(D; θ)A1 = ∂θ θˆML \n∂2 log L(D; θ)A2 = ∂θ∂θT \nθˆML \nThe matrix A2 has interesting properties (it’s called the Fisher Information Matri x), but its most \nrelevant property here is that the determin ant |A2|≈ nC(r) where C(r) does not depend on the \nnumber of examples n. \nWe have performed a second-order Taylor series expansion , i.e. up to the second deriv ative. What \nwould the problem be if we had performe d only a ﬁrst-ord er expan sion? \n(e) Optional (Extra Credit): Now, assume that the prior is uninformativ e, i.e P (θ) \nspeakin g, it should be 1/( dθ), but the constan t factor will not matter). Show that with the second \norder approximation = 1 (strictly\nL(D; θ)P (θ)dθ ≈ L(D; θˆML) 2π �d/2 \nn\nC1(r) \nwhere C1(r) consists of terms that do not depend on the number of exam ples n. \n(f) Optional (Extra Credit): Using your result from part (g), argue that\nlim log P (D|Fr)= BIC(D, Fr)\nn→∞ \nThis is the motivation behind the use of the BIC score for model selection. \n2. Kenn y and Cartman are playing a game ; in this game, Cartman draws samples from a normal distribu­\ntion and gives them to Kenny, who must determine what the parame ters (i.e., the mean and variance) \nof the distribution are. Kenny knows that he can simply report the sample mean and sample variance , \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare \n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "(D|F2) the factors involving feature i. Use a similar analysis to evaluate the terms involving featu re i in P (D|F1). U", "source_title": "f11ea4792f43a775187d45477bfd7952 hw4", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "11fccbf4-cf78-42a4-b17c-a1178537386f", "text": "tion and gives them to Kenny, who must determine what the parame ters (i.e., the mean and variance) of the distribution are. Kenny knows that he can simply report the sample mean and sample variance , Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� � � � � � as they are the maximum-likelihood estimators for the mean and variance . However, it turns out that \nCartman is cheatin g. Inste ad of keeping the parame ters ﬁxed, he occassional ly changes them without \ntellin g Kenny. \nKenn y is, however, very cunning, and catches on. He decides to use BIC to determin e when Cartman \nis changing the paramete rs. \n(a) Given data X1,X2,...,XN , write a formula for the BIC for the hypothes is that there are m change-\npoints t1, t2, ..., tm, with \nXi ∼ N(µk, Σk) when tk−1 ≤ i<tk, (9) \nwhere t0 = 0 and tm+1 = N for convenience. \n(b) How can Kenny use this to determine the number of change-points and where they are? Write \na MATLAB program to execute this procedure assuming that there is at most one change- point \nand plot the BIC as a function of the hypothesized change-point for the dataset hw4p3b. Your \nalgor ithm should run in linear time in the number of sampl es! \n(c) Kenn y realize s that this procedure is broadly applicable. He ﬁnds an audio ﬁle of three students \nsaying hello and, by assuming that students generate speech by drawing cepstra from multivariate-\nnormal distributions, whose parameters depend on things like the studen t’s vocal chords and their \nposition relativ e to the microphone and so on, he discovers that he can use BIC to segmen t the \naudio ﬁle to ﬁnd a speaker change! What’s more, using a sliding-win dow algor ithm that his TA \nprovided (multisplit.m), he ﬁnds that he can ﬁnd multiple change-points. Try it out! (Use the \ndata in hw4p3d.) How many speaker changes are there and when do they occur? Now try hw4p3e. \nWhat are the speaker changes now? Do they make sense? If not, what went wron g? \n3. AdaBo ost is a simple algor ithm for estimating ensembles \nm\nhm(x)= αj h(x; θj ) (10) \nj=1 \nwhere αj ≥ 0 (not necessarily summing to one) and we have so far assumed that each h(x; θj ) is a \ndecis ion stump. The basic boosting algori thm can be easily extended, however. For exam ple, we can \nreplace the decision stumps with diﬀeren t base classiﬁers and/or we can use a diﬀerent loss function to \ntrain the ensemble. The exponential loss used in AdaBoost is not very forgiving about outliers and so \nwe will try to use another loss function, along with diﬀere nt base learners. \nAt stage m of the algori thm, we we will try to minimize \nn n\nJ(αm,θm) = Loss ythm(xt) = Loss ythm−1(xt)+ ytαmh(xt; θm) (11) \nt=1 t=1 \nwhere we assume that hm−1(x) is ﬁxed from m−1 previous rounds and view J as a function of parameters \nαm and θm associated with the new base classiﬁer. The margi n loss Loss() could be any function that ·\nis convex and decreas ing in its argume nt (smaller loss for predictions with larger voting margin ). \nLet’s elaborate on the revised algori thm a bit. We can initialize h0(x) = 0 for the ensemble with no base \nlearn ers. Then, after m − 1 rounds, we perform two distin ct steps to optimize θm and αm, respectively. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "convex", "section_heading": "tion and gives them to Kenny, who must determine what the parame ters (i.e., the mean and variance) of the distribution ", "source_title": "f11ea4792f43a775187d45477bfd7952 hw4", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "0403585b-0b59-420c-b7a6-7d2f441cad9b", "text": "algori thm a bit. We can initialize h0(x) = 0 for the ensemble with no base learn ers. Then, after m − 1 rounds, we perform two distin ct steps to optimize θm and αm, respectively. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � \n� � � \n� \n� � First, we ﬁnd parameters θˆm that minimize \n� ��−Wm��−1(t) \n�� \nd � n n\ndαm J(αm,θm)��\nαm=0 = \nt=1 dL ythm−1(xt) yth(xt; θm)= − \nt=1 Wm−1(t) yth(xt; θm) (12) \nwhere dL(z)= d/dz Loss(z) is always negative because the loss is decreasing. The form of the opti­\nmization problem for θm is exactly as in AdaBoost except that the weights will be diﬀere nt due to the \ndiﬀerent margin loss. We can also normalize the weights in the algorith m since the normalization would \nnot aﬀect the resulting θˆm. \nOnce we have θˆm, we can ﬁnd ˆαm that minimizes \nn\nJ(αm,θˆm) = Loss ythm−1(xt)+ ytαmh(x; θˆm) (13) \nt=1 \nThis is easy to solve since Loss() is convex and we only have one parame ter to optimize . ·\nWe can now write the general boosting algorith m more succinctly . After initializing W˜0(t)=1/n, each \nboosting iteration consists of the following three steps: \n(Step 1) Find θˆm that (approximately) minimizes the weighted error �m or 2�m − 1 given by \nn\n− W˜m−1(t) yth(xt; θm) (14) \nt=1 \n(Step 2) Find ˆαm that minimizes \n�n � � \nJ(αm,θˆm) = Loss ythm−1(xt)+ ytαmh(x; θˆm) (15) \nt=1 \n(Step 3) Reweight the exampl es \nW˜m(t)= −cm dL ythm−1(xt)+ ytαˆmh(x; θˆm (16) \nwhere cm normalizes the new weights so they sum to one. \nNow that we have a boosting algorith m for any loss function , we can select a particular one. Speciﬁcally, \nwe will consider the logistic loss: \nLoss(z) = log (1 + exp(−z)) (17) \n(a) Show that the unnormalized weights Wm(t) from the logistic loss are bounded by 1. What can you \nsay about the resulti ng normalized weights for exampl es that are clearly misclassiﬁed in compari son \nto those that are just slightly misclas siﬁed by the current ensemble? \n(b) Suppose the training set is linearl y separable and we would use a linear support vector machine \n(no slack penalties ) as a base classiﬁer. In the ﬁrst boosting iteration , what would the resulting ˆα1 \nbe? \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "convex", "section_heading": "algori thm a bit. We can initialize h0(x) = 0 for the ensemble with no base learn ers. Then, after m − 1 rounds, we perf", "source_title": "f11ea4792f43a775187d45477bfd7952 hw4", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "23503060-2170-41c0-a3a1-4d13350378ff", "text": "the training set is linearl y separable and we would use a linear support vector machine (no slack penalties ) as a base classiﬁer. In the ﬁrst boosting iteration , what would the resulting ˆα1 be? Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n(c) Let’s consider a bit simpler base learners but still those th at can be com bined into strong predictors. \nSo, we deﬁne our base learners as radial basis functions centered at particular training points, i.e., \nh(x; θ)= yt exp(−β�x − xt�2) (18) \nwhere the parame ter θ simply identiﬁes the training point t. These base learners retur n real valued \npredictions h(x; t) ∈ [−1, 1]. Provide an argume nt for why an ensemble with n such base learners \ncan in princip le classify a set of n points in all possible ways. \n(d) It seems that the boosting algor ithm with such base learn ers migh t seriou sly overﬁt. Let’s see if \nthis is true in practice . We have provided you with data and all the necessary matlab code to run \nthe boosting algor ithm with the radial basis learners. The ﬁles are availab le in hw4/pr ob3. The \nfunction you need to run is call boosting.m. Does it overﬁt? \n(e) To understand this a bit better let’s plot the voting margin errors as a function of the number of \nboosting iterations. Recall that a margin error at level ρ occurs whenev er yth˜m(xt) − ρ ≤ 0 where \nαj ’s in the ensemble h˜m(xt) are normalized to sum to one. Plot the margin errors at levels ρ =0.1 \nand ρ =0.5. You should be able to do this with only a minor modiﬁcation to the code we have \nprovided. Can you use these plots to explai n your observation s in part d)? \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "the training set is linearl y separable and we would use a linear support vector machine (no slack penalties ) as a base", "source_title": "f11ea4792f43a775187d45477bfd7952 hw4", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "8f819be6-c23d-4fd0-b9f1-e7338b80f6dc", "text": "� 1 6.867 Mac hine learning , lectur e 16 (Jaak kola) \nLecture topics: \n• Mixture of Gaussians (cont’d) \n• The EM algorithm: some theory \n• Additio nal mixture topics \n– regula rizat ion \n– stage-wise mixt ures \n– conditio nal mixtur es \n• Mixture models and clustering \nMixture of Gaussi ans (cont’d) \nRecall the simple Gaussian mixture model \nm\nP (x; θ)= P (j)N(x; µj , Σj ) (1) \nj=1 \nWe have already discussed how to estima te the parameters of such models with the EM \nalgorithm. The E and M steps of the algorithm were: \n(E-st ep) Evaluate the posterior assignmen t proba bilities p(l)(j|t)= P (j|xt,θ(l)) \nbased on the current setting of the parameters θ(l). \n(M-s tep) Update the parameters accor ding to \nn\nP (l+1)(j)= nˆ(\nnj) , where nˆ(j)= � \np(l)(j|t) (2) \nt=1 \nn\nµj (l+1) = nˆ(1 \nj) � \np(l)(j|t)xt (3) \nt=1 \nn\nΣ(l+1) = nˆ(1 \nj) � \np(l)(j|t)(xt − µ(l+1) )(xt − µ(l+1) )T (4) j j j\nt=1\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Unsupervised Learning", "subtopic": "clustering", "section_heading": "� 1 6.867 Mac hine learning , lectur e 16 (Jaak kola) ", "source_title": "735de5b1396c8364cc51a5b1d479760e lec16", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "bfd25591-5250-437e-8b0b-f5f08da056db", "text": "nj) , where nˆ(j)= � p(l)(j|t) (2) t=1 n µj (l+1) = nˆ(1 j) � p(l)(j|t)xt (3) t=1 n Σ(l+1) = nˆ(1 j) � p(l)(j|t)(xt − µ(l+1) )(xt − µ(l+1) )T (4) j j j t=1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2 6.867 Mac hine learning , lectur e 16 (Jaak kola) \nThe fact that the EM algorithm is itera tive, and can get stuck in locally optima l solut ions, \nmeans that we have to pay attention to how the parameters are initialized. For example, \nif we initia lly set all the components to have identical means and covariances, then they \nwould remain identical even after the EM iteratio ns. In other words, they would be changed \nin exactly the same way. So, eﬀectively, we would be estimat ing only a single Gauss ian \ndistributio n. To understand this, note that the parameter updates (above) are based solely \non the posterio r assignmen ts. If the parameters of any two comp onents are identical, so will \ntheir posterio r proba bilities. Identical posterio rs then lead to identical updates. Note that \nsetting the prior probabilit ies diﬀerently while keeping the comp onen t models initialized \nthe same would still lead to this degenerate result. \nIt is necessary to provide suﬃcie nt variatio n in the initializa tion step. We can still set the \nmixing proportions to be unifor m, P (0)(j)=1/m, and let all the covariance matr ices equal \nthe overall data covariances. But we should randomly position the Gaussian components, \ne.g., by equa ting the means with rando mly chosen data points. Even with such initial­\nizatio n the algorithm would have to be run multiple times to ensure we will ﬁnd a good \nsolutio n. Figure 1 below exempliﬁes how a particular EM run with four components (with \nthe suggested initializatio n) can get stuck in a locally optimal solut ion. The data in the \nﬁgur e came from a four comp onen t mixt ure. \n−4 −2 0 2 4 6 8−4−2024681012\nFigur e 1: A locally optimal Gaussian mixture with four componen ts\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "if ", "section_heading": "nj) , where nˆ(j)= � p(l)(j|t) (2) t=1 n µj (l+1) = nˆ(1 j) � p(l)(j|t)xt (3) t=1 n Σ(l+1) = nˆ(1 j) � p(l)(j|t)(xt − µ(", "source_title": "735de5b1396c8364cc51a5b1d479760e lec16", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 1}
{"id": "591e7a56-2017-41bc-a348-afa54877b56f", "text": "solut ion. The data in the ﬁgur e came from a four comp onen t mixt ure. −4 −2 0 2 4 6 8−4−2024681012 Figur e 1: A locally optimal Gaussian mixture with four componen ts Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� \n� � \n� � � � 3 6.867 Mac hine learning , lectur e 16 (Jaak kola) \nEM theory \nLet’s understand the EM-algorithm a bit better in a context of a general mixture model of \nthe form \nm\nP (x; θ)= P (j)P (x|θj ) (5) \nj=1 \nwhere θ collects together both {P (j)} and {θj }. The goal is the maximize the log-likeliho od \nof data D = {x1,..., xn} \nn\nl(D; θ) = log P (xt; θ) (6) \nt=1 \nAs before, let’s start with a complete data version of the problem and assume that we are \ngiven D = {x1,..., xn} as well as the corresp onding assignmen ts J = {j1,...,jn}. The \ncomplete log-likelihood of all the observ ations is given by: \nm n� � \nl(D, J ; θ)= δ(j|t)log P (j)P (xt|θj ) (7) \nj=1 t=1 \nwhere δ(j|t) = 1if j = jt and zero otherwis e. If θ(l) denote the curren t setting of the \nparameters, then the E-step of the EM-algorithm corresp onds to replacing the hard assign­\nments δ(j|t) with soft posterior assignmen ts p(l)(j|t)= P (j|x,θ(l)). This replacement step \ncorresponds to evaluat ing the expected complete log-likeliho od \nm n�� \nEl(D, J ; θ)D,θ(l) = p(l)(jt)log P (j)P (xθj ) (8) |\nj=1 t=1 | |\nThe expectation is over the assignmen ts given D = {x1,..., xn} and the current setting \nof the parameters so that E{δ(j|t)|xt,θ(l)} = P (j|x,θ(l))= p(l)(j|t). The rationale here \nis that we average over variables whose values we do not know but can guess relat ive \nto the curren t model, completing the incomplete observ ations. Note that there are two \nsets of parameters involved in the above expression. The current setting θ(l) that deﬁne d \nthe posterior assignments p(l)(j|t) for completing the data, and θ that we are now free to \noptimize over. Once we have completed the data, i.e., evaluat ed the posterio r assignmen ts \np(l)(j|t), they won’t change as a function of θ. The E-step simply casts the incomplete \ndata prob lem back into a complete data problem. In the M-step of the EM algorithm, we \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "function", "section_heading": "solut ion. The data in the ﬁgur e came from a four comp onen t mixt ure. −4 −2 0 2 4 6 8−4−2024681012 Figur e 1: A local", "source_title": "735de5b1396c8364cc51a5b1d479760e lec16", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 2}
{"id": "d090e5d1-cb21-4fae-859d-d2264093b068", "text": "posterio r assignmen ts p(l)(j|t), they won’t change as a function of θ. The E-step simply casts the incomplete data prob lem back into a complete data problem. In the M-step of the EM algorithm, we Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� � � 4 6.867 Mac hine learning , lectur e 16 (Jaak kola) \ntreat D = {x1,..., xn} and the soft completio ns p(l)(j|t) as if they were observ ed data, and \nmaximize the expected log-likeliho od with respect to θ while keeping everything else ﬁxed. \nLet’s relate the EM algorithm a bit more ﬁrmly to the goal of maximizing the log-likelihood \nl(D; θ). We will show that the EM algorithm is actua lly optimizing an auxilia ry objective \nthat forces the log-likeliho od up from below. To this end, let Q = {q(j|t)} be any set of \ndistributio ns over the unde rlying assignmen ts, not necessarily the posterior assignmen ts \np(l)(j|t). The auxiliar y objectiv e that the EM algorithm is using is \nm n n��� �� \nl(D,Q; θ)= q(j|t)log P (j)P (x|θj )+ H(q(·|t)) (9) \nj=1 t=1 t=1 \nwhere H(q(·|t)) = − m q(j|t)log q(j|t) is the entropy of the assignment distribut ion j=1 \nq(·|t). We view l(D,Q; θ) as a function of Q and θ and have used q(j|t) for the assignmen t \ndistributio ns so as to emphasize that they can be set indep enden tly of the parameters \nθ in this objectiv e. The EM algorith m can be seen as an algorithm that alternating ly \nmaximizes l(D,Q; θ), with respect to Q for ﬁxed θ (E-step) and with respect to θ for a \nﬁxed Q (M-s tep). To make this a bit more precise , we use θ(l) for the curren t setting of the \nparameters and let Q(l) denote the assignmen t distribut ions that are really the posterio rs, \ni.e., q(l)(j|t)= p(l)(j|t)= P (j|x,θ(l)). It is possible to show relativ ely easily that l(D,Q; θ(l)) \nattains the maximum with respect to Q exact ly when Q = Q(l), i.e., when the assignment \ndistributio ns are the posterio rs P (j|x,θ(l)). The EM-algorithm is now deﬁned as \n(E-st ep) Q(l) = arg max Q l(D,Q; θ(l)) \n(M-s tep) θ(l+1) = arg max θ l(D,Q(l); θ) \nThe E-step above recovers the posterior assignmen ts, q(l)(j|t)= p(l)(j|t), and the M-step, \nwith these posterio r assignmen ts, maximizes \nm n�� n\nl(D,Q(l); θ)= p(l)(j|t)log P (j)P (x|θj )+ H(p(l)(·|t)) (10) \nj=1 t=1 t=1 \nwhich is exactly as before since the entropy term is ﬁxed for the purpose of optimizing θ. \nSince each step in the algorithm is a maximizatio n step, the objectiv e l(D,Q; θ) has to \nincrease monotonically . This monotone increa se is precisely what unde rlies the monot one \nincrease of the log-likelihood l(D; θ) in the EM algorithm. Indeed, we claim that our \nauxiliary objectiv e equals the log-likeliho od after any E-step. In other words, \nl(D,Q(l); θ(l))= l(D; θ(l)) (11) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "posterio r assignmen ts p(l)(j|t), they won’t change as a function of θ. The E-step simply casts the incomplete data pro", "source_title": "735de5b1396c8364cc51a5b1d479760e lec16", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "213f0b26-d47b-4a50-bf70-bef9b599a62d", "text": "the monot one increase of the log-likelihood l(D; θ) in the EM algorithm. Indeed, we claim that our auxiliary objectiv e equals the log-likeliho od after any E-step. In other words, l(D,Q(l); θ(l))= l(D; θ(l)) (11) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � � \n� � � � \n� � \n� 5 6.867 Mac hine learning , lectur e 16 (Jaak kola) \nIt is easy (but a bit tedious) to verify this by substituting in the posterior assignmen ts: \nm n� � n\nl(D,Q(l); θ(l))= p(l)(j|t)log P (l)(j)P (x|θj (l))+ H(p(l)(·|t)) (12) \nj=1 t=1 t=1 \n�m n� n m\n= p(l)(j|t)log P (l)(j)P (x|θj (l)) − p(l)(j|t)log p(l)(j|(13) t)\nj=1 t=1 t=1 j=1 \n�m n (l)� \n(l)(jP (l)(j)P (xθj ) = \nj=1 t=1 p |t)log p(l)(j|t)|(14) \nm n (l) \n= �� \nP (jxt,θ(l))log P (l)(j)P (x|θj ) (15) \nj=1 t=1 |P (j|xt,θ(l)) \nm n\n= P (j|xt,θ(l))log P (xt; θ(l)) (16) \nj=1 t=1 \nn\n= log P (xt; θ(l))= l(D; θ(l)) (17) \nt=1 \nNote that the entropy term in the auxilia ry objectiv e is neces sary for this to happ en. Now, \nwe are ﬁnally ready to state that \nl(D;θ(l)) l(D;θ(l+1))� �� � M-step E-step � �� � M-step\nl(D,Q(l); θ(l)) ≤ l(D,Q(l); θ(l+1)) ≤ l(D,Q(l+1); θ(l+1)) ≤ ... (18)\nwhich demonstra tes that the EM-alg orithm monotonically increases the log-likeliho od. The \nequalit y above holds only at convergence . \nAddi tional mixture topics: regulariza tion \nRegularizat ion plays an important role in the context of any ﬂexible model and this is true \nfor mixtures as well. The number of parameters in an m−comp onent mixtur e of Gaussians \nmodel in d−dimensions is exactly m − 1+ md + md(d + 1)/2 and can easily become a \nproblem when d and/ or m are large. To include regular ization we need to revise the basic \nEM-algorithm formulated for maxim um likelihood estima tion. From the point of view of \nderiving new update equa tions, the eﬀect of regular ization will be the same in the case of \ncomplete data (with again the resulting δ(j|t)’s replaced by the posterio rs). Let us therefore \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "the monot one increase of the log-likelihood l(D; θ) in the EM algorithm. Indeed, we claim that our auxiliary objectiv e", "source_title": "735de5b1396c8364cc51a5b1d479760e lec16", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "09119774-ee5d-49f1-bf58-6de4b195865a", "text": "of view of deriving new update equa tions, the eﬀect of regular ization will be the same in the case of complete data (with again the resulting δ(j|t)’s replaced by the posterio rs). Let us therefore Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � � � \n� � � � 6 6.867 Mac hine learning , lectur e 16 (Jaak kola) \nsimplify the setting a bit and consider regularizing the model when the observ ations are \ncomplete. The key terms to regularize are the covariance matrices. \nWe can assign a Wishar t prior over the covariance matrices. This is a conjuga te prior over \ncovariance matrices and will behave nicely in terms of the estimatio n equa tions. The prior \ndistributio n depends on two parameters, one of which is a common covariance matr ix S \ntowards which the estimat es of Σj ’s will be pulled. The degree of “pull” is speciﬁed by n� \nknown as the equivalent sample size. n� speciﬁes the number of data points we would have \nto observ e for the data to inﬂuenc e the solut ion as much as the prior. More forma lly, the \nlog-wishar t penalty is given by \nn� n� \nlog P (Σj |S,n�) = const. − 2 Trace(Σ−\nj 1S) − 2 log |Σj| (19) \nGiven data D = {x1,..., xn}, and assignmen ts J = {j1,...,jn}, the penalized complete \nlog-likeliho od function is given by \nm n\nl(D, J ; θ)= δ(j|t) log P (j) \nj=1 t=1 \nm n\n+ δ(j|t)log N(xt; µj , Σj ) + log P (Σj |S,n�) (20) \nj=1 t=1 \nThe solutio n (steps not provided) chang es only slightly and, natu rally, only in terms of the \ncovariance matrices1: \nn\nPˆ(j)= nˆ(\nnj) , where nˆ(j)= � \nδ(j|t) (21) \nt=1 \nn1 � \nµˆj = nˆ(j) δ(j|t)xt (22) \nt=1] ⎡ ⎤ \nn1 � \nΣˆj = nˆ(j)+ n� ⎣ δ(j|t)(x t − µˆj )(xt − µˆj )T +n�S⎦ (23) \nt=1] \nThe res ulting co variance up dates can b e used as part of the EM alg orithm simply by repla c­\ning δ(jt) with the posterior assignments p(l)(jt), as before. The choice of S depends on | |\nΣ (the overall data covariance). the problem but could be, e.g., either the identity matrix or ˆ\n1Note that the regularization penalty corresponds to having observed n� samples with covarian ce S \nfrom the same Gaussian. This update rule is therefore not cast in terms of inverse covarian ce matrice s as \nit would when combining noisy sources with diﬀerent noise covarian ces. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "of view of deriving new update equa tions, the eﬀect of regular ization will be the same in the case of complete data (w", "source_title": "735de5b1396c8364cc51a5b1d479760e lec16", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "887542ff-8c01-41c0-ae1b-0198fd8faaaf", "text": "samples with covarian ce S from the same Gaussian. This update rule is therefore not cast in terms of inverse covarian ce matrice s as it would when combining noisy sources with diﬀerent noise covarian ces. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� \n� \n� � � 7 6.867 Mac hine learning , lectur e 16 (Jaak kola) \nAddi tional mixture topics: sequen tial estimation \nMixture models serve two diﬀeren t purposes. They provide eﬃcien tly parameterized den­\nsities (distributio ns) over x but can also help unco ver structure in the data, i.e., identify \nwhich data points belong to which groups. We will discus s the latter more in the context \nof clustering and focus here only on predicting x. In this setting, we can estima te mixtur e \nmodels a bit more eﬃciently in stages, similarly to boosting. In other words, supp ose we \nhave already estimat ed an m − 1 mixtu re \nm−1\nPˆm−1(x)= Pˆ(j)P (x|θˆj ) (24) \nj=1 \nThe comp onen t distributio ns could be Gaussians or other densities or distributions. We \ncan now imagine adding one more component while keeping Pˆm−1(x) ﬁxed. The resulting \nm−comp onen t mixture can be parameterized as \nP (x; pm,θm) = (1 − pm)Pˆm−1(x)+ pmP (x|θm) (25) \nm−1\n= [(1 − pm)Pˆ(j)]P (x|θˆj )+ pmP (x|θm) (26) \nj=1 \nNote that by scaling down Pˆm−1(x) with 1−pm we can keep the overall mixture prop ortions \nto sum to one regardless of pm ∈ [0, 1]. We can now estima te pm and θm via the EM \nalgorithm. The beneﬁt is that this estimatio n problem is much simpler than estimatin g \nthe full mixt ure; we only have one component to adjust as well as the weight assigned to \nthat componen t. The EM-algorithm for estimating the comp onen t to add is given by \n(E-st ep) Evaluate the posterior assignment probabilities pertaining to the new com­\nponen t: \n(l) (l) \np(l)(mt)= (l) pm P (x|θm ) \n(l) (l) (27) |\n(1 − pm )Pˆm−1(x)+ pm P (x|θm ) \n(l+1) (l+1) (M-s tep) Obtain new parameters pm and θm \n(l+1) t=1 pm = n p(l)(m|t) (28) n \nn\nθ(l+1) = arg max p(l)(mt)log P (xtθm) (29) m θm | |\nt=1 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "samples with covarian ce S from the same Gaussian. This update rule is therefore not cast in terms of inverse covarian c", "source_title": "735de5b1396c8364cc51a5b1d479760e lec16", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "3786fe4a-53d4-4089-aa55-6652d296536b", "text": "pm P (x|θm ) (l+1) (l+1) (M-s tep) Obtain new parameters pm and θm (l+1) t=1 pm = n p(l)(m|t) (28) n n θ(l+1) = arg max p(l)(mt)log P (xtθm) (29) m θm | | t=1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n8 6.867 Mac hine learning , lectur e 16 (Jaak kola) \nNote that in Eq.(29) we maximize the expected log-likeliho od where the hard assignmen ts \nδ(m|t) have been replaced with their expected values, i.e., soft posterior assignmen ts \np(l)(m|t). This is exactly how the updates were obtained in the case of Gaussian mix­\ntures as well. \nNote that it is often necess ary to regula rize the mixture comp onen ts even if they are added \nsequen tially . \nAddi tional mixture topics: conditional mixtures \na) \n−1 −0.5 0 0.5 1−1.4−1.2−1−0.8−0.6−0.4−0.200.2 b) \n−1 −0.5 0 0.5 1−1.4−1.2−1−0.8−0.6−0.4−0.200.2\nc) \n−1 −0.5 0 0.5 100.10.20.30.40.50.60.70.80.91 d) \n−1 −0.5 0 0.5 1−1.4−1.2−1−0.8−0.6−0.4−0.200.2\nFigur e 2: a) A regression problem, b) linear regression models (experts) and their assigned \nregio ns, c) gating network output (probabilit y that we select the blue expert on the right), \nd) the mean output from the conditiona l mixture distributio n as a function of the input . \nWe can also use mixt ure models for regression. These are known as conditional mixtur es \nor mixtur es of experts models. Consider a simple linear regression model \nP (y|x,θ,σ2)= N(y; θT x,σ2) (30) \nThe goal is to predict the responses as a functio n of the input x. In many cases the responses \nare not linear functions of the input (see Figure 2a). This leaves us with a few choices. We \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "linear regression", "section_heading": "pm P (x|θm ) (l+1) (l+1) (M-s tep) Obtain new parameters pm and θm (l+1) t=1 pm = n p(l)(m|t) (28) n n θ(l+1) = arg max ", "source_title": "735de5b1396c8364cc51a5b1d479760e lec16", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 7}
{"id": "2b9899b9-510a-444b-9004-ceb4b3bd11de", "text": "is to predict the responses as a functio n of the input x. In many cases the responses are not linear functions of the input (see Figure 2a). This leaves us with a few choices. We Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� � \n� 9 6.867 Mac hine learning , lectur e 16 (Jaak kola) \ncould try to ﬁnd a feature mapping φ(x) so as to ensure that a linear function in the featur e \nspace can capture the non-line arities that relate x and y. It may not be always clear what \nthe feature vector should be, however (in case of Figure 2a the relevant feature mapp ing \nwould be φ(x)= |x|). Another approa ch is to divide the input space into regio ns such that \nwithin each regio n the relation between x and y can be captured with a linear function. \nSuch a model requires us to be able to predict how to assign linear functions (experts) to \nregio ns. In conditio nal mixt ures, this assignmen t is carried out by another model, so called \ngating network. The gating network could, for example, be a logistic regression model \nP (j =1|x,η,η0)= g( ηT x + η0 ) (31) \nand P (j =2|x,η,η0)=1 − g( ηT x + η0 ) (we would have to use a softmax function for \ndividing the space into m regio ns). The overall model for the responses y given x would \nthen be a conditio nal mixtur e \n2\nP (yx)= P (jx,η,η0)N(y; θjT x,σj 2) (32) |\nj=1 |\nNote that the real diﬀerence between this and a simple mixture is that the mixing propor­\ntions as well as the comp onent models are conditio ned on the input. Such models can be \nagain estimated via the EM-alg orithm: \n(E-st ep) Evaluate the posterior assignmen t probabilit ies: \np(l)(j|t)= P (j|xt,η(l),η0(l\nP ))\n(N\nl)((\nyy\ntt\n|;(\nxtθ\n) (l))T xt, (σ(l))2) (33) \nNote that the assignments are based on both xt and yt. \n(M-s tep) Obtain new parameters by maximizing the expected log-likeliho ods relativ e \nto the posterio r assignmen ts: \n2 n\n{η(l+1),η(l+1) (l)(j = arg max p t)log P (jxt,η,η0) (34) 0 } \nη,η0 | |\nj=1 t=1 \nn\n{θj (l+1) ,σj (l+1) = arg max p(l)(jt)log N(yt; θjT xt,σj 2) (35) \nθj ,σj} \nt=1 |\nThes e estima tion equatio ns are readily obtained from the comp lete log-likelihood \nversio n pertaining to the diﬀerent conditio nal probabilit ies and by replacing δ(j|t) \nwith the posterior assignmen ts. \nFigur e 2 illustra tes the resulting mixtur e of experts model in the toy problem. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "is to predict the responses as a functio n of the input x. In many cases the responses are not linear functions of the i", "source_title": "735de5b1396c8364cc51a5b1d479760e lec16", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 8}
{"id": "495733de-1dac-491c-af93-af0957e02b0f", "text": "log-likelihood versio n pertaining to the diﬀerent conditio nal probabilit ies and by replacing δ(j|t) with the posterior assignmen ts. Figur e 2 illustra tes the resulting mixtur e of experts model in the toy problem. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� 10 6.867 Mac hine learning , lectur e 16 (Jaak kola) \nMixture models and clustering \nWe have so far used mixture models as ﬂexib le ways of constructing probabilit y models for \nprediction tasks. The motivation behind the mixt ure model was that the available data \nmay include unobserv ed (sub)g roups and by incorp orating such structur e in the model we \ncould obtain more accurate predictions. We are also interested in uncovering that group \nstructure. This is a clustering problem. For example, in the case of modeling exam scores it \nwould be useful to understa nd what types of studen ts there are. In a biological context, it \nwould be useful to unco ver which genes are activ e (expressed) in which cell types when the \nmeasuremen ts are from tissue samples involving multiple cell types in unkno wn quantities. \nClustering problems are ubiquito us. \nMixture models as generative models requir e us to articulate the type of clusters or sub­\ngroups we are looking to identify. The simplest type of clusters we could look for are \nspherical Gaussian clusters, i.e., we would be estimating Gaussian mixt ures of the form \nm\nP (x; θ,m)= P (j)N(x; µj ,σj 2I) (36) \nj=1 \nwhere the parameters θ include {P (j)}, {µj }, and {σj 2}. Note that we are estimat ing the \nmixtur e models with a diﬀeren t objective in mind. We are more interes ted in ﬁnding where \nthe clusters are than how good the mixture model is as a generative model. \nThere are many questions to resolve. For example, how many such spherical Gauss ian \nclusters are there in the data? This is a model selection problem. If such clusters exist, \ndo we have enough data to identify them? If we have enough data, can we hope to ﬁnd \nthe clusters via the EM algorithm? Is our appro ach robust, i.e., does our metho d degrade \ngracefully when data contain “background samples” or impurities along with spherical \nGaussian clusters? Can we make the clustering algorithm more robust? \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Unsupervised Learning", "subtopic": "clustering", "section_heading": "log-likelihood versio n pertaining to the diﬀerent conditio nal probabilit ies and by replacing δ(j|t) with the posterio", "source_title": "735de5b1396c8364cc51a5b1d479760e lec16", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 9}
{"id": "53e64ceb-134e-4065-a9a0-b7cc5e36819a", "text": "� \n� 1 6.867 Mac hine learning , lectur e 15 (Jaak kola) \nLecture topics: \n• Diﬀeren t types of mixture models (cont’d) \n• Estimating mixtures: the EM algorithm \nMixture models (cont’d) \nBasic mixture model \nMixture models try to capture and resolve observ able ambiguit ies in the data. E.g., an \nm−comp onen t Gaussian mixt ure model \nm\nP (x; θ)= P (j)N(x; µj , Σj ) (1) \nj=1 \nThe parameters θ include the mixing proportio ns (prio r distribut ion) {P (j)}, means of \ncomp onen t Gaussians {µj }, and covariances {Σj }. The notation {P (j)} is a shorthand for \n{P (j),j =1,...,m}. \nTo genera te a sample x from such a model we would ﬁrst sample j from the prior distribu­\ntion {P (j)}, then samp le x from the selected comp onen t N(x; µj , Σj ). If we genera ted n \nsamples, then we would get m potentially overlapping clusters of points, where each cluster \ncenter would corresp ond to one of the means µj and the number of points in the clusters \nwould be approximately nP (j). This is the type of structure in the data that the mixture \nmodel is trying to captur e if estima ted on the basis of observ ed x samples. \nStuden t exam model: 1-year \nWe can model vectors of exam scores with mixture models. Each x is a vector of scores from \na particular studen t and samples corresp ond to students. We expect that the populati on \nof studen ts in a particular year consists of m diﬀeren t types (e.g., due to diﬀerences in \nbackground). If we expect each type to be presen t with an overall probabilit y P (j), then \neach studen t score is modeled as a mixt ure \nm\nP (xθ)= P (xθj )P (j) (2) |\nj=1 |\nwhere we sum over the types (weighted by P (j)) since we don’t know what type of student \nt is prior to seeing their exam score xt. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare \n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "� ", "source_title": "a5a5b7b5a8c2eb7d7d23eac5c7f3a9af lec15", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "f597e5c9-57cc-4461-bad4-3d893e2b4866", "text": "(xθ)= P (xθj )P (j) (2) | j=1 | where we sum over the types (weighted by P (j)) since we don’t know what type of student t is prior to seeing their exam score xt. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � � � � \n� \n� � � � � � 2 6.867 Mac hine learning , lectur e 15 (Jaak kola) \nIf there are n studen ts taking the exam a particular year, then the likeliho od of all the \nstuden t score vectors, D1 = {x1,..., xn}, would be \nn n m\nL(D 1; θ)= P (xt|θ)= P (x|θj )P (j) (3) \nt=1 t=1 j=1 \nStuden t exam model: K-years \nSupp ose now that we have student data from K years of oﬀering the course. In year k we \nhave nk studen ts who took the course. Let xk,t denote the score vector for a student t in \nyear k. Note that t is just an index to identify samples each year and the same index does \nnot imply that the same studen t took the course multiple years. We can now assume that \nthe number of studen t types as well as P (x|θj ) remain the same from year to year (the \nparameters θj are the same for all years). However, the populati on of studen ts may easily \nchang e from year to year, and thus the prior probabilities over the types have to be set \ndiﬀeren tly. Let P (j|k) deno te the prior proba bilities over the types in year k (all of these \nwould have to be estimated of course). Now, according to our mixtur e distribution, we \nexpect example scores for students in year k be sampled from \nm\nP (xk,θ)= P (xθj )P (jk) (4) |\nj=1 | |\nThe likelihood of all the data, across K years, D = {D1,...,DK }, is given by \n�Knk Knk m\nL(D; θ)= P (xk,t|k,θ)= P (xk,t|θj )P (j|k) (5) \nk=1 t=1 k=1 t=1 j=1 \nThe parameters θ here include the mixing portio ns {P (j|k)} that change from year to year \nin additio n to {θj }. \nCollaborati ve ﬁltering \nMixture models are useful also in recommender systems. Supp ose we have n users and \nm movies and our task is to recommend movies for users. The users have each rated a \nsmall fractio n of movies and our task is to ﬁll-in the rating matrix, i.e., provide a predicted \nrating for each user across all m movies. Such a prediction task is known as a collaborative \nﬁltering problem (see Figure 1). \nSay the possible ratings are rij ∈{1,..., 5} (i.e., how many stars you assign to each movie). \nWe will use i to index users and j for movies; a rating rij , if provided, speciﬁes how user \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare \n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "(xθ)= P (xθj )P (j) (2) | j=1 | where we sum over the types (weighted by P (j)) since we don’t know what type of student", "source_title": "a5a5b7b5a8c2eb7d7d23eac5c7f3a9af lec15", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "d0588a40-f0b5-4c52-bae6-5c80f73c4e9a", "text": "the possible ratings are rij ∈{1,..., 5} (i.e., how many stars you assign to each movie). We will use i to index users and j for movies; a rating rij , if provided, speciﬁes how user Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � 3 6.867 Mac hine learning , lectur e 15 (Jaak kola) \nrij \n2 \n1 3 \n5 \n5 \n2 2 users i movies j \nFigur e 1: Partially observed rating matrix for a collaborative ﬁltering task. \ni rated movie j. Since only a small fraction of movies are rated by each user, we need a \nway to index these elemen ts of the user/mo vie matrix: we say (i,j) ∈ ID if rating rij is \navailable (observ ed). D denotes all the observed rating s. \nWe can build on the previous discuss ion on mixture models. We can represen t each movie \nas a distributio n over “movie types” zm ∈{1,...,Km}. Simila rly, a user is represen ted \nby a distribution over “user types” zu ∈{1,...,Ku}. We do not assume that each movie \ncorresponds to a single movie type across all users. Instea d, we interpret the distribut ion \nover movie types as a bag of features corresponding to the movie and we resample from this \nbag in the context of each user. This is analogous to predic ting exam scores for studen ts \nin a particular year (we didn’t assume that all the studen ts had the same type). We also \nmake the same assumptio n about user types, i.e., that the type is sampled from the “bag” \nfor each rating, resulting potentially in diﬀerent types for diﬀeren t movies. Since all the \nunobserv ed quan tities are summed over, we do not explicitly assign any ﬁxed movie/user \ntype to a rating. \nWe imagine generatin g the rating rij associated with (i,j) eleme nt of the rating matrix as \nfollows. Sample a movie type from P (zm|j), sample a user type from P (zu|i), then sample \na rating rij with proba bility P (rij |zu,zm). All the probabilit ies involved have to estimated \nfrom the available data. Note that we will resample movie and user types for each ratin g. \nThe resulting mixture model for rating rij is given by \nKuKm\nP (rij i,j,θ)= P (rij zu,zm)P (zui)P (zmj) (6) |\nzu=1 zm=1 | | |\nwhere the parameters θ refer to the mapping from types to rating s {P (r|zu,zm)} and the \nuser and movie speciﬁc distributio ns {P (zu|i)} and {P (zm|j)}, respectiv ely. The likeliho od \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare \n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "the possible ratings are rij ∈{1,..., 5} (i.e., how many stars you assign to each movie). We will use i to index users a", "source_title": "a5a5b7b5a8c2eb7d7d23eac5c7f3a9af lec15", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "a3fe51c9-da51-4b4f-963f-efad1f712839", "text": "| | | where the parameters θ refer to the mapping from types to rating s {P (r|zu,zm)} and the user and movie speciﬁc distributio ns {P (zu|i)} and {P (zm|j)}, respectiv ely. The likeliho od Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� �� � 4 6.867 Mac hine learning , lectur e 15 (Jaak kola) \nof the observed data D is \nL(D; θ)= P (rij|i,j,θ) (7) \n(i,j)∈ID \nUsers rate movies diﬀeren tly. For example, some users may only use a part of the scale \n(e.g., 3, 4 or 5) while others may be bi-mo dal, rating movies either very bad 1 or very good \n5. We can impr ove the model by assuming that each user has a rating style s ∈{1,...,Ks}. \nThe styles are assumed to be the same for all users, we just don’t know how to assign each \nuser to a particular style. The prior probabilit y that any randomly chosen user would have \nstyle s is speciﬁed by P (s). These parameters are commo n to all users. We also assume \nthat the rating predictions P (rij |zu,zm) associated with user/mo vie types now depend on \nthe style s as well: P (rij |zu,zm,s). \nWe have to be a bit careful in writing the likelihood of the data. All the ratings of one user \nhave to come from one rating style s but we can sum over the possibilit ies. As a result, the \nlikeliho od of the observ ed ratings is modiﬁed to be \nlikeliho od of user i’s rating s with style s \n� � �� n� KuKm � Ks� �� \nL�(D; θ)= P (s) P (rij |zu,zm,s)P (zu|i)P (zm|j) (8) \ni=1 s=1 j:(i,j)∈ID zu=1 zm=1 \nThe model does not actually involve that many parameters to estima te. There are exactly \n{P (s)} {P (r|zu,zm,s)} {P (zu|i)} {P (zm|j)}����� �� ������ �� � \n(Ks − 1)+ (5 − 1)KuKmKs + n(K u − 1)+ m(K m − 1) (9) \nindep enden t parameters in the model. \nA realistic model would include, in addition, a prediction of the “missing elements” in the \nrating matrix, i.e., a model of why the entry was missing (a user failed to rate a movie they \nhad seen, not seen but could, chosen not to see, etc.). \nEstimating mixtures: the EM-al gorithm \nWe have seen a number of diﬀeren t types of mixtur e models. The advantage of mixture \nmodels lies in their abilit y to incorp orate and resolv e ambiguities in the data, especially \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare \n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "| | | where the parameters θ refer to the mapping from types to rating s {P (r|zu,zm)} and the user and movie speciﬁc di", "source_title": "a5a5b7b5a8c2eb7d7d23eac5c7f3a9af lec15", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "d19e72b9-b52b-4782-8a3e-5941c68a54a1", "text": "EM-al gorithm We have seen a number of diﬀeren t types of mixtur e models. The advantage of mixture models lies in their abilit y to incorp orate and resolv e ambiguities in the data, especially Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� \n� � \n� � � \n� � � � 5 6.867 Mac hine learning , lectur e 15 (Jaak kola) \nin terms of uniden tiﬁed sub-groups. However, we can make use of them only if we can \nestimate such models easily from the available data. \nComplete data. The simplest way to unde rstand how to estima te mixt ure models is \nto start by pretending that we knew all the sub-typing (comp onen t) assignm ents for each \navailable data point. This is analo gous to knowing the label for each example in a clas­\nsiﬁcatio n context. We don’t actually know these (they are unobserved in the data) but \nsolving the estimat ion problem in this context will help us later on. \nLet’s begin with the simple Gaussian mixture model in Eq.(1 ), \nm\nP (x; θ)= P (j)N(x; µj , Σj ) (10) \nj=1 \nand pretend that each observ ation x1,..., xn also had informa tion about the comp onen t \nthat was responsible for generating it, i.e., we also observ ed j1,...,jn. This additio nal \ncomp onen t informat ion is convenien t to include in the form of 0-1 assignmen ts δ(j|t) where \nδ(jt|t)=1 and δ(j|t)=0 for all j �= jt. The log-likeliho od of this complete data is \nn� � \nl(x1,..., xn,j1,...,jn; θ) = log P (jt)N(xt; µjt , Σjt ) (11) \nt=1 \nn m� � \n= δ(j|t)log P (j)N(xt; µj , Σj ) (12) \nt=1 j=1 \n�m n\n= δ(j|t) log P (j) \nj=1 t=1 \nm n\n+ δ(j|t)log N(xt; µj , Σj ) (13) \nj=1 t=1 \nIt’s important to note that in trying to maximize this log-likeliho od, all the Gaussians can be \nestimated separa tely from each other. Put another way, because our pretend observations \nare “complete”, we can estimate each comp onent from only data pertaining to it; the \nproblem of resolv ing which component should be responsible for which data points is not \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare \n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "EM-al gorithm We have seen a number of diﬀeren t types of mixtur e models. The advantage of mixture models lies in their", "source_title": "a5a5b7b5a8c2eb7d7d23eac5c7f3a9af lec15", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "d4598c7c-d1bd-43ff-ab82-96ea4614645c", "text": "another way, because our pretend observations are “complete”, we can estimate each comp onent from only data pertaining to it; the problem of resolv ing which component should be responsible for which data points is not Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n6 6.867 Mac hine learning , lectur e 15 (Jaak kola) \npresen t. As a result, the maximizing solut ion can be written in closed form: \nˆP (j) = ˆn(j) \nn , where ˆn(j) = n� \nt=1 δ(j|t) (14) \n1 n� \nˆµj = ˆn(j) t=1] δ(j|t)x t (15) \nˆΣj = 1 \nˆn(j) n� \nt=1] δ(j|t)(x t − ˆµj )(xt − ˆµj )T (16) \nIn other words, the prior probabilities simply recover the empirical fractio ns from the “ob­\nserved” j1,...,jn, and each Gaussian comp onent is estimat ed in the usual way (evaluat ing \nthe empir ical mean and the covariance) based on data points explicitly assigned to that \ncomp onen t. So, the estimatio n of mixture models would be very easy if we knew the \nassignmen ts j1,...,jn. \nIncom plete data. What changes if we don’t know the assignmen ts? We can always guess \nwhat the assignmen ts should be based on the curren t setting of the parameters. Let θ(l) \ndenote the curren t (initia l) parameters of the mixt ure distributio n. Using these parameters, \nwe can evaluate for each data point xt the posterior proba bility that it was generat ed from \ncomp onen t j: \nP (l)(j)N(xt; µj (l) , Σj (l)) P (l)(j)N(xt; µj (l) , Σj (l))P (j|xt,θ(l))= � m P (l)(j�)N(xt; µ(l) (l) = P (xt; θ(l)) (17) \nj�=1 j� , Σj� ) \nInstead of using the 0-1 assignments δ(jt) of data points to comp onen ts we can use “soft \n(l)(j|\nassignmen ts” p|t)= P (j|xt,θ(l)). By substituting these in the above closed form es­\ntimating equa tions we get an iterat ive algorithm for estimat ing Gaussian mixtures. The \nalgorithm is iterative since the soft posterio r assignmen ts were evaluated based on the cur­\nrent setting of the parameters θ(l) and may have to revised later on (once we have a better \nidea of where the clusters are in the data). \nThe resulting algorithm is known as the Expectation Maximization algorithm (EM for short) \nand applies to all mixtur e models and beyond. For Gaussian mixtures, the EM-algorithm \ncan be writt en as \n(Step 0) Initialize the Gaussian mixtur e, i.e., specify θ(0). A simple initializat ion \n(0)consists of setting P (0)(j)=1/m, equatin g each µj with a rando mly chosen data \n(0)point, and making all Σj equal to the overall data covariance. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare \n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "if ", "section_heading": "another way, because our pretend observations are “complete”, we can estimate each comp onent from only data pertaining ", "source_title": "a5a5b7b5a8c2eb7d7d23eac5c7f3a9af lec15", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 5}
{"id": "353ce6bb-a519-4520-acd0-fae654d253d5", "text": "Gaussian mixtur e, i.e., specify θ(0). A simple initializat ion (0)consists of setting P (0)(j)=1/m, equatin g each µj with a rando mly chosen data (0)point, and making all Σj equal to the overall data covariance. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� 7 6.867 Mac hine learning , lectur e 15 (Jaak kola) \n(E-st ep) Evaluate the posterior assignmen t proba bilities p(l)(j|t)= P (j|xt,θ(l)) \nbased on the current setting of the parameters θ(l). \n(M-s tep) Update the parameters accor ding to \nn\nP (l+1)(j)= nˆ(\nnj) , where nˆ(j)= � \np(l)(j|t) (18) \nt=1 \nn\nµj (l+1) = nˆ(1 \nj) � \np(l)(j|t)xt (19) \nt=1 \nn1 � \nΣ(l+1) = nˆ(j) p(l)(j|t)(xt − µ(l+1) )(xt − µ(l+1) )T (20) j j j \nt=1 \nPerhaps surprisingly , this iterative algorithm is guaranteed to converge and each iterati on \nincreases the log-likeliho od of the data. In other words, if we write \nn\nl(D; θ(l))= P (xt; θ(l)) (21) \nt=1 \nthen \nl(D; θ(0)) <l(D; θ(1)) <l(D; θ(2)) < ... (22) \nuntil convergence. The main downside of this algorithm is that we are only guaranteed \nto converge to a locally optimal solution where d/dθ l(D; θ) = 0. In other words, there \ncould be a setting of the parameters for the mixtur e distribution that leads to a higher \nlog-likeliho od of the data1 . For this reason, the algorithm is typically run multiple times \n(recall the random initiali zation of the means) so as to ensure we ﬁnd a reaso nably good \nsolutio n, albeit perhaps only locally optima l. \nExampl e. Consider a simple mixt ure of two Gauss ians. Figure 2 demonstra tes how \nthe EM-algorithm changes the Gaussian comp onents after each iteration. The ellipsoids \nspecify one standar d devia tion distances from the Gaussian mean. The mixing proportions \nP (j) are not visible in the ﬁgure. Note that it takes many iterations for the algorithm to \nresolv e how to prop erly assign the data points to mixture components. At convergence, \nthe assignmen ts are still soft (not 0-1) but nevertheless clearly divide the responsibilities \nof the two Gaussian componen ts across the clusters in the data. \n1In fact, the highest likelihood for Gaussian mixt ures is always ∞. This happens when one of the \nGaussians conce ntrates arou nd a single data point. We do not look for such solutions, however, and they \ncan be removed by constrai ning the covariance matrice s or via regularization . The real compar ison is to a \nnon-triv ial mixture that achieves the highest log-lik elihood. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare \n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "regularization", "section_heading": "Gaussian mixtur e, i.e., specify θ(0). A simple initializat ion (0)consists of setting P (0)(j)=1/m, equatin g each µj w", "source_title": "a5a5b7b5a8c2eb7d7d23eac5c7f3a9af lec15", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "6f9ddab5-abcf-4826-bb75-33e69e4fc4b2", "text": "for such solutions, however, and they can be removed by constrai ning the covariance matrice s or via regularization . The real compar ison is to a non-triv ial mixture that achieves the highest log-lik elihood. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n8 6.867 Mac hine learning , lectur e 15 (Jaak kola) \n−0.5 0 0.5 1 1.5 2−0.500.511.52\n−0.5 0 0.5 1 1.5 2−0.500.511.52\n−0.5 0 0.5 1 1.5 2−0.500.511.52\ninitia lization iteration 1 iteration 2\n−0.5 0 0.5 1 1.5 2−0.500.511.52\n−0.5 0 0.5 1 1.5 2−0.500.511.52\n−0.5 0 0.5 1 1.5 2−0.500.511.52\niteration 4 iteration 6 iterati on 10 \nFigur e 2: An example of the EM algorithm with a two-componen t mixture of Gaussians \nmodel. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare \n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "regularization", "section_heading": "for such solutions, however, and they can be removed by constrai ning the covariance matrice s or via regularization . T", "source_title": "a5a5b7b5a8c2eb7d7d23eac5c7f3a9af lec15", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 7}
{"id": "02293386-c3d4-4b2b-844c-6407598a537c", "text": "6.867 Machine learning \nMid-t erm exam \nOcto ber 8, 2003 \n(2 points) Your name and MIT ID: \nProblem 1 \nIn this problem we use sequential activ e learning to estimate a linear model \ny = w1x + w0 + � \nwhere the input space (x values) are restricted to be within [−1, 1]. The noise term � \nis assumed to be a zero mean Gaussian with an unknown variance σ2 . Recall that our \nsequen tial active learning metho d selects input points with the highest variance in the \npredicted outputs. Figur e 1 below illustra tes what outputs would be returned for each \nquery (the outputs are not available unless speciﬁcall y queried). \nWe start the learning algorith m by querying output s at two input points, x = −1 and \nx = 1, and let the sequen tial active learning algorithm select the remaining query points. \n1. (4 points) Give the next two inputs that the sequen tial active learning metho d would \npick. Explai n why. \n1\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n−1 −0.5 0 0.5 1−0.500.511.5\nxyFigur e 1: Samples from the underlying relation between the inputs x and outputs y. The \noutputs are not available to the learning algorithm unles s speciﬁcally queried. \n2. (4 points) In the ﬁgure 1 above, draw (appro xima tely) the linear relatio n between \nthe inputs and outputs that the active learning metho d would ﬁnd after a large \nnumber of iterat ions. \n3. (6 points) Would the result be any diﬀeren t if we started with query points x =0 \nand x = 1 and let the sequen tial active learning algorithm select the rema ining query \npoints? Explain why or why not. \n2\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "6.867 Machine learning ", "source_title": "9010fa78deb635f95826725a14edbc6f midterm f01", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "178f99b0-4827-4182-a06a-af1dbfe2aa0d", "text": "be any diﬀeren t if we started with query points x =0 and x = 1 and let the sequen tial active learning algorithm select the rema ining query points? Explain why or why not. 2 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 2\n00.5 11.5 22.5 33.5 4−0.4−0.20\nregularization parameter Clog−probabilityaverage log−probability of test labelsaverage log−probability of training labels\nFigur e 2: Log-pro babilit y of labels as a functio n of regularizat ion parameter C \nHere we use a logistic regression model to solve a classiﬁca tion problem. In Figur e 2, we have \nplotted the mean log-proba bility of labels in the training and test sets after having trained \nthe classiﬁer with quadra tic regulariza tion penalty and diﬀeren t values of the regula rizati on \nparameter C. \n1. (T/F – 2 points) In training a logistic regression model by maximizing \nthe likelihood of the labels given the inputs we have multiple locally \noptimal solutions.\n2. (T/F – 2 points) A stochastic gradient algorithm for training logistic \nregression models with a ﬁxed learning rate will ﬁnd the optima l setting \nof the weights exactly .\n3. (T/F – 2 points) The average log-probabilit y of training labels as in \nFigur e 2 can never increase as we increase C. \n3\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "stochastic gradient", "section_heading": "be any diﬀeren t if we started with query points x =0 and x = 1 and let the sequen tial active learning algorithm select", "source_title": "9010fa78deb635f95826725a14edbc6f midterm f01", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "8b390a90-f5e0-43da-a420-7843294cf53a", "text": "will ﬁnd the optima l setting of the weights exactly . 3. (T/F – 2 points) The average log-probabilit y of training labels as in Figur e 2 can never increase as we increase C. 3 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n4. (4 points) Explain why in Figur e 2 the test log-probability of labels decrea ses for \nlarge values of C. \n5. (T/F – 2 points) The log-probabilit y of labels in the test set would \ndecrease for large values of C even if we had a large number of training \nexamples. \n6. (T/F – 2 points) Adding a quadratic regula rizatio n penalty for the \nparameters when estimat ing a logistic regression model ensures that \nsome of the parameters (weights associated with the components of the \ninput vectors) vanish. \nProblem 3 \nConsider a training set consisting of the following eight examples: \nExamples labeled “0” Examples labeled “1” \n3,3,0 2,2,0 \n3,3,1 1,1,1 \n3,3,0 1,1,0 \n2,2,1 1,1,1 \nThe questio ns below pertain to various feature selectio n metho ds that we could use with \nthe logistic regression model. \n1. (2 points) What is the mutual informat ion between the third feature \nand the target label based on the training set? \n2. (2 points) Whic h feature(s) would a ﬁlter feature selectio n metho d \nchoose? You can assume here that the mutual informa tion criterion is \nevaluated between a single feature and the label. \n4 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "will ﬁnd the optima l setting of the weights exactly . 3. (T/F – 2 points) The average log-probabilit y of training labe", "source_title": "9010fa78deb635f95826725a14edbc6f midterm f01", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "404721c0-990d-4773-9934-fc855358b485", "text": "set? 2. (2 points) Whic h feature(s) would a ﬁlter feature selectio n metho d choose? You can assume here that the mutual informa tion criterion is evaluated between a single feature and the label. 4 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3. (2 points) Whic h two feature(s) would a greedy wrapp er process \nchoose? \n4. (4 points) Whic h featur es would a regula rizatio n approach with a 1-norm penalt y �3 \ni=1 |wi| choose? Explain brieﬂy . \nProblem 4 \n1. (6 points) Figur e 3 shows the ﬁrst decis ion stump that the AdaBoost algorithm \nﬁnds (starting with the unifor m weights over the training examples). We claim that \nthe weights associated with the training examples after includin g this decision stump \nwill be [1/8, 1/8, 1/8, 5/8] (the weights here are enumera ted as in the ﬁgure). Are \nthese weights correct, why or why not? \nDo not provide an explicit calculatio n of the weights. \n2. (T/F – 2 points) The votes that AdaBo ost algorithm assigns to the \ncomp onen t classiﬁe rs are optima l in the sense that they ensure larger \n“margins” in the training set (higher majority predictio ns) than any \nother setting of the votes. \n3. (T/F – 2 points) In the boosting iterat ions, the training error of each \nnew decis ion stump and the training error of the combined classiﬁer \nvary roughly in concert \n5 .\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nxxox+1\n+1+1−1+1\n−1\n4321Figur e 3: The ﬁrst decision stump that the boosting algorithm ﬁnds. \nProblem 5 \nx1x2\nxx\nx\nxxxx\nx\nx\nx\nooo\noo\noox\nxoo\no\nFigur e 4: Training set, maximum marg in linea r separ ator, and the supp ort vectors (in \nbold). \n1. (4 points) What is the leave-one-out cross-validation error estimate for \nmaximum margin separa tion in ﬁgure 4? (we are asking for a number) \n2. (T/F – 2 points) We would expect the supp ort vectors to remain \nthe same in genera l as we move from a linea r kernel to higher order \npolyno mial kernels.\n3. (T/F – 2 points) Structur al risk minimizat ion is guaranteed to ﬁnd\nthe model (among those considered) with the lowest expected loss\n6\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "set? 2. (2 points) Whic h feature(s) would a ﬁlter feature selectio n metho d choose? You can assume here that the mutua", "source_title": "9010fa78deb635f95826725a14edbc6f midterm f01", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "b6486472-864a-4e33-8c62-af11b7186493", "text": "from a linea r kernel to higher order polyno mial kernels. 3. (T/F – 2 points) Structur al risk minimizat ion is guaranteed to ﬁnd the model (among those considered) with the lowest expected loss 6 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n4. (6 points) What is the VC-dimension of a mixtur e of two Gaussians model in the \nplane with equal covariance matrices? Why? \nProblem 6 \nUsing a set of 100 labeled training examples (two classes), we train the following models: \nGaussI A Gaussian mixture model (one Gaussian per class), where the covariance matrices \nare both set to I (iden tity matr ix). \nGaussX A Gaussian mixture model (one Gaussian per class) witho ut any restrictio ns on \nthe covariance matrices. \nLinLog A logistic regression model with linear features. \nQuadLo g A logistic regression model, using all linear and quadr atic features. \n1. (6 points) After training, we measure for each model the aver age log probability of \nlabels given examples in the training set. Specify all the equalit ies or inequa lities that \nmust always hold between the models relative to this performa nce measure. We are \nlooking for statemen ts like “model 1 ≤ model 2” or “model 1 = model 2”. If no such \nstatement holds, write “none”. \n7\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2. (4 points) Whic h equa lities and inequalit ies must always hold if we instead use the \nmean classiﬁc ation error in the training set as the performance measure? Again use \nthe forma t “model 1 ≤ model 2” or “model 1 = model 2”. Write “none” if no such \nstatement holds. \n8\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nAnother set of ﬁgur es\n−1 −0.5 0 0.5 1−0.500.511.5\nxy\nFigur e 1. Samples from the underlying relation between the input s x and outputs y. The \noutputs are not available to the learning algorithm unles s speciﬁcally queried \n00.5 11.5 22.5 33.5 4−0.4−0.20\nregularization parameter Clog−probabilityaverage log−probability of test labelsaverage log−probability of training labels\nFigur e 2. Log-pro babilit y of labels as a functio n of regularizat ion parameter C\n9\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "regularization", "section_heading": "from a linea r kernel to higher order polyno mial kernels. 3. (T/F – 2 points) Structur al risk minimizat ion is guarant", "source_title": "9010fa78deb635f95826725a14edbc6f midterm f01", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "fa55ce44-b88a-4903-bf7c-60db685f8b52", "text": "speciﬁcally queried 00.5 11.5 22.5 33.5 4−0.4−0.20 regularization parameter Clog−probabilityaverage log−probability of test labelsaverage log−probability of training labels Figur e 2. Log-pro babilit y of labels as a functio n of regularizat ion parameter C 9 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nxxox+1\n+1+1−1+1\n−1\n4321Figur e 3. The ﬁrst decision stump that the boosting algorithm ﬁnds.\nx1x2\nxx\nx\nxxxx\nx\nx\nx\nooo\noo\noox\nxoo\no\nFigur e 4. Training set, maximum margin linear separa tor, and the supp ort vectors (in \nbold). \n10\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "speciﬁcally queried 00.5 11.5 22.5 33.5 4−0.4−0.20 regularization parameter Clog−probabilityaverage log−probability of t", "source_title": "9010fa78deb635f95826725a14edbc6f midterm f01", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "d758dc29-cfe3-4ef7-9224-2a0bbd93d386", "text": "6.867 Machine learning \nFinal exam \nDecem ber 3, 2004 \nYour name and MIT ID: \nJ. D. 00000000\n(Optiona l) The grade you would give to yourself + a brief justiﬁcation.\nA... why not?\n1\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 1\n0 1 2 3 4 5 600.511.522.533.544.55\n+−(1)\n+−(2)\nFigur e 1: Labeled training points for problem 1. \nConsider the labeled training points in Figur e 1, where ’x’ and ’o’ denote positive and \nnegative labels, respectiv ely. We wish to apply AdaBo ost with decision stumps to solve \nthe classiﬁc ation problem. In each boosting iterat ion, we select the stump that minimizes \nthe weighted training error, breaking ties arbitra rily. \n1. (3 points) In ﬁgure 1, draw the decision boundary corresp onding to the ﬁrst decis ion \nstump that the boosting algorithm would choose. Label this boundar y (1), and also \nindicate +/-side of the decision bounda ry. \n2. (2 points) In the same ﬁgure 1 also circle the point(s) that have the highest weight \nafter the ﬁrst boosting iteration. \n3. (2 points) What is the weighted error of the ﬁrst decis ion stump after\nthe ﬁrst boosting iteration, i.e., after the points have been reweighted?\n4. (3 points) Draw the decision boundary corresp onding to the second decision stump, \nagain in Figure 1, and label it with (2), also indicating the +/-side of the bounda ry. \n5. (3 points) Would some of the points be misclassiﬁed by the combined classiﬁer after \nthe two boosting iterat ions? Provide a brief justiﬁcatio n. (the points will be awarded \nfor the justiﬁcation, not whether your y/n answ er is correct) \nYes. For example, the circled point in the ﬁgure is misclassiﬁe d by the ﬁrst decision \nstump and could be classiﬁe d correctly in the combination only if the weight/votes of \nthe second stump is higher than the ﬁrst. If it were higher, however, then the points \nmisclassiﬁe d by the second stump would be miscla ssiﬁe d in the combination. \n2\n0.5 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "if ", "section_heading": "6.867 Machine learning ", "source_title": "e49f26ee15eafbbdcba4c0f491af009e final f04soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 0}
{"id": "a177c0b8-0c59-4139-9327-f25c607724b2", "text": "if the weight/votes of the second stump is higher than the ﬁrst. If it were higher, however, then the points misclassiﬁe d by the second stump would be miscla ssiﬁe d in the combination. 2 0.5 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 2 \n1. (2 points) Consider a linear SVM trained with n labeled points in R2 without slack \npenalties and resulting in k = 2 supp ort vectors (k<n). By adding one additiona l \nlabeled training point and retraining the SVM classiﬁe r, what is the maximum number \nof supp ort vectors in the resulting solution? \n() k\n() k +1\n() k +2\n(X) n +1 \n2. We train two SVM classiﬁers to separa te points in R2 . The classiﬁe rs diﬀer only in \nterms of the kernel function. Classiﬁe r 1 uses the linear kernel K1(x, x�)= xT x�, and \nclassiﬁer 2 uses K2(x, x�)= p(x)p(x�), where p(x) is a 3-component Gaussian mixtur e \ndensit y, estima ted on the basis of related other problems. \n(a) (3 points) What is the VC-dimension of the second SVM classiﬁer 2 \nthat uses kernel K2(x, x�)? \nThe feature space is 1-dim ensional; each point x ∈R2 is mapp ed to a \nnon-ne gative numb er p(x). \n(b) (T/F – 2 points) The second SVM classiﬁer can only separa te points T \nthat are likely according to p(x) from those that have low probabilit y \nunder p(x). \n(c) (4 points) If both SVM classiﬁers achieve zero training error on n labeled \npoints, which classiﬁer would have a better genera lization guarantee? Provide a \nbrief justiﬁca tion. \nThe ﬁrst classiﬁer has VC-dimension 3 while the second one has VC-dimension 2. \nThe complexity penalty for the ﬁrst one is therefore higher . When the numb er of \ntraining errors is the same for the two classiﬁers, the bound on the expected error is \nsmal ler for the second classiﬁer. \n3\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "if the weight/votes of the second stump is higher than the ﬁrst. If it were higher, however, then the points misclassiﬁe", "source_title": "e49f26ee15eafbbdcba4c0f491af009e final f04soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "a9170af4-ee27-4d66-93a2-ea4c7c8b65ec", "text": "for the ﬁrst one is therefore higher . When the numb er of training errors is the same for the two classiﬁers, the bound on the expected error is smal ler for the second classiﬁer. 3 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 3\n0 2 4 6 8 10 12 14 16 18−4−202468\nx1x2\n−2 0 2 4 6 8 10−6−4−20246\nx1x2\n(a) (b) \nFigur e 2: Data sets for clustering. Points are located at integer coordinates. \n1. (4 points) First consider the data plott ed in Figure 2a, which consist of two rows of \nequally spaced points. If k-means clustering (k = 2) is initialised with the two points \nwhose coordina tes are (9, 3) and (11, 3), indicate the ﬁnal clusters obtained (after the \nalgorithm converges) on Figur e 2a. \n2. (4 points) Now consider the data in Figure 2b. We will use spectra l clustering \nto divide these points into two clusters. Our version of spectral clustering uses a \nneigh bourhood graph obtained by connecting each point to its two nearest neigh bors \n(brea king ties rando mly), and by weighting the resulting edges between points xi and \nxj by Wij = exp(−||xi − xj||). Indicate on Figure 2b the clusters that we will obtain \nfrom spectral clustering. Provide a brief justiﬁca tion. \nThe random walk induc ed by the weights can switch between the clusters in the \nﬁgure in only two places, (0,-1) and (2,0). Since the weights decay with distanc e, \nthe weights corresponding to transitions within cluster s are higher than those going \nacross in both places. The random would would therefore tend to remain within the \nclusters indic ated in the ﬁgure. \n3. (4 points) Can the solution obtained in the previo us part for the data in Figure 2b \nalso be obtained by k-means clustering (k = 2)? Justify your answ er. \nNo. In the k-me ans algorithm points are assigne d to the closest mean (cluster cen­\ntroid). The centroids of the left and right clusters in the ﬁgure are (0,0) and (5,0), \nrespectively. Point (2,0), for example, is closer to the left cluster centroid (0,0) and \nwouldn ’t be assigne d to the right cluster . The two cluster s in the ﬁgure therefore \ncannot be ﬁxed points of the k-me ans algorithm. \n4\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Unsupervised Learning", "subtopic": "k-means", "section_heading": "for the ﬁrst one is therefore higher . When the numb er of training errors is the same for the two classiﬁers, the bound", "source_title": "e49f26ee15eafbbdcba4c0f491af009e final f04soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "65453c60-6850-4850-86ce-b62f8ee99620", "text": "closer to the left cluster centroid (0,0) and wouldn ’t be assigne d to the right cluster . The two cluster s in the ﬁgure therefore cannot be ﬁxed points of the k-me ans algorithm. 4 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxyFigur e 3: Training sample from a mixture of two linear models \nProblem 4 \nThe data in Figure 3 comes from a mixtu re of two linear regression models with Gaussian \nnoise: \nP (y|x; θ)= p1N (y ; w10 + w11x, σ2)+ p2N (y ; w20 + w21x, σ2)1 2 \nwhere p1 + p2 = 1 and θ =(p1,p2,w10,w11,w20,w21,σ1,σ2). We hope to estimat e θ from \nsuch data via the EM algorithm.\nTo this end, let z ∈{1, 2} be the mixture index, variable indica ting which of the regression\nmodels is used to generate y given x.\n1. (6 points) Connect the random variables X, Y , and Z with directed edges so that \nthe graphi cal model on the left represen ts the mixture of linea r regression models \ndescribed above, and the one on the right represe nts a mixture-of-experts model. For \nboth models, Y denot es the output variable, X the input, and Z is the choice of the \nlinear regression model or expert. \nmixture of linear regressions\nX Z\nY\nmixture of experts\nX Z\nY\n5\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "linear regression", "section_heading": "closer to the left cluster centroid (0,0) and wouldn ’t be assigne d to the right cluster . The two cluster s in the ﬁgu", "source_title": "e49f26ee15eafbbdcba4c0f491af009e final f04soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "daa09b35-e6c4-4911-b924-42772ffbf96e", "text": "models, Y denot es the output variable, X the input, and Z is the choice of the linear regression model or expert. mixture of linear regressions X Z Y mixture of experts X Z Y 5 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nWe use a single plot to repres ent the model parameters (see the ﬁgure below). Each \nlinear regression model appears as a solid line (y = wi0 + wi1x) in between two \nparallel dotted lines at vertica l dista nce 2σi to the solid line. Thus each regression \nmodel “covers” the data that falls between the dotted lines. When w10 = w20 and \nw11 = w21 you would only see a single solid line in the ﬁgure; you may still see two \ndiﬀeren t sets of dotted lines corresp onding to diﬀeren t values of σ1 and σ2. The solid \nbar to the right represe nts p1 (and p2 =1 − p1). \nFor example, if \nθ =( p1,p2,w10,w11,w20,w21,σ1,σ2) \n=( 0.35, 0.65, 0.5, 0, 0.85, −0.7, 0.05, 0.15) \nthe plot is \n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np1\n2. (6 points) We are now ready to estima te the parameter s θ via EM. There are, \nhowever, many ways to initialize the parameters for the algorithm. \nOn the next page you are asked to connect 3 diﬀeren t initializat ions (left column) with \nthe paramet ers that would result after one EM iterat ion (right column). Diﬀe rent \ninitia lizations may lead to the same set of parameters. Your answ er should consist of \n3 arrows, one from each initializat ion. \n6\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np1\n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np1\n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np1\n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np10 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np10 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np10 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np1Next iteration Initialization7 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "linear regression", "section_heading": "models, Y denot es the output variable, X the input, and Z is the choice of the linear regression model or expert. mixtu", "source_title": "e49f26ee15eafbbdcba4c0f491af009e final f04soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "03e089ea-161b-4b8a-ba63-5b6563b80ac3", "text": "p1 0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2 xy 00.10.20.30.40.50.60.70.80.91 p10 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2 xy 00.10.20.30.40.50.60.70.80.91 p10 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2 xy 00.10.20.30.40.50.60.70.80.91 p10 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2 xy 00.10.20.30.40.50.60.70.80.91 p1Next iteration Initialization7 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 5\nAssume that the following sequences are very long and the pattern highligh ted with spaces \nis repeated: \nSequence 1: 100 100 100 100 ... 100 \nSequence 2: 1 100 100 100 ... 100 \n1. (4 points) If we model each sequence with a diﬀerent ﬁrst-or der HMM , what is the \nnumber of hidden states that a reasona ble model selectio n metho d would report? \nHMM for Sequence 1 HMM for Sequence 2 \nNo. of hidden states 3 4 \n2. (2 points) The following Bayesian network depic ts a sequence of 5 observ ations from \nan HMM, where s1,s2,s3,s4,s5 is the hidden state sequence. \ns1 s2 s3 s4 s5 \nx1 x2 x4 x5 \nAre x1 and x5 indep endent given x3? Brieﬂy justify your answ er. \nThey are not indep endent. The moralized ancestral graph corresponding to x1, x3, \nand x5 is the same graph with arrows replac ed with undir ected edges. x1 and x5 are \nnot separated given x3, and thus not indep endent. \n3. (3 points) Does the order of Markov dependenc ies in the observ ed sequence always \ndetermine the number of hidden states of the HMM that generated the sequenc e? \nProvide a brief justiﬁcation. \nNo. The answer to the previous question implies that obser vations corresponding to \n(typic al) HMMs have no Markov properties (of any order). This holds, for exam­\nple, when there are only two possible hidden states. Thus Markov properties of the \nobservation sequenc e cannot in gener al determine the numb er of hidden states. \n8\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "p1 0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2 xy 00.10.20.30.40.50.60.70.80.91 p10 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2 ", "source_title": "e49f26ee15eafbbdcba4c0f491af009e final f04soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "57cd0272-9aa4-477b-ad11-d4497459f0de", "text": "(of any order). This holds, for exam­ ple, when there are only two possible hidden states. Thus Markov properties of the observation sequenc e cannot in gener al determine the numb er of hidden states. 8 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nx3\nProblem 6 \nWe wish to develop a graphica l model for the following transportation problem. A transport \ncompa ny is trying to choose between two alternativ e routes for commuting between Boston \nand New York. In an experimen t, two identical busses leave Bosto n at the same but \notherwise rando m time, TB . The busses take diﬀeren t routes, arriving at their (comm on) \ndestinatio n at times TN1 and TN2. \nTransit time for each route depends on the congestion along the route, and the two con­\ngestio ns are unrela ted. Let us represent the rando m delays introduced along the routes by \nvariables C1 and C2. Finally , let F represen t the identity of the bus which reaches New \nYork ﬁrst. We view F as a random variable that takes values 1 or 2. \n1. (6 points) Complete the following directed graph (Bayesian network) with edges \nso that it captures the relationships between the variables in this transp ortatio n \nproblem. \nF TB \nC1 C2 \nTN2 TN1 \n9\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2. (3 points) Consider the following directed graph as a possible represen tation of the \nindep endences between the variables TN1, TN2, and F only: \nWhic h of the following factor izatio ns of the joint are consisten t with the graph?\nP (TN1)P (TN2)P (F |TN1, TN 2) X \nP (TN1)P (TN2)P (F |TN1) X \nP (TN1)P (TN2)P (F ) X \n10\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "(of any order). This holds, for exam­ ple, when there are only two possible hidden states. Thus Markov properties of the", "source_title": "e49f26ee15eafbbdcba4c0f491af009e final f04soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "23770e9e-a47f-4689-a017-84d187be11e1", "text": "� �� 6.867 Mac hine learning , lectur e 7 (Jaakkola) 1 \nLecture topics: \n• Kernel form of linear regression \n• Kernels , examples, constructio n, properties \nLinear regression and kernels \nConsider a slightly simpler model where we omit the oﬀset parameter θ0, reducing the \nmodel to y = θT φ(x) + � where φ(x) is a particular featur e expansio n (e.g., polyno mial). \nOur goal here is to turn both the estimatio n problem and the subsequen t predictio n task \ninto forms that involve only inner products between the feature vectors. \nWe have already emphasized that regula rization is necessary in conjunctio n with mapp ing \nexamples to higher dimens ional feature vector s. The regular ized least squares objectiv e to \nbe minimized, with parameter λ, is given by \nn\nJ(θ)= �� \nyt − θT φ(xt) �2 + λ�θ�2 (1) \nt=1 \nThis form can be deriv ed from penalized log-likeliho od estima tion (see previo us lecture \nnotes). The eﬀect of the regulariza tion penalty is to pull all the parameters towards zero. \nSo any linear dime nsions in the parameters that the training feature vectors do not pertain \nto are set explicitly to zero. We would therefore expect the optima l parameters to lie in \nthe span of the featur e vector s corresp onding to the training example s. This is indee d the \ncase. \nAs before, the optima lity conditio n for θ follows from setting the gradien t to zero: \nαt \ndJ(θ)= −2 n�� \nyt − θT φ(xt) �� \nφ(xt)+2λθ = 0 (2) dθ t=1 \nWe can therefore construct the optima l θ in terms of prediction diﬀerences αt and the \nfeature vectors: \nn1 � \nθ = αtφ(xt) (3) λ t=1 \nThe implica tion is that the optimal θ (however high dimensiona l) will lie in the span of the \nfeature vectors corresp onding to the train ing example s. This is due to the regula rizati on \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "� �� 6.867 Mac hine learning , lectur e 7 (Jaakkola) 1 ", "source_title": "1b55295adbca700abfa7356a4f832e33 lec7", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "1db65cdd-17af-4caf-926d-ef5364c041cd", "text": "implica tion is that the optimal θ (however high dimensiona l) will lie in the span of the feature vectors corresp onding to the train ing example s. This is due to the regula rizati on Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� �\n� � 6.867 Mac hine learning , lectur e 7 (Jaakkola) 2 \npenalt y we added. But how do we set αt? The values for αt can be found by insisting that \nthey indeed can be interpreted as predictio n diﬀerences : \nn1 � \nαt = yt − θT φ(x t)= yt − αt� φ(xt� )T φ(xt) (4) λ t�=1 \nThus αt depends only on the actual responses yt and the inner products between the \ntraining examples, the Gram matrix : \n⎡ ⎤ φ(x1)T φ(x1) φ(x 1)T φ(x n) ··· \nK = ⎣ ⎦ (5) ··· ··· ··· \nφ(xn)T φ(x1) ... φ(xn)T φ(x n) \nIn a vector form, \na =[α1,...,αn]T , (6) \ny =[y1,...,yn]T , (7) \n1 a = Ka (8) y − λ \nthe solution is \n−1 \naˆ= λλI + K y (9) \nNote that ﬁnding the estima tes ˆαt requires inverting a n × n matrix. This is the cost of \ndealing with inner products as opposed to handing featur e vectors direc tly. In some cases, \nthe beneﬁt is substa ntial since the feature vector s in the inner products may be inﬁnite \ndimensional but never needed explicitly . \nAs a result of ﬁnding ˆαt we can cast the predic tions for new examples also in terms of inner \nproducts: \nn n\ny = θˆT φ(x)= (ˆαt/λ)φ (xt� )T φ(x)= αˆtK(xt� , x) (10) \nt=1 t=1 \nwhere we view K(xt� , x) as a kernel function, a function of two argumen ts xt� and x. \nKernels \nSo we have now success fully turned a regula rized linear regress ion problem into a kernel \nform. This means that we can simply substitute diﬀerent kernel functions K(x, x�) into the \nestimatio n/predictio n equatio ns. This gives us an easy access to a wide range of possible \nregression functions. Here are a couple of standa rd examples of kernels:\n\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "implica tion is that the optimal θ (however high dimensiona l) will lie in the span of the feature vectors corresp ondin", "source_title": "1b55295adbca700abfa7356a4f832e33 lec7", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "16dc4d83-8496-4272-ad51-37bdf85206d4", "text": "diﬀerent kernel functions K(x, x�) into the estimatio n/predictio n equatio ns. This gives us an easy access to a wide range of possible regression functions. Here are a couple of standa rd examples of kernels: Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � \n� 6.867 Mac hine learning , lectur e 7 (Jaakkola) 3 \n• Polyno mial kernel \nK(x, x�) = (1+ x T x�)p,p =1, 2,... (11) \nRadial basis kernel • \nβ K(x, x�) = exp − 2 �x − x��2 , β> 0 (12) \nWe have already discuss ed the feature vectors corresponding to the polyno mial kernel. The \ncomp onen ts of these feature vectors were polynomia l terms up to degree p with speciﬁcally \nchosen coeﬃcients. The restricted choice of coeﬃcien ts was necessary in order to collapse \nthe inner product calculations. \nThe feature “vector s” corresp onding to the radial basis kernel are inﬁnite dimension al! \nThe comp onen ts of these “vector s” are indexed by z ∈Rd where d is the dimension of the \noriginal input x. More precisely, the featur e vectors are functio ns: \nφz(x)= c(β,d) N(z; x, 1/2β) (13) \nwhere N(z; x, (1/β)) is a norma l pdf over z and c(β,d) is a constan t. Roughly speaking, \nthe radial basis kernel measures the proba bility that you would get the same sample z (in \nthe same small region) from two norma l distributio ns with means x and x� and a common \nvariance 1/2β. This is a reason able measure of “similarit y” between x and x� and kernels \nare often deﬁne d from this perspectiv e. The inner product giving rise to the radial basis \nkernel is deﬁned throug h integra tion \nK(x, x�)= φz(x)φ z(x�)dz (14) \nWe can also constr uct various types of kernels from simpler ones. Here are a few rules to \nguide us. Assume K1(x, x�) and K2(x, x�) are valid kernels (corr espond to inner products \nof some feature vectors), then \n1. K(x, x�)= f(x)K 1(x, x�)f(x�) for any functio n f(x), \n2. K(x, x�)= K1(x, x�)+ K2(x, x�), \n3. K(x, x�)= K1(x, x�)K 2(x, x�) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "diﬀerent kernel functions K(x, x�) into the estimatio n/predictio n equatio ns. This gives us an easy access to a wide r", "source_title": "1b55295adbca700abfa7356a4f832e33 lec7", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "24de2411-2834-43d0-9da9-0547e4dc0448", "text": "kernels (corr espond to inner products of some feature vectors), then 1. K(x, x�)= f(x)K 1(x, x�)f(x�) for any functio n f(x), 2. K(x, x�)= K1(x, x�)+ K2(x, x�), 3. K(x, x�)= K1(x, x�)K 2(x, x�) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � 6.867 Mac hine learning , lectur e 7 (Jaakkola) 4 \nare all valid kernels . While simple, these rules are quite powerful. Let’s ﬁrst under stand \nthese rules from the point of view of the implicit feature vectors. For each rule, let φ(x) be \nthe featur e vector corresp onding to K and φ(1)(x) and φ(2)(x) the feature vectors associated \nwith K1 and K2, respectiv ely. The feature mapping for the ﬁrst rule is given simply by \nmultiply ing with the scalar functio n f(x): \nφ(x)= f(x)φ(1)(x) (15) \nso that φ(x)T φ(x�)= f(x)φ(1)(x)T φ(1)(x�)f(x�)= f(x)K 1(x, x�)f(x�). The second rule, \nadding kernels, corresponds to just concatena ting the feature vectors \nφ(1)(x)φ(x)= φ(2)(x) (16) \nThe third and the last rule is a little more complica ted but not much. Supp ose we use a \ndouble index i,j to index the componen ts of φ(x) where i ranges over the comp onen ts of \nφ(1)(x) and j refers to the comp onents of φ(2)(x). Then \n(1) (2)φi,j (x)= φi (x)φj (x) (17) \nIt is now easy to see that \nK(x, x�) = φ(x)T φ(x�)� (18) \n= φi,j (x)φ i,j(x�) (19) \ni,j� \n= φ(1) \ni (x)φ(2) j (x)φ(1) i (x�)φ(2) j (x�) (20) \ni,j� � \n= [ φ(1) \ni (x)φ(1) i (x�)][ φ(2) j (x)φ(2) j (x�)] (21) \ni j \n= [φ(1)(x)T φ(1)(x�)][φ(2)(x)T φ(2)(x�)] (22) \n= K1(x, x�)K 2(x, x�) (23) \nThes e construction rules can also be used to verify that something is a valid kernel. As an \nexample, let’s ﬁgure out why a radia l basis kernel \nK(x, x�) = exp{−21 �x − x��2} (24) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "kernels (corr espond to inner products of some feature vectors), then 1. K(x, x�)= f(x)K 1(x, x�)f(x�) for any functio n", "source_title": "1b55295adbca700abfa7356a4f832e33 lec7", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "ca2b24cd-d56d-4d18-b925-891616555579", "text": "Thes e construction rules can also be used to verify that something is a valid kernel. As an example, let’s ﬁgure out why a radia l basis kernel K(x, x�) = exp{−21 �x − x��2} (24) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� 6.867 Mac hine learning , lectur e 7 (Jaakkola) 5 \nis a valid kernel. \nexp{− 1 \n2�x − x��2} = exp{− 1 2 x \nT x + x T x� − 1 2 x\n�T x�} (25) \n� f(x)�� � � f(x�)�� � \n= exp{− 1 2 x \nT x} · exp{x T x�} · exp{− 1 2 x\n�T x�} (26) \nHere exp{xT x�} is a sum of simple products xT x� and is therefore a kernel based on the \nsecond and third rules ; the ﬁrst rule allows us to incorp orate f(x) and f(x�). \nString kernels. It is often necessary to make predictions (classify, assess risk, determine \nuser ratings) on the basis of more complex objects such as variable length sequence s or \ngraphs that do not neces sarily permit a simple descript ion as points in Rd . The idea of \nkernels extends to such objects as well. Consider, for example, the case where the inputs x \nare variable lengt h sequences (e.g., documen ts or biosequence s) with elemen ts from some \ncommo n alpha bet A (e.g., letters or protein residues). One way to compar e such sequence s \nis to consider subsequences that they may share. Let u ∈Ak denote a lengt h k sequenc e \nfrom this alpha bet and i a sequence of k indexes. So, for example, we can say that u = x[i] \nif u1 = xi1 , u2 = xi2 , ..., uk = xik . In other words, x contains the elemen ts of u in \npositio ns i1 <i2 < <ik. If the elemen ts of u are found in successiv e positio ns in x, ··· \nthen ik − i1 = k − 1. A simple string kernel corresp onds to feature vectors with coun ts of \noccurences of length k subse quences: \nφu(x)= δ(ik − i1,k − 1) (27) \ni:u=x[i] \nIn other words, the comp onents are index ed by subse quences u and the value of u-\ncomp onen t is the number of times x contains u as a contiguous subsequenc e. For example, \nφon(the common construct) = 2 (28) \nThe number of comp onents in such feature vectors is very large (exponential in k). Yet, \nthe inner product \nφu(x)φ u(x�) (29) \nu∈Ak \ncan be computed eﬃcie ntly (there are only a limited number of possible contiguo us subse ­\nquences in x and x�). The reaso n for this diﬀerence , and the argumen t in favor of kernels \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "Thes e construction rules can also be used to verify that something is a valid kernel. As an example, let’s ﬁgure out wh", "source_title": "1b55295adbca700abfa7356a4f832e33 lec7", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "b5dd2f14-ae57-44f9-b437-3cf79e80017d", "text": "be computed eﬃcie ntly (there are only a limited number of possible contiguo us subse ­ quences in x and x�). The reaso n for this diﬀerence , and the argumen t in favor of kernels Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� � � � \n� 6.867 Mac hine learning , lectur e 7 (Jaakkola) 6 \nmore generally , is that the feature vectors have to aggregate the informa tion necess ary to \ncompa re any two sequences while the inner product is evaluated for two speciﬁc sequenc es. \nWe can also relax the requireme nt that matches must be contiguo us. To this end, we deﬁne \nthe length of the window of x where u appears as l(i)= ik − i1. The feature vectors in a \nweighte d gapp ed substring kernel are given by \nφu(x)= λl(i) (30) \ni:u=x[i] \nwhere the parameter λ ∈ (0, 1) speciﬁes the penalty for non-con tiguous matches to u. The \nresulting kernel \n⎛⎞⎛ ⎞ \nK(x, x�)= φu(x)φ u(x�)= ⎝ λl(i)⎠⎝ λl(i)⎠ (31) \nu∈Ak u∈Ak i:u=x[i] i:u=x�[i] \ncan be computed recursively. It is often useful to normalize such a kernel so as to remo ve \nany immedia te eﬀect from the sequence lengt h: \nK˜(x, x�)= � K(x�, x�) (32) \nK(x, x) K(x�, x�) \nApp endix (optional): Kernel linear regre ssion with oﬀset \nGiven a feature expansion speciﬁed by φ(x) we try to minimize \nn�� �2 J(θ,θ0)= yt − θT φ(xt) − θ0 + λ�θ�2 (33) \nt=1 \nwhere we have chosen not to regularize θ0 to preserve the simila rity to classiﬁcation dis­\ncussed later on. Not regularizing θ0 means, e.g., that we do not care whether all the \nresponses have a consta nt added to them; the value of the objective, after optimizing θ0, \nwould rema in the same with or witho ut such consta nt. \nSetting the deriv atives with respect to θ0 and θ to zero gives the following optima lity \nconditio ns: \ndJ(θ,θ0) n� � \n= −2 yt − θT φ(xt) − θ0 = 0 (34) dθ0 t=1 \nαt ndJ(θ,θ0)=2λθ − 2 ��� \nyt − θT φ�� \n(xt) − θ0 �� \nφ(xt) = 0 (35) dθ t=1\n\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "be computed eﬃcie ntly (there are only a limited number of possible contiguo us subse ­ quences in x and x�). The reaso ", "source_title": "1b55295adbca700abfa7356a4f832e33 lec7", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "44773cee-96d1-4bf0-88c1-3234e4816d05", "text": "ns: dJ(θ,θ0) n� � = −2 yt − θT φ(xt) − θ0 = 0 (34) dθ0 t=1 αt ndJ(θ,θ0)=2λθ − 2 ��� yt − θT φ�� (xt) − θ0 �� φ(xt) = 0 (35) dθ t=1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � \n� � \n� � \n� �� � 6.867 Mac hine learning , lectur e 7 (Jaakkola) 7 \nWe can therefore construct the optima l θ in terms of prediction diﬀerences αt and the \nfeature vectors as before: \nn1 � \nθ = αtφ(xt) (36) λ t=1 \nUsing this form of the solut ion for θ and Eq.(34) we can also express the optimal θ0 as a \nfunction of the predictio n diﬀerences αt: \nn n n1 �� � 1 � 1 � \nθ0 = yt − θT φ(xt)= yt − αt� φ(xt� )T φ(xt) (37) n n λ t=1 t=1 t�=1 \nWe can now constra in αt to take on values that can indeed be interpr eted as prediction \ndiﬀerenc es: \nαi = yi − θT φ(xi) − θ0 (38) \nn1 � \n= yi − λαt� φ(xt� )T φ(xi) − θ0 (39) \nt�=1 \nn n n1 � 1 � 1 � \n= yi − αt� φ(xt� )T φ(xi) − yt − αt� φ(xt� )T φ(xt) (40) λ n λ t�=1 t=1 t�=1 \nn n n1 � 1 � 1 � \n= yi − yt − αt� φ(xt� )T φ(xi) − φ(x t� )T φ(xt) (41) n λ n t=1 t�=1 t=1 \nWith the same matrix notation as before, and letting 1 = [1,..., 1]T , we can rewrite the \nabove conditio n as \nC \n1 a =(I − 11T /n) y − (I − 11T /n)Ka (42) λ\nwhere C = I − 11T /n is a centering matrix. Any solutio n to the above equatio n has \nto satisfy 1T a = 0 (just left multiply the equatio n with 1T ). Note that this is exactly \nthe optima lity conditio n for θ0 in Eq.(34). Using this “summing to zero” property of the \nsolutio n we can rewrite the above equatio n as \n1 a = Cy − CKCa (43) λ\n\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "ns: dJ(θ,θ0) n� � = −2 yt − θT φ(xt) − θ0 = 0 (34) dθ0 t=1 αt ndJ(θ,θ0)=2λθ − 2 ��� yt − θT φ�� (xt) − θ0 �� φ(xt) = 0 (", "source_title": "1b55295adbca700abfa7356a4f832e33 lec7", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "b6919b95-5dcd-4ffc-afb8-464a6ff78ab4", "text": "exactly the optima lity conditio n for θ0 in Eq.(34). Using this “summing to zero” property of the solutio n we can rewrite the above equatio n as 1 a = Cy − CKCa (43) λ Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n6.867 Mac hine learning , lectur e 7 (Jaakkola) 8 \nwhere we have introduced an addit ional centering operation on the right hand side. This \ncanno t change the solution since Ca = a whene ver 1T a = 0. The solut ion aˆis then \naˆ= λ (λI + CKC)−1 Cy (44) \nOnce we have aˆwe can reconstruct θˆ0 from Eq.(37). θˆT φ(x) reduces to the kernel form as \nbefore. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "kernel", "section_heading": "exactly the optima lity conditio n for θ0 in Eq.(34). Using this “summing to zero” property of the solutio n we can rewr", "source_title": "1b55295adbca700abfa7356a4f832e33 lec7", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 7}
{"id": "ce607c1f-45e5-49b8-9e98-8240a54d65f1", "text": "6.867 Machine learning \nMid-t erm exam \nOcto ber 15, 2003 \n(2 points) Your name and MIT ID: \nSOLU TIONS\nProblem 1 \nSupp ose we are trying to solve an activ e learning problem, where the possible inputs you \ncan select form a discrete set. Speciﬁcally, we have a set of N unlab eled documen ts, \nΦ1,..., ΦN , where each documen t is represen ted as a binary feature fector \nΦ=[φ1,...,φm]T \nand φi = 1 if word i appears in the docume nt and zero otherwise. Our goal is to quickly label \nthese N documen ts with 0/1 labels. We can request a label for any of the N documen ts, \npreferably as few as possible. We also have a small number n of these documents already \nlabeled to get us started. \nWe use a logistic regress ion model to solve the classiﬁcatio n task: \nP (y =1|Φ, w)= g( w T Φ) \nwhere g() is the logistic function. Note that we do not include the bias term . ·\n1. (T/F – 2 points) Any word that appears in all the N documen ts T \nwould eﬀectively provide a bias term for the logistic regression model. \n2. (T/F – 2 points) Any word that appears only in the available n la- F \nbeled documen ts used for initia lly training the logistic regression model, \nwould serve equally well as a bias term. \n1\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "6.867 Machine learning ", "source_title": "a5dae01d24632b23c2755e6eb7e0b4ad midterm f03soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "3128accc-cf9b-4f96-99a3-0500d96923d9", "text": "(T/F – 2 points) Any word that appears only in the available n la- F beled documen ts used for initia lly training the logistic regression model, would serve equally well as a bias term. 1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3. Having trained the logistic regression model on the basis of the n labeled documen ts, \nobtaining wˆn, we’d like to request additiona l labeled documen ts. For this, we will \nuse the following measure of uncerta inty in our predictions: \nEy∼pt |y − pt| = pt|1 − pt| + (1 − pt)|0 − pt| =2pt(1 − pt) \nwhere pt = P (y =1Φt, wˆn), our current predic tion of the proba bility that y = 1 for |\nthe tth unlab eled docume nt Φt. \na) (4 points) We would request the label for the docume nt/query point Φt that \nhas \n( ) the smallest value of 2pt(1 − pt) \n( X ) the largest value of 2pt(1 − pt) \n( ) an intermedia te value of 2pt(1 − pt) \nBrieﬂy expla in the rationa le behind the selection criterion that you chose. \n2pt(1 − pt) is a measure of uncertainty about the label for document t. We expect to \nbe able to reduce the uncertainty the most by requesting labels for those documents \nwith the largest value of 2pt(1 − pt), points that are closest to the decision boundary. \nPut another way, getting additional training examples that are close to the boundar y \nought to help the most in terms of ﬁgur ing out exactly wher e the boundary should \nlie. \nb) (2 points) Sketch wˆn in Figur e 1.1. Write down the equati on, express ed solely \nin terms of Φ and wˆn, that Φ has to satisfy for it to lie exactly on the decision \nbounda ry: \nPoints Φ that lie on the decision boundary must satisfy wˆnT Φ=0. Henc e, wˆnT \nis ortho gonal to the decision bounda ry. Moreover, since we decide y =1 when \nwˆnT Φ > 0, wˆnT points into the ’+’ region. \n2\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "(T/F – 2 points) Any word that appears only in the available n la- F beled documen ts used for initia lly training the l", "source_title": "a5dae01d24632b23c2755e6eb7e0b4ad midterm f03soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "f90afa14-a875-43dd-a43b-5f162cfa4569", "text": "the decision boundary must satisfy wˆnT Φ=0. Henc e, wˆnT is ortho gonal to the decision bounda ry. Moreover, since we decide y =1 when wˆnT Φ > 0, wˆnT points into the ’+’ region. 2 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nc) (4 points) In ﬁgure 1.2, circle the next point we would select according to the \ncriterio n. Draw two decision boundari es that would result from incor porating \nthe new point in the training set, labeling the boundaries as y = 1 and y = 0, \ndepending on the outcome of the query . \nThe uncertainty is largest when P (y = 1Φ) = g(wˆ\nT\nnΦ) is near one-half,\n |\ne.g. when wˆ\nT\nnΦ is near zero so that the point Φ is close to the decision \nboundary. Henc e, we circle the point nearest the boundar y. If this point \n(initial ly in the y =0 region) turns out to be a y =1 document, then \nwe ”tilt” the boundary towar ds that point so as to tend to move that \nexample to the other side of the decision boundar y. Other wise, if the \ndocument is in fact a y =0, then we tilt the boundar y away from that \npoint so that it is ”deeper” in the y =0 region. \n+ o ˆwn \n+ o y = 0 y = 1 \nFigur e 1.1. Two labeled points, unlabeled Figur e 1.2. Two labeled points, unlabeled \npoints, and the decis ion boundar y. The points, and the decis ion boundar y. The \npoint “+” corresponds to y = 1. point “+” corresponds to y = 1. \n4. (T/F – 2 points) The criterio n we have used here for activ e learning F \nguarantees that the measure of uncertainty about the labels of the \nunlab eled points will decrease mono tonica lly for each point after each \nquery . \nRough ly speaking, when we ”tilt” the boundary to incorporate each new \nlabeled example some unlab eled points may end up closer to the bound­\nary which tends to increase the uncertainty in those points. \n3 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "if ", "section_heading": "the decision boundary must satisfy wˆnT Φ=0. Henc e, wˆnT is ortho gonal to the decision bounda ry. Moreover, since we d", "source_title": "a5dae01d24632b23c2755e6eb7e0b4ad midterm f03soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 2}
{"id": "9cc096d0-795d-480c-9f35-8160c7f82040", "text": "Rough ly speaking, when we ”tilt” the boundary to incorporate each new labeled example some unlab eled points may end up closer to the bound­ ary which tends to increase the uncertainty in those points. 3 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 2 \nConsider a regression problem where the two dimensiona l input points x =[x1,x2]T are \nconstra ined to lie within the unit square: xi ∈ [−1, 1], i =1, 2. The training and test input \npoints x are sampled uniformly at rando m within the unit squar e. The target output s y \nare governed by the following model \ny ∼ N(x13 x25 − 10x1x2 +7x12 +5x2 − 3, 1) \nIn other words, the outputs are norma lly distributed with mean given by \nx13 x25 − 10x1x2 +7x 2 +5x2 − 31 \nand variance 1.\nWe learn to predict y given x using linear regression models with 1st through 10th order\npolyno mial features. The models are nested in the sense that the higher order models will\ninclude all the lower order features. The estimation criterio n is the mean squared error.\nWe ﬁrst train a 1st, 2nd, 8th, and 10th order model using n = 20 training points, and then\ntest the predictio ns on a large number of indep enden tly sampled points.\n1. (6 points) Selec t all the appro priat e model(s) for each column. If you think the \nhighest, or lowest, error would be shared among several models, be sure to list all \nmodels. \nLowest test error \nLowest training error Highest training error (typically) \n1st order ( ) ( X ) \n2nd order ( ) ( ) ( X ) \n8th order ( X ) ( ) \n10th order ( X ) ( ) ( ) \nBrieﬂy explain your selection in the last column, i.e., the model you would expect to \nhave the lowest test error : \nThe 10th order regression model would seriously overﬁt when presente d only with \nn = 20 training points. The second order model on the other hand might ﬁnd \nsome useful structur e in the data based only on 20 points. The true model is also \ndominate d by the second order terms. Since |x1|≤ 1 and |x2|≤ 1 any higher order \nterms without large coeﬃcients are vanishingly small. \n2. (6 points) We now train the polyno mial regression models using n = 106 (one \nmillion) training points. Again select the appropria te model(s) for each column. If \n4 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "Rough ly speaking, when we ”tilt” the boundary to incorporate each new labeled example some unlab eled points may end up", "source_title": "a5dae01d24632b23c2755e6eb7e0b4ad midterm f03soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "9151613e-59b0-4315-a179-ca39cbd4a6a4", "text": "without large coeﬃcients are vanishingly small. 2. (6 points) We now train the polyno mial regression models using n = 106 (one million) training points. Again select the appropria te model(s) for each column. If 4 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nyou think the highest, or lowest, error would be shared among several models, be sure \nto list all models. \nLowest structur al error Highest approx. error Lowest test error \n1st order ( ) ( ) ( ) \n2nd order ( ) ( ) ( ) \n8th order ( X ) ( ) ( X ) \n10th order ( X ) ( X ) ( ) \n3. (T/F – 2 points) The approximation error of a polynomia l regression T \nmodel depends on the number of training points. \n4. (T/F – 2 points) The structural error of a polynomia l regression F \nmodel depends on the number of training points. \n5\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 3 \nWe consider here linear and non-linea r supp ort vector machines (SVM) of the form: \nmin w12/2 subject to yi(w1xi + w0) − 1 ≥ 0,i =1,...,n, or \nmin w T w/2 subject to yi(w T Φi + w0) − 1 ≥ 0,i =1,...,n \nwhere Φi is a feature vector constructed from the corresp onding real valued input xi. We \nwish to compa re the simple linear SVM classiﬁer (w1x + w0) and the non-linear classiﬁer \n(wT Φ+ w0), where Φ=[x,x2]T . \n1. (3 points) Provide three input points x1, x2, and x3 and their associated ±1 labels \nsuch that they cannot be separ ated with the simple linear classiﬁer, but are separ able \nby the non-linea r classif er with Φ = [x,x2]T . You may ﬁnd Figure 3.1. helpf ul in \nansw ering this question. \n(x =1,y = 1), (x =2,y = −1), (x =3,y = 1)\n2. (3 points) In the ﬁgure below (Figure 3.1), mark your three points x1, x2, and x3 as \npoints in the featur e space with their associated labels. Draw the decision boundary \nof the non-line ar SVM classiﬁer with Φ = [x,x2]T that separ ates the points. \n0 1 2 3 40246810121416\nf1=xf2=x2\n(x,x2) \nXXX\ny=1y=−1y=1\nFigur e 3.1. Feature space.\n6\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "without large coeﬃcients are vanishingly small. 2. (6 points) We now train the polyno mial regression models using n = 1", "source_title": "a5dae01d24632b23c2755e6eb7e0b4ad midterm f03soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "4f643446-d2d6-4966-ac2e-c0a498936d02", "text": "associated labels. Draw the decision boundary of the non-line ar SVM classiﬁer with Φ = [x,x2]T that separ ates the points. 0 1 2 3 40246810121416 f1=xf2=x2 (x,x2) XXX y=1y=−1y=1 Figur e 3.1. Feature space. 6 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3. (3 points) Consider two labeled points (x =1,y = 1) and (x =3,y = −1). Is the \nmargin we attain using feature vector s Φ = [x,x2]T \n( X ) greater\n( ) equal\n( ) smaller\nthan the margin resulting from using the input x directly? \n4. (2 points) In general, is the margin we would attain using scaled feature vectors \nΦ = [2x, 2x2]T \n( X ) greater\n( ) equal\n( ) smaller\n( ) any of the above\nin compar ison to the margin resulting from using Φ = [x,x2]T ? \n5. (T/F – 2 points) The values of the marg ins obtained by two diﬀeren t T \nkernels K(x,x�) and K˜(x,x�) on the same training set do not tells us \nwhich classiﬁer will perfor m better on the test set. \nWe need to normalize the margin for it to be meaningful. For example,\na simple scaling of the feature vectors would lead to a larger margin.\nSuch a scaling does not change the decision boundar y, however, and so\nthe larger margin cannot directly inform us about gener alization.\n7\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "associated labels. Draw the decision boundary of the non-line ar SVM classiﬁer with Φ = [x,x2]T that separ ates the poin", "source_title": "a5dae01d24632b23c2755e6eb7e0b4ad midterm f03soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "b478112e-0ee4-458f-8f72-0663f4648c40", "text": "simple scaling of the feature vectors would lead to a larger margin. Such a scaling does not change the decision boundar y, however, and so the larger margin cannot directly inform us about gener alization. 7 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 4 \nWe consider here generativ e and discriminativ e approa ches for solving the classiﬁcat ion \nproblem illustra ted in Figur e 4.1. Speciﬁca lly, we will use a mixture of Gaussians model \nand regula rized logistic regression models. \n0 0.5 1 1.5 200.511.52\nx1x2\nXX\nFigur e 4.1. Labeled training set, where “+” corresponds to class y = 1. \n1. We will ﬁrst estimate a mixture of Gaussians model, one Gaussian per class, with the \nconstra int that the co variance matrices are iden tity matr ices. The mixing proportions \n(class frequencies) and the means of the two Gaussians are free parameters. \na) (3 points) Plot the maximum likeliho od estimates of the means of the two class \nconditio nal Gaussians in Figure 4.1. Mark the means as points “x” and label \nthem “0” and “1” according to the class. \nThe means should be close to the center of mass of the points. \nb) (2 points) Draw the decision boundary in the same ﬁgure. \nSince the two classes have the same numb er of points and the same co­\nvarianc e matr ices, the decision boundar y is a line and, moreover, should \nbe drawn as the ortho gonal bisector of the line segment conne cting the \nclass means. \n8\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "simple scaling of the feature vectors would lead to a larger margin. Such a scaling does not change the decision boundar", "source_title": "a5dae01d24632b23c2755e6eb7e0b4ad midterm f03soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "af63aa18-fb37-4e95-81c8-2e549d5c7fc4", "text": "points and the same co­ varianc e matr ices, the decision boundar y is a line and, moreover, should be drawn as the ortho gonal bisector of the line segment conne cting the class means. 8 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2. We have also trained regula rized linea r logistic regression models \nP (y =1x, w)= g(w0 + w1x1 + w2x2) |\nfor the same data. The regularizat ion penalties, used in penalized conditio nal log-\nlikeliho od estimatio n, were −Cwi 2, where i =0, 1, 2. In other words, only one of the \nparameters were regular ized in each case. Based on the data in Figure 4.1, we gen­\nerated three plots, one for each regularized parameter, of the number of misclassiﬁed \ntraining points as a function of C (Figure 4.2). The three plots are not identiﬁed \nwith the corresponding parameters, however. Please assign the “top”, “middle”, and \n“bottom” plots to the correct parameter, w0, w1, or w2, the parameter that was \nregula rized in the plot. Provide a brief justiﬁcatio n for each assignm ent. \n• (3 points) “top” = (w1) \nBy strongly regularizing w1 we force the boundar y to be horizontal in the ﬁgure. \nThe logistic regression model tries to maximize the log-probability of classifyi ng the \ndata correctly. The highest penalty comes from the misclass iﬁed points and thus \nthe boundary will tend to balanc e the (wor st) errors. In the ﬁgure, this is roughly \nspeaking x2 =1 line, resulting in 4 errors. \n• (3 points) “middle” = (w0) \nIf we regularize w0, then the boundary will eventual ly go through the origin (bias \nterm set to zero). Based on the ﬁgure we can ﬁnd a good linear boundary through \nthe origin with only one error. \n• (3 points) “bottom” = (w2) \nThe training error is unaﬀe cted if we regular ize w2 (constr ain the boundar y to be \nvertic al); the value of w2 would be smal l already without regularization. \n9\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "regularization", "section_heading": "points and the same co­ varianc e matr ices, the decision boundar y is a line and, moreover, should be drawn as the orth", "source_title": "a5dae01d24632b23c2755e6eb7e0b4ad midterm f03soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 7}
{"id": "b6c3d147-60f8-49cb-b26d-5a66bdd6f704", "text": "points) “bottom” = (w2) The training error is unaﬀe cted if we regular ize w2 (constr ain the boundar y to be vertic al); the value of w2 would be smal l already without regularization. 9 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n0 0.5 1 1.5 200.511.52\nx1x2\n0 0.5 1 1.5 2 2.5 3024top\n0 0.5 1 1.5 2 2.5 3024middle\n0 0.5 1 1.5 2 2.5 3024bottom\nregularization parameter CFigur e 4.1 Labeled training set Figure 4.2. Training errors as a functio n \n(repro duced here for clarity) of regula rizatio n penalty \n10\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "regularization", "section_heading": "points) “bottom” = (w2) The training error is unaﬀe cted if we regular ize w2 (constr ain the boundar y to be vertic al)", "source_title": "a5dae01d24632b23c2755e6eb7e0b4ad midterm f03soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 8}
{"id": "1976f126-d7e5-4944-b16b-c70f153b0947", "text": "� 1 6.867 Mac hine learning , lectur e 17 (Jaak kola) \nLecture topics: \n• Mixture models and clustering , k-means \n• Dista nce and clustering \nMixture models and clustering \nWe have so far used mixture models as ﬂexib le ways of constructing probabilit y models for \nprediction tasks. The motivation behind the mixture model was that the available data \nmay include unobserv ed (sub)g roups and by incorp orating such structur e in the model we \ncould obtain more accurate predictions. We are also interested in uncovering that group \nstructure. This is a clustering problem. For example, in the case of modeling exam scores it \nwould be useful to understa nd what types of studen ts there are. In a biological context, it \nwould be useful to uncover which genes are activ e (expressed) in which cell types when the \nmeasuremen ts are from tissue samples involving multiple cell types in unkno wn quantities. \nClustering problems are ubiquito us. \nThere are many diﬀerent types of clustering algorithms. Some generate a series of nested \nclusters by merg ing simple clusters into larger ones (hierarchical agglomera tive clustering ), \nwhile others try to ﬁnd a pre-s peciﬁed number of clusters that best capture the data (e.g., \nk-means). Whic h algorithm is the most appro priat e to use depends in part on what we are \nlooking for. So it is very useful to know more than one clustering metho d. \nMixture models as generative models requir e us to articulate the type of clusters or sub­\ngroups we are looking to identify. The simplest type of clusters we could look for are \nspherical Gaussian clusters, i.e., we would be estimating Gaussian mixtures of the form \nm\nP (x; θ,m)= P (j)N(x; µj ,σj 2I) (1) \nj=1 \nwhere the parameters θ include {P (j)}, {µj }, and {σj 2}. Note that we are estimat ing the \nmixtur e models with a diﬀeren t objective in mind. We are more interes ted in ﬁnding where \nthe clusters are than how good the mixture model is as a generative model. \nThere are many questions to resolve. For example, how many such spherical Gauss ian \nclusters are there in the data? This is a model selection problem. If such clusters exist, \ndo we have enough data to identify them? If we have enough data, can we hope to ﬁnd \nthe clusters via the EM algorithm? Is our appro ach robust, i.e., does our metho d degrade \n Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Unsupervised Learning", "subtopic": "k-means", "section_heading": "� 1 6.867 Mac hine learning , lectur e 17 (Jaak kola) ", "source_title": "5450b1187d84d8ea1cd99ac75526212c lec17", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "dc139759-e1be-45ca-b045-acfe1fe5c382", "text": "exist, do we have enough data to identify them? If we have enough data, can we hope to ﬁnd the clusters via the EM algorithm? Is our appro ach robust, i.e., does our metho d degrade Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� 2 6.867 Mac hine learning , lectur e 17 (Jaak kola) \ngracefully when data contain “background samples” or impurities along with spherical \nGaussian clusters? Can we make the clustering algorithm more robust? Similar questions \napply to other clustering algorithms. We will touch on some of these issues in the context \nof each algorithm. \nMixtures and K-means \nIt is often helpf ul to try to search for simple clusters. For example, consider a mixtur e of \ntwo Gaussians where the variances of the spherical Gaussians may be diﬀeren t: \n2\nP (x; θ)= P (j)N(x; µj ,σj 2I) (2) \nj=1 \nPoints far away from the two means, in whatev er direction, would always be assigned to \nthe Gaussian with the larger variance (tails of that distribution approa ch zero much slower; \nthe posterior assignmen t is based on the ratio of probabili ties). While this is perfectly ﬁne \nfor modeling the density, it does not quite agree with the clustering goal. \nTo avoid such issue s (and to keep the discussion simpler) we will restrict the covariance \nmatrices to be all identical and equal to σ2I where σ2 is commo n to all clusters. Moreover, \nwe will ﬁx the mixing proportions to be unifor m P (j)=1/m. With such restrictio ns we \ncan more easily understand the type of clusters we will discover. The resulting simpliﬁed \nmixtur e model has the form \nm1 � \nP (x; θ)= N(x; µj ,σ2I) (3) m j=1 \nLet’s begin by trying to understand how points are assigned to clusters within the EM \nalgorithm. To this end, we can see how the mixture model partitions the space into \nregio ns where within each regio n a speciﬁc component has the highest posterio r prob­\nabilit y. To start with, consider any two components i and j and the bounda ry where \nP (i|x,θ)= P (j|x,θ), i.e., the set of points for which the component i and j have the same \nposterio r proba bility. Since the Gaussians have equal prior proba bilities, this happens when \nN(x; µj ,σ2I)= N(x; µi,σ2I). Both Gaussians have the same spherical covariance matrix \nso the probabil ity that they assign to points is based on the Euclide an distances to their \nmean vectors. The posteriors therefore can be equal only when the comp onent means are \nequidistan t from the points: \n2 2 2 2�x − µi�= �x − µj �or 2x T (µj − µi)= �µj �−�µj � (4) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "exist, do we have enough data to identify them? If we have enough data, can we hope to ﬁnd the clusters via the EM algor", "source_title": "5450b1187d84d8ea1cd99ac75526212c lec17", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "fdf3da92-a9a4-400e-b2bc-cfc2ea28a32c", "text": "posteriors therefore can be equal only when the comp onent means are equidistan t from the points: 2 2 2 2�x − µi�= �x − µj �or 2x T (µj − µi)= �µj �−�µj � (4) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3 6.867 Mac hine learning , lectur e 17 (Jaak kola) \nThe boundary is therefore linea r1 in x. We can draw such bounda ries between any pair of \ncomp onents as illustr ated in Figure 1. The pairwis e compariso ns induce a Voronoi partition \nof the space where, for example, the regio n enclosing µ1 in the ﬁgure corresp onds to all the \npoints whose closest mean is µ1. This is also the regio n where P (1|x,θ) takes the highest \nvalue among all the posterio r proba bilities . \nµ2 \n2 x 21 \n3 \nx \nµ1 x \nµ3 \n13 \nFigur e 1: Voronoi regio ns resulting from pairwise compar ison of posterio r assignmen t prob­\nabilit ies. Each line corresp onds to a decision between posterio r probabilities of two comp o­\nnents. The line is highligh ted (solid) when the boundary is an activ e constrain t for deciding \nthe highest probability comp onent. \nNote that the posterior assignment probabilities P (j|x,θ) evaluated in the E-step of the \nEM algorithm do not vanish across the boundaries. The region s merely highligh t when \none assignmen t has a higher probabilit y than the others. The overall variance parameter \nσ2 controls how sharply the posterio r probabilities chang e when we cross each boundary . \nSmall σ2 results in very sharp transitions (e.g., from near zero to nearly one) while the \nposterio rs change smoothly when σ2 is large. \nK-m eans. We can deﬁne a simpler and faster version of the EM algorithm by just assigning \neach point to the comp onent with the highest posterior probabilit y (its closest mean). \nGeometrically , the points within each Voronoi regio n are assigned to one component. The \nM-step then simply reposition s each mean vector to the center of the points within the \nregio n. The updated means deﬁne new Voronoi regions and so on. The resulting algorithm \nfor ﬁnding cluster means is known as the K-me ans algorithm: \nE-step: assign each point xt to its closes t mean, i.e., jt = arg min j �xt − µj �2 \nM-step: recomput e µj ’s as means of the assigned points. \n1The lineari ty holds when the two Gaussians have the same covarian ce matrix, spherical or not. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "posteriors therefore can be equal only when the comp onent means are equidistan t from the points: 2 2 2 2�x − µi�= �x −", "source_title": "5450b1187d84d8ea1cd99ac75526212c lec17", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "7d75c51a-32d0-4c4b-902c-2389ddc0b757", "text": "= arg min j �xt − µj �2 M-step: recomput e µj ’s as means of the assigned points. 1The lineari ty holds when the two Gaussians have the same covarian ce matrix, spherical or not. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� 4 6.867 Mac hine learning , lectur e 17 (Jaak kola) \nThis is a highly popular hard assignmen t version of the EM-algorithm. We can view \nthe algorithm as optimizing the complete log-likeliho od of the data with respect to the \nassignmen ts j1,...,jn (E-step) as well as the means µ1,...,µm (M-step). This is equiv alent \nto minimizing the overall squar ed error (distortion) to the cluster means: \nn\nJ(j1,...,jn,µ1,...,µm)= �xt − µjt �2 (5) \nt=1 \nEach step of the algorithm, E or M-step, decreases the objectiv e until convergence. The \nalgorithm can never come back to the same assignmen ts j1,...,jn as it would result in the \nsame value of the objectiv e. Since there are only O(nk) possible partitions of points with \nk means, the algorithm has to converge in a ﬁnite number of steps. At convergence, the \ncluster means ˆµ1,..., µˆm are locally optimal solutio ns to the minim um distor tion objective \nn\nJ(ˆµ1,..., µˆm) = min �xt − µˆj �2 (6) \nj \nt=1 \nThe speed of the K-means algorithm comes at a cost. It is even more sensitive to proper \ninitia lization than a mixtur e of Gaussians trained with EM. A typical and reaso nable \ninitia lization corresp onds to setting the ﬁrst mean to be a rando mly selected training point, \nthe second as the furthest training point from the ﬁrst mean, the third as the furthest \ntraining point from the two means, and so on. This initiali zation is a greedy packing \nalgorithm. \nIdentiﬁabil ity. The k-means or mixture of Gaussians with the EM algorithm can only do \nas well as the maximum likeliho od solution they aim to recover. In other words, even when \nthe underlying clusters are spherical, the number of training example s may be too small for \nthe globa lly optima l (non-trivia l) maxim um likelihood solution to provide any reaso nable \nclusters. As the number of points increases , the qualit y of the maximum likeliho od soluti on \nimpro ves but may be hard to ﬁnd with the EM algorithm. A large number of training points \nalso makes it easier to ﬁnd the maxim um likeliho od solutio n. We could therefore roughly \nspeaking divide the clustering problem into three regimes depending on the number of \ntraining points: not solvable, solvable but hard, and easy. For a recen t discus sion on such \nregimes, see Srebro et al., “An Investiga tion of Computatio nal and Infor matio nal Limits in \nGaussian Mixture Clustering ”, ICML 2006. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "= arg min j �xt − µj �2 M-step: recomput e µj ’s as means of the assigned points. 1The lineari ty holds when the two Gau", "source_title": "5450b1187d84d8ea1cd99ac75526212c lec17", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "602acff1-8d4c-4ee9-b45d-5bf4bf51b099", "text": "solvable but hard, and easy. For a recen t discus sion on such regimes, see Srebro et al., “An Investiga tion of Computatio nal and Infor matio nal Limits in Gaussian Mixture Clustering ”, ICML 2006. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� � 5 6.867 Mac hine learning , lectur e 17 (Jaak kola) \nDistance and clustering \nGaussian mixture models and the K-means algorithm make use of the Euclidean distance \nbetween points. Why should the points be compa red in this manner? In many cases the \nvector represe ntatio n of objects to be clustered is deriv ed, i.e., comes from some feature \ntransformat ion. It is not at all clear that the Euclidean dista nce is an appropria te way of \ncompa ring the resulting feature vectors. \nConsider, for example , a docume nt clustering problem. We migh t treat each documen t x \nas a bag of words and map it into a feature vector based on term frequencies: \nnw(x) = number of times word w appears in x (7) \nf(w|x) = nw(x)� \nw� nw� (x) = term frequency (8) \n� � IDF�� �� # of docs φw(x) = f(w|x) · log # of docs with word w (9) \nwhere the featu re vector φ(x) is known as the TFID F mapping (there are many variatio ns \nof this). IDF stands for inverse document frequency and aims to de-emphasize words that \nappear in all documen ts, words that are unlik ely to be useful for clustering or classiﬁcatio n. \nWe can now interpret the vector s φ(x) as points in a Euclidean space and apply , e.g., the \nK-means clustering algorithm. There are, however, many other ways of deﬁning a distance \nmetric for clustering documen ts. \nDista nce metr ic plays a central role in clustering , regardless of the algorithm. For example, \na simple hierarchical agglomerat ive clustering algorithm is deﬁned almo st entirely on the \nbasis of the dista nce functi on. The algorithm proceeds by succe ssively merging two closest \npoints or closest clusters (average squared distance) and is illustrat ed in Figure 2. \nIn solving clustering problems, the focus should be on deﬁning the dista nce (simila rity) \nmetric, perhaps at the expense of a speciﬁc algorithm. Most algorithms could be applied \nwith the chosen metr ic. One avenue for deﬁning a dista nce metric is throug h model selec­\ntion. Let’s see how this can be done. The simplest model over the words in a documen t is a \nunigr am model where each word is an indep enden t sample from a multi-nomia l distributio n \n{θw}, w θw = 1. The probabilit y of all the words in documen t x is therefore \nP (x|θ)= θw = θnw (x) (10) w \nw∈x w \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "solvable but hard, and easy. For a recen t discus sion on such regimes, see Srebro et al., “An Investiga tion of Computa", "source_title": "5450b1187d84d8ea1cd99ac75526212c lec17", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "5d6dfc04-ed3f-47ca-862b-c13f797e9bd6", "text": "t sample from a multi-nomia l distributio n {θw}, w θw = 1. The probabilit y of all the words in documen t x is therefore P (x|θ)= θw = θnw (x) (10) w w∈x w Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� � � 6 6.867 Mac hine learning , lectur e 17 (Jaak kola) \n3 1 x \na) 2x \nb) \n4 x \nx 2 x \nx 1 3 \nc) x 4 x \n 4 x \nx 2 x \nx 1 3 \n2 4 3 d) 1 \nFigur e 2: a-c) successive merg ing of clusters in a hierar chical clustering algorithm and d) \nthe resulting cluster hiera rchy. \nIt is often a good idea to normalize the documen ts so they have the same length. We could, \nfor examp le, say that each documen t is of lengt h one and word “counts” are given by term \nfrequencies f(w|x). According ly, \nP (x˜|θ)= θf (w|x) (11) w \nw \nwhere x˜denotes a normalized version of documen t x. Consider a cluster of documen ts C. \nThe unigram model that best describes the normalized documen ts in the cluster can be \nobtained by maximizing the log-likelihood \nl(C; θ) = log P (x˜|θ)= f(w|x)log θw (12) \nw x∈C x∈C \nThe maxim um likeliho od solutio n is, as before, obtained throug h empirical counts (repr e­\nsented here by term frequencie s): \n1 � \nθˆw = Cf(w|x) (13) || x∈C \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "t sample from a multi-nomia l distributio n {θw}, w θw = 1. The probabilit y of all the words in documen t x is therefor", "source_title": "5450b1187d84d8ea1cd99ac75526212c lec17", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "1cf6b7f3-cd9e-4fba-837a-dbf105e24e85", "text": "θw (12) w x∈C x∈C The maxim um likeliho od solutio n is, as before, obtained throug h empirical counts (repr e­ sented here by term frequencie s): 1 � θˆw = Cf(w|x) (13) || x∈C Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � \n� \n� \n| || ||\n� \n|� |\n� |� 7 6.867 Mac hine learning , lectur e 17 (Jaak kola) \nThe resulting log-likeliho od value is \nl(C; θˆ) = f(w|x)log θˆw (14) \nw x∈C \n= C� 1 � \nf(wx)log θˆw (15) || \nw |C| x∈C |\n= |C| θˆw log θˆw = −|C|H(θˆ) (16) \nw \nwhere H(θˆ) is the entropy of the unigram distributio n. \nWe can now proceed to deﬁne a distance corresponding to the unigram model. Consider \nany two clusters Ci and Cj and their combinat ion C = Ci ∪Cj . When Ci and Cj are treat ed \nas distinct clusters, we will use a diﬀerent unigram model to capture their term frequencies. \nThe combined cluster C, on the other hand, is modeled with a single unigr am model. We \ncan now treat the documen ts in the two clusters as observ ations and devise a model selection \nproblem for deciding whether the clusters should be viewed as separate or merged. To this \nend, we will evaluate the resulting log-likeliho ods in two ways corresponding to whether \nthe clusters are modeled as separa te or combined. When separ ate: \nl(Ci|θˆ·|i)+ l(Cj |θˆ·|j )= −|Ci|H(θˆ·|i) −|Cj |H(θˆ·|j ) (17) \n= −(|Ci| + |Cj |) Pˆ(y)H(θˆ·|y) (18) \ny∈{i,j} \nwhere Pˆ(y = i)= |Ci|/(|Ci| + |Cj |) is the proba bility that a randomly drawn documen t \nfrom C = Ci ∪ Cj is from cluster Ci. The parameter estima tes θˆw|i and θˆw|j are obtained \nas before. Similarly , when the two clusters are modeled with the same unigra m: \nl(Ci,Cj θˆ) = −(Ci+ Cj )H(θˆ) (19) \nwhere the parameter estimat e θˆw can be written as θˆw = y∈{i,j} Pˆ(y)θˆw|y, i.e., as a cluster \nsize weighted combinatio n of the estimates from the individua l clusters. \nWe can now deﬁne a (squared) dista nce between Ci and Cj accor ding to how much better \nthey are modeled as separa te clusters (log-likeliho od ratio statistic): \nd2(Ci,Cj )= l(Ciθˆ·|i)+ l(Cj θˆ·|j ) − l(Ci,Cj θˆ) (20) \n=(|Ci| + |Cj |) H(θˆ) − Pˆ(y)H(θˆ·|y) (21) \ny∈{i,j} \n=(|Ci| + |Cj |) Iˆ(w; y) (22) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "θw (12) w x∈C x∈C The maxim um likeliho od solutio n is, as before, obtained throug h empirical counts (repr e­ sented h", "source_title": "5450b1187d84d8ea1cd99ac75526212c lec17", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "77dffd6b-046d-40c7-816a-5036f0271051", "text": "are modeled as separa te clusters (log-likeliho od ratio statistic): d2(Ci,Cj )= l(Ciθˆ·|i)+ l(Cj θˆ·|j ) − l(Ci,Cj θˆ) (20) =(|Ci| + |Cj |) H(θˆ) − Pˆ(y)H(θˆ·|y) (21) y∈{i,j} =(|Ci| + |Cj |) Iˆ(w; y) (22) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n8 6.867 Mac hine learning , lectur e 17 (Jaak kola) \nwhere Iˆ(w; y) is the mutual informa tion between words w sampled from the combined \ncluster C and the identity of the cluster (i or j). (Recall a similar deriv ation in the feature \nselection context). More precise ly, the mutual informa tion is comput ed on the basis of \nPˆ(w,y)= Pˆ(y)θˆw|y. Note that d2(Ci,Cj ) is symmetr ic and non-negative but need not \nsatisfy all the properties of a metr ic. It nevertheless compa res the two clusters in a very \nnatural way: we measure how disting uishable they are based on their word distribu tions. \nIn other words, Iˆ(w; y) tells us how much a word sample d at rando m from a documen t in \nthe combined cluster C tells us about the cluster Ci or Cj that the sample came from. Low \ninforma tion content means that the two clusters have nearly identical distributions over \nwords. \nThe (squar ed) metric d2(Ci,Cj ) can be immedia tely used, e.g., in a hiera rchical clustering \nalgorithm (the same algorithm deriv ed from a diﬀeren t perspective is known as an agglom­\nerative informatio n bottleneck metho d). We could take the model selection idea further \nand decide when to stop merging clusters rather than simply providing a scale for deciding \nhow diﬀeren t two clusters are. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Unsupervised Learning", "subtopic": "clustering", "section_heading": "are modeled as separa te clusters (log-likeliho od ratio statistic): d2(Ci,Cj )= l(Ciθˆ·|i)+ l(Cj θˆ·|j ) − l(Ci,Cj θˆ) ", "source_title": "5450b1187d84d8ea1cd99ac75526212c lec17", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 7}
{"id": "43daf007-f2fb-4041-9b2a-96940905a91a", "text": "6.867 Machine learning \nMid-t erm exam \nOcto ber 8, 2003 \n(2 points) Your name and MIT ID: \nJ. J. Doe, MIT ID# 0000000 00\nProblem 1 \nIn this problem we use sequential activ e learning to estimate a linear model \ny = w1x + w0 + � \nwhere the input space (x values) are restricted to be within [−1, 1]. The noise term � \nis assumed to be a zero mean Gaussian with an unknown variance σ2 . Recall that our \nsequen tial active learning metho d selects input points with the highest variance in the \npredicted outputs. Figur e 1 below illustra tes what outputs would be returned for each \nquery (the outputs are not available unless speciﬁcall y queried). \nWe start the learning algorith m by querying output s at two input points, x = −1 and \nx = 1, and let the sequen tial active learning algorithm select the remaining query points. \n1. (4 points) Give the next two inputs that the sequen tial active learning metho d would \npick. Explain why. \n1\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n−1 −0.5 0 0.5 1−0.500.511.5\nxyFigur e 1: Samples from the underlying relation between the inputs x and outputs y. The \noutputs are not available to the learning algorithm unles s speciﬁcally queried. \nA linear function is constr ained the most by the end points (the varianc e is always \nhighest at the end points) and thus the next two points are -1 and 1 (order not \ndetermine d). \nAlternatively, you can look more closely at \n��T �� \nV ar{yˆ(x)} =1 (XT X)−1 1 \nx x \nwhere, initial ly, X = [1, −1;1, 1] (MATLAB notation). Since (XT X)−1 is positive \n(semi-)deﬁnite regardless of the points include d in X, the variance is a convex-up \nparabola and takes its largest value at (one of) the end points. \n2. (4 points) In the ﬁgure 1 above, draw (appro xima tely) the linear relatio n between \nthe inputs and outputs that the active learning metho d would ﬁnd after a large \nnumber of iterat ions. \nSince only the points -1 and 1 are queried, the linear function is deter­\nmine d by the outputs at these points. Henc e the line in the ﬁgure.\n3. (6 points) Would the result be any diﬀeren t if we started with query points x =0 \n2 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "convex", "section_heading": "6.867 Machine learning ", "source_title": "af6e5e8e769e8e4561393320c9e93db6 midterm f01soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "d81c0f67-1b4a-4dfd-988a-08e0c8f8a815", "text": "is deter­ mine d by the outputs at these points. Henc e the line in the ﬁgure. 3. (6 points) Would the result be any diﬀeren t if we started with query points x =0 2 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nand x = 1 and let the sequen tial active learning algorithm select the rema ining query \npoints? Explain why or why not. \nThe same argument applie s here and we would start querying at x=-1 and x=1 in \nan alternating fashion. Note that while the varianc e is a function of inputs only, the \nresulting linear function surely depends on the outputs as well. Initial ly, therefore, \nthere will be a diﬀer ence due to the single non-extr emal query point x=0 but as more \nend points are selected, the diﬀer ence will vanish \nProblem 2\n00.5 11.5 22.5 33.5 4−0.4−0.20\nregularization parameter Clog−probabilityaverage log−probability of test labelsaverage log−probability of training labels\nFigur e 2: Log-pro babilit y of labels as a functio n of regularizat ion parameter C \nHere we use a logistic regression model to solve a classiﬁca tion problem. In Figur e 2, we have \nplotted the mean log-proba bility of labels in the training and test sets after having trained \nthe classiﬁer with quadra tic regulariza tion penalty and diﬀeren t values of the regula rizati on \nparameter C. \n3\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "regularization", "section_heading": "is deter­ mine d by the outputs at these points. Henc e the line in the ﬁgure. 3. (6 points) Would the result be any diﬀ", "source_title": "af6e5e8e769e8e4561393320c9e93db6 midterm f01soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "61b0b4d3-c052-41e0-8a31-d8ee1099746e", "text": "plotted the mean log-proba bility of labels in the training and test sets after having trained the classiﬁer with quadra tic regulariza tion penalty and diﬀeren t values of the regula rizati on parameter C. 3 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n1. (T/F – 2 points) In training a logistic regression model by maximizing \nthe likelihood of the labels given the inputs we have multiple locally \noptimal solutions. F \nThe log-probability of labels given examples implie d by the logistic re­\ngression model is a concave (convex down) function with respect to the \nweights. The (only) locally optimal solution is also globally optima l (if \nthere are only a few examples, there might be a “ﬂat” region with many \nequally optimal solutions, as in, e.g., question 3.4). \n2. (T/F – 2 points) A stochastic gradient algorithm for training logistic \nregression models with a ﬁxed learning rate will ﬁnd the optima l setting \nof the weights exactly . F \nA ﬁxed learning rate means that we are always taking a ﬁnite step to­\nwards improving the log-probability of any single training example in the \nupdate equation. Unless the examples are somehow “aligne d”, we will \ncontinue jumping from side to side of the optimal solution, and will not \nbe able to get arbitrarily close to it. The learning rate has to approach \nto zero in the course of the updates for the weights to conver ge. \n3. (T/F – 2 points) The average log-probabilit y of training labels as in \nFigur e 2 can never increase as we increase C. T \nStronger regular ization means more constr aints on the solution and thus \nthe (aver age) log-probability of the trainin g examples can only get worse. \n4. (4 points) Explain why in Figur e 2 the test log-probability of labels decrea ses for \nlarge values of C. \nAs C increases, we give more weight to constr aining the predictor , and thus give less \nﬂexibility to ﬁtting the training set. The increased regularization guarantees that the \ntest performanc e gets closer to the training perfor manc e, but as we over-c onstr ain \nour allowed predictors, we are not able to ﬁt the training set at all, and although the \ntest perfor manc e is now very close to the training performanc e, both are low. \n5. (T/F – 2 points) The log-probabilit y of labels in the test set would \ndecrease for large values of C even if we had a large number of training \nexamples. \n4 T\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "stochastic gradient", "section_heading": "plotted the mean log-proba bility of labels in the training and test sets after having trained the classiﬁer with quadra", "source_title": "af6e5e8e769e8e4561393320c9e93db6 midterm f01soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "a66ea7f8-ff1a-411d-bc3e-676c662811e6", "text": "both are low. 5. (T/F – 2 points) The log-probabilit y of labels in the test set would decrease for large values of C even if we had a large number of training examples. 4 T Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nThe above argument still holds, but the value of C for which we will \nobserve such a decrease will scale up with the numb er of examples. \n6. (T/F – 2 points) Adding a quadratic regula rizatio n penalty for the \nparameters when estimat ing a logistic regression model ensures that \nsome of the parameters (weights associated with the components of the \ninput vectors) vanish. \nA regular ization penalty for feature selection must have non-zer o deriva­\ntive at zero. Otherwise, the regularization has no eﬀect at zero, and \nweight will tend to be slightly non-zer o, even when this does not im­\nprove the log-probabilities by much. \nProblem 3 \nConsider a training set consisting of the following eight examples: F\nExamples labeled “0” Examples labeled “1” \n3,3,0 2,2,0 \n3,3,1 1,1,1 \n3,3,0 1,1,0 \n2,2,1 1,1,1 \nThe questio ns below pertain to various feature selectio n metho ds that we could use with \nthe logistic regression model. \n1. (2 points) What is the mutual informat ion between the third feature\nand the target label based on the training set?\nThe third feature has the same distribution conditione d on both labels,\nhence it is statistic ally indep endent of the target label and the mutual\ninformation between them is zero.\n2. (2 points) Whic h feature(s) would a ﬁlter feature selectio n metho d 1,2 \nchoose? You can assume here that the mutual informa tion criterion is \nevaluated between a single feature and the label. \nBoth of these features have high, and equal, mutual infor mation with\nthe target label.\n1,33. (2 points) Whic h two feature(s) would a greedy wrapp er process or 2,3 choose? \n5\n0 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "both are low. 5. (T/F – 2 points) The log-probabilit y of labels in the test set would decrease for large values of C ev", "source_title": "af6e5e8e769e8e4561393320c9e93db6 midterm f01soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "47d634dd-3914-491d-b454-01d9b2142180", "text": "and the label. Both of these features have high, and equal, mutual infor mation with the target label. 1,33. (2 points) Whic h two feature(s) would a greedy wrapp er process or 2,3 choose? 5 0 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nFirst, one of the ﬁrst two features will be chosen, as this allows us to\neasily classify correctly six of the eight features. Once one of these fea­\ntures is chosen, the other one does not help us. However, the third\nfeature can help us discriminate between (2,2,1) and (2,2,0)— includ­\ning it will enable us to classify all examples correctly, and will thus be\ninclude d.\n4. (4 points) Whic h featur es would a regula rizatio n approach with a 1-norm penalt y �3 \ni=1 |wi| choose? Explain brieﬂy . \nSince the ﬁrst two features are identic al, shifting weight between them has no eﬀect \non the log-probabilities. The 1-norm penalty is oblivious to shifting weight in this \nway. Thus, it will not favor setting the weight of one of the ﬁrst two features to \nzero, and such a setting will be one of many equally optimal solutions. Most optimal \nsolutions would thus include both features, but depending on how we optimize the \nobjective function, we might choose the rare solutions in which one of the weights is \nset to zero. \nIncluding the third features improve the average log-probabilities, but it also increases \nthe penalty. Whether or not it will be include d depends on the regular ization param­\neter C that controls the relative importanc e of the average log-probabiliti es and the \nregularization penalty. To check what values of C will allow inclus ion of the third \nfeature, we can check the derivative of the objective function at w3 =0: The deriva­\ntive of the log probability of the sample (2,2,1) will be g�(0)x3 = 1 . The derivatives 2 \nfor samples with x3 =0 will be zero, and the derivatives for the other four samples \nwill cancel each other . Thus the total derivative of the log-probabilities term will be \n1 . This will be more signiﬁc ant than the regularization only if C< 1 .2 2 \nProblem 4 \n1. (6 points) Figur e 3 shows the ﬁrst decis ion stump that the AdaBoost algorithm \nﬁnds (starting with the unifor m weights over the training examples). We claim that \nthe weights associated with the training examples after includin g this decision stump \nwill be [1/8, 1/8, 1/8, 5/8] (the weights here are enumera ted as in the ﬁgure). Are \nthese weights correct, why or why not? \nDo not provide an explicit calculatio n of the weights. \n6\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "and the label. Both of these features have high, and equal, mutual infor mation with the target label. 1,33. (2 points) ", "source_title": "af6e5e8e769e8e4561393320c9e93db6 midterm f01soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "f630925b-c49e-496b-858d-0f9a66315a09", "text": "stump will be [1/8, 1/8, 1/8, 5/8] (the weights here are enumera ted as in the ﬁgure). Are these weights correct, why or why not? Do not provide an explicit calculatio n of the weights. 6 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nxxox+1\n+1+1−1+1\n−1\n4321Figur e 3: The ﬁrst decision stump that the boosting algorithm ﬁnds.\nThe boosting algorithm adjusts the weights so that the current hypothesis is at chanc e \nlevel relative to the new weights. Since there is only one miss-classiﬁe d example, it \nhas to receive weight = 1/2. So the weights are not correct. \n2. (T/F – 2 points) The votes that AdaBo ost algorithm assigns to the F \ncomp onen t classiﬁe rs are optima l in the sense that they ensure larger \n“margins” in the training set (higher majority predictio ns) than any \nother setting of the votes. \nThe votes in the boosting algor ithm are optimize d sequential ly and never \n“reoptimize d” after all the hypotheses have been gener ated. These votes \ncannot therefore be optimal in the sense of achieving the largest majority \npredictions for the training examples. \n3. (T/F – 2 points) In the boosting iterat ions, the training error of each F \nnew decis ion stump and the training error of the combined classiﬁer \nvary roughly in concert \nWhile the training error of the combine d classiﬁer typically decreases \nas a function of boosting iterations, the error of the individual decision \nstumps typically increases since the example weights become concen­\ntrated at the most diﬃcult examples. \n7\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "margin", "section_heading": "stump will be [1/8, 1/8, 1/8, 5/8] (the weights here are enumera ted as in the ﬁgure). Are these weights correct, why or", "source_title": "af6e5e8e769e8e4561393320c9e93db6 midterm f01soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "f61895f1-8ca9-4e47-a818-f76b4dcd88d3", "text": "error of the combine d classiﬁer typically decreases as a function of boosting iterations, the error of the individual decision stumps typically increases since the example weights become concen­ trated at the most diﬃcult examples. 7 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nx1x2\nxx\nx\nxxxx\nx\nx\nx\nooo\noo\noox\nxoo\noFigur e 4: Training set, maximum marg in linea r separ ator, and the supp ort vectors (in \nbold). \nProblem 5 \n1. (4 points) What is the leave-one-out cross-validation error estimate for \nmaximum margin separa tion in ﬁgure 4? (we are asking for a number) 0 \nBased on the ﬁgure we can see that removing any single point would not \nchanc e the resulting maximum margin separator. Since all the points \nare initial ly classiﬁe d correctly, the leave-one-out error is zero. \n2. (T/F – 2 points) We would expect the supp ort vectors to remain \nthe same in genera l as we move from a linea r kernel to higher order \npolyno mial kernels. \nThere are no guarantees that the support vectors remain the same. The \nfeature vectors corresponding to polynomial kernels are non-line ar func­\ntions of the original input vectors and thus the support points for max­\nimum margin separation in the feature space can be quite diﬀer ent. F \n3. (T/F – 2 points) Structur al risk minimizat ion is guaranteed to ﬁnd \nthe model (among those considered) with the lowest expected loss F \nWe are guaranteed to ﬁnd only the model with the lowest upper bound \non the expected loss. \n4. (6 points) What is the VC-dimension of a mixtur e of two Gaussians model in the \nplane with equal covariance matrices? Why? \n8 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "error of the combine d classiﬁer typically decreases as a function of boosting iterations, the error of the individual d", "source_title": "af6e5e8e769e8e4561393320c9e93db6 midterm f01soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "2428cf33-58f0-4cdb-8fca-aaccb68b944c", "text": "only the model with the lowest upper bound on the expected loss. 4. (6 points) What is the VC-dimension of a mixtur e of two Gaussians model in the plane with equal covariance matrices? Why? 8 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nA mixtur e of two Gaussians with equal covariance matric es has a linear decision \nboundary. Linear separators in the plane have VC-dim exactly 3. \nProblem 6 \nUsing a set of 100 labeled training examples (two classes), we train the following models: \nGaussI A Gaussian mixture model (one Gaussian per class), where the covariance matrices \nare both set to I (iden tity matr ix). \nGaussX A Gaussian mixture model (one Gaussian per class) witho ut any restrictio ns on \nthe covariance matrices. \nLinLog A logistic regression model with linear features. \nQuadLo g A logistic regression model, using all linear and quadr atic features. \n1. (6 points) After training, we measure for each model the average log probability of \nlabels given examples in the training set. Specify all the equalit ies or inequa lities that \nmust always hold between the models relative to this performa nce measure. We are \nlooking for statemen ts like “model 1 ≤ model 2” or “model 1 = model 2”. If no such \nstatement holds, write “none”. \n9\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "logistic regression", "section_heading": "only the model with the lowest upper bound on the expected loss. 4. (6 points) What is the VC-dimension of a mixtur e of", "source_title": "af6e5e8e769e8e4561393320c9e93db6 midterm f01soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 7}
{"id": "7be052e8-0b75-42a8-b86f-58ee0abc5736", "text": "hold between the models relative to this performa nce measure. We are looking for statemen ts like “model 1 ≤ model 2” or “model 1 = model 2”. If no such statement holds, write “none”. 9 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nGaussI ≤ LinLog (both have logistic postirior s, and LinLog is the logistic model \nmaximizing the average log probabilities) \nGaussX ≤ QuadL og (both have logistic postirior s with quadr atic features, and \nQuadLo g is the model of this class maximizing the average log probabilities) \nLinLog ≤ QuadL og (logistic regression models with linear features are a subclass \nof logistic regression models with quadr atic funct ions— the maximum from the su­\nperclass is at least as high as the maximum from the subclass) \nGaussI ≤ QuadL og (follows from above inequalities) \n(GaussX will have higher average log joint probabilities of examples and labels, then \nwill GaussI. But have higher average log joint probabilities does not necessar ily \ntranslate to higher average log conditional probabilities) \n2. (4 points) Whic h equa lities and inequalit ies must always hold if we instead use the \nmean classiﬁc ation error in the training set as the performance measure? Again use \nthe forma t “model 1 ≤ model 2” or “model 1 = model 2”. Write “none” if no such \nstatement holds. \nNone. Having higher average log conditional probabilities, or average log joint prob­\nabilities, does not necessarily translate to higher or lower classiﬁc ation error. Coun­\nterexamples can be constr ucted for all pairs in both directions. \nAlthough there is no inequalities which is always correct, it is commonly \nthe case that GaussX ≤ GaussI and that QuadL og ≤ LinLog. \nPartial credit of up to two points was awarded for these inequalities. \n10\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "logistic regression", "section_heading": "hold between the models relative to this performa nce measure. We are looking for statemen ts like “model 1 ≤ model 2” o", "source_title": "af6e5e8e769e8e4561393320c9e93db6 midterm f01soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 8}
{"id": "84350c8a-b355-4ebe-861a-df4bbd46c593", "text": "� � 1 6.867 Mac hine learning , lectur e 2 (Jaakkola) \nPerceptron, convergence, and generalization \nRecall that we are dealing with linear classiﬁers through origin, i.e., \nf(x; θ) = sign θT x (1) \nwhere θ ∈Rd speciﬁes the parameters that we have to estimate on the basis of training \nexamples (imag es) x1,..., xn and labels y1,...,yn. \nWe will use the perceptron algorithm to solve the estimatio n task. Let k denote the number \nof parameter updates we have performed and θ(k) the parameter vector after k updates. \nInitially k = 0 and θ(k) = 0. The algorithm then cycles throug h all the training insta nces \n(xt,yt) and updates the parameters only in response to mistak es, i.e., when the label is \npredicted incorrectly . More precisely , we set θ(k+1) = θ(k) + ytxt when yt(θ(k))T xt < 0 \n(mista ke), and otherwise leave the parameters unchang ed. \nConvergence in a ﬁnite number of updates \nLet’s now show that the perceptron algorithm indeed convergences in a ﬁnite number of \nupdates. The same a nalysis will also help us understa nd ho w the linear classiﬁer genera lizes \nto unseen images. To this end, we will assume that all the (training ) images have bounded \nEuclide an norms, i.e., �xt�≤ R for all t and some ﬁnite R. This is clearly the case for any \npixel images with bounded intensit y values. We also make a much stronger assumpti on \nthat there exists a linear classiﬁer in our class with ﬁnite parameter values that correctly \nclassiﬁes all the (training) images. More precisely, we assume that there is some γ> 0 such \nthat yt(θ∗)T xt ≥ γ for all t =1,...,n. The addit ional number γ> 0 is used to ensure that \neach example is classiﬁed correctly with a ﬁnite margin. \nThe convergence proof is based on combining two results: 1) we will show that the inner \nproduct (θ∗)T θ(k) increases at least linearly with each update, and 2) the squar ed norm \n�θ(k)�2 increases at most linearly in the number of updates k. By combinin g the two \nwe can show that the cosine of the angle between θ(k) and θ∗ has to increase by a ﬁnite \nincremen t due to each update. Since cosine is bounded by one, it follows that we can only \nmake a ﬁnite number of updates. \nPart 1: we simply take the inner product (θ∗)T θ(k) before and after each update. When \nmaking the kth update, say due to a mistak e on image xt, we get \n(θ∗)T θ(k) =(θ∗)T θ(k−1) + yt(θ∗)T xt ≥ (θ∗)T θ(k−1) + γ (2) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "� � 1 6.867 Mac hine learning , lectur e 2 (Jaakkola) ", "source_title": "df535ce359aad52f48c203db64558fe8 lec2", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "7210cf67-59b5-4ae5-8d09-67b0eb76dcfb", "text": "(θ∗)T θ(k) before and after each update. When making the kth update, say due to a mistak e on image xt, we get (θ∗)T θ(k) =(θ∗)T θ(k−1) + yt(θ∗)T xt ≥ (θ∗)T θ(k−1) + γ (2) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2 6.867 Mac hine learning , lectur e 2 (Jaakkola) \nsince , by assumptio n, yt(θ∗)T xt ≥ γ for all t (θ∗ is always correct). Thus, after k updates, \n(θ∗)T θ(k) ≥ kγ (3) \nPart 2: Our second claim follows simply from the fact that updates are made only on \nmistak es: \n�θ(k)2 = �θ(k−1) + ytxt�2 (4) �\n= �θ(k−1)2 +2yt(θ(k−1))T xt + �xt�2 (5) \n�θ(k−1)�\n2 2≤\n�θ(k−1)�\n2 + �xt� (6) \n≤ �+ R2 (7) \nsince yt(θ(k−1))T xt < 0 whenev er an update is made and, by assumptio n, �xt�≤ R. Thus,\n�θ(k)�2 ≤ kR2 (8)\nWe can now combine parts 1) and 2) to bound the cosine of the angle between θ∗ and θ(k):\nkγ kγ cos(θ∗,θ(k))= (θ∗)T θ(k) 1) 2) \n(9) �θ(k)��θ∗�≥�θ(k)��θ∗�≥√\nkR2 �θ∗� \nSince cosine is bounded by one, we get \nkγ R2 2 \n1 ≥√\nkR2 �θ∗� or k ≤�\nγθ\n2 ∗� (10) \nMargin and geom etry \nIt is worthwhile to understand this result a bit further. For example, does �θ∗�2/γ2 relate \nto how diﬃcult the classiﬁcatio n problem is? Indeed, it does. We claim that its inverse, \ni.e., γ/�θ∗� is the smallest dista nce in the imag e space from any example (image) to the \ndecis ion boundar y speciﬁed by θ∗. In other words, it serves as a measure of how well the \ntwo classes of imag es are separated (by a linear boundary ). We will call this the geometric \nmargin or γgeom (see ﬁgure 1). γ−1 is then a fair measure of how diﬃcult the problem is: geom \nthe smaller the geometr ic margin that separates the training imag es, the more diﬃcult the \nproblem. \nTo calculate γgeom we measure the distance from the decision boundary θ∗T x = 0 to one of \nthe images xt for which ytθ∗T xt = γ. Since θ∗ speciﬁes the normal to the decision boundar y, \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "(θ∗)T θ(k) before and after each update. When making the kth update, say due to a mistak e on image xt, we get (θ∗)T θ(k", "source_title": "df535ce359aad52f48c203db64558fe8 lec2", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "2a6c2f5f-d6ab-42a8-bb8b-af3c804a7337", "text": "calculate γgeom we measure the distance from the decision boundary θ∗T x = 0 to one of the images xt for which ytθ∗T xt = γ. Since θ∗ speciﬁes the normal to the decision boundar y, Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3 6.867 Mac hine learning , lectur e 2 (Jaakkola) \nθ∗ x \nx x decis ion boundary \nθ∗T x = 0 \nx x x \nγgeom \nFigur e 1: Geometric margin \nthe shortest path from the boundar y to the image xt will be parallel to the normal. The \nimag e for which ytθ∗T xt = γ is therefore among those closest to the boundary . Now, let’s \ndeﬁne a line segment from x(0) = xt, parallel to θ∗, towards the bounda ry. This is given \nby \nytθ∗ \nx(ξ)= x(0) − ξ (11) �θ∗� \nwhere ξ deﬁnes the length of the line segmen t since it multiplies a unit length vector. It \nremains to ﬁnd the value of ξ such that θ∗T x(ξ) = 0, or, equivalently, ytθ∗T x(ξ) = 0. This \nis the point where the segmen t hits the decision bounda ry. Thus \n� � \nytθ∗T x(ξ) = \n= \n= \n= ytθ∗T x(0) − ξ ytθ∗ \n�θ∗� \nytθ∗T � \nxt − ξ ytθ∗ \n�θ∗� � \nytθ∗T xt − ξ �θ∗�2 \n�θ∗� \nγ − ξ�θ∗� = 0 (12) \n(13) \n(14) \n(15) \nimplying that the distance is exactly ξ = γ/�θ∗� as claimed. As a result, the bound on the \nnumber of perceptron updates can be written more succinctly in terms of the geometric \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "calculate γgeom we measure the distance from the decision boundary θ∗T x = 0 to one of the images xt for which ytθ∗T xt ", "source_title": "df535ce359aad52f48c203db64558fe8 lec2", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "4854ace8-fdab-4a56-b334-185b2d0ae355", "text": "(12) (13) (14) (15) implying that the distance is exactly ξ = γ/�θ∗� as claimed. As a result, the bound on the number of perceptron updates can be written more succinctly in terms of the geometric Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n4 6.867 Mac hine learning , lectur e 2 (Jaakkola) \nmargin γgeom (dista nce to the boundar y): \n� �2R k ≤ (16) γgeom \nwith the unde rstanding that γgeom is the largest geomet ric margin that could be achieved \nby a linear classiﬁer for this problem. Note that the result does not depend (directly) on \nthe dime nsion d of the examples, nor the number of training examples n. It is neverthele ss � �2 Rtempting to interpr et γgeom as a measure of diﬃcult y (or complexit y) of the problem of \nlearning linear classiﬁers in this setting. You will see later in the course that this is exactly \nthe case, cast in terms of a measure known as VC-dimension. \nGenerali zation guarantees \nWe have so far discus sed the perceptron algorithm only in relation to the training set but \nwe are more interested in how well the perceptron classiﬁe s images we have not yet seen, \ni.e., how well it genera lizes to new images. Our simple analysis above actually provides \nsome informat ion about generalizatio n. Let’s assume then that all the images and labels \nwe could possibly encounter satisfy the same two assumptions. In other words, 1) �xt�≤ R \nand 2) ytθ∗T xt ≥ γ for all t and some ﬁnite θ∗. So, in essence, we assume that there is a \nlinear classiﬁe r that works for all images and labels in this problem, we just don’t know \nwhat this linear classiﬁer is to start with. Let’s now imag ine getting the imag es and labels \none by one and performing only a single update per image, if misclassiﬁed, and move on. \nThe previous situatio n concerning the training set corresp onds to encoun tering the same \nset of images repeatedly. How many mistak es are we now going to make in this inﬁnite \narbitrar y sequence of images and labels, subject only to the two assumpt ions? The same \nnumber k ≤ (R/γgeom)2 . Once we have made this many mistakes we would classif y all the \nnew images correctly. So, provided that the two assumpt ions hold, especially the second \none, we obtain a nice guarantee of generalizatio n. One caveat here is that the perceptr on \nalgorithm does need to know when it has made a mistake. The bound is after all cast in \nterms of the number of updates based on mista kes. \nMaxi mum margi n classiﬁer? \nWe have so far used a simple on-line algorithm, the perceptro n algorithm, to estimate a \nlinear classiﬁer. Our reference assumpt ion has been, however, that there exists a linear \nclassiﬁer that has a large geometr ic marg in, i.e., whose decision boundar y is well separa ted \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "(12) (13) (14) (15) implying that the distance is exactly ξ = γ/�θ∗� as claimed. As a result, the bound on the number of", "source_title": "df535ce359aad52f48c203db64558fe8 lec2", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "da0cddb7-8171-426c-99ff-bc6f02f9f9ca", "text": "algorithm, to estimate a linear classiﬁer. Our reference assumpt ion has been, however, that there exists a linear classiﬁer that has a large geometr ic marg in, i.e., whose decision boundar y is well separa ted Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n5 6.867 Mac hine learning , lectur e 2 (Jaakkola) \nfrom all the training imag es (examples). Can’t we ﬁnd such a large margin classiﬁer \ndirectly? Yes, we can. The classiﬁe r is known as the Supp ort Vector Machine or SVM for \nshort . See the next lecture for details. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "algorithm, to estimate a linear classiﬁer. Our reference assumpt ion has been, however, that there exists a linear class", "source_title": "df535ce359aad52f48c203db64558fe8 lec2", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "cd1a0c16-a473-42e6-8599-3f66c7097e56", "text": "1 6.867 Mac hine learning , lectur e 19 (Jaak kola) \nLecture topics: \n• Mark ov chains (cont’d) \nHidden Markov Models • \nMarkov chains (cont’d) \nIn the context of spectral clustering (last lecture) we discusse d a rando m walk over the \nnodes induce d by a weighted graph. Let Wij ≥ 0 be symmetric weights associated with the \nedges in the graph; Wij = 0 whenever edge doesn’t exist. We also assumed that Wii = 0 for \nall i. The graph deﬁnes a rando m walk where the proba bility of transitionin g from node \n(state) i to node j is given by \nWijP (X(t +1) = j|X(t)= i)= Pij = � Wij� (1) \nj� \nNote that self-transitio ns (going from i to i) are disallo wed because Wii = 0 for all i. \nWe can understand the random work as a homo gene ous Markov chain: the probabilit y of \ntransitioning from i to j only depends on i, not the path that took the process to i. In other \nwords, the curren t state summa rizes the past as far as the future transitions are concerned. \nThis is a Markov (condit ional indep endence) prop erty: \nP (X(t +1) = j|X(t)= i,X(t − 1) = it−1,...,X(1) = i1)= P (X(t +1) = j|X(t)= i) (2) \nThe term “homog eneous” speciﬁes that the transition probabilities are indep enden t of time \n(the same probabilities are used whenev er the rando m walk returns to i). \nWe also deﬁned ergodicity as follows: Mark ov chain is ergodic if there exist a ﬁnite m such \nthat \nP (X(t + m)= j|X(t)= i) > 0 for all i and j (3) \nSimple weighted graphs need not deﬁne ergodic chains. Consider, for example, a weighted \ngraph between two nodes 1 − 2 where W12 > 0. The resulting rando m walk is necessarily \nperiodic, i.e., 121212 .... A Markov chain is ergodic only when all the states are communi­\ncating and the chain is aperiodic which is clearly not the case here. Similarly , even a graph \n1 − 2 − 3 with positiv e weights on the edges would not deﬁne an ergodic Markov chain. \nEvery other state would neces sarily be 2, thus the chain is periodic. The reason here is \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "1 6.867 Mac hine learning , lectur e 19 (Jaak kola) ", "source_title": "bd962d39492e55697cfa6bb418ae1642 lec19", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "20462b4f-fb50-427d-b4ea-65af66740dda", "text": "1 − 2 − 3 with positiv e weights on the edges would not deﬁne an ergodic Markov chain. Every other state would neces sarily be 2, thus the chain is periodic. The reason here is Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2 6.867 Mac hine learning , lectur e 19 (Jaak kola) \nthat Wii = 0. By adding a positive self-transitio n, we can remo ve periodicity (rando m walk \nwould stay in the same state a variable number of steps). Any connected weighted graph \nwith positive weights and positiv e self-transitions gives rise to an ergodic Markov chain. \nOur deﬁnition of the random walk so far is a bit incomplete. We did not specify how \nthe process start ed, i.e., we didn’t specify the initial state distribut ion. Let q(i) be the \nprobabilit y that the rando m walk is started from state i. We will use q as a vector of \nprobabilities across k states (reserving n for the number training examples as usual). \nThere are two ways of describing Markov chains: through state transition diagr ams or as \nsimple graphical models. The descriptions are complemen tary. A transition diagram is a \ndirected graph over the possible states where the arcs between states specify all allowed \ntransitions (those occuring with non-zero proba bility). See Figure 1 for examples. We \ncould also add the initia l state distributio n as transitions from a dedicated initial (null) \nstate (not shown in the ﬁgure). \n1 \n1 \n2 \n2 \n3 \nFigur e 1: Examples of transition diagrams deﬁning non-e rgodic Markov chains. \nIn graphical models, on the other hand, we focus on explicating variables and their de­\npendencies. At each time point the random walk is in a particular state X(t). This isa \nrandom variable. It’s value is only aﬀected by the random variable X(t − 1) specifying \nthe state of the random walk at the previous time point. Graphically , we can theref ore \nwrite a sequence of random variables where arcs specify how the values of the variables are \ninﬂuence d by others (dependen t on others). More precisely, X(t − 1) X(t) means that →\nthe value of X(t) depends on X(t − 1). Put another way, in simulating the random walk, \nwe would have to know the value of X(t − 1) in order to sample a value for X(t). The \ngraphical model is shown in Figure 2. \nState predict ion \nWe will cast the problem of calcula ting the predic tive probabilities over states in a form \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "1 − 2 − 3 with positiv e weights on the edges would not deﬁne an ergodic Markov chain. Every other state would neces sar", "source_title": "bd962d39492e55697cfa6bb418ae1642 lec19", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "36f119f1-f3e1-4f78-ba77-348b39afc91f", "text": "in order to sample a value for X(t). The graphical model is shown in Figure 2. State predict ion We will cast the problem of calcula ting the predic tive probabilities over states in a form Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � \n� \n� �� � \n� 3 6.867 Mac hine learning , lectur e 19 (Jaak kola) \nFigur e 2: Markov chain as a graphi cal model. \nthat will be useful for Hidden Markov Models later on. Since \nP (X(t + m)= j|X(t)= i)=[P m]ij (4) \nwe can also write for any n \nk k\nP (X(n)= j)= q(i)P (X(n)= j|X(1) = i)= q(i)[P n−1]ij (5) \ni=1 i=1 \nIn a vector form qT P n−1 is a row vector whose jth comp onent is P (X(n)= j). Note that \nthe matr ix products involve summing over all the intermediat e states until X(n)= j. More \nexplicitly , let’s evaluate the sum over all the states x1,...,xn in the matrix form as \nn − 1 times � n−1� �� � \nP (X(1) = x1) P (X(t +1) = xt+1|X(t)= xt)= q T PP ··· PP 1 = 1 (6) \nx1,...,xn t=1 \nThis is a sum over kn possible state conﬁgura tions (settings of x1,...,xn) but can be easily \nperformed in terms of matrix products. We can understand this in terms of recursive eval­\nuation of t step probabilities αt(i)= P (X(t)= i). We will write αt for the corresp onding \ncolumn vector so that \nt − 1 times \nq T PP PP = αtT (7) ··· \nClearly , \nq T = α1 T (8) \nαtT \n−1P = αtT , t> 1 (9) \nk\nαt−1(i)P ij = αt(j) (10) \ni=1 \n. . . X(t + 1) X(t − 1) X(t) \n. . . \nEstimation\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "in order to sample a value for X(t). The graphical model is shown in Figure 2. State predict ion We will cast the proble", "source_title": "bd962d39492e55697cfa6bb418ae1642 lec19", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "81112ac0-a4ec-44ef-84ad-406cfa2c5490", "text": ", q T = α1 T (8) αtT −1P = αtT , t> 1 (9) k αt−1(i)P ij = αt(j) (10) i=1 . . . X(t + 1) X(t − 1) X(t) . . . Estimation Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � � \n� \n� 4 6.867 Mac hine learning , lectur e 19 (Jaak kola) \nMark ov models can be estimated easily from observed sequences of states. Given x1,...,xn \n(e.g., 1212221), the log-likeliho od of the observ ed sequence is given by \nn−1\nlog P (x1,...,xn) = log P (X(1) = x1) P (X(t +1) = xt+1|X(t)= xt) (11) \nt=1 \nn−1\n= log q(x1) + log Pxt,xt+1 (12) \nt=1 \n= log q(x1)+ nˆ(i,j)log Pij (13) \ni,j \nwhere ˆn(i,j) is the number of observ ed transition s from i to j in the sequence x1,...,xn. \nThe resulting maximum likeliho od setting of Pij is obtained as an empirical fractio n \nnˆ(i,j)Pˆij = � \nj� nˆ(i,j�) (14) \nNote that q(i) can only be reliably estimated from multiple observ ed sequences. For ex­\nample, based on x1,...,xn, we would simply set ˆq(i)= δ(i,x1) which is hardly accurate \n(sample size one). Regulariza tion is useful here, as before. \nHidden Mark ov Models \nHidden Markov Models (HMMs) extend Mark ov models by assuming that the states of the \nMark ov chain are not observ ed directly, i.e., the Markov chain itself remains hidden. We \ntherefore also model how the states relate to the actua l observations. This assumptio n of a \nsimple Mark ov model underly ing a sequence of observations is very useful in many practical \ncontexts and has made HMMs very popular models of sequence data, from speech recogni­\ntion to bio-sequence s. For example, to a ﬁrst appro xima tion, we may view words in speech \nas Mark ov sequences of phonemes . Phonemes are not observed directly , however, but have \nto be related to acoustic observations. Similar ly, in modeling protein sequences (sequences \nof amino acid residues), we may, again approxima tely, describe a protein molecule as a \nMark ov sequence of structura l characteristics. The structura l features are typically not \nobserv able, only the actual residues. \nWe can understa nd HMMs by combining mixture models and Markov models. Consider \nthe simple example in Figure 3 over four discrete time points t =1, 2, 3, 4. The ﬁgure \nsummarizes multiple sequences of observ ations y1,...,y4, where each observ ation sequence \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "hmm", "section_heading": ", q T = α1 T (8) αtT −1P = αtT , t> 1 (9) k αt−1(i)P ij = αt(j) (10) i=1 . . . X(t + 1) X(t − 1) X(t) . . . Estimation C", "source_title": "bd962d39492e55697cfa6bb418ae1642 lec19", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "755769fd-d59a-4b61-8fb3-01851fd33587", "text": "mixture models and Markov models. Consider the simple example in Figure 3 over four discrete time points t =1, 2, 3, 4. The ﬁgure summarizes multiple sequences of observ ations y1,...,y4, where each observ ation sequence Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� 5 6.867 Mac hine learning , lectur e 19 (Jaak kola) \ncorresponds to a single value yt per time point. Let’s begin by ignoring the time informatio n \nand instead collapse the observations across the time points. The observ ations form two \nclusters are now well modeled by a two comp onen t mixt ure: \nP (y)= P (j)P (y|j) (15) \nj=1 2\nwhere, e.g., P (y|j) could be a Gaussian N(y; µj ,σ\nare eﬀectively modeling the data at each time point with the same mixt ure model. If we 2). By collapsing the observations we\nj \ngenera te data form the resulting mixt ure model we would select a mixture component at \nrandom at each time step and generate the observ ation from the corresp onding comp onent \n(cluster). There’s nothing that ties the selectio n of mixture comp onents in time so that \nsamples from the mixture yield “pha ntom” clusters at successiv e time points (we select the \nwrong componen t/cluster with equa l proba bility). By omitting the time infor matio n, we \ntherefore place half of the probabilit y mass in locations with no data. Figur e 4 illustrates \nthe mixture model as a graphica l model. \na) \ntime t = 1 x x x \nx xx xx x \nx \nx xx x x x\nt = 2 t = 3 t = 4 y \nb) \ntime t = 1 x x x \nx xx xx x \nx \nx xx x x x\nt = 2 t = 3 t = 4 y \nFigur e 3: a) Example data over four time points, b) actual data and ranges of samples \ngenera ted from a mixture model (red ovals) estima ted witho ut time infor matio n. \nThe solutio n is to model the selection of the mixt ure comp onen ts as a Mark ov model, i.e., \nthe componen t at t = 2 is selected on the basis of the componen t used at t = 1. Put another \nway, each state in the Mark ov model now uses one of the components in the mixture model \nto generate the corresponding observ ation. As a graphica l model, the mixture model is a \ncombination of the two as shown in Figure 5. \nProba bility model \nOne advantage of represe nting the HMM as a graphical model is that we can easily write \ndown the joint probabilit y distribution over all the variables. The graph explicates how the \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "hmm", "section_heading": "mixture models and Markov models. Consider the simple example in Figure 3 over four discrete time points t =1, 2, 3, 4. ", "source_title": "bd962d39492e55697cfa6bb418ae1642 lec19", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "47a438ec-2fd9-4598-9fd0-866fe39bcc19", "text": "5. Proba bility model One advantage of represe nting the HMM as a graphical model is that we can easily write down the joint probabilit y distribution over all the variables. The graph explicates how the Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� � � 6 6.867 Mac hine learning , lectur e 19 (Jaak kola) \nX(1) X(2) X(3) X(4) \nY (1) Y (2) Y (3) Y (4) \nFigur e 4: A graphical model view of the mixtur e model over the four time points. The \nvariables are indexed by time (diﬀerent samples would be drawn at each time point) but \nthe parameters are shared across the four time points. X(t) refers to the selection of the \nmixtur e comp onent while Y (t) refers to the observ ations. \nvariables depend on each other (who inﬂuences who) and thus highlig hts which condition al \nprobabilities we need to write down: \nP (x1,...,xn,y1,...,yn)= P (x1)P (y1x1)P (x2x1)P (y2x2) ... (16) |\nn−1| |\n= P (x1)P (y1x1) [P (xt+1xt)P (yt+1xt+1)] (17) |\nt=1 | |\nn−1\n= q(x1)P (y1|x1) Pxt,xt+1 P (yt+1|xt+1) (18) \nt=1 \nwhere we have used the same notation as before for the Mark ov chains. \nX(1) X(2) X(3) X(4) \nY (1) Y (2) Y (3) Y (4) \nFigur e 5: HMM as a graphical model. It is a Markov model where each state is associated \nwith a distributio n over observ ations. Alterna tively, we can view it as a mixt ure model \nwhere the mixtur e comp onents are selected in a time dependen t manner. \nThree probl ems to solve \nWe typically have to be able to solve the following three problems in order to use these \nmodels eﬀectiv ely: \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "hmm", "section_heading": "5. Proba bility model One advantage of represe nting the HMM as a graphical model is that we can easily write down the j", "source_title": "bd962d39492e55697cfa6bb418ae1642 lec19", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "f299a5ed-3c0f-4548-a542-9c1040784a7b", "text": "e comp onents are selected in a time dependen t manner. Three probl ems to solve We typically have to be able to solve the following three problems in order to use these models eﬀectiv ely: Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� \n� � 7 6.867 Mac hine learning , lectur e 19 (Jaak kola) \n1. Evaluate the proba bility of observ ed data or \nP (y1,...,yn)= P (x1,...,xn,y1,...,yn) (19) \nx1,...,xn \n2. Find the most likely hidden state sequenc e x1,...,x∗ ∗ \nn given observ ations y1,...,yn, \ni.e.,\n{x∗,...,x1∗ \nn} = arg \n3. Estimate the parameters of the model from multiple sequences of y1(l),...,yn(l\nl ) , l = \n1,...,L. \nProbl em 1 \nAs in the context of Mark ov chains we can eﬃcien tly sum over the possible hidden state \nsequenc es. Here the summa tion means evaluating P (y1,...,yn). We will perform this in \ntwo ways depending on whether the recursio n moves forward in time, comput ing αt(j), or \nbackward in time, evaluating βt(i). The only change from before is the fact that whatev er \nstate we happen to visit at time t, we will also have to generat e the observ ation yt from \nthat state. This additiona l requiremen t of genera ting the observ ations can be included via \ndiagonal matrices \n⎡ ⎤ P (y|1) 0 \nDy = ⎣ ⎦ (21) ··· \n0 P (y|k) \nSo, for example , \nk\nq T Dy1 1 = q(i)P (y1|i)= P (y1) (22) \ni=1 \nSimilar ly, \nk k\nq T Dy1 PDy2 1 = q(i)P (y1|i) Pij P (y2|j)= P (y1,y2) (23) \ni=1 j=1 max \nx1,...,xn P (x1,...,xn,y1,...,yn) (20) \nWe can therefore write the forward and backward algorithms as metho ds that perfor m the \nmatrix multiplicat ions in \nq T Dy1 PDy2 P PDyn 1 = P (y1,...,yn) (24) ··· \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "e comp onents are selected in a time dependen t manner. Three probl ems to solve We typically have to be able to solve t", "source_title": "bd962d39492e55697cfa6bb418ae1642 lec19", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "966e967c-9075-4b14-8984-041aadab966a", "text": "x1,...,xn P (x1,...,xn,y1,...,yn) (20) We can therefore write the forward and backward algorithms as metho ds that perfor m the matrix multiplicat ions in q T Dy1 PDy2 P PDyn 1 = P (y1,...,yn) (24) ··· Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� \n� 8 6.867 Mac hine learning , lectur e 19 (Jaak kola) \neither in the forward or backward direction. In terms of the forward pass algorithm: \nq T Dy1 = αT \n1 (25) \nαT PDyt = αT\nt , or equiv alently (26) � �t−1\nk\nαt−1(i)P ij P (yt|j)= αt(j) (27) \ni=1 \nThes e values hold exactly αt(j)= P (y1,...,yt,X(t)= j) since we have generated all the \nobserv ations up to and including yt and have summed over all the states except for the last \none X(t). \nThe backward pass algorithm is simila rly deﬁne d as: \nβn = 1 (28) \nβt = PDyt+1 βt+1, or equiv alently (29) \nk\nβt(i)= Pij P (yt+1|j)βt+1(j) (30) \nj=1 \nIn this case βt(i)= P (yt+1,...,yn|X(t)= i) since we have summed over all the possible \nvalues of the state variables X(t + 1),...,X(n), start ing from a ﬁxed X(t)= i, and the \nﬁrst observ ation we have generated in the recursion is yt+1. \nBy combining the two recursions we can ﬁnally evaluate \nk\nP (y1,...,yn)= αtT βt = αt(i)β t(i) (31) \ni=1 \nwhich holds for any t =1,...,n. You can understa nd this result in two ways: either in \nterms of performing the remaining matrix multiplicat ion corresp onding to the two parts \nαT\nt βt����� �� � \nP (y1,...,yn)= (q T Dy1 P PDyt )(PDyt+1 PDyn 1) (32) ··· ··· \nor as an illustra tion of the Markov prop erty: \nαt(i) βt(i)k������ �� � \nP (y1,...,yn)= P (y1,...,yt,X(t)= i) P (yt+1,...,yn|X(t)= i) (33) \ni=1 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n6.867 Mac hine learning , lectur e 19 (Jaak kola) 9 \nAlso, since βn(i) = 1 for all i, clearly \nP (y1, . . . , yn) = k� \ni=1 αn(i) = k� \ni=1 P (y1, . . . , yt, X(t) = i) (34) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "x1,...,xn P (x1,...,xn,y1,...,yn) (20) We can therefore write the forward and backward algorithms as metho ds that perfo", "source_title": "bd962d39492e55697cfa6bb418ae1642 lec19", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 7}
{"id": "e1fa8379-cdba-4e51-b215-0b4eb0d5a6c4", "text": "� \n� � |\n� \n� 1 6.867 Mac hine learning , lectur e 20 (Jaak kola) \nLecture topics: \n• Hidden Markov Models (cont’d) \nHidden Mark ov Models (cont’d) \nWe will continue here with the three problems outlined previ ously . Consider having given \na set of sequences of observ ations y1,...,yn. The observations typically do not contain the \nhidden state sequence and we are left with the following problems to solve: \n1. Evaluate the proba bility of observ ed data or \nP (y1,...,yn)= P (x1,...,xn,y1,...,yn) (1) \nx1,...,xn \n2. Find the most likely hidden state sequenc e x∗ \n1,...,x∗ \nn given observ ations y1,...,yn, \ni.e., \n{x∗ \n1,...,x∗ \nn} = arg max P (x1,...,xn,y1,...,yn) (2) \nx1,...,xn \n3. Estimate the parameters of the model from multiple sequences of y1(l),...,yn(l\nl ) , l = \n1,...,L. \nWe have alrea dy solved the ﬁrst problem. For example, this can be done with the forward \nalgorithm \nq(j)P (y1j)= α1(j) (3) \nk\nαt−1(i)P ij P (yt|j)= αt(j) (4) \ni=1 \nwhere αt(j)= P (y1,...,yt,X(t)= j) so that P (y1,...,yn)= j αn(j). \nProbl em 2: most likely hidden state sequence \nThe most likely hidden state sequence can be found with a small modiﬁca tion to the forward \npass algorithm. The goal is to ﬁrst evaluate “max -probabilit y” of data \nmax P (y1,...,yn,x1,...,xn)= P (y1,...,yn,x∗ \n1,...,xn∗ ) (5) \nx1,...,xn \n Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "hidden markov", "section_heading": "� ", "source_title": "1ad9ace4da67d4c396fa56c250dc2b12 lec20", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "4e29a30b-733f-428d-874d-08818123ea11", "text": "hidden state sequence can be found with a small modiﬁca tion to the forward pass algorithm. The goal is to ﬁrst evaluate “max -probabilit y” of data max P (y1,...,yn,x1,...,xn)= P (y1,...,yn,x∗ 1,...,xn∗ ) (5) x1,...,xn Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� 2 6.867 Mac hine learning , lectur e 20 (Jaak kola) \nand subsequen tly reconstruct the maximizing sequence x∗ \n1,...,x∗ \nn. The max operation is \nsimilar to evaluating \nP (y1,...,yn,x1,...,xn)= P (y1,...,yn) (6) \nx1,...,xn \nwhich we were able to do with just the forward algorithm. In fact, we can obtain the \nmax-probabilit y of data by merely chang ing the ’sum’ in the forward algorithm to a ’max’: \n� q(j�)P (y1|j)= d1(j) (7) \nmax dt−1(i)P ij P (ytj)= dt(j) (8) \ni |\nwhere \ndt(j) = max P (y1,...,yt,x1,...,xt−1,X(t)= j) (9) \nx1,...,xt−1 \nIn the forward algorithm we ﬁnally summed over the last state j in αn(j) to get the \nprobabilit y of observ ations. Analog ously , here we need to maximize over that last state so \nas to get the max-pro babilit y: \nmax P (y1,...,yn,x1,...,xn) = max dn(j) (10) \nx1,...,xn j \nWe now have the maximum value but not yet the maximizing sequence. We can easily \nreconstruct the sequence by backtracking search, i.e., by sequen tially ﬁxing states starting \nwith the last one: \nx∗ \nn = arg maxdn(j) (11) \nj \nxt = arg maxdt(i)P i,x∗ (12) \ni t+1 \nIn other words, the backward iteration simply ﬁnds i that attains the maximum in Eq.(8 ) \nwhen j has already been ﬁxed to the maximizing value x∗ \nt+1 for the next state. The resulting \nalgorithm that evaluates dt(j) through the above recursive formula, and follows up with \nthe backtracking search to realize (one of) the most likely hidden state sequence s, is known \nas the Viter bi algorithm. \nConsider an HMM with two underly ing states and transition probabilit ies as describ ed in \nFigur e 1. Note that the model canno t return to state 1 after it has left it. Each state \nj =1, 2 is associated with a Gaussian output distribution P (y|j)= N(y; µj ,σ2), where \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "hmm", "section_heading": "hidden state sequence can be found with a small modiﬁca tion to the forward pass algorithm. The goal is to ﬁrst evaluate", "source_title": "1ad9ace4da67d4c396fa56c250dc2b12 lec20", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "e5c7e2c6-951c-4f83-85cf-f5181608e06c", "text": "Figur e 1. Note that the model canno t return to state 1 after it has left it. Each state j =1, 2 is associated with a Gaussian output distribution P (y|j)= N(y; µj ,σ2), where Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3 6.867 Mac hine learning , lectur e 20 (Jaak kola) \nµ1 = 3, µ2 = 1, and the variances are assumed to be the same. We are given 8 observ ations \ny1,...,y8 shown in the ﬁgure. Now, for any speciﬁc value of σ2, we can ﬁnd the most likely \nhidden state sequence x∗ \n1,...,x∗ \n8 with the Viterbi algorithm. \nLet’s try to understa nd how this state sequence behaves as a functio n of the comm on \nvariance σ2 . When σ2 is large, the two output distribut ions, P (y|1) and P (y|2), assign \nessentially the same probabilit y to all the observ ations in the ﬁgure. Thus the most likely \nhidden state sequence is one that is guided solely by the Mark ov model (no observ ations). \nThe resulting sequence is all 2’s. The proba bility of this sequence under the Markov model \nis just 1/2 (there’s only one choice, the initia l selection) . The probabilit y of any other state \nsequenc e is at most 1/4. Now, let’s consider the other extreme, when the variance σ2 is \nvery small. In this case, the state sequenc e is essen tially only guided by the observations \nwith the constra int that the you cannot transition out of state 2. The most likely state \nsequenc e in this case is ﬁve 1’s followed by three 2’s, i.e., 11111222. The two observations \ny4 and y5 keep the model in state 1 even though y3 is low. This is because the Mark ov chain \nforces us to either capture y3 or {y4,y5} but not both. If the model could return to state 1, \nthe most likely state sequence would become 11211222. For intermediate values of σ2 the \nmost likely state sequenc e tries to balance the tendency of the Mark ov chain to choose 2 \nas soon as possible and the need to assign a reason able probabilit y to all observ ations. For \nexample, if σ2 ≈ 1 then the resulting most likely state sequence is 11222222. \na) \n0.5 21 0.5\n0.5\n10.5 \nb) \n0.5 \n2 1 X(1) \n2 1 X(4) \n2 1 \n2 1 \n1 1 1 X(3) X(2) \n0.5 0.5 0.5 \n0.5 0.5 0.50.5 \nFigur e 1: a) A two-state markov chain with transitio n probabilit ies, b) the same chain \nunfolded in time with the corresp onding state variables, c) example observations over 8 \ntime points and the output distributio ns. c) \ny \nx x \nx x x \nx x x 1 3 \n2 \nt = 1 t = 8 P (y|2) = N(y; 1, σ2) P (y|1) = N(y; 3, σ2) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "Figur e 1. Note that the model canno t return to state 1 after it has left it. Each state j =1, 2 is associated with a G", "source_title": "1ad9ace4da67d4c396fa56c250dc2b12 lec20", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "977044bc-786c-467d-8d28-d38da29b0954", "text": "and the output distributio ns. c) y x x x x x x x x 1 3 2 t = 1 t = 8 P (y|2) = N(y; 1, σ2) P (y|1) = N(y; 3, σ2) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� \n� � � � � \n� � � � � 4 6.867 Mac hine learning , lectur e 20 (Jaak kola) \nProbl em 3: estimation \nIn most cases we have to estimat e HMM s only on the basis of output sequences such as \ny1,...,yn without knowing the corresponding states of the Markov chain. This is akin \nto mixture models discusse d earlier. These are incomplete data estimat ion problems, i.e., \nwe do not have observations for all the variables involved in the model. As before, the \nestimatio n can be performed iteratively via the EM algorithm. \nA simple way to deriv e the EM algorithm is to start with complete observations, i.e., we \nassume we have x1,...,xn as well as y1,...,yn. Note that while typica lly we would have \nmultiple observ ation sequenc es, we will focus here on a single sequence to keep the equa tions \nsimple. Now, we can encode the complete observations by deﬁning \n1, if xt = i δ(i|t)= 0, otherwise (13) \n1, if xt = i and xt+1 = jδ(i,j|t)= 0, otherwise (14) \nThe comp lete log-likeliho od then becomes \nk k n\nl({xt}, {yt})= δ(i|1)log q(i)+ δ(i|t)log P (yt|i) \ni=1 i=1 t=1 \nk k n\n+ δ(i,j|t) log Pij (15) \ni=1 j=1 t=1 \nwhere the ﬁrst term simply picks out log q(x1); in the second term, for each i, we consider \nall the observ ations that had to be generated from state i; in the last expression, the term \nin the brackets coun ts how many times each i j transition occurred in the sequence → \nx1,...,xn. Given the counts δ(), we can now solve for the maximizing parameters as in ·\nthe case of simple Markov chains (the ﬁrst and the last express ion), and as in mixture \nmodels (second expression). \nThe EM algorithm now follows directly from replacing the hard counts, δ(i|t) and δ(i,j|t), \nwith the corresponding posterior proba bilities p(i|t) and p(i,j|t), evaluated on the basis of \nthe current HMM parameters. The posterio rs we need are \np(i|t)= P (X(t)= i|y1,...,yn) (16) \np(i,j|t)= P (X(t)= i,X(t +1) = j|y1,...,yn) (17) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "and the output distributio ns. c) y x x x x x x x x 1 3 2 t = 1 t = 8 P (y|2) = N(y; 1, σ2) P (y|1) = N(y; 3, σ2) Cite a", "source_title": "1ad9ace4da67d4c396fa56c250dc2b12 lec20", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "5f89fdb3-7d1e-42af-bfbc-489c73c50b24", "text": "the corresponding posterior proba bilities p(i|t) and p(i,j|t), evaluated on the basis of the current HMM parameters. The posterio rs we need are p(i|t)= P (X(t)= i|y1,...,yn) (16) p(i,j|t)= P (X(t)= i,X(t +1) = j|y1,...,yn) (17) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n5 6.867 Mac hine learning , lectur e 20 (Jaak kola) \nIt remains to show how these posterior probabilit ies can be computed. We can start by \nwriting \nP (y1,...,yn,X(t)= i)= P (y1,...,yt,X(t)= i)P (yt+1,...,yn|X(t)= i) (18) \n= αt(i)β t(i) (19) \nwhich follows directly from the Mark ov property (future observ ations do not depend on the \npast ones provided that we know which state we are in curren tly). The posterio r is now \nobtained by norma lization \nαt(i)β t(i)P (X(t)= i|y1,...,yn)= �k αt(i�)βt(i�) (20) \ni�=1 \nSimilar ly, \nP (y1,...,yn,X(t)= i,X(t +1) = j) \n= P (y1,...,yt,X(t)= i) Pij P (yt+1|j) P (yt+2,...,yn|X(t +1) = j) (21) \n= αt(i)P ij P (yt+1|j)βt+1(j) (22) \nThe posterio r again results from normalizing across i and j: \nP (X(t)= i,X(t)= jy1,...,yn)= �k �α\nkt(i)P ij P (yt+1|j)βt+1(j) (23) |\ni�=1 j�=1 αt(i�)Pi�j� P (yt+1|j�)βt+1(j�) \nIt is important to understa nd that P (X(t)= i,X(t)= j|y1,...,yn) are not the transition \nprobabilities we have as parameters in the HMM model. Thes e are posterior probabilities \nthat a likely hidden state sequence that had to generate the observ ation sequence went \nthrough i at time t and transitioned into j; they are evaluated on the basis of the model \nand the observ ed sequence. \nMult iple (parti al) alignment \nAs another example of the use of the Viterbi algorithm as well as the EM algorithm for \nestimating HMM models, let’s consider the problem of multiple alignmen t of sequences. \nHere we are interes ted in ﬁnding a pattern, a fairly conserv ed sequence of observations, \nembedded in unkno wn locations in multiple observ ed sequences. We assume we know very \nlittle about the pattern other than that it appeared once in all the sequences (a constra int \nwe could easily relax further). For simplicit y, we will assume here that we know the length \nof the pattern (four time points/positio ns). The sequences could be speech signals where \na particular word was uttered in each but we don’t know when the word appeared in \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "the corresponding posterior proba bilities p(i|t) and p(i,j|t), evaluated on the basis of the current HMM parameters. Th", "source_title": "1ad9ace4da67d4c396fa56c250dc2b12 lec20", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "4fcfd553-a16a-4065-b930-0232bd83d67d", "text": "here that we know the length of the pattern (four time points/positio ns). The sequences could be speech signals where a particular word was uttered in each but we don’t know when the word appeared in Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n6 6.867 Mac hine learning , lectur e 20 (Jaak kola) \nthe signal, nor what exactly the word was. The sequenc es could also be protein or DNA \nsequenc es where we are looking for a sequenc e fragmen t that appears in all the available \nsequenc es (e.g., a binding site). \nPerhaps the simplest possible HMM model we could specify is given in Figure 2. The states \nm1,...,m4 are “mat ch states” that we envision will be used to genera te the pattern we \nare looking for. States I1 and I2 are “insert states” that generate the remaining parts of \neach observation sequence, before and after the pattern. Each state is associated with an \noutput distribut ion: P (y|Ii), i =1, 2, P (y|mi), i =1,..., 4. The parameters p and the \noutput distributions need to be learned from the available data. \nNote that this is a model that genera tes a ﬁnite length sequence. We will ﬁrst enter the \ninsert state, spend there on average 1/(1 − p) time steps, then generate the pattern, i.e., \none observation in succession from each of the matc h states, and ﬁnally spend another \n1/(1 − p) time steps on average to generate ﬂanking observations. We have speciﬁcally set \nthe p parameter associated with the ﬁrst insert state to agree with that of the second. This \ntying of parameters (bala ncing the cost of repeating insert states) ensures that, given any \nspeciﬁc observation sequence, y1,...,yn, there is no bias towards ﬁnding the pattern in any \nparticular location of the sequence . \nThis model is useless for ﬁnding a pattern in a single observ ation sequence. However, it \nbecomes more useful when we have multiple observ ation sequences that can all be assumed \nto contain the pattern. The stereot ypical way that the pattern is generated, one observ ation \nfrom each successive matc h states, and the freedom to associate any observation with each \nof the matc h states, encourages the matc h states to take over the recurring pattern in the \nsequenc es (rather than being generated from the insert states). The output distrib utions \nfor the insert states canno t become very speciﬁc since they will have to be used to generate \nmost of the observ ations in the sequence s. \nm4 Begin End 1 − p \np 1 − p \npI1 I2 m1 m2 m3 \nFigur e 2: A simple HMM model for the multiple alignmen t task (only the Markov chain \npart shown). \nNow, given multiple sequenc es, we can simply train the parameters in our HMM model via \nthe EM algorithm to maximize the log-likelihood that the model assigns to those sequences. \nAt this point we are not concerned about where the patterns actually occur, just interested \nin ﬁnding appropr iate parameter values (output distributions). Similar ly to mixt ure models \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "hmm", "section_heading": "here that we know the length of the pattern (four time points/positio ns). The sequences could be speech signals where a", "source_title": "1ad9ace4da67d4c396fa56c250dc2b12 lec20", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "7b702b62-74a0-4321-a157-d5c85d809e6f", "text": "that the model assigns to those sequences. At this point we are not concerned about where the patterns actually occur, just interested in ﬁnding appropr iate parameter values (output distributions). Similar ly to mixt ure models Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n7 6.867 Mac hine learning , lectur e 20 (Jaak kola) \nfor clustering, the location of the pattern (and what the pattern is), is resolv ed in a soft \nmanner through the posterior assignments of observ ations to matc h or insert states. \nOnce the parameters are found, we can use the Viterb i algorithm to “label” each observ ation \nin a sequenc e with the corresp onding most likely hidden state. So, for example , for a \nparticular observ ation sequence, y1,...,yn, we might get \nI1 I1 ... I1 m1 m2 m3 m4 I2 I2 ... I2 (24) y1 y2 ... yt−1 yt yt+1 yt+2 yt+3 yt+4 yt+5 ... yn \nas the most likely hidden state sequence . The states in the most likely state sequence are \nin one to one corresp ondence with the observations. So, in this case, the pattern clear ly \noccurs exactly at time/p osition t, where the sequence of match states begins. \nThe sequence fragmen ts in all the observ ation sequences that were identiﬁed with the \npattern can be subsequen tly aligned as in the ﬁgure below. \nm1,m2,m3,m4 \nm1,m2,m3,m4 \n... \nm1,m2,m3,m4 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare \n \n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Unsupervised Learning", "subtopic": "clustering", "section_heading": "that the model assigns to those sequences. At this point we are not concerned about where the patterns actually occur, j", "source_title": "1ad9ace4da67d4c396fa56c250dc2b12 lec20", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "be444b2a-3ff8-4c8b-86fa-f3d1e724c996", "text": "6.867 Machine learning \nMid-t erm exam \nOcto ber 13, 2004 \n(2 points) Your name and MIT ID: \nProblem 1\n−1 0 1−101\nxnoise\n−1 0 1−101\nxnoise\n−1 0 1−101\nxnoise\nA B C \n1. (6 points) Each plot above claims to represent predictio n errors as a function of \nx for a trained regression model based on some dataset. Some of these plots could \npotentially be prediction errors for linear or quadra tic regression models, while oth­\ners couldn’t. The regression models are trained with the least squares estimatio n \ncriterio n. Please indicate compat ible models and plots. \nA B C \nlinear regression ( ) ( ) ( ) \nquadratic regression ( ) ( ) ( ) \n1\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 2 \nHere we explore a regression model where the noise variance is a function of the input \n(variance increases as a function of input). Speciﬁcally \ny = wx + � \nwhere the noise � is normally distr ibuted with mean 0 and standard deviation σx. The \nvalue of σ is assumed known and the input x is restricted to the interval [1, 4]. We can \nwrite the model more compa ctly as y ∼ N(wx,σ2x2). \nIf we let x vary within [1, 4] and sample outputs y from this model with some w, the \nregression plot might look like \n1 2 3 40246810\nxy\n1. (2 points) How is the ratio y/x distributed for a ﬁxed (consta nt) x?\n2. Supp ose we now have n training points and targets {(x1,y1), (x2,y2),..., (xn,yn)}, \nwhere each xi is chosen at random f rom [1, 4] and the corresponding y i is subsequen tly \nsampled from yi ∼ N(w∗xi,σ2xi 2) with some true underlying parameter value w∗; the \nvalue of σ2 is the same as in our model. \n2\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "6.867 Machine learning ", "source_title": "5ca07f780e61497d0da9137e218901a0 midterm f04", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "29c99592-a1c0-4f03-ad0a-8e04936bf2d7", "text": "rom [1, 4] and the corresponding y i is subsequen tly sampled from yi ∼ N(w∗xi,σ2xi 2) with some true underlying parameter value w∗; the value of σ2 is the same as in our model. 2 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n(a) (3 points) What is the maximum-lik eliho od estimate of w as a function of the \ntraining data? \n(b) (3 points) What is the variance of this estimat or due to the noise in the target \noutputs as a function of n and σ2 for ﬁxed inputs x1,...,xn? For later utility \n(if you omit this answ er) you can deno te the answ er as V (n,σ2). \nSome potentially useful relations: if z ∼ N(µ,σ2), then az ∼ N(aµ,σ2a2) for a \nﬁxed a. If z1 ∼ N(µ1,σ12) and z2 ∼ N(µ2,σ22) and they are indep enden t, then \nVar(z 1 + z2)= σ12 + σ22 . \n3. In sequen tial activ e learning we are free to choose the next training input xn+1, here \nwithin [1, 4], for which we will then receiv e the corresp onding noisy target yn+1, sam­\npled from the unde rlying model. Supp ose we already have {(x1,y1), (x2,y2),..., (xn,yn)}\nand are trying to ﬁgure out which xn+1 to select. The goal is to choose xn+1 so asto \nhelp minimize the variance of the predictions f(x;ˆwn)= wˆnx, where ˆwn is the maxi­\nmum likeliho od estimat e of the parameter w based on the ﬁrst n training examples. \n(a) (2 points) What is the variance of f(x;ˆwn) due to the noise in the training out­\nputs as a function of x, n, and σ2 given ﬁxed (alrea dy chosen) inputs x1,...,xn? \n(b) (2 points) Whic h xn+1 would we choose (within [1, 4]) if we were to next select \nx with the maximum variance of f(x;ˆwn)? \n(c) (T/F – 2 points) Since the variance of f(x;ˆwn) only depends on x, \nn, and σ2, we could equally well select the next point at random from \n[1, 4] and obtain the same reductio n in the maximum variance. \n3 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "function", "section_heading": "rom [1, 4] and the corresponding y i is subsequen tly sampled from yi ∼ N(w∗xi,σ2xi 2) with some true underlying paramet", "source_title": "5ca07f780e61497d0da9137e218901a0 midterm f04", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 1}
{"id": "dc5ae771-7823-476f-9ed8-f210b468b0a3", "text": "Since the variance of f(x;ˆwn) only depends on x, n, and σ2, we could equally well select the next point at random from [1, 4] and obtain the same reductio n in the maximum variance. 3 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n−2−1.5 −1−0.5 00.5 11.5 200.10.20.30.40.50.60.70.80.91(1) P (y = 1|x, ˆw) (2) P (y = 1|x, ˆw) \ny = 0 y = 1 y = 0 \nFigur e 1: Two possible logistic regression solutio ns for the three labeled points. \nProblem 3 \nConsider a simple one dimens ional logistic regression model \nP (y =1|x, w)= g(w0 + w1x) \nwhere g(z) = (1+exp(−z))−1 is the logistic functio n. \n1. Figur e 3 shows two possible conditio nal distributio ns P (y =1|x, w), viewed as a \nfunction of x, that we can get by changing the parameters w. \n(a) (2 points) Please indic ate the number of classiﬁcatio n errors for each condi­\ntional given the labeled example s in the same ﬁgure \nConditio nal (1) makes ( ) classiﬁc ation errors \nConditio nal (2) makes ( ) classiﬁc ation errors \n(b) (3 points) One of the conditiona ls in Figure 3 corresp onds to the \nmaximum likeliho od setting of the parameters wˆbased on the labeled \ndata in the ﬁgur e. Whic h one is the ML solution (1 or 2)? \n(c) (2 points) Would adding a regular ization penalt y |w1|2/2 to the log-\nlikeliho od estima tion criterio n aﬀect your choice of solutio n (Y/N)? \n4\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "logistic regression", "section_heading": "Since the variance of f(x;ˆwn) only depends on x, n, and σ2, we could equally well select the next point at random from ", "source_title": "5ca07f780e61497d0da9137e218901a0 midterm f04", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "40cbb15c-1a83-4214-8313-159937f65499", "text": "one is the ML solution (1 or 2)? (c) (2 points) Would adding a regular ization penalt y |w1|2/2 to the log- likeliho od estima tion criterio n aﬀect your choice of solutio n (Y/N)? 4 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n0 50 100 150 200 250 300−1.5−1−0.500.51\nnumber of training examplesexpected log−likelihood of test labelsFigur e 2: The expected log-likeliho od of test labels as a functio n of the number of train ing \nexamples. \n2. (4 points) We can estima te the logistic regression parameters more accurately with \nmore training data. Figur e 2 shows the expected log-lik eliho od of test labels for a \nsimple logistic regression model as a function of the number of training example s and \nlabels. Mark in the ﬁgur e the structural error (SE) and appr oximatio n error (AE), \nwhere “error” is measured in terms of log-likelihood. \n3. (T/F – 2 points) In general for small training sets, we are likely \nto reduc e the appr oxima tion error by adding a regular ization penalt y \n|w1|2/2 to the log-likeliho od criterion. \n5\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "one is the ML solution (1 or 2)? (c) (2 points) Would adding a regular ization penalt y |w1|2/2 to the log- likeliho od ", "source_title": "5ca07f780e61497d0da9137e218901a0 midterm f04", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "d44aceb9-7554-4193-919e-0b1f22bb6e80", "text": "3. (T/F – 2 points) In general for small training sets, we are likely to reduc e the appr oxima tion error by adding a regular ization penalt y |w1|2/2 to the log-likeliho od criterion. 5 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nx\n2 \n(0,1) (1,1) o x \n(0,0) (1,0) x o x1 \nFigur e 3: Equa lly likely input conﬁgura tions in the training set \nProblem 4 \nHere we will look at metho ds for selecting input featur es for a logistic regression model \nP (y =1|x, w)= g(w0 + w1x1 + w2x2) \nThe available training examples are very simple, involving only binary valued inputs: \nNum ber of copies x1 x2 y\n10 1 11\n10 0 10\n10 1 00\n10 0 01\nSo, for example, there are 10 copies of x = [1, 1]T in the training set, all labeled y = 1. \nThe correct label is actually a deterministic functio n of the two features: y =1 if x1 = x2 \nand zero otherwise. \nWe deﬁne greedy selection in this context as follows: we start with no featur es (train only \nwith w0) and success ively try to add new featur es provided that each addition strictl y \nimpro ves the training log-likeliho od. We use no other stopping criterio n. \n1. (2 points) Could greedy selection add either x1 or x2 in this case? \nAnsw er Y or N. \n2. (2 points) What is the classiﬁc ation error of the training examples that \nwe could achieve by including both x1 and x2 in the logistic regression \nmodel? \n6 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3. (3 points) Supp ose we deﬁne another possible feature to include, a function of x1 \nand x2. Whic h of the following features, if any, would permit us to correctly classify \nall the training example s when used in combinat ion with x1 and x2 in the logistic \nregression model: \n() x1 − x2 \n() x1x2 \n() x22 \n4. (2 points) Could the greedy selection metho d choose this feature as \nthe ﬁrst feature to add when the available featur es are x1, x2 and your \nchoice of the new feature? Answ er Y or N.\n7\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "3. (T/F – 2 points) In general for small training sets, we are likely to reduc e the appr oxima tion error by adding a r", "source_title": "5ca07f780e61497d0da9137e218901a0 midterm f04", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "cc6d93df-4691-4448-afc4-5b2b0664765d", "text": "Could the greedy selection metho d choose this feature as the ﬁrst feature to add when the available featur es are x1, x2 and your choice of the new feature? Answ er Y or N. 7 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 5\n00.5 11.5 22.5 300.511.522.53\n(0,0)(2,2)(0,3)\n(h,1)\nFigur e 4: Labeled trainin g exampl es \nSupp ose we only have four training examples in two dimensions (see Figure 4): \npositiv e exam ples at x1 = [0, 0]T , x2 = [2, 2]T and \nnega tive examples at x3 =[h, 1]T , x4 = [0, 3]T . \nwhere we treat h ≥ 0 as a parameter. \n1. (2 points) How large can h ≥ 0 be so that the training points are still \nlinearly separ able? \n2. (2 points) Does the orientation of the maxim um margin decision \nbounda ry change as a functio n of h when the points are separ able? \nAnsw er Y or N. \n8\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3. (4 points)What is the margin achieved by the maximum margin bounda ry as a \nfunction of h? \n4. (3 points) Assume that h =1/2 (as in the ﬁgur e) and that we can \nonly observ e the x2-componen t of the input vector s. Witho ut the other \ncomp onen t, the labeled training points reduce to (0,y = 1), (2,y = 1), \n(1,y = −1), and (3,y = −1). What is the lowest order p of polyno mial \nkernel that would allow us to correctly classif y these points? \n9\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nAddi tional set of ﬁgures\n−1 0 1−101\nxnoise\n−1 0 1−101\nxnoise\n−1 0 1−101\nxnoise\nA B C\n1 2 3 40246810\nxy\n−2−1.5 −1−0.5 00.5 11.5 200.10.20.30.40.50.60.70.80.91\n(1) P (y = 1|x, ˆw) (2) P (y = 1|x, ˆw) \ny = 0 y = 1 y = 0 \n10\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n0 50 100 150 200 250 300−1.5−1−0.500.51\nnumber of training examplesexpected log−likelihood of test labels\nx1 x2 \nx \nx (1,1) (0,1) \n(1,0) (0,0) o \no \n00.5 11.5 22.5 300.511.522.53\n(0,0)(2,2)(0,3)\n(h,1)\n11", "topic": "Math for ML", "subtopic": "vector", "section_heading": "Could the greedy selection metho d choose this feature as the ﬁrst feature to add when the available featur es are x1, x", "source_title": "5ca07f780e61497d0da9137e218901a0 midterm f04", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "4828ff8e-f829-43b3-9e1d-ff180145bb75", "text": "y = 0 10 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]. 0 50 100 150 200 250 300−1.5−1−0.500.51 number of training examplesexpected log−likelihood of test labels x1 x2 x x (1,1) (0,1) (1,0) (0,0) o o 00.5 11.5 22.5 300.511.522.53 (0,0)(2,2)(0,3) (h,1) 11\n\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "for ", "section_heading": "y = 0 10 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw", "source_title": "5ca07f780e61497d0da9137e218901a0 midterm f04", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 6}
{"id": "8abd129b-3b89-4d25-9d9c-47d9d342b966", "text": "1 6.867 Mac hine learning , lectur e 3 (Jaakkola) \nThe Supp ort Vector Machine \nSo far we have used a referenc e assumpt ion that there exists a linea r classiﬁer that has \na large geometric margin, i.e., whose decision boundar y is well separa ted from all the \ntraining images (examples). Such a large margin classiﬁer seems like one we would like to \nuse. Can’t we ﬁnd it more directly? Yes, we can. The classiﬁer is known as the Supp ort \nVector Machine or SVM for short . \nYou could imag ine ﬁnding the maximum margin linear classiﬁer by ﬁrst identifying any \nclassiﬁer that correctly classiﬁes all the example s (Figure 2a) and then increasing the ge­\nometr ic margin until the classiﬁer “locks in place” at the point where we cannot increase \nthe marg in any further (Figur e 2b). The solutio n is unique. \na) b)\nθ decision boundary x \nx x \nx x x θT x = 0 \n x x decision boundary \nˆθT x = 0 \nx x \nγgeom ˆθ \nx \nx \nFigur e 1: a) A linea r classiﬁer with a small geometric margin, b) maximum margin linea r \nclassiﬁer. \nMore formally , we can set up an optimizatio n problem for directly maximizing the geometric \nmargin. We will need the classiﬁer to be correct on all the training examples or ytθT xt ≥ γ \nfor all t =1,...,n. Subject to these constraints, we would like to maximize γ/�θ�, i.e., \nthe geomet ric margin. We can alternativ ely minimize the inverse �θ�/γ or the inverse \nsquar ed 1 (�θ�/γ)2 subject to the same constra ints (the facto r 1/2 is included merely for 2 \nlater convenience ). We then have the following optimization problem for ﬁnding θˆ: \n1 minimize 2�θ�2/γ2 subject to ytθT xt ≥ γ for all t =1,...,n (1) \nWe can simplify this a bit further by getting rid of γ. Let’s ﬁrst rewrite the optimizat ion \nproblem in a way that highlig hts how the solut ion depends (or doesn’t depend) on γ: \n1 minimize 2�θ/γ�2 subject to yt(θ/γ)T xt ≥ 1 for all t =1,...,n (2) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "1 6.867 Mac hine learning , lectur e 3 (Jaakkola) ", "source_title": "c8442240a87cdacbb2b90f8b53393e9d lec3", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "78a34a2e-0a13-4616-9974-db3a69111ab6", "text": "ﬁrst rewrite the optimizat ion problem in a way that highlig hts how the solut ion depends (or doesn’t depend) on γ: 1 minimize 2�θ/γ�2 subject to yt(θ/γ)T xt ≥ 1 for all t =1,...,n (2) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2 6.867 Mac hine learning , lectur e 3 (Jaakkola) \nIn other words, our classiﬁca tion problem (the data, setup) only tells us something about \nthe ratio θ/γ, not θ or γ separa tely. For example, the geometric margin is deﬁne d only on \nthe basis of this ratio. Scalin g θ by a constan t also doesn’t change the decision bounda ry. \nWe can therefore ﬁx γ = 1 and solve for θ from \nminimize 1\n2�θ�2 subject to ytθT xt ≥ 1 for all t =1,...,n (3) \nThis optimiza tion problem is in the standar d SVM form and is a quadr atic programming \nproblem (objective is quadratic in the parameters with linear constrain ts). The resulting \ngeometric margin is 1/�θˆ� where θˆis the unique solution to the above problem. The \ndecis ion boundar y (separa ting hyper-plane) nor the value of the geometric margin were \naﬀected by our choice γ = 1. \nGeneral formulation, oﬀset param eter \nWe will modify the linea r classiﬁer here sligh tly by adding an oﬀset term so that the decis ion \nbounda ry does not have to go through the origin. In other words, the classiﬁer that we \nconsider has the form \nf(x; θ,θ0) = sign( θT x + θ0 ) (4) \nwith parameters θ (normal to the separating hyper-plane) and the oﬀset parameter θ0,a \nreal number. As before, the equatio n for the separ ating hyper-plane is obtained by setting \nthe argumen t to the sign functio n to zero or θT x + θ0 = 0. This is a genera l equa tion for a \nhyper-plane (line in 2-dim) . The additi onal oﬀset parameter can lead to a classiﬁe r with a \nlarger geometric marg in. This is illustr ated in Figures 2a and 2b. Note that the vector θˆ\ncorrespondin g to the maximum margin solution is diﬀerent in the two ﬁgur es. \nThe oﬀset parameter changes the optimizatio n problem only sligh tly: \n1 minimize 2�θ�2 subject to yt(θT xt + θ0) ≥ 1 for all t =1,...,n (5) \nNote that the oﬀset parameter only appears in the constra ints. This is diﬀeren t from \nsimply modifying the linear classiﬁer through origin by feeding it with examples that have \nan addition al consta nt comp onent, i.e., x� =[x; 1]. In the above formulation we do not bias \nin any way where the separa ting hyper-plane should appear, only that it should maximize \nthe geometr ic margin. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "ﬁrst rewrite the optimizat ion problem in a way that highlig hts how the solut ion depends (or doesn’t depend) on γ: 1 m", "source_title": "c8442240a87cdacbb2b90f8b53393e9d lec3", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "5f4ed336-cd4b-43db-8bcf-d6ff65b6f66f", "text": "al consta nt comp onent, i.e., x� =[x; 1]. In the above formulation we do not bias in any way where the separa ting hyper-plane should appear, only that it should maximize the geometr ic margin. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3 6.867 Mac hine learning , lectur e 3 (Jaakkola) \ndecision boundary \nθˆT x + θˆ0 =0 \ndecision boundary \nθˆT x =0 \nx x \nθˆ\nx x \nθˆ\nx x x x \nγgeom x γgeom x \nx x \na) b) \nFigur e 2: a) Maximum margin linear classiﬁe r throug h origin; b) Maximum marg in linea r \nclassiﬁer with an oﬀset parameter \nProp erties of the maximum margi n linear classiﬁer \nThe maxim um marg in classiﬁer has several very nice properties, and some not so advanta­\ngeous features. \nBeneﬁts. On the positiv e side, we have already motivated these classiﬁe rs based on the \nperceptron algorithm as the “best reference classiﬁers”. The solution is also unique based \non any linearly separable training set. Moreover, drawing the separ ating boundary as far \nfrom the training example s as possible makes it robust to noisy examples (thoug h not \nnoisy labels). The maximum marg in linear bounda ry also has the curio us property that \nthe solutio n depends only on a subset of the examples, those that appear exactly on the \nmargin (the dashed lines parallel to the boundary in the ﬁgures). The examples that lie \nexactly on the marg in are called supp ort vectors (see Figu re 3). The rest of the examples \ncould lie anywhere outside the margin without aﬀec ting the solution. We would therefore \nget the same classiﬁer if we had only receiv ed the supp ort vector s as training examples. Is \nthis is a good thing? To answer this question we need a bit more forma l (and fair) way of \nmeasuring how good a classiﬁe r is. \nOne possible “fair ” performance measure evaluated only on the basis of the training exam­\nples is cross-validation. This is simply a method of retrainin g the classiﬁer with subsets of \ntraining examples and testing it on the rema ining held- out (and therefore fair) examples, \npretending we had not seen them before. A particular version of this type of procedure \nis called leave-one-out cross-validation. As the name suggests, the procedure is deﬁned as \nfollows: select each training example in turn as the single example to be held- out, train the \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "al consta nt comp onent, i.e., x� =[x; 1]. In the above formulation we do not bias in any way where the separa ting hype", "source_title": "c8442240a87cdacbb2b90f8b53393e9d lec3", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "e9f37ab7-f5c9-4c19-a46b-7c9522412a64", "text": "version of this type of procedure is called leave-one-out cross-validation. As the name suggests, the procedure is deﬁned as follows: select each training example in turn as the single example to be held- out, train the Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n4 6.867 Mac hine learning , lectur e 3 (Jaakkola) \nˆθ x \nx \nx \nx x ˆθT x + ˆθ0 = 0 decision boundary \nγgeom x \nFigur e 3: Supp ort vectors (circled) associated the maximum marg in linear classiﬁer \nclassiﬁer on the basis of all the rema ining training examples, test the resulting classiﬁer on \nthe held- out example, and coun t the errors. More precisely , let the superscript ’−i’ denote \nthe parameters we would obtain by ﬁnding the maximum marg in linear separa tor witho ut \nthe ith training example. Then \nn� � 1 � \nleave-one-out CV error = Loss yi,f(xi; θ−i,θ0−i) (6) n i=1 \nwhere Loss(y ,y�) is the zero-o ne loss. We are eﬀectiv ely trying to gauge how well the \nclassiﬁer would genera lize to each training example if it had not been part of the traini ng \nset. A classiﬁer that has a low leave-one-out cross- validatio n error is likely to generalize \nwell thoug h it is not guaranteed to do so. \nNow, what is the leave-one-out CV error of the maxim um margin linear classiﬁer? Well, \nexamples that lie outside the margin would be classiﬁed correctly regardless of whether \nthey are part of the training set. Not so for supp ort vectors. They are key to deﬁning the \nlinear separ ator and thus, if remo ved from the training set, may be misclassiﬁed as a result. \nWe can therefore deriv e a simple upper bound on the leave-one-out CV error : \n# of supp ort vectors leave-one-out CV error ≤ (7) n \nA small number of supp ort vectors – a sparse solutio n – is therefor e advantageous. This is \nanother argumen t in favor of the maxim um margin linear separa tor. \nProbl ems. There are problems as well, however. Even a single training example , if misla­\nbeled, can radically change the maxim um marg in linear classiﬁer. Consider, for example, \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "version of this type of procedure is called leave-one-out cross-validation. As the name suggests, the procedure is deﬁne", "source_title": "c8442240a87cdacbb2b90f8b53393e9d lec3", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "70614159-90fd-4df3-ad98-1966a6994f83", "text": "maxim um margin linear separa tor. Probl ems. There are problems as well, however. Even a single training example , if misla­ beled, can radically change the maxim um marg in linear classiﬁer. Consider, for example, Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n5 6.867 Mac hine learning , lectur e 3 (Jaakkola) \nwhat would happen if we switched the label of the top right supp ort vector in Figur e 3. \nAllowing misclassiﬁed exam ples, relaxation \nLabeling errors are comm on in many practical problems and we should try to mitiga te \ntheir eﬀect. We typically do not know whether examples are diﬃcult to classify because \nof labeling errors or because they simply are not linearly separa ble (there isn’t a linea r \nclassiﬁer that can classify them correctly). In either case we have to articulat e a trade-oﬀ \nbetween misclassifying a training example and the potential beneﬁt for other examples. \nPerhaps the simplest way to permit errors in the maximum marg in linea r classiﬁer is to \nintroduce “slac k” variables for the classiﬁcatio n/mar gin constraints in the optimizati on \nproblem. In other words, we measure the degree to whic h each margin constra int is vio­\nlated and associate a cost for the violation. The costs of violating constr aints are minimized \ntogether with the norm of the parameter vector. This gives rise to a simple relaxed opti­\nmizatio n problem: \nn1 2 � \nminimize 2�θ�+ C ξt (8) \nt=1 \nsubject to yt(θT xt + θ0) ≥ 1 − ξt and ξt ≥ 0 for all t =1,...,n (9) \nwhere ξt are the slack variables. The margin constra int is violated when we have to set \nξt > 0 for some example. The penalt y for this violation is Cξt and it is traded-oﬀ with \nthe possible gain in minimizing the squar ed norm of the parameter vector or �θ�2 . If we \nincrease the penalt y C for margin viola tions then at some point all ξt = 0 and we get back \nthe maxim um margin linear separa tor (if possible). On the other hand, for small C many \nmargin constr aints can be violated. Note that the relaxed optimization problem speciﬁes \na particula r quantitative trade-oﬀ between the norm of the parameter vector and marg in \nviolations. It is reasonable to ask whether this is indeed the trade-oﬀ we want. \nLet’s try to understa nd the setup a little further. For example, what is the resulting marg in \nwhen some of the constr aints are violated? We can still take 1/�θˆ� as the geometr ic margin. \nThis is indee d the geom etric marg in based on examples for whic h ξt ∗ = 0 where ‘*’ indicates \nthe optimized value. So, is it the case that we get the maximum margin linear classiﬁer \nfor the subset of examples for whic h ξt ∗ = 0? No, we don’t . The examples that violate the \nmargin constrain ts, includin g those training examples that are actually misclassiﬁed (larg er \nviolation), do aﬀect the solut ion. In other words, the parameter vector θˆis deﬁned on the \nbasis of examples right at the margin, those that violate the constraint but not enough to \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "maxim um margin linear separa tor. Probl ems. There are problems as well, however. Even a single training example , if m", "source_title": "c8442240a87cdacbb2b90f8b53393e9d lec3", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "973bd9d8-f3e1-4358-9968-ebf9ea905e80", "text": "actually misclassiﬁed (larg er violation), do aﬀect the solut ion. In other words, the parameter vector θˆis deﬁned on the basis of examples right at the margin, those that violate the constraint but not enough to Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n6 6.867 Mac hine learning , lectur e 3 (Jaakkola) \nbe misclassiﬁed, and those that are misclassiﬁed. All of these are supp ort vectors in this \nsense. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "actually misclassiﬁed (larg er violation), do aﬀect the solut ion. In other words, the parameter vector θˆis deﬁned on t", "source_title": "c8442240a87cdacbb2b90f8b53393e9d lec3", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "70ec96d9-8c07-4721-b4d1-3989ac353f0c", "text": "6.867 Machine learning \nFinal exam \nDecem ber 3, 2004 \nYour name and MIT ID: \n(Optiona l) The grade you would give to yourself + a brief justiﬁcation.\n1\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 1\n0 1 2 3 4 5 600.511.522.533.544.55\nFigur e 1: Labeled training points for problem 1. \nConsider the labeled training points in Figur e 1, where ’x’ and ’o’ denote positive and \nnegative labels, respectiv ely. We wish to apply AdaBo ost with decision stumps to solve \nthe classiﬁc ation problem. In each boosting iterat ion, we select the stump that minimizes \nthe weighted training error, breaking ties arbitra rily. \n1. (3 points) In ﬁgure 1, draw the decision boundary corresp onding to the ﬁrst decis ion \nstump that the boosting algorithm would choose. Label this boundar y (1), and also \nindicate +/-side of the decision bounda ry. \n2. (2 points) In the same ﬁgure 1 also circle the point(s) that have the highest weight \nafter the ﬁrst boosting iteration. \n3. (2 points) What is the weighted error of the ﬁrst decis ion stump after \nthe ﬁrst boosting iteration, i.e., after the points have been reweighted? \n4. (3 points) Draw the decision boundary corresp onding to the second decision stump, \nagain in Figure 1, and label it with (2), also indicating the +/-side of the bounda ry. \n2\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "for ", "section_heading": "6.867 Machine learning ", "source_title": "57667e1d2442cc589da5051663606fc6 final f04", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 0}
{"id": "3291a569-4c28-41da-8794-3d3b8ca34151", "text": "points have been reweighted? 4. (3 points) Draw the decision boundary corresp onding to the second decision stump, again in Figure 1, and label it with (2), also indicating the +/-side of the bounda ry. 2 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n5. (3 points) Would some of the points be misclassiﬁed by the combined classiﬁer after \nthe two boosting iterati ons? Provide a brief justiﬁcatio n. (the points will be awarded \nfor the justiﬁcation, not whether your y/n answ er is correct) \nProblem 2 \n1. (2 points) Consider a linear SVM trained with n labeled points in R2 without slack \npenalties and resulting in k = 2 supp ort vectors (k<n). By adding one additiona l \nlabeled training point and retraining the SVM classiﬁe r, what is the maximum number \nof supp ort vectors in the resulting solution? \n() k\n() k +1\n() k +2\n() n +1\n2. We train two SVM classiﬁers to separa te points in R2 . The classiﬁe rs diﬀer only in \nterms of the kernel function. Classiﬁe r 1 uses the linear kernel K1(x, x�)= xT x�, and \nclassiﬁer 2 uses K2(x, x�)= p(x)p(x�), where p(x) is a 3-component Gaussian mixtur e \ndensit y, estima ted on the basis of related other problems. \n(a) (3 points) What is the VC-dimension of the second SVM classiﬁer \nthat uses kernel K2(x, x�)? \n(b) (T/F – 2 points) The second SVM classiﬁer can only separa te points \nthat are likely according to p(x) from those that have low probabilit y \nunder p(x). \n(c) (4 points) If both SVM classiﬁers achieve zero training error on n labeled \npoints, which classiﬁer would have a better genera lization guarantee? Provide a \nbrief justiﬁca tion. \n3\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "points have been reweighted? 4. (3 points) Draw the decision boundary corresp onding to the second decision stump, again", "source_title": "57667e1d2442cc589da5051663606fc6 final f04", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "b951f673-dbdc-4c78-905f-582f9eb01dce", "text": "have low probabilit y under p(x). (c) (4 points) If both SVM classiﬁers achieve zero training error on n labeled points, which classiﬁer would have a better genera lization guarantee? Provide a brief justiﬁca tion. 3 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 3\n0 2 4 6 8 10 12 14 16 18−4−202468\nx1x2\n−2 0 2 4 6 8 10−6−4−20246\nx1x2\n(a) (b) \nFigur e 2: Data sets for clustering. Points are located at integer coordinates. \n1. (4 points) First consider the data plott ed in Figure 2a, which consist of two rows of \nequally spaced points. If k-means clustering (k = 2) is initialised with the two points \nwhose coordina tes are (9, 3) and (11, 3), indicate the ﬁnal clusters obtained (after the \nalgorithm converges) on Figur e 2a. \n2. (4 points) Now consider the data in Figure 2b. We will use spectra l clustering \nto divide these points into two clusters. Our version of spectral clustering uses a \nneigh bourhood graph obtained by connecting each point to its two nearest neigh bors \n(brea king ties rando mly), and by weighting the resulting edges between points xi and \nxj by Wij = exp(−||xi − xj||). Indicate on Figure 2b the clusters that we will obtain \nfrom spectral clustering. Provide a brief justiﬁca tion. \n3. (4 points) Can the solution obtained in the previo us part for the data in Figure 2b \nalso be obtained by k-means clustering (k = 2)? Justify your answ er. \n4\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "svm", "section_heading": "have low probabilit y under p(x). (c) (4 points) If both SVM classiﬁers achieve zero training error on n labeled points,", "source_title": "57667e1d2442cc589da5051663606fc6 final f04", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "32ec8595-d851-44c4-8607-b9d277b7ee6e", "text": "a brief justiﬁca tion. 3. (4 points) Can the solution obtained in the previo us part for the data in Figure 2b also be obtained by k-means clustering (k = 2)? Justify your answ er. 4 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n�� �� �� �� �� �� �� �� \n�� �� �� �� \n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxyFigur e 3: Training sample from a mixture of two linear models \nProblem 4 \nThe data in Figure 3 comes from a mixtu re of two linear regression models with Gaussian \nnoise: \nP (y|x; θ)= p1N (y ; w10 + w11x, σ2)+ p2N (y ; w20 + w21x, σ2)1 2 \nwhere p1 + p2 = 1 and θ =(p1,p2,w10,w11,w20,w21,σ1,σ2). We hope to estimat e θ from \nsuch data via the EM algorithm. \nTo this end, let z ∈{1, 2} be the mixture index, variable indica ting which of the regression \nmodels is used to generate y given x. \n1. (6 points) Connect the random variables X, Y , and Z with directed edges so that \nthe graphi cal model on the left represen ts the mixture of linea r regression models \ndescribed above, and the one on the right represe nts a mixture-of-experts model. For \nboth models, Y denot es the output variable, X the input, and Z is the choice of the \nlinear regression model or expert. \nmixtur e of linear regressions mixtur e of experts \nX Z X Z \nY Y \n5 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "linear regression", "section_heading": "a brief justiﬁca tion. 3. (4 points) Can the solution obtained in the previo us part for the data in Figure 2b also be o", "source_title": "57667e1d2442cc589da5051663606fc6 final f04", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "83b919ed-0f7e-40ba-b9ae-24f09180a3d2", "text": "denot es the output variable, X the input, and Z is the choice of the linear regression model or expert. mixtur e of linear regressions mixtur e of experts X Z X Z Y Y 5 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nWe use a single plot to repres ent the model parameters (see the ﬁgure below). Each \nlinear regression model appears as a solid line (y = wi0 + wi1x) in between two \nparallel dotted lines at vertica l dista nce 2σi to the solid line. Thus each regression \nmodel “covers” the data that falls between the dotted lines. When w10 = w20 and \nw11 = w21 you would only see a single solid line in the ﬁgure; you may still see two \ndiﬀeren t sets of dotted lines corresp onding to diﬀeren t values of σ1 and σ2. The solid \nbar to the right represe nts p1 (and p2 =1 − p1). \nFor example, if \nθ =( p1,p2,w10,w11,w20,w21,σ1,σ2) \n=( 0.35, 0.65, 0.5, 0, 0.85, −0.7, 0.05, 0.15) \nthe plot is \n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np1\n2. (6 points) We are now ready to estima te the parameter s θ via EM. There are, \nhowever, many ways to initialize the parameters for the algorithm. \nOn the next page you are asked to connect 3 diﬀeren t initializat ions (left column) with \nthe paramet ers that would result after one EM iterat ion (right column). Diﬀe rent \ninitia lizations may lead to the same set of parameters. Your answ er should consist of \n3 arrows, one from each initializat ion. \n6\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nInitialization Next Iteration\n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np1\n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np1\n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np1\n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np1\n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np1\n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np1\n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np1\n7\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "linear regression", "section_heading": "denot es the output variable, X the input, and Z is the choice of the linear regression model or expert. mixtur e of lin", "source_title": "57667e1d2442cc589da5051663606fc6 final f04", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "c877f561-7152-4b00-adc3-930bb4df733c", "text": "0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2 xy 00.10.20.30.40.50.60.70.80.91 p1 0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2 xy 00.10.20.30.40.50.60.70.80.91 p1 0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2 xy 00.10.20.30.40.50.60.70.80.91 p1 0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2 xy 00.10.20.30.40.50.60.70.80.91 p1 7 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 5 \nAssume that the following sequences are very long and the pattern highligh ted with spaces \nis repeated: \nSequence 1: 100 100 100 100 ... 100 \nSequence 2: 1 100 100 100 ... 100 \n1. (4 points) If we model each sequence with a diﬀerent ﬁrst-or der HMM , what is the \nnumber of hidden states that a reasona ble model selectio n metho d would report? \nHMM for Sequence 1 HMM for Sequence 2 \nNo. of hidden states \n2. (2 points) The following Bayesian network depic ts a sequence of 5 observ ations from \nan HMM, where s1,s2,s3,s4,s5 is the hidden state sequence. \ns1 s2 s3 s4 s5 \nx1 x2 x4 x5 \nAre x1 and x5 indep endent given x3? Brieﬂy justify your answ er. \n3. (3 points) Does the order of Markov dependenc ies in the observ ed sequence always \ndetermine the number of hidden states of the HMM that generated the sequenc e? \nProvide a brief justiﬁcation. \n8\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nx3\nProblem 6 \nWe wish to develop a graphica l model for the following transportation problem. A transport \ncompa ny is trying to choose between two alternativ e routes for commuting between Boston \nand New York. In an experimen t, two identical busses leave Bosto n at the same but \notherwise rando m time, TB . The busses take diﬀeren t routes, arriving at their (comm on) \ndestinatio n at times TN1 and TN2. \nTransit time for each route depends on the congestion along the route, and the two con­\ngestio ns are unrela ted. Let us represent the rando m delays introduced along the routes by \nvariables C1 and C2. Finally , let F represen t the identity of the bus which reaches New \nYork ﬁrst. We view F as a random variable that takes values 1 or 2. \n1. (6 points) Complete the following directed graph (Bayesian network) with edges \nso that it captures the relationships between the variables in this transp ortatio n \nproblem. \n9\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2 xy 00.10.20.30.40.50.60.70.80.91 p1 0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2 xy 0", "source_title": "57667e1d2442cc589da5051663606fc6 final f04", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "c03e8934-419c-436d-b212-3f6dc4a9c9b1", "text": "random variable that takes values 1 or 2. 1. (6 points) Complete the following directed graph (Bayesian network) with edges so that it captures the relationships between the variables in this transp ortatio n problem. 9 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2. (3 points) Consider the following directed graph as a possible represen tation of the \nindep endences between the variables TN1, TN2, and F only: \nWhic h of the following factor izatio ns of the joint are consisten t with the graph? \nP (TN1)P (TN2)P (F |TN1,TN2) \nP (TN1)P (TN2)P (F |TN1) \nP (TN1)P (TN2)P (F ) \n10\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nAddi tional set of ﬁgures\n0 1 2 3 4 5 600.511.522.533.544.55\n0 2 4 6 8 10 12 14 16 18−4−202468\nx1x2\n−2 0 2 4 6 8 10−6−4−20246\nx1x2\n(a) \nmixtu re of linear regressions \n�� �� \n�� �� X Z (b) \nmixtur e of experts \n�� �� \n�� �� X Z \n�� �� Y \n�� �� Y \n11 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nInitialization Next Iteration\n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np1\n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np1\n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np1\n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np1\n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np1\n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np1\n0 0.2 0.4 0.6 0.8 1−0.200.20.40.60.811.2\nxy\n00.10.20.30.40.50.60.70.80.91\np1\n12\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\ns5 s1 s2 \nx1 x2 x3 x4 x5 s3 s4 \n13\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "linear regression", "section_heading": "random variable that takes values 1 or 2. 1. (6 points) Complete the following directed graph (Bayesian network) with ed", "source_title": "57667e1d2442cc589da5051663606fc6 final f04", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "2aab129c-e4dc-46ce-9967-1d6c90e8b3a2", "text": "� 6.867 Mac hine learning , lectur e 9 (Jaakkola) 1 \nLecture topics: \n• Kernel optimizatio n \n• Model (kernel) selectio n \nKernel optimization \nWhether we are interes ted in (linea r) classiﬁcat ion or regression we are faced with the \nproblem of selecting an appro priate kernel function. A step in this direction migh t be to \ntailor a particular kernel a bit better to the available data. We could, for example, introduce \naddit ional parameters in the kernel and optimize those parameters so as to impro ve the \nperforma nce. These parameters could be simple as the β parameter in the radia l basis \nkernel, weight each dimension of the input vectors, or more ﬂexible as ﬁnding the best \nconvex combinatio n of basic (ﬁxed) kernels . Key to such an approach is the measure we \nwould optimize . Ideally , this measure would be the genera lization error but we obviously \nhave to settle for a surro gate measure. The surro gate measure could be cross-v alidatio n or \nan alternativ e criterio n related to the generalizat ion error such as the geometric margin. \nWe need additiona l safeguar ds if we are to use the geometric margin. For example, simply \nmultiply ing the feature vectors by two would double the geometric margin. So, witho ut \nnormalizat ion, the margin cannot serve as an appro priate criterio n. The simples t way to \nnormalize the featur e vector s prior to estima tion would be to requi re that �φ(x)� =1 \nfor all x regardless of the kernel. This normalization can be done directly in the kernel \nrepresen tation as follows \nK(x, x�)K˜(x, x�)= � (1) \nK(x, x)K(x�, x�) \nAnot her appro ach to optimizing the kernel functio n is kernel alignment. In other words, \nwe would adjust the kernel parameters so as to make it, or its Gram matr ix, more towards \nan ideal target kernel. For example, in a classiﬁcat ion setting, we could use \nKij ∗ = yiyj (2) \nas the Gram matrix of the target kernel. One argument for selecting this as the target is \nthat if we set αj =1/n then \nn\nαj yj Kij ∗ = yi (3) \nj=1 \n Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "convex", "section_heading": "� 6.867 Mac hine learning , lectur e 9 (Jaakkola) 1 ", "source_title": "2051efc0159bf145f2050469b7589fc5 lec9", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "7d05d01f-2d3c-48f8-8c98-005b0c751fc7", "text": "= yiyj (2) as the Gram matrix of the target kernel. One argument for selecting this as the target is that if we set αj =1/n then n αj yj Kij ∗ = yi (3) j=1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� 2 6.867 Mac hine learning , lectur e 9 (Jaakkola) \nand all the training examples are classiﬁed correctly with the same margin. (You could \nargue that another target should be used instead). Let’s see how we can align the kernel \ntowards this target. Supp ose our parameterized kernel is a convex combinatio n of kernels \n(e.g., constru cted on the basis of diﬀeren t sources of input data) \nm\nwhere θi ≥ 0 and θi = 1. These are the parameters we can adjust. We can now set θ K(x, x�; θ) = θiKi(x, x�) (4) \ni=1 \n� m \ni=1 \nso as to make the Gram matrix of this kernel, Kij (θ), more similar to the Gram matrix of \nthe target kernel, Kij ∗ . To do this we view the Gram matr ices as vectors and deﬁne their \ninner product in the usual way \nn\n�K∗,Kθ� = Kij ∗ Kij (θ) (5) \ni,j=1 \nThe parameters θ can be now set so as to maximi ze the cosine of the angle between the \nGram matrices: \n� �K∗,Kθ� (6) \n�K∗,K∗��Kθ,Kθ� \nModel (kernel) selection \nOptimizing the kernel in a parameterized form involved little consideratio n for the com­\nplexit y of the set of classiﬁers we are ﬁtting to ﬁnite data. It therefor e did not address \nthe problem of over-ﬁtting or ﬁtting too complex a model to too few data points. In many \ncases it makes sense to explicitly cast the problem of selecting a kernel as a model selection \nproblem. \nBy choosing a kernel we specify the feature vectors on the basis of which linear predictions \nare made. Each model1 (class) refers to a set of linear functio ns (classiﬁers) based on \nthe chosen feature represe ntatio n. In many cases the models are nested in the sense that \nthe more “comple x” model contains the “simpler” one. Consider, for example, solving a \nclassiﬁcatio n problem with either \nK1(x, x�) = (1+ x T x�) or (7) \nK2(x, x�) = (1+ x T x�)2 (8) \n1In statistics , a model is a family /set of distr ibutions or a family/se t of linear separators. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "convex", "section_heading": "= yiyj (2) as the Gram matrix of the target kernel. One argument for selecting this as the target is that if we set αj =", "source_title": "2051efc0159bf145f2050469b7589fc5 lec9", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "150ed3ba-127f-4dfb-b48d-8c8b39e33fad", "text": "K1(x, x�) = (1+ x T x�) or (7) K2(x, x�) = (1+ x T x�)2 (8) 1In statistics , a model is a family /set of distr ibutions or a family/se t of linear separators. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � � 3 6.867 Mac hine learning , lectur e 9 (Jaakkola) \nClassiﬁers making use of the quadratic polyno mial kernel can in principle repro duce the \nclassiﬁers based on the linear kernel. As a model, i.e., as a set of linear classiﬁers based on \nthe quadratic kernel, it theref ore contains the simpler linear one. We can state this a bit \nmore formally in terms of discriminan t functions. For example, based on the linear kernel \nK1, our discriminan t functio ns are of the form \nf1(x; θ,θ0)= θT φ(1)(x)+ θ0 (9) \nwhere φ(1)(x) is the feature represen tation corresp onding to K1 such that K1(x, x�)= \nφ(1)(x)T φ(1)(x�). By varying the parameters θ and θ0 we can generat e the set of possible \ndiscriminan t functio ns corresponding to this kernel: \nF1 = {f1(·; θ,θ0): θ ∈Rd1 ,θ0 ∈ R} (10) \nF2 is deﬁne d analo gously for the quadratic kernel. The fact that the two models are nested \nmeans that F1 ⊆F2. For purposes of classiﬁcatio n, we wouldn’t actua lly have to assert \nthat the families of discrimina nt functio ns are nested, only that the discriminan t functions \nin F2 can produce the signs of those in F1. \nThe forma l problem for us to solve is then to select a kernel Ki from a set of possible kernels \nK1,K2,..., where the models associated with the kernels are nested F1 ⊆F2 ⊆ .... This \nis a model selection problem in a standard nested form. \nFrom here on we will be referring to discriminan t functions rather than kernels so as to \nemphasize the point that the discussion applies to other types of classiﬁers as well. \nModel selection preliminaries \nBefore getting into speciﬁc selection criteria let’s understa nd a bit better what exactly we \nare doing here. Recall that our goal is to accurately classify new test examples. Model \nselection is intende d to facilita te this process. In other words, we switch from one model \n(kernel) to another so as to generalize better. The model we select will deﬁne how we \nwill respond to any training data, i.e., which classiﬁer we choose to make predic tions on \nnew examples. Model selectio n canno t therefor e be decoupled from how we ﬁnd the “best \nﬁtting ” classiﬁer from a given model. After all, it is that best ﬁtting classiﬁer that will \ndetermine how well we generalize. \nLet Sn = {(x1,y1),..., (xn,yn)} denot e a training set of n examples and labels. If we chose \nmodel Fi then we would ﬁnd the best ﬁtting discrimina nt functio n fˆ i ∈Fi by minimizing \nn\nJ(θ,θ0) = Loss yt,f(xt; θ,θ0)+ λn�θ�2 (11) \nt=1 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "K1(x, x�) = (1+ x T x�) or (7) K2(x, x�) = (1+ x T x�)2 (8) 1In statistics , a model is a family /set of distr ibutions ", "source_title": "2051efc0159bf145f2050469b7589fc5 lec9", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "b3a5a766-7a91-4471-8f1e-926c681f7687", "text": "set of n examples and labels. If we chose model Fi then we would ﬁnd the best ﬁtting discrimina nt functio n fˆ i ∈Fi by minimizing n J(θ,θ0) = Loss yt,f(xt; θ,θ0)+ λn�θ�2 (11) t=1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n4 6.867 Mac hine learning , lectur e 9 (Jaakkola) \nwhere the loss could be the hinge loss (SVM), logistic, or other. The regularizat ion param­\neter λn would in general depend on the number of training examples. We are interested in \nhow the classiﬁer fˆ i(x)= f(x; θ, ˆθˆ0) resulting from our estima tion procedure generalizes to \nnew examples . \nEach parameter setting (θ,θ0), i.e., each discrimina nt function in our set, has an associated \nexpected loss or risk \n�� �� \nR(θ,θ0)= E(x,y)∼P Loss∗ y,f(x; θ,θ0) (12) \nwhere the new test example and label, (x,y), is sampled from an underly ing distribut ion \nP which is typically unknown to us. This is the generalizatio n error we would like to \nminimize. Note that we have used Loss∗(·, ) above rather than the loss used in training. ·\nThes e need not be the same and often they are not. For example, our goal may be to \nminimize classiﬁcat ion error so that Loss∗(y,f(x)) = 1 − δ(y, sign(f (x))) , i.e., the zero-one \nloss. We could still estima te the SVM classiﬁer from the training set in the usual way, \noptimizing the hinge loss. The hinge loss can be viewed as a convex surrogate for the \nzero-one loss and it behaves much better in terms of the resultin g optimiza tion problem we \nhave to solve during training (quadratic rather than integer programming problem). \nThe quan tity of interest to us is the genera lization error R(θ, ˆθˆ0), or R(fˆ i) for short, corre­\nsponding to the classiﬁer or discriminan t function we would choose from Fi in response to \nthe training data Sn. Ideally , we would then select the model Fi that leads to the smallest \ngenera lization error, minimizing \n�� �� \nR(fˆ i)= E(x,y)∼P Loss∗ y, fˆ i(x) (13) \nNote that the risk R(fˆ i) is still a random variable as fˆ i depends on the training data that \nwe assume was also sampled from the same underly ing distributio n P . If the training data \nwere sample d from a diﬀeren t distribution , how could we expect to generalize? Actually , \nthe only thing we really need is that the relatio nship between the labels and example s is \nthe same for the training and test samples , along with some guarantee that the trainin g \nexamples cover the areas of input space that we will be tested on. In theoretica l analysis \nit is nevertheless much more convenien t to assume that the distributio ns are the same. \nNow, we clearly do not have access to the unde rlying distribut ion and therefore canno t \nevaluate R(fˆ i). In fact, the whole model selection problem would go away if had access to \nthe under lying distribution P (x,y). To classify new instances , we would simply forget about \nthe training set and use the minim um probabilit y of error classiﬁer ˆy(x) = arg maxy P (y|x) \n(see the appendix). No classiﬁer could lead to a lower probabilit y of error. Our task is \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "convex", "section_heading": "set of n examples and labels. If we chose model Fi then we would ﬁnd the best ﬁtting discrimina nt functio n fˆ i ∈Fi by", "source_title": "2051efc0159bf145f2050469b7589fc5 lec9", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "d71f5516-7f32-4c44-be7e-5acbcc376950", "text": "the training set and use the minim um probabilit y of error classiﬁer ˆy(x) = arg maxy P (y|x) (see the appendix). No classiﬁer could lead to a lower probabilit y of error. Our task is Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n5 6.867 Mac hine learning , lectur e 9 (Jaakkola) \nmuch more diﬃcult since we have to select fˆ i ∈Fi as well as the model Fi on the basis of \nthe trainin g data alone, without acces s to P . \nLet’s try to under stand ﬁrst intuitiv ely what the model selection criteri on has to be able \nto do. To make this a bit more concrete, consider just choosing between F1 and F2 \ncorrespondin g to linear or quadr atic feature vectors. Since the models are nested, F1 ⊆F2, \nwe can always achieve lower classiﬁc ation error on the training set by adopting F2. This \nis regar dless of whether the true underlying model is linear . So, by choosing F2, we may \nbe over-ﬁtting. If the true relationship between the labels and examples were linear (the \nminim um probabi lity of error classiﬁer is linea r), then the quadr atic nature of the resulting \ndecis ion bounda ry would simply be due to noise and couldn’t generalize very well. So we \nshould be able to see an increasing gap between the training and test error s as a function \nof the model complexit y as in Figure 1 below. Clearly , all thing s being equal, we should \nselect F1 as it is a simple r model. The real questio n is how to balan ce the “complexit y” \nof the model, some measure of size or power of Fi, against their ﬁt to the training data. \nThere are a number of answers to this question depending on your perspective. We will \nbrieﬂy go over a few possibilities and return to them later on. \n0 10 20 30 40 5000.10.20.30.40.50.60.70.80.91\nmeasure of complexitytrain/test errors\ntraining error test error \nFigur e 1: Training and test error s as a functio n of model order (e.g., degree of polynom ial \nkernel). \nModel selection criteria: structural risk minimization \nOne appro ach to model selectio n is to try to directly relate the (expected) risk R(fˆ i) \n�� �� \nR(fˆ i)= E(x,y)∼P Loss∗ y, fˆ i(x) (14) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "the training set and use the minim um probabilit y of error classiﬁer ˆy(x) = arg maxy P (y|x) (see the appendix). No cl", "source_title": "2051efc0159bf145f2050469b7589fc5 lec9", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "c9c73873-e8a5-4836-88d4-5cd09bed6c21", "text": "ial kernel). Model selection criteria: structural risk minimization One appro ach to model selectio n is to try to directly relate the (expected) risk R(fˆ i) �� �� R(fˆ i)= E(x,y)∼P Loss∗ y, fˆ i(x) (14) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � � 6 6.867 Mac hine learning , lectur e 9 (Jaakkola) \nthat we would like to have and the empiric al risk Rn(fˆ i) \nn\nRn(fˆ i) = 1 Loss∗ yt,fˆ i(xt) (15) n t=1 \nthat we can compute. If we can do this, then we have a partial access to R(fˆ i) through its \nempirical counterpart Rn(fˆ i). Note that the empirical risk here is computed on the basis \nof the available training set Sn = {(x1,y1),..., (xn,yn)} and Loss∗() rather than say the ·, ·\nhinge loss. For our purp oses here, fˆ i ∈Fi could be any estimate deriv ed from the training \nset that approxima tely tries to minimiz ing the empirical risk. \nWe are interested in quan tifying how much R(fˆ i) can deviat e from Rn(fˆ i). The larger the \ndeviat ion the less represen tative the training error is about the generaliza tion error. This \nhapp ens with more complex models Fi. Indeed, we aim to show that \nR(fˆ i) ≤ Rn(fˆ i)+ C(n, Fi,δ) (16) \nwhere the complexity penalty C(n, Fi) only depends on the model Fi, the number of training \ninstances, and a parameter δ. The peanalty does not depend on the actual training data. \nWe will discuss the parameter δ below in more detail. For now, it suﬃces to say that 1 − δ \nspeciﬁes the probability that the bound holds. We can only give a proba bilistic guarantee \nin this sense since the empirica l risk (training error) is a random quan tity that depends on \nthe speciﬁc insta ntiation of the data. \nFor nested models, F1 ⊆F2 ⊆ ..., the penalt y is necessarily an increasing function of i, \nthe model order (e.g., the degree of polynomia l kernel). Moreover, the penalt y should go \ndown as a function n. In other words, the more data we have, the more complex models \nwe expect to be able to ﬁt and still have the training error close to the generaliza tion error. \nThe type of result in Eq.(16 ) gives us an upper bound guarante e of gener alization error. \nWe can then select the model with the best guarantee, i.e., the one with the lowest bound. \nFigur e 2 shows how we would expect the upper bound to behave as a function of increa singly \ncomplex models in our nested “hierar chy” of models. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "empirical risk", "section_heading": "ial kernel). Model selection criteria: structural risk minimization One appro ach to model selectio n is to try to direc", "source_title": "2051efc0159bf145f2050469b7589fc5 lec9", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "bbc15f96-fcf0-479c-adb1-000bae446d72", "text": "guarantee, i.e., the one with the lowest bound. Figur e 2 shows how we would expect the upper bound to behave as a function of increa singly complex models in our nested “hierar chy” of models. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n7 6.867 Mac hine learning , lectur e 9 (Jaakkola) \n0 10 20 30 40 5000.10.20.30.40.50.60.70.80.91\nVC dimensionBound \nComplexity penalty \nTraining error \nFigur e 2: Bound on the generalizatio n error as a functio n of model order (e.g., degree of \npolyno mial kernel). \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "kernel", "section_heading": "guarantee, i.e., the one with the lowest bound. Figur e 2 shows how we would expect the upper bound to behave as a funct", "source_title": "2051efc0159bf145f2050469b7589fc5 lec9", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "f93d7ee1-11e8-4e8e-9e5b-d28b2e26fe7a", "text": "� � 6.867 Mac hine learning , lectur e 5 (Jaakkola) 1 \nLinear regression, active learning \nWe arrived at the logistic regression model when trying to explicitly model the uncertainty \nabout the labels in a linear classiﬁer. The same genera l modeling approa ch permits us to \nuse linear predictio ns in various other contexts as well. The simples t of them is regress ion \nwhere the goal is to predict a continuous response yt ∈R to each example vector. Here \ntoo focusing on linear predictions won’t be inheren tly limiting as linea r predictions can be \neasily extended (next lecture) . \nSo, how should we model continuous responses? The linear function of the input alrea dy \nproduces a “mean predictio n” or θT x + θ0. By treating this as a mean predic tion more \nforma lly, we are stating that the expected value of the response variable, conditio ned on \nx, is θT x + θ0. More succinctly , we say that E{y|x} = θT x + θ0. It rema ins to associate \na distributio n over the responses around such mean predic tion. The simples t symmetric \ndistributio n is the norma l (Gaussian) distr ibution. In other words, we say that the responses \ny follow the normal pdf \n1 1 N(y; µ,σ2)= √\n2πσ2 exp −2σ2 (y − µ)2 (1) \nwhere µ = θT x + θ0. Our response model is therefore deﬁned as \nP (y|x,θ,θ0)= N(y; θT x + θ0,σ2) (2) \nSo, when the input is 1-dimensiona l, we predict a mean response that is a line in (x,y) \nspace, and assume that noise in y is normally distributed with zero mean and variance σ2 . \nNote that the noise variance σ2 in the model does not depend on the input x. Moreover, we \nonly model variation in the y-direc tion while expecting to know x with perfect precision. \nTaking into accoun t the eﬀect of potential noise in x on the responses y would tie parameters \nθ and θ0 to the noise variance σ2, potentially in an input depende nt mann er. The speciﬁcs \nof this coupling depend on the form of noise added to x. We will discuss this in a bit more \ndetail later on. \nWe can also write the linear regression model in another way to explica te how exactly the \naddit ive noise appears in the responses: \ny = θT x + θ0 + � (3) \nwhere � ∼ N(0,σ2) (mea ning that noise � is distributed norma lly with mean zero and \nvariance σ2). Clearly for this model E{y|x} = θT x + θ0 since � has zero mean. Moreo ver, \nadding Gaussian noise to a determinis tic prediction θT x + θ0 makes y normally distributed \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "� � 6.867 Mac hine learning , lectur e 5 (Jaakkola) 1 ", "source_title": "9e7f8fb89d65d7572980907fd6d12315 lec5", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "ea3a7360-c59a-427d-9d87-762d4bc15d78", "text": "and variance σ2). Clearly for this model E{y|x} = θT x + θ0 since � has zero mean. Moreo ver, adding Gaussian noise to a determinis tic prediction θT x + θ0 makes y normally distributed Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� 6.867 Mac hine learning , lectur e 5 (Jaakkola) 2 \nwith mean θT x + θ0 and variance σ2, as before. So, in particular, for the training input s \nx1,..., xn and outputs y1,...,yn, the model relat ing them is \nyt = θT xt + θ0 + �t,t =1,...,n (4) \nwhere et ∼ N(0,σ2) and ei is indep enden t of ej for any i =�i. \nRegardless of how we choose the write the model (both forms are useful) we can ﬁnd the \nparameter estima tes by maximizing the conditio nal likeliho od. Simila rly to the logistic \nregression case, the condit ional likelihood is written as \nn � �� 1 1 L(θ,θ0,σ2)= √\n2πσ2 exp −2σ2 (yt − θT xt − θ0)2 (5) \nt=1 \nNote that σ2 is also a parameter we have to estimat e. It accounts for errors not captured \nby the linear model. In terms of the log-likeliho od, we try to maximize \nl(θ, θ0, σ2) = n� \nt=1 log � 1 √\n2πσ2 exp � \n− 1 \n2σ2 (yt − θT xt − θ0)2 �� \n(6) \n= n� � \n− 1 \n2 log(2π) − 1 2 log σ\n2 − 1 \n2σ2 (yt − θT xt − θ0)2 � \n(7) \nt=1 \n= const. − n \n2 log σ2 − 1 \n2σ2 n� \n(yt − θT xt − θ0)2 (8) \nt=1 \nwhere ’const.’ absorbs terms that do not depend on the parameters. Now, the problem of \nestimating the parameters θ and θ0 is nicely decoupled from estimating σ2 . In other words, \nwe can ﬁnd the maximizing θˆand θˆ0 by simply minimizing the mean squared error \nn\n(yt − θT xt − θ0)2 (9) \nt=1 \nIt is perhaps easiest to write the solutio n based on a bit of matrix calculat ion. Let X be \na matrix whose rows, indexed by training examples, are given by [xtT , 1] (xt turned into a \nrow vector and 1 added at the end). In terms of this matrix, the minimizatio n problem \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "and variance σ2). Clearly for this model E{y|x} = θT x + θ0 since � has zero mean. Moreo ver, adding Gaussian noise to a", "source_title": "9e7f8fb89d65d7572980907fd6d12315 lec5", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "9556235b-076d-4e42-abfd-c492ccb360df", "text": "be a matrix whose rows, indexed by training examples, are given by [xtT , 1] (xt turned into a row vector and 1 added at the end). In terms of this matrix, the minimizatio n problem Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � � � � �\n�\n� \n������\n�\n�\n������\n����\n� \n�\n�\n����\n�\n�\n�\n�\n�\n� 6.867 Mac hine learning , lectur e 5 (Jaakkola) 3 \nbecomes \nn n �T � ��2 ��2 θ\n θ\n xt T \nt , 1 (10)\n yt − θ0 1= yt − x θ0 t=1 t=1 \n2⎤⎡⎤⎡ y1 x1 T , 1 θ\n⎣\n··· − ··· θ0 yn xnT , 1⎣\n⎦\n⎦\n (11)\n =\n2 \ny − X\nθ\nθ0 (12)\n =\n�T �T \n= y T y − 2 θ XT y + θ XT X θ (13) θ0 θ0 θ0 \nwhere y =[y1,...,yn]T is a vector of training responses. Solving it yields \nθˆ\nθˆ0 =(XT X)−1XT y (14) \nNote that the optimal parameter values are linea r functions of the observ ed responses y. We \nwill make use of this prop erty later on. The dependence on the training inputs x1,..., xn \n(or the matrix X) is non-linea r, however. \nThe noise variance can be subsequen tly set to account for the remaining predictio n errors. \nIndee d, the the maximizing value of σ2 is given by \nσˆ2 1\nn\n= n t=1 (yt − θˆT xt − θˆ0)2 (15)\nwhich is the average squared prediction error. Note that we cannot compute ˆσ2 before \nknowing how well the linear model explains the responses. \nBias and variance of the parameter estimates \nWe can make use of the closed form parameter estima tes in Eq.(1 4) to analyze how good \nthese estimates are. For this purpose let’s make the strong assumptio n that the actual \nrelat ion between x and y follows a linear model of the same type that we are estimat ing \n(we just don’t know the correct parameter values θ∗, θ0∗, and σ∗2). We can therefor e describ e \nthe observ ed responses yt as \nyt = θ∗T xt + θ0 ∗ + �t,t =1,...,n (16) \n \n \n Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "be a matrix whose rows, indexed by training examples, are given by [xtT , 1] (xt turned into a row vector and 1 added at", "source_title": "9e7f8fb89d65d7572980907fd6d12315 lec5", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "93c02196-5279-4d2c-80b2-9686202ed88a", "text": "ing (we just don’t know the correct parameter values θ∗, θ0∗, and σ∗2). We can therefor e describ e the observ ed responses yt as yt = θ∗T xt + θ0 ∗ + �t,t =1,...,n (16) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � \n� � � � \n� � \n� � \n� � � � � � 6.867 Mac hine learning , lectur e 5 (Jaakkola) 4 \nwhere �t ∼ N(0,σ∗2). In a matrix form \nθ∗ \ny = X + e (17) θ0 ∗ \nwhere e =[�1,...,�n]T , E{e} = 0 and E{eeT } = σ∗2 I. The noise vector e is also indep en­\ndent of the input s or X. Pluggin g this form of responses into Eq.(14) we get \nθˆ θ∗ \nθˆ0 =(XT X)−1XT (X θ0 ∗ + e) (18) \n=(XT X)−1XT X θ∗ \n+(XT X)−1XT e (19) θ0 ∗ \n= θ∗ \n+(XT X)−1XT e (20) θ0 ∗ \nIn other words, our parameter estimat es can be decomp osed into the sum of correct under­\nlying parameters and estima tes based on noise alone (i.e., based on e). Thus, on average \nwith ﬁxed input s \nθˆ θ∗ θ∗ \nE{ θˆ0 |X} = θ0 ∗ +(XT X)−1XT E{e|X} = θ0 ∗ (21) \nOur parameter estima tes are therefore unbiase d or correct on average when averaging is \nover possible training sets we could generate. The averaging here is condit ioned on the \nspeciﬁc train ing inputs. \nUsing Eq.(2 0) and Eq.(2 1) we can also evaluat e the conditi onal co-variance of the parameter \nestimates where the expectation is again over the noise in the outputs: \nCov{ � ˆθ \nˆθ0 � \n|X} = E ��� ˆθ \nˆθ0 � \n− � θ∗ \nθ∗ \n0 �� �� ˆθ \nˆθ0 � \n− � θ∗ \nθ∗ \n0 ��T \n|X � \n(22) \n= \n= \n= \n= E �� \n(XT X)−1XT e � � \n(XT X)−1XT e �T |X � \nE � \n(XT X)−1XT ee T X(XT X)−1|X � \n(XT X)−1XT E{ee T |X} X(XT X)−1 \n(XT X)−1XT (σ∗2 I) X(XT X)−1 (23) \n(24) \n(25) \n(26) \n= σ∗2(XT X)−1XT X(XT X)−1 (27) \n= σ∗2(XT X)−1 (28) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "ing (we just don’t know the correct parameter values θ∗, θ0∗, and σ∗2). We can therefor e describ e the observ ed respon", "source_title": "9e7f8fb89d65d7572980907fd6d12315 lec5", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "ea1cd781-d391-4a8e-ad9d-049b2a301405", "text": "E � (XT X)−1XT ee T X(XT X)−1|X � (XT X)−1XT E{ee T |X} X(XT X)−1 (XT X)−1XT (σ∗2 I) X(XT X)−1 (23) (24) (25) (26) = σ∗2(XT X)−1XT X(XT X)−1 (27) = σ∗2(XT X)−1 (28) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � � � \n� � \n� � � � \n� � � � \n� � � � \n� � � � \n� � �� 6.867 Mac hine learning , lectur e 5 (Jaakkola) 5 \nSo the way in which the parameters vary in response to noise in the outputs is a function \nof the inputs or X. We will use this prop erty in the next section to select inputs so as to \nimpro ve the quality of the parameter estima tes or to reduc e the variance of predictio ns. \nBased on the bias and variance calcula tions we can evaluat e the mean squared error of \nthe parameter estimates. To this end, we use the fact that the expectation of the squared \nnorm of any vector valued rando m variable can be decomposed into a bias and variance \ncomp onen ts as follows: \nE �z − z∗�2 = E �z − E{z} + E{z}− z∗�2 (29) \n= E �z − E{z}�2 + 2(z − E{z})T (E{z}− z∗)+ �E{z}− z∗�2 (30) \n= E �z − E{z}�2 +2E (z − E{z})T (E{z}− z∗)+ �E{z}− z∗�2 \nvariance � �� � bias2 � ����� \n= E �z − E{z}�2 + �E{z}− z∗�2 (31) \nwhere we have assumed that z∗ is ﬁxed. Make sure you unde rstand how this decomposition \nis deriv ed. We will further elaborate the variance part to better use the result in our context: \nE �z − E{z}�2 = E (z − E{z})T (z − E{z}) (32) \n= E Tr (z − E{z})T (z − E{z}) (33) \n= E Tr (z − E{z})(z − E{z})T (34) \n= Tr E (z − E{z})(z − E{z})T (35) \n= Tr [Cov{z}] (36) \nwhere Tr[] is the matr ix trace, the sum of its diagonal components, and therefore a linea r ·\noperation (exchangeable with the expectation) . We have also used the fact that Tr[aT b]= \nTr[abT ] for any vectors a and b. \n \n Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "E � (XT X)−1XT ee T X(XT X)−1|X � (XT X)−1XT E{ee T |X} X(XT X)−1 (XT X)−1XT (σ∗2 I) X(XT X)−1 (23) (24) (25) (26) = σ∗2", "source_title": "9e7f8fb89d65d7572980907fd6d12315 lec5", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "e4780cdf-e395-4632-9b32-76998849499d", "text": "trace, the sum of its diagonal components, and therefore a linea r · operation (exchangeable with the expectation) . We have also used the fact that Tr[aT b]= Tr[abT ] for any vectors a and b. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � \n� \n�� � � 6.867 Mac hine learning , lectur e 5 (Jaakkola) 6 \nNow, adapting the result to our setting, we get \nvariance bias2 =0 \n��� �� ��2 � �� ���� � ��� �� �� ���� � ��� \n2 \nE ��� \nθˆθˆ\n0 − θ\nθ∗\n∗ ��� \n|X = Tr Cov θˆθˆ\n0 |X + ��� \nEθˆθˆ\n0 |X − θ\nθ∗\n∗ ��� \n0 0 \n= σ∗2 Tr (XT X)−1 (37) \nLet’s understa nd this result a bit further. How does it depend on n, the number of training \nexamples? In other words, how quickly does the mean squar ed error decrease as the number \nof training examples increases, assuming the input examples x are samp led indep enden tly \nfrom some underly ing distributio n P (x)? To answer this let’s start by analyzing what \nhapp ens to the matrix XT X: \nn�� \nXT X = xt [x T\nt , 1] (38) 1 \nt=1 \nn�� 1 � xt T = n [xt , 1] (39) · n 1 \nt=1 \n≈ n · Ex∼P x [x T , 1] = n · C (40) 1 \nwhere for large n the average will be close to the corresponding expected value. For large \nn the mean squared error of the parameter estima tes will therefore be close to \nσ∗2 \nTr[C−1] (41) n · \nThe variance of simply averaging the (noise in the) outputs would behave as σ∗2/n. Since \nwe are estima ting d + 1 parameters where d is the input dimension, this dependence would \nhave to be in Tr[C−1]. Indeed it is. This term, a trace of a (d + 1) × (d + 1) matrix C−1 , \nis directly proportional to d + 1. \nPenalized log-likelihood and Ridge regression \nWhen the number of training examples is small, i.e., not too much larger than the number \nof parameters (dimension of the inputs), it is often beneﬁcial to regular ize the parameter \nestimates. We will deriv e the form of regula rizatio n here by assigning a prior distributio n \nover the parameters P (θ,θ0). The purpose of the prior is to prefer small parameter values \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "trace, the sum of its diagonal components, and therefore a linea r · operation (exchangeable with the expectation) . We ", "source_title": "9e7f8fb89d65d7572980907fd6d12315 lec5", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "78f16bb1-dc7b-48ab-9554-33ea12164b5b", "text": "the parameter estimates. We will deriv e the form of regula rizatio n here by assigning a prior distributio n over the parameters P (θ,θ0). The purpose of the prior is to prefer small parameter values Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � 6.867 Mac hine learning , lectur e 5 (Jaakkola) 7 \n(predict values close to zero) in the absence of data. Speciﬁcally , we will look at simple \nzero mean Gaussian distr ibutions \n� ��� dθ 0 � \nP (θ,θ0; σ�2)= N( θ0 ;0 ,σ�2I)= N(θ0;0,σ�2) N(θj ;0,σ�2) (42) \nj=1 \nwhere the variance parameter σ�2 in the prior distr ibution speciﬁes how strongly we wish \nto bias the parameters towards zero. \nBy combining the log-lik eliho od criterion with the prior we obtain a penalize d log-likeliho od \nfunction (penalized by the prior): \nn�� ��� 1 1 l�(θ,θ0,σ2) = log √\n2πσ2 exp −2σ2 (yt − θT xt − θ0)2 + log P (θ,θ0; σ�2) (43) \nt=1\nn\nn 1 � \n= const. − 2 log σ2 − 2σ2 (yt − θT xt − θ0)2 \nt=1 \nd1 � d +1 −2σ�2 (θ02 + θj 2) − 2 log σ�2 (44) \nj=1 \nIt is convenient to tie the prior variance σ�2 to the noise variance σ2 accor ding to σ�2 = σ2/λ. \nThis has the eﬀect that if the noise variance σ2 is large, we penalize the parameters very \nlittle (permit large deviat ions from zero by assuming a large σ�2). On the other hand, if \nthe noise variance is small, we could be over-ﬁtting the linear model. This happens, for \nexample, when the number of training example s is small. In this case most of the responses \ncan be explained directly by the linear model making the noise variance very small. In such \ncases our penalty for the parameters will be larger as well (prior variance is smaller). \nIncorp orating this parameter tie into the penali zed log-likelihood functio n gives \nnn 1 � \nl�(θ,θ0,σ2) = const. − 2 log σ2 − 2σ2 (yt − θT xt − θ0)2 \nt=1 \ndλ � d +1 −2σ2 (θ02 + θj 2) − 2 log(σ2/λ) (45) \nj=1 \nn + d +1 d +1 = const. − log σ2 + log λ (46) 2 2 \nn d1 � � \n−2σ2 (yt − θT xt − θ0)2 + λ(θ02 + θj 2) (47) \nt=1 j=1 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "the parameter estimates. We will deriv e the form of regula rizatio n here by assigning a prior distributio n over the p", "source_title": "9e7f8fb89d65d7572980907fd6d12315 lec5", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "e00dcef3-ad99-4835-a443-806a31aeb3a4", "text": "n + d +1 d +1 = const. − log σ2 + log λ (46) 2 2 n d1 � � −2σ2 (yt − θT xt − θ0)2 + λ(θ02 + θj 2) (47) t=1 j=1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � \n�� � � 6.867 Mac hine learning , lectur e 5 (Jaakkola) 8 \nwhere again the estimat ion of θ and θ0 separa tes from setting the noise variance σ2 . Note \nthat this separa tion is achieved because we tied the prior and noise variance parameters. \nThe above regular ized problem of ﬁnding the parameter estimat es θˆand θˆ0 is known as \nRidge regression. \nAs before, we can get closed form estimat es for the parameters (we omit the analogous \nderiv ation): \nθˆ\n=(λI + XT X)−1XT y (48) θˆ0 \nIt is now useful to understa nd how the properties of these parameter estimates depend on \nλ. For example, are the parameter estimates unbiase d? No, they are not: \nE �� ˆθ \nˆθ0 � \n|X � \n= (λI + XT X)−1XT X � θ∗ \nθ∗ \n0 � \n� � (49) \n= (λI + XT X)−1(XT X + λI − λI) θ∗ \nθ∗ \n0 (50) \n� � � bias�� � �� \n= \n= θ∗ \nˆθ∗ \n0 −λ(λI + XT X)−1 θ∗ \nθ∗ \n0 \n� \nI − λ(λI + XT X)−1 � � θ∗ \nθ∗ \n0 � (51) \n(52) \n� � \nIt is straightforward to check that I − λ(λI + XT X)−1 is a positiv e deﬁnite matrix with \neigen values all less than one. The parameter estimates are therefore shrunk towards zero \nand more so the larger the value of λ. This is what we would expect since we explicitly \nfavored small parameter values with the prior penalt y. What do we gain from such biase d \nparameter estimates? Let’s evaluate the mean squared error, starting with the covariance: \nθˆ\nCov ˆ|X = σ∗2(λI + XT X)−1XT X(λI + XT X)−1 (53) θ0 \n= σ∗2(λI + XT X)−1(λI + XT X − λI)(λI + XT X)−1 (54) \n= σ∗2(λI + XT X)−1 − λσ∗2(λI + XT X)−2 (55) \nThe mean squared error in the parameters is therefore given by (we again omit the deriv a-\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "n + d +1 d +1 = const. − log σ2 + log λ (46) 2 2 n d1 � � −2σ2 (yt − θT xt − θ0)2 + λ(θ02 + θj 2) (47) t=1 j=1 Cite as: ", "source_title": "9e7f8fb89d65d7572980907fd6d12315 lec5", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 7}
{"id": "8aacfca3-9abf-4a48-b035-69e09c81a041", "text": "XT X − λI)(λI + XT X)−1 (54) = σ∗2(λI + XT X)−1 − λσ∗2(λI + XT X)−2 (55) The mean squared error in the parameters is therefore given by (we again omit the deriv a- Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n�\n����\n�\n�\n�\n�\n����\n�\n� � \n�\n � \n� � \n� � � � � � \n�\n����\n�\n�\n�\n�\n����\n�\n� � \n� � \n�\n����\n�\n�\n�\n�\n����\n�\n6.867 Mac hine learning , lectur e 5 (Jaakkola) 9 \ntion that can be obtained simila rly to previo us expressions): \nE\n2 \n|\nX\nθˆ\n θ∗ \nσ∗2 (λI + XT X)−1 − λ(λI + XT X)−2Tr\n −\n =\nθˆ0 ·\nθ0 ∗ \n�T θ∗ � ∗θ +λ2 (λI + XT X)−2 (56)\nθ0 ∗ θ0 ∗ \nCan this be smaller than the mean squared error corresp onding to the unregularized esti­\nmates σ∗2 Tr (XT X)−1 ? Yes, it can. This is indeed the beneﬁt from regular ization: we · \ncan reduce large variance at the cost of introducing a bit of bias. We will get back to this \ntrade-oﬀ in the context of model selection. \nLet’s exemplify the eﬀect of λ on the mean squared error in a context of a very simple \n1-dimensiona l example . Suppose, we have observ ed responses for only two points, x = −1 \nand x = 1. In this case, \n−11 20 1/(2 + λ) 0 X = 11 , XT X = 02 , (λI + XT X)−1 = 0 1/(2 + λ) (57) \nThe expression for the mean squared error therefor e becomes \nE\n2 \n|\nX\nθˆ\n λ2θ∗ 2 2λ\n=\nσ∗2 (θ∗2 + θ0 ∗2)\n(2 + λ) −\n +\n(2 + λ)2 (2 + λ)2−\nθˆ0 θ0 ∗ \n=4σ∗2 \n+ λ2 \n(θ∗2 + θ0 ∗2) (58) (2 + λ)2 (2 + λ)2 \nWe should compar e this to σ∗2Tr (XT X)−1 = σ∗2 obtained without regula rizatio n (cor­\nresponds to setting λ = 0). In the noisy case σ∗2 >θ∗2 + θ0 ∗2 we can set λ = 2 and \nobtain \nE\n2 4σ∗2 4 8σ∗2θˆ\n θ∗ 1\n(θ∗2 + θ0 ∗2σ∗2 (59)\n |\nX\n=\n +\n16 16\n) <\n −\n =\n16 2\n θˆ0 θ0 ∗ \nThe mean squared error of the parameter s is therefor e clearly smaller than without regu­\nlarization. \nActive learning \nWe can use the expressions for the mean squar ed error to activ ely select input points \nx1,..., xn, when possible, so as to reduce the resulting estimatio n error . This is an active \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "for ", "section_heading": "XT X − λI)(λI + XT X)−1 (54) = σ∗2(λI + XT X)−1 − λσ∗2(λI + XT X)−2 (55) The mean squared error in the parameters is the", "source_title": "9e7f8fb89d65d7572980907fd6d12315 lec5", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 8}
{"id": "7a814e15-bc9e-4711-93b7-2f13ef824d93", "text": "learning We can use the expressions for the mean squar ed error to activ ely select input points x1,..., xn, when possible, so as to reduce the resulting estimatio n error . This is an active Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n�\n����\n�\n�\n�\n�\n����\n�\n� � \n� � 6.867 Mac hine learning , lectur e 5 (Jaakkola) 10 \nlearning (experiment design ) problem. The goal is to get by with as few responses as \npossible without sacriﬁcing the estimatio n accura cy. For example, in training a classiﬁer \nwe would like to minimize the number of training images we would have to label in order \nobtain a certain class iﬁcation accuracy . In the regression context, we may be selecting \namong possible experimen ts to carry out (e.g., choosing diﬀerent operating points for a \nfacto ry or gauging mark et/custo mer responses to diﬀeren t strategies available to us). By \nletting the metho d guide the selectio n of the training examples (inputs), we will genera lly \nneed far fewer examples in compar ison to selecting them at rando m from some underly ing \ndistributio n, database, or trying available experimen ts at rando m. Fewer responses means \nless human eﬀort, less cost, or both. \nTo develop this further let’s go back to the unreg ularized case where \nE\n2 \n|\nX\nθˆ\n θ∗ \n= σ∗2Tr\n(XT X)−1 (60)\n −\nθˆ0 θ0 ∗ \nWe do not know the noise variance σ∗2 for the correct model but it only appears as a \nmultiplica tive consta nt in the above express ion and therefore won’t aﬀect how we should \nchoose the inputs. When the choice of inputs is indeed up to us (e.g., which experiments to \ncarry out) we can select them so as to minimize Tr (XT X)−1 . One caveat of this appro ach \nis that it relies on the underlying relatio nship between the input s and the responses to be \nlinear. When this is no longer the case we may end up with clearly suboptima l selections. \nWe will discuss next time how we can ﬁnd say n input examples x1,..., xn that minimiz e \nthe criterion. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "regression", "section_heading": "learning We can use the expressions for the mean squar ed error to activ ely select input points x1,..., xn, when possib", "source_title": "9e7f8fb89d65d7572980907fd6d12315 lec5", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 9}
{"id": "4841b910-bd4c-4e3c-bcbe-39bce2eff5d0", "text": "6.867 Machine learning \nMid-t erm exam \nOcto ber 18, 2006 \n(2 points) Your name and MIT ID: \n1\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\na) \n0 0.5 1 1.5 2 2.5 300.511.522.53\nxy b) \n0 0.5 1 1.5 2 2.5 300.511.522.53\nxy\nc) \n0 0.5 1 1.5 2 2.5 300.511.522.53\nxy d) \n0 0.5 1 1.5 2 2.5 300.511.522.53\nxy\nFigur e 1: Plots of linear regression results with diﬀeren t types of regulariza tion \nProblem 1 \nFigur e 1 plots linear regression results on the basis of only three data points. We used \nvarious types of regular izatio n to obtain the plots (see below) but got confuse d about \nwhich plot corresp onds to which regula rizatio n method. Please assign each plot to one \n(and only one) of the following regula rizatio n metho d. \n2\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "6.867 Machine learning ", "source_title": "ffb24092649a63cd06dd4c3443ae9f2a midterm f06soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "9d68ceee-fbf3-481d-9249-a5e2b5b4a96a", "text": "plots (see below) but got confuse d about which plot corresp onds to which regula rizatio n method. Please assign each plot to one (and only one) of the following regula rizatio n metho d. 2 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� 1.1 (2 points) �3 \nt=1(yt − θxt − θ0)2 + λθ2 where λ = 1 c \n1.2 (4 points) �3 t=1(yt − θxt − θ0)2 + λθ2 where λ = 10 b \nBrieﬂy expla in why \nThe slope is strongly regularize d making the regression function ﬂat. Since we don’t \nregularize the oﬀset parameter it is still possible to lift the ﬂat functi on in the midd le \nof the responses. \nNote: since θ0 is not regularize d, the sum of positive and negative errors will be \nexactly zero at the optimal setting of the parameters \n3\nθxt − θˆ0)=0 \nt=1 (yt − ˆ\n�31.3 (2 points) (yt − θxt − θ0)2 + λ(θ2 + θ02) where λ =1 at=1\n�31.4 (2 points) (yt − θxt − θ0)2 + λ(θ2 + θ2) where λ = 10 dt=1 0\nProblem 2 \nWe are trying to solve a regression problem with kernel linear regression models using \ndiﬀeren t degree polyno mial kernels. Our regression problem is a little unusual in the sense \nthat the train ing input points are 1-dimens ional and ﬁxed, x1,...,xn (all distinct). Our \ntask is to ﬁnd the unde rlying functio n values at the same points and speciﬁcally at x1. \nThe underlying function is f∗(x)= |x}, where the expectation is over the underlying E{y\ndistributio n (pdf) p(yx) governing how y depends probabilistica lly on x. We have no |\nknowledge of f∗(x) or p(yx) beyond real valued training responses y1,...,yn, sampled |\nfrom p(yx) at the training inputs. |\nLet’s assume that our linear regression model (not in the kernel form) is given by \nf(x; θ,θ0)= θT φ(x)+ θ0 \nwhere φ(x) is the featur e vector corresp onding to our choice of the kernel functio n. We will \nestimate the parameter θ and θ0 (or α and θ0 in a kernel form) by minimizing the mean \n3\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "plots (see below) but got confuse d about which plot corresp onds to which regula rizatio n method. Please assign each p", "source_title": "ffb24092649a63cd06dd4c3443ae9f2a midterm f06soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "b87f0305-da7b-4588-8109-9b01d7e65800", "text": "is the featur e vector corresp onding to our choice of the kernel functio n. We will estimate the parameter θ and θ0 (or α and θ0 in a kernel form) by minimizing the mean 3 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nsquar ed predictio n error without regula rization:\nn� �21 � \nyi − f(xi; θ,θ0) n i=1 \nWe will then use f(x1; θ, ˆθˆ0) as an estimat or of f∗(x1). In other words, all we care about is \nthe prediction at x1. Assume that n = 3. \n2.1 (3 points) Write down an expression for the bias of this estima tor. Your expression \nshould involve just f(x1; θ, ˆθˆ0), E{·}, and f∗(x1), as well as an expla natio n for what the \nexpectation is over. \nBias at x1 = E{ f(x1; θ, ˆθˆ0) }− f∗(x1) \nwher e the expectation is over the responses y1, y2, and y3 corresponding to the three \nﬁxed training points x1, x2, and x3. Each response is sample d from p(y ixi), inde­|\npendently from the other s. \n2.2 (2 points) Whic h degree polynomia l kernel would we need to get zero \ntraining error, i.e., ﬁt the three training responses perfectly? \nYou need a quadr atic feature vector to perfectly ﬁt three points. \n2.3 (2 points) Would we get an unbiased estima tor at x1 if we achieve Y \nzero training error (Y/N )? \nSince we are ﬁtting the responses perfectly, our estimator f(x1; θ, ˆθˆ0) \nsimply returns y1, the training response . The expected value of this \nresponse is by deﬁnition f∗(x1). The estimator is therefore unbiase d. \n2.4 (3 points) Supp ose the noise variance at x1 is E{(y1 − f∗(x1))2} = σ2 . What is the \nvariance of our “zero training error estima tor”, again at x1? \nSince the estimator returns the obser ved training response y1 at x1, f(x1; θ, ˆθˆ0)= y1, \nand is unbiase d so that E{f(x1; θ, ˆθˆ0)} = f∗(x1), we have \n�� �2 � \nE f(x1; θ, ˆθˆ0) − E{f(x1; θ, ˆθˆ0)} = E{(y1 − f∗(x1))2} = σ2 \nwhich is just the noise varianc e. So our estimator is as noisy as the responses \n(seriously overﬁtting). \n4\n2 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "is the featur e vector corresp onding to our choice of the kernel functio n. We will estimate the parameter θ and θ0 (or", "source_title": "ffb24092649a63cd06dd4c3443ae9f2a midterm f06soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "82688977-40a8-4765-8d2f-1e7bf4123144", "text": "�2 � E f(x1; θ, ˆθˆ0) − E{f(x1; θ, ˆθˆ0)} = E{(y1 − f∗(x1))2} = σ2 which is just the noise varianc e. So our estimator is as noisy as the responses (seriously overﬁtting). 4 2 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 3 \nWe are trying to solve a classiﬁcat ion problem with supp ort vector machines. In our prob­\nlem there are only a few positive training examp les and we are certa in that they are classiﬁed \ncorrectly. We also have a large number of negative training examples , some of which may \nbe misclassiﬁed. We’d like to modify the basic dual form of the SVM optimizatio n problem, \nn� 1 n� \n(1) maximize αi − 2 αiαj yiyj K(xi, xj ) \ni=1 i,j=1 \nn� \n(2) subject to αi ≥ 0, αiyi = 0 \ni=1 \nto better solve this type of problem. We would like to ensure that we won’t misclassify any \nof the positive examples but could misclassify some of the negative examples. We believe \nyou have to introduce additiona l parameter( s) (or consta nts for the purp ose of solving the \nquadratic programming problem) in order to achieve this. \nIn your solution, please use I+ to index positively labeled examples (yi = +1) and I− for \nnegative example s (yi = −1). In other words, i ∈ I+ means that yi = +1, and |I+| is the \nnumber of positiv e examples. \n3.1 (6 points) Your solut ion must be in the dual form. You can refer to (1) and (2) above. \nMaximize \n(1), as above\nsubject to\n(2) and αi ≤ C− for i ∈ I− (negative examples). \nIn other words, we limit how strongly the margin constr aints are enfor ced for the \nnegative examples. Positive examples have no such limit and the classiﬁer will have \nto satisfy the margin constr aints exactly for the positive examples. \n3.2 (6 points) Chec k (Y/N) which of the following alternativ e criteria would work for opti­\nmizing your new parameters. We have underlined any diﬀerences between the alterna tives. \n5 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "�2 � E f(x1; θ, ˆθˆ0) − E{f(x1; θ, ˆθˆ0)} = E{(y1 − f∗(x1))2} = σ2 which is just the noise varianc e. So our estimator i", "source_title": "ffb24092649a63cd06dd4c3443ae9f2a midterm f06soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "d3692b12-976e-440d-a720-704cb8e553be", "text": "exactly for the positive examples. 3.2 (6 points) Chec k (Y/N) which of the following alternativ e criteria would work for opti­ mizing your new parameters. We have underlined any diﬀerences between the alterna tives. 5 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n( ) We train your SVM algorithm |I+| times, each time leaving out one of the positiv e \nexamples, and testing the classiﬁer on the left out example. The parameter(s) are set \nto minimize the resulting number of misclassiﬁe d example s. \nSince we are only focusing on how well the positive examples are classi­\nﬁed, setting C− =0 would be optimal. As a result, we wouldn ’t enfor ce\nany classiﬁc ation constr aints on the negative examples.\n( X ) We train your SVM algorithm |I−| times, each time leaving out one of the negative \nexamples, and testing the classiﬁer on the left out example. The parameter(s) are set \nto minimize the resulting number of misclassiﬁe d example s. \nBrieﬂy explain why this would or would not work: \nThe optimization problem descr ibed above strictly enfor ces the classiﬁc ation con­\nstraints for the positive examples. Thus no matter how we set C− it won’t be possi­\nble to misclassify any of them on the training set. However , focusin g solely on the \nnegative examples will not try to gauge how well we gener alize in terms of classify­\ning positive examples. We are simply trying to gener alize well in terms of correctly \nclassifying negative examples (by optimizing this CV error) with the constr aint that \nwe still have have to classify all the positive training examples correctly. \n( X ) We train your SVM algorithm n times , each time leaving out one of the examp les, \npositiv e or negative, and testing the classiﬁe r on the left out example. The constant \nis set to minimize the resulting number of misclassiﬁed examples. \nThis is the standar d CV error and would work here as well. \nProblem 4 \nA studen t in a machine learning course claimed that the points in Figure 2a can be separa ted \nwith “almost a linear kernel” . Hard to believe, we responded, since the points are clearly \nnot linearly separa ble. But the studen t insisted. The “almo st a linear kernel” they had in \nmind was the following norm alized kernel: \nxT x� \nKnorm(x, x�)= �x��x�� \n4.1 (2 points) What are the feature vectors corresponding to this kernel? \n6 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "exactly for the positive examples. 3.2 (6 points) Chec k (Y/N) which of the following alternativ e criteria would work f", "source_title": "ffb24092649a63cd06dd4c3443ae9f2a midterm f06soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "ec6da7b6-d654-44d8-ada7-8cff0793a1d1", "text": "t insisted. The “almo st a linear kernel” they had in mind was the following norm alized kernel: xT x� Knorm(x, x�)= �x��x�� 4.1 (2 points) What are the feature vectors corresponding to this kernel? 6 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nFigur e 2: a) Points that should be separ able with a normalized linear kernel. b) feature \nspace with the original points overlaid with their original coordina te values. a) \n00.2 0.4 0.6 0.8 100.10.20.30.40.50.60.70.80.91 b) \n00.2 0.4 0.6 0.8 100.10.20.30.40.50.60.70.80.91x \nx φ(x) \nφ(x) φ(x) \nThe feature vectors are just φ(x)= x/�x�. These are two dimensional vectors \n(although they only vary along the unit circle). \n4.2 (4 points) Using Figure 2b (righ t), graphically map the points to their new feature \nrepresen tation using the ﬁgure as the feature space. \nThe points are mapp ed radial ly to the unit circle (the largest dotte d \ncircle in the ﬁgur e) \n4.3 (4 points) Draw the resulting maxim um margin decision bounda ry in the feature \nspace. Use the same Figure 2b (right). The studen t was right, the points are separable! \nSee the ﬁgur e. \n7\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "t insisted. The “almo st a linear kernel” they had in mind was the following norm alized kernel: xT x� Knorm(x, x�)= �x�", "source_title": "ffb24092649a63cd06dd4c3443ae9f2a midterm f06soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "761f2145-b50b-4d52-8020-b4f2c045a7b9", "text": "4.3 (4 points) Draw the resulting maxim um margin decision bounda ry in the feature space. Use the same Figure 2b (right). The studen t was right, the points are separable! See the ﬁgur e. 7 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n4.4 (2 points) Does the value of the discriminan t function corresponding N \nto your solut ion change if we scale any point, i.e., evaluate it at s x \ninstead of x for some s> 0? (Y/N ) \n4.5 (4 points) Draw the decision bounda ry in the origina l input space resulting from the \nnormalized linea r kernel. Use Figure 2a (left). \nThe maximum margin boundar y in the feature space crosses the unit \ncircle in two places. These are the feature vectors right on the boundary. \nSince points that are already normalize d map onto themselves in the \nfeature space, these are also points right on the boundar y in the original \nspace. We know that scaling doesn’t aﬀect the discr iminant function \nand thus you can simply draw lines from these points on the unit circle \nto/fr om the origin to get the decision boundary in the original space. \nProblem 5 \nThere are many criteria for activ e learning . In particular, in the context of linear regress ion, \nwe derived such criteria by assuming that the underlying model was also linear (in the fea­\nture space). One of the resulting criteria was based on ﬁnding points where our predictions \nvaried the most (relativ e to resample d training sets from an assumed underlying model). \nWe will focus here on simple activ e learning methods for classiﬁcatio n tasks with the \nperceptron algorithm. We assume that you can only ask labels for the training examples \nx1,..., xn (those we don’t alrea dy have labels for). The labels are ﬁxed once revealed so \nthere’s no reason to query the same point multiple times. The perceptro n algorithm, in \nresponse to mista kes, updates its parameters according to \nθ ← θ + ytxt iﬀ ytθT xt ≤ 0 \n5.1 (2 points) In our setting, would it be useful to get a label for a point N \nthat we can classif y correctly? (Y/N ) \nTher e are two plausible ways of applying the perceptron algorithm in this \ncontext. You either run the algor ithm until it conver ges with the labels \nyou already have, or consider each example only once in the order in \nwhich they were asked to be labeled. In either case, a point that does not \nresult in an update, i.e., is not misclassiﬁe d, won’t have any imme diate \neﬀect on θ and therefore not on the next point to be labeled. \n5.2 (6 points) Given the current θ we have to select which example x1,..., xn would be \nthe most useful to label. Check all of the following criteria you believe would work as a \nselection criterion. We could select the point xt with \n( ) the largest norm �xt� \n( ) the largest value of |θT xt| \n8 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "4.3 (4 points) Draw the resulting maxim um margin decision bounda ry in the feature space. Use the same Figure 2b (right", "source_title": "ffb24092649a63cd06dd4c3443ae9f2a midterm f06soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "b437d383-1c5f-4fc3-9f93-01191711aae6", "text": "Check all of the following criteria you believe would work as a selection criterion. We could select the point xt with ( ) the largest norm �xt� ( ) the largest value of |θT xt| 8 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n( X ) the smallest value of |θT xt|\nBrieﬂy expla in why your chosen criterio n (criteria) would work in our activ e learning settin g:\nWe are looking for points that are potential ly misclassiﬁe d. These are the ones that \nare close to the boundary. We don’t know whet her they are misclassiﬁe d before seeing \nthe label but we can measure how close they are to the boundary, which is what |θT x|\ntries to do (you could normalize this by �θ� so as to get an actual distanc e to the \ncurrent boundary). \n9\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "Check all of the following criteria you believe would work as a selection criterion. We could select the point xt with (", "source_title": "ffb24092649a63cd06dd4c3443ae9f2a midterm f06soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 7}
{"id": "a62770cd-eb11-44e7-bb0c-583e0a353191", "text": "� � \n�� � � \n� � \n� � Massa chusetts Institu te of Techn ology \n6.867 Machine Lear ning, Fall 2006 \nProblem Set 4: Solutions \n1. (a) (8 points) We have \nnd\nL(D; θ)= P (xri |yr)P (yr) (1) \nr=1 i=1 \nwhere the number of exampl es is n. We can parame terize P (y) with the paramete r ψ, as \n2 2 P (y)= ψ y+1 (1 − ψ) 1−y (2) \nThis is the same kind of parame trization we have used for P (xi|y). Eqn 1 thus becomes \nn d xri +1 \n2L(D; θ,ψ)= �� \nθi|yr (1 − θi|yr ) 1−\n2 xri ψ yr \n2+1 (1 − ψ) 1−\n2 yr (3) \nr=1 i=1 \nIn the above equation , θi|+1 occurs whenev er xri = 1 and yr = 1 and (1 − θi|+1) occurs whenev er \nxri = −1 and yr = 1. Simil arly, ψ occurs whenver yr = 1 and (1 − ψ) occurs whenver yr = −1. \nWith this intuition we now have: \n⎡ ⎤ \nd � � \nL(D; θ,ψ)= ⎣ θinˆ\n|y iy (1,y)(1 − θi|y)nˆiy (−1,y)⎦ ψnˆy (1)(1 − ψ)nˆy (−1) (4) \ni=1 y={−1,1} \nIn the above, we have used the ˆn notation used in the lectures; e.g., ˆniy(1, −1) counts the number \nof examples with xri = 1 and yr = −1. We then have \nL(D; θ,ψ)P (θ,ψ)= L(D; θ,ψ)P (θ)P (ψ) (5) \nWe assume a uniform prior on ψ i.e. P (ψ) = 1. Since ψ ∈ [0, 1], this is already normalize d. Then \nwe have: \n⎡ ⎤ \nd � � \nL(D; θ,ψ)P (θ,ψ)= ⎣ θinˆ\n|y iy (1,y)(1 − θi|y)nˆiy (−1,y)⎦ ψnˆy(1)(1 − ψ)nˆy (−1) (6)× \n⎡ i=1 y={−1,1} ⎤ \nd�� 1 +⎣ \nB(r+ +1,r− + 1)θir\n|y (1 − θi|y)r− ⎦ (7) \ni=1 y={−1,1} \nΓ(a+b)where B(a,b)= Γ(a)Γ(a) . We collect the terms togethers \n⎡ ⎤ \nd\nL(D; θ,ψ)P (θ,ψ)= Q1 \nr ⎣� � \nθinˆ\n|y iy (1,y)+r+ \n(1 − θi|y)nˆiy (−1,y)+r− ⎦� \nψnˆy (1)(1 − ψ)nˆy (−1) � \n(8) \ni=1 y={−1,1} \nwhere Qr = B(r+ +1,r− + 1)2d . Thus, m + \ni|y =ˆniy(1,y)+ r+ and m−\ni|y =ˆniy(−1,y)+ r−. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "� � ", "source_title": "ebc7892b924d0778dce0580bc121c9f2 hw4 soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "93b825a7-ce0c-4b70-a927-585c35d4028a", "text": "|y iy (1,y)+r+ (1 − θi|y)nˆiy (−1,y)+r− ⎦� ψnˆy (1)(1 − ψ)nˆy (−1) � (8) i=1 y={−1,1} where Qr = B(r+ +1,r− + 1)2d . Thus, m + i|y =ˆniy(1,y)+ r+ and m− i|y =ˆniy(−1,y)+ r−. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � \n� � � � \n� � \n� (b) (8 points) We have\n⎡ ⎤ \nd + � � �� mi|y )m−ny (1)(1 − ψ)nˆy (−1)P (θ,ψ|D) ∝ ⎣ θi|y (1 − θi|yi|y ⎦ ψˆ (9) \ni=1 y={−1,1} \nThe right-han d side (RHS ) consists of a product of Beta distributions. To normalize it, we could \nintegrate over each θi|y over the range θi|y = [0, 1], using integration by parts for each such case. \nHowever, there’s a much simpler method. Since the posterior has the form of a product of Beta \ndistributions, we could directly use the corres ponding normalization constan t. The normalization \nconstan t for the Beta distri bution is described in the problem-set. Using it, we have \n⎡ ⎤ \nd\n+P (D|F)= ⎣ B(mi|y +1,m−\ni|y + 1) ⎦ B(ˆny(1) + 1,nˆy(−1)+1) (10) \ni=1 y={−1,1} \ni.e., \n⎡ ⎤ \n1 ⎣ dm + \ni|y )m−⎦ ψnˆy (1)(1 − ψ)nˆy (−1)P (θ,ψ|D)= θ (1 − θi|yi|y (11) \ni=1 P (D|F) y={−1,1} i|y \nIf you chose to preserve the constant 1/Q r as part of the initial P (θ,ψ|D), your answ er in Eqn 10 \nshoul d be multiplied by 1/Q r. \n+(c) (9 points) If feature i is inclu ded (F2), the corresponding terms in P (D|F) will be B(mi|1 + \n1,m−\ni|1 + 1)B(m + \ni|−1 +1,m−\ni|−1 + 1). If it is not included (F1), there will only be one term θi which \nwill combine counts for both y = 1 and y = −1, i.e., the term corresponding to feature i will be \nB(ˆni(1) + r+ +1,nˆi(−1) + r− + 1). \nTo choose F1 over F2, we need \nB(ˆni(1) + r+ +1,nˆi(−1) + r− + 1) > 1 or (12)B(m + +1,m−+ 1)B(m + +1,m−+ 1) i|1 i|1 i|−1 i|−1 \nor, \nlog B(ˆni(1) + r + +1,nˆi(−1) + r− + 1)) (13) \n+− log B(mi|1 +1,m−\ni|1 + 1) (14) \n− log B(m + +1,m−+ 1) > 0 (15)i|−1 i|−1 \n(d) (2 points) At the MLE value, the ﬁrst derivative is zero (the deriv ative of a diﬀerentiable function \nis zero at maxi ma and minima). As such, a ﬁrst-order expansion will not buy us much— it will \nonly lead to a constant-valued function. \n(e) (6 points) From previous part, A1 =0. LetΣ =(−A2)−1 . Since A2 is the Hessian (i.e. the \nmatrix of second derivatives) evaluated at a maxima, it is negative deﬁnite, so that the negativ e of \nits inverse Σ is positiv e deﬁnite. Also, we are given that |Σ|≈ (nrC(r))−1 . We now have \nL(D; θ)P (θ)dθ = L(D; θ)1 dθ (16) ·· \n= exp(l og L(D; θ))dθ (17) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "|y iy (1,y)+r+ (1 − θi|y)nˆiy (−1,y)+r− ⎦� ψnˆy (1)(1 − ψ)nˆy (−1) � (8) i=1 y={−1,1} where Qr = B(r+ +1,r− + 1)2d . Thu", "source_title": "ebc7892b924d0778dce0580bc121c9f2 hw4 soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "76d1bf78-a1db-483d-83d0-78c74da6d5d6", "text": "negativ e of its inverse Σ is positiv e deﬁnite. Also, we are given that |Σ|≈ (nrC(r))−1 . We now have L(D; θ)P (θ)dθ = L(D; θ)1 dθ (16) ·· = exp(l og L(D; θ))dθ (17) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� � \n� � = exp(l og L(D; θˆML) − 1(θ − θˆML)T Σ−1(θ − θˆML))dθ (18)2\n= �� \nL(D; θˆML)dθ � �� \nexp(1(θ − θˆML)T Σ−1(θ − θˆML))dθ � \n(19)2\n(20) \nL(D; θˆML) is a constan t, i.e., it doesn’t depend on θ. Also, the second term looks a lot like a \nGaussian distribution. So we have \n��� � 1 1 � \nL(D; θˆML) (2π)r/2|Σ|1/2 exp( (θ − θˆML)T Σ−1(θ − θˆML))dθ (21)(2π)r/2|Σ|1/2 2\n≈ L(D; θˆML) · (2π)r/2(n rC(r))−1/2 · 1 (22) \n≈ L(D; θˆML) � 2\nnπ �r/2 \nC1(r) (23) \n(f) (2 points) Taking the log of the expression from the previou s part, we have: \nlog P (D; Fr) ≈ log L(D; θˆML) − 2 r log n +2 r log2π + C2(r) (24) \nAs n →∞, the terms that depend only on r and not on n can be ignor ed. So that limn→∞ log P (D; Fr) \nbecomes \nlog L(D; θˆML) − r log n (25)2 \n2. (a) The likelihood is: \nm tk\nL(D;Θ) = N(xi; µk, Σk). (26) \nk=0 i=tk−1 \nThe log-likelihood is: \nm tk\n�(D; Θ) = log N(xi; µk, Σk); (27) \nk=0 i=tk−1 \nFurthermore, the number of free variables in m +1, d-dime nsional multivariate Gaussians is m(d + \nd(d + 1)/2). Conse quently, the BIC is: \nm t�k−1 � � m\nBIC = � \nlog N(xi; µk, Σk) − d + d(d \n2+ 1)/2 � \nlog(tk − tk−1). \nk=0 i=tk−1 k=0 \nIt is worth noting that: \nt�k−1 \nlog N(xi; µk, Σk)= − tk −\n2 tk−1 (log |Σ| + d log(2π) + 1) . (28) \ni=tk−1 \n(b) \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "negativ e of its inverse Σ is positiv e deﬁnite. Also, we are given that |Σ|≈ (nrC(r))−1 . We now have L(D; θ)P (θ)dθ = ", "source_title": "ebc7892b924d0778dce0580bc121c9f2 hw4 soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "b8bdedd7-6f61-4c46-9ef6-5fd7edd41484", "text": "d(d 2+ 1)/2 � log(tk − tk−1). k=0 i=tk−1 k=0 It is worth noting that: t�k−1 log N(xi; µk, Σk)= − tk − 2 tk−1 (log |Σ| + d log(2π) + 1) . (28) i=tk−1 (b) Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nfunct ion [bestat,t opscore,sc ores]=split(X )\nN = size(X, 1); · \nd = size(X, 2); · \nλ = 1; · \ns1 =0; s2 = sum(X); · \nS1 =0; S2 = 0; · \nfor i =1: N · \nS2 = S2 + X(i, :)�X(i, :); ·· \nend· \nµ = s2/N; · \nΣ= S2/N − µ�µ; · \ntopscore = −10; · \nbestat = −1; · \nscores = []; · \nfor i =1: N − 2 · \ns1 = s1 + X(i, :); ·· \nS1 = S1 + X(i, :)�X(i, :); ·· \ns2 = s2 − X(i, :); ·· \nS2 = S2 − X(i, :)�X(i, :); ·· \nµ1 = s1/i; ·· \nΣ1 = S1/i − µ�\n1µ1; ·· \nµ2 = s2/(N − i); ·· \nΣ2 = S2/(N − i) − µ2�µ2; ·· \nif i> 30 and i<N − 30 ·· \nscore = N log(det(Σ)) − i log(det(Σ1)) ··· \n− (N − i)log(det(Σ 2)) ···· \n− λ/2(d + d(d + 1)/2) log(N); ···· \nif score > topscore ··· \nbestat = i; ···· \ntopscore = score; ···· \nend ··· \nscores(i, :) = [score, det(Σ), det(Σ 1), det(Σ 2)]; ··· \nend ·· \nend· \nend \n� load -ascii ‘data1’ \n� C = data1; \n� [bestat,t opscore,sc ores] = split(C ); \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n0 200 400 600 800 1000 1200 1400 1600 1800 2000−1000100200300400500600700(c) \n� load -ascii ‘cepstra1.mat’ \n� C = cepstra1; \n� multisplit(C ) \nans =\n149\n· \n194\n· \n291\n· \n421\n· \n492\n· \n556\n· \n668\n· \n738\n· \n1470\n· \n1587\n· \n1693\n· \n1751· \n1840· \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� load -ascii ‘cepstra2.mat’ \n� C = cepstra2; \n� multisplit(C ) \nans \n· \n· · · · · · \n· \n· · · · · · · · = \n32\n198\n230\n285\n449\n514\n684\n813\n852\n897\n1040\n1197\n1229\n1397\n1534 \n1683 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "if ", "section_heading": "d(d 2+ 1)/2 � log(tk − tk−1). k=0 i=tk−1 k=0 It is worth noting that: t�k−1 log N(xi; µk, Σk)= − tk − 2 tk−1 (log |Σ| + ", "source_title": "ebc7892b924d0778dce0580bc121c9f2 hw4 soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 3}
{"id": "a17267e1-f555-4605-8be0-1f7d4e13738a", "text": "multisplit(C ) ans · · · · · · · · · · · · · · · · = 32 198 230 285 449 514 684 813 852 897 1040 1197 1229 1397 1534 1683 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� 3. (a) Take the negative deriv ative of the loss function to get the weights: \ne−z \n. (29)1+ e−z \nThe numerator and denominat or are both positiv e and the numerator is less than the denominator. \nThus, the quotien t is between 0 and 1. Given that the unnormalize d weights are bounded, exampl es \nthat are badly misclassiﬁed and those that are just barely misclassiﬁed will end up with comparab le \nweights after normalization. \n(b) The value of ˆα1 is inﬁnite. Increas ing α1 will decreas e all the train ing losses since yth(x t; θˆ1) > 0 \nfor all t. \n(c) There are two ways to show this. First, by construction when β is ﬁxed. It suﬃc es to provide a set \nof points such that an ensemble with the radial basis learners can classify them in all possible ways. \nWe will use n base learn ers, each associate d with one train ing point. The points can be placed \nfar enough apart such that the only relevant contribution to the ensem ble output comes from the \nbase learn er associated with each point. Since the base learners reproduce the training labels for \nindividual points, so will the ensemble. \nAnot her way is to use the result in problem set 2 that the gram matrix for the radial basis kernel \nis invertible so that the discriminant function \nn\nh(x; θ)= αtyt exp(−β�x − xt�2) (30) \nt=1 \ncan be chosen to take any values over n-points x1,..., xn. Strictly speaking we’d have to show, \nin addition, that αt’s in the above expression can be all non-negativ e. The product αtyt is not \nconstrain ed if we can choose yt ∈ {−1, 1} for each base learner (yt here need not be the label we \naim to reproduce with h(x t; θ)). \n(d) It does not overﬁt; the test error decreases initially, but does not increas e again after many iteration s \nas it would if it were overﬁtting. \n0 5 10 15 20 25 300.050.10.150.20.250.30.350.40.45Number test examples misclassified\nNumber of Boosting Iterations\n(e) Replace lines 9 and 10 in call boosting.m with: \n[y est,sum of alpha]= eval boost(model(1:k ),data.xtr ain); · \nerr(k )=sum(y est.*data.yt rain/sum of alpha≤0.5)/lengt h(data.ytr ain); · \nThe margi n errors for ρ =0.1 tend to decrease . \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "multisplit(C ) ans · · · · · · · · · · · · · · · · = 32 198 230 285 449 514 684 813 852 897 1040 1197 1229 1397 1534 168", "source_title": "ebc7892b924d0778dce0580bc121c9f2 hw4 soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "19818f53-8f42-4854-935c-5ee9accd0901", "text": "9 and 10 in call boosting.m with: [y est,sum of alpha]= eval boost(model(1:k ),data.xtr ain); · err(k )=sum(y est.*data.yt rain/sum of alpha≤0.5)/lengt h(data.ytr ain); · The margi n errors for ρ =0.1 tend to decrease . Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n0 5 10 15 20 25 300.20.220.240.260.280.30.32Number test examples misclassified\nNumber of Boosting IterationsCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "for ", "section_heading": "9 and 10 in call boosting.m with: [y est,sum of alpha]= eval boost(model(1:k ),data.xtr ain); · err(k )=sum(y est.*data.", "source_title": "ebc7892b924d0778dce0580bc121c9f2 hw4 soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 5}
{"id": "82643076-f4ff-4122-84ee-64a713d10acb", "text": "� 1 6.867 Mac hine learning , lectur e 4 (Jaakkola) \nThe Supp ort Vector Machine and regularization \nWe proposed a simple relax ed optimizatio n problem for ﬁnding the maxim um margin sep­\narator when some of the examples may be misclassiﬁed: \nminimize 1 \n2�θ�2 + C n� \nξt (1) \nt=1 \nsubject to yt(θT xt + θ0) ≥ 1 − ξt and ξt ≥ 0 for all t = 1, . . . , n (2) \nwhere the remaining parameter C could be set by cross-validat ion, i.e., by minimizing the \nleave-one-out cross-validat ion error . \nThe goal here is to brieﬂy unde rstand the relaxed optimizatio n problem from the point of \nview of regular ization. Regular ization problems are typically form ulated as optimizatio n \nproblems involving the d esired objective (classiﬁcatio n loss in our case) and a regula rizatio n \npenalt y. The regulariza tion penalt y is used to help stabilize the minim ization of the ob­\njective or infuse prior knowledg e we might have about desirable solutions. Many machine \nlearning metho ds can be viewed as regulariza tion meth ods in this manner. For later utilit y \nwe will cast SVM optimizatio n problem as a regulariza tion problem. \na) \n−3 −2 −1 0 1 2 3−1−0.500.511.522.53 b) \n−3 −2 −1 0 1 2 3−1−0.500.511.522.53\nFigur e 1: a) The hinge loss (1 − z)+ as a functio n of z. b) The logistic loss log[1 + exp(−z)] \nas a function of z. \nTo turn the relaxed optimizatio n problem into a regular ization problem we deﬁne a loss \nfunction that corresp onds to individually optimize d ξt values and speciﬁe s the cost of vio­\nlating each of the margin constra ints. We are eﬀectively solving the optimiza tion problem \nwith respect to the ξ values for a ﬁxed θ and θ0. This will lead to an expression of C t ξt \nas a function of θ and θ0. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "� 1 6.867 Mac hine learning , lectur e 4 (Jaakkola) ", "source_title": "59a63d2efbe8aa01041937ff539a449a lec4", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "0620254a-ab96-4c30-ad36-a06dce79f9c4", "text": "are eﬀectively solving the optimiza tion problem with respect to the ξ values for a ﬁxed θ and θ0. This will lead to an expression of C t ξt as a function of θ and θ0. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � �+ 2 6.867 Mac hine learning , lectur e 4 (Jaakkola) \nThe loss function we need for this purp ose is based on the hinge loss Lossh(z) deﬁned as \nthe positiv e part of 1 − z, written as (1 − z)+ (see Figure 1a). The relaxed optimizatio n \nproblem can be writt en using the hinge loss as \n=ξˆt \nn� �� � \nminimize 1\n2�θ�2 + C �� \n1 − yt(θT xt + θ0) �+ (3) \nt=1 \nHere �θ�2/2, the inverse squar ed geometric margin, is viewed as a regulariza tion penalt y \nthat helps stabilize the objective \nn\nC 1 − yt(θT xt + θ0) (4) \nt=1 \nIn other words, when no margin constra ints are violated (zero loss), the regula rizatio n \npenalt y helps us select the solution with the largest geometric margin. \nLogistic regressi on, maxi mum likelihood estimation \n−3 −2 −1 0 1 2 300.10.20.30.40.50.60.70.80.91\nFigur e 2: The logistic function g(z) = (1+exp(−z))−1 . \nAnot her way of dealing with noisy labels in linear classiﬁcat ion is to model how the noisy \nlabels are generated. For example, human assigned labels tend to be very good for “typical \nexamples” but exhibit some variation in more diﬃcult cases. One simple model of noisy \nlabels in linear class iﬁcation is a logistic regression model. In this model we assign a \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "logistic regression", "section_heading": "are eﬀectively solving the optimiza tion problem with respect to the ξ values for a ﬁxed θ and θ0. This will lead to an ", "source_title": "59a63d2efbe8aa01041937ff539a449a lec4", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "e13e38cb-3aaf-4719-b777-35be87072fc7", "text": "to be very good for “typical examples” but exhibit some variation in more diﬃcult cases. One simple model of noisy labels in linear class iﬁcation is a logistic regression model. In this model we assign a Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � \n� � \n� � \n� 3 6.867 Mac hine learning , lectur e 4 (Jaakkola) \nprobabilit y distribut ion over the two labels in such a way that the labels for examples \nfurther away from the decision boundar y are more likely to be correct. More precisely , we \nsay that \nP (y =1|x,θ,θ0)= gθT x + θ0 (5) \nwhere g(z) = (1+exp(−z))−1 is known as the logistic functio n (Figure 2). One way to \nderiv e the form of the logistic functio n is to say that the log-odds of the predicted class \nprobabilities should be a linear function of the inputs: \nlog P (y =1|x,θ,θ0)= θT x + θ0 (6) P (y = −1|x,θ,θ0) \nSo for example, when we predict the same probabilit y (1/2) for both classe s, the log-odds \nterm is zero and we recover the decision boundar y θT x + θ0 = 0. The precise functiona l \nform of the logistic function, or, equiv alently, the fact that we chose to model log-odds \nwith the linear predictio n, may seem a little arbit rary (but perhaps not more so than the \nhinge loss used with the SVM classiﬁer). We will deriv e the form of the logistic function \nlater on in the course based on certa in assumptio ns about class-c onditional distributio ns \nP (x|y = 1) and P (x|y = −1). \nIn order to better compar e the logistic regression model with the SVM we will write the \nconditio nal probabil ity P (y|x,θ,θ0) a bit more succinctly . Speciﬁc ally, since 1 − g(z)= \ng(−z) we get \nP (y = −1|x,θ,θ0)=1 − P (y =1|x,θ,θ0)=1 − g( θT x + θ0 )= g −(θT x + θ0) (7) \nand therefor e \nP (y|x,θ,θ0)= gy(θT x + θ0) (8) \nSo now we have a linear classiﬁer that makes probabilistic predictions about the labels. \nHow should we train such models? A sensible criterion would seem to be to maximize the \nprobabilit y that we predict the correct label in response to each example. Assuming each \nexample is labeled independen tly from others, this proba bility of assigning correct labels \nto examples is given by the product \nn\nL(θ,θ0)= P (yt|xt,θ,θ0) (9) \nt=1 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "to be very good for “typical examples” but exhibit some variation in more diﬃcult cases. One simple model of noisy label", "source_title": "59a63d2efbe8aa01041937ff539a449a lec4", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "2ed40a8d-6f68-4792-83a9-eacf51fc1e74", "text": "correct label in response to each example. Assuming each example is labeled independen tly from others, this proba bility of assigning correct labels to examples is given by the product n L(θ,θ0)= P (yt|xt,θ,θ0) (9) t=1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� � � 4 6.867 Mac hine learning , lectur e 4 (Jaakkola) \nL(θ,θ0) is known as the (conditiona l) likeliho od functio n and is interpreted as a function \nof the parameters for a ﬁxed data (labels and examples). By maximizing this condition al \nlikeliho od with respect to θ and θ0 we obtain maximum likeliho od estimates of the param­\neters. Maxim um likeliho od estimators1 have many nice prop erties. For example, assuming \nwe have selected the right model class (logistic regression model) and certa in regularit y \nconditio ns hold, then the ML estima tor is a) consistent (we will get the right parameter \nvalues in the limit of a large number of training examples ), and b) eﬃcient (no other esti­\nmator will converge to the correct parameter values faster in the mean squared sense). But \nwhat if we do not have the right model class? Neither property may hold as a result. More \nrobust estima tors can be found in a larger class of estima tors called M-estimator s that \nincludes maximum likeliho od. We will nevertheless use the maximum likeliho od principle \nto set the parameter values. \nThe product form of the condit ional likeliho od functio n is a bit diﬃcult to work with \ndirectly so we will maximize its logarithm instea d: \nn\nl(θ,θ0) = log P (yt|xt,θ,θ0) (10) \nt=1 \nAlterna tively, we can minimize the negat ive logarithm \nlog-loss n�� �� � \n− l(θ,θ0)= − log P (yt|xt,θ,θ0) (11) \nt=1 \nn\n= − log gyt(θT xt + θ0) (12) \nt=1 \nn� � � �� \n= log 1+exp −yt(θT xt + θ0) (13) \nt=1 \nWe can interpret this simila rly to the sum of the hinge losse s in the SVM appr oach. As \nbefore, we have a base loss function, here log[1 + exp(−z)] (Figure 1b), simila r to the hinge \nloss (Figure 1a), and this loss depends only on the value of the “marg in” yt(θT xt + θ0) for \neach example. The diﬀerence here is that we have a clear proba bilistic interpretat ion of \nthe “strength” of the prediction, i.e., how high P (yt|xt,θ,θ0) is for any particular example. \nHaving a proba bilistic interpretat ion does not, however, mean that the probabilit y values \nare in any way sensible or calibr ated. Predicted proba bilities are calibr ated when they \n1An estim ator is a function that maps data to paramete r values . An estim ate is the value obtained in \nresponse to speciﬁc data. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "logistic regression", "section_heading": "correct label in response to each example. Assuming each example is labeled independen tly from others, this proba bilit", "source_title": "59a63d2efbe8aa01041937ff539a449a lec4", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "c04c00fc-32fe-4dc5-b054-51d0326583bb", "text": "calibr ated. Predicted proba bilities are calibr ated when they 1An estim ator is a function that maps data to paramete r values . An estim ate is the value obtained in response to speciﬁc data. Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� � 5 6.867 Mac hine learning , lectur e 4 (Jaakkola) \ncorrespond to observ ed frequencies. So, for example, if we group together all the examples \nfor which we predict positiv e label with proba bility 0.5, then roughly half of them should be \nlabeled +1. Proba bility estimates are rarely well-calibra ted but can nevertheless be useful. \nThe minimiza tion problem we have deﬁned above is convex and there are a number of \noptimization metho ds available for ﬁnding the minimizing θˆand θˆ0 including simple gradi­\nent desce nt. In a simple (stochastic) gradient descent, we would modify the parameters in \nresponse to each term in the sum (based on each training example). To specify the updates \nwe need the following deriv atives \nd log � \n1 + exp � \n−yt(θT xt + θ0) �� \n= −yt exp −yt(θT xt + θ0) (14) dθ0 1 + exp( −yt(θT xt + θ0)) \n= −yt[1 − P (yt|xt, θ, θ0)] (15) \nand \nd \ndθ log � \n1 + exp � \n−yt(θT xt + θ0) �� \n= −ytxt[1 − P (yt|xt, θ, θ0)] (16) \nThe parameters are then updated by selecting training examples at rando m and moving \nthe parameters in the opposite direc tion of the deriv atives: \nθ0 ← θ0 + η · yt[1 − P (yt|xt,θ,θ0)] (17) \nθ ← θ + η · ytxt[1 − P (yt|xt,θ,θ0)] (18) \nwhere η is a small (positiv e) learning rate. Note that P (yt|xt,θ,θ0) is the probabilit y that \nwe predict the training label correctly and [1−P (yt|xt,θ,θ0)] is the probabilit y of making a \nmistak e. The stochastic gradien t descent updates in the logistic regression context therefore \nstrongly resem ble the perceptro n mistake driven updates. The key diﬀerence here is that \nthe updates are graded, made in proportion to the proba bility of making a mistake. \nThe stochastic gradient descent algorithm leads to no signiﬁcan t change on average when \nthe gradient of the full objectiv e equals zero. Setting the gradient to zero is also a neces sary \nconditio n of optimalit y: \nnd � \ndθ0 (−l(θ,θ0)= − yt[1 − P (yt|xt,θ,θ0)]=0 (19) \nt=1 \nnd � \ndθ (−l(θ,θ0)) = − ytxt[1 − P (yt|xt,θ,θ0)]=0 (20) \nt=1 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "gradient descent", "section_heading": "calibr ated. Predicted proba bilities are calibr ated when they 1An estim ator is a function that maps data to paramete ", "source_title": "59a63d2efbe8aa01041937ff539a449a lec4", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "09c067ca-8b9a-432f-aca8-6cf34cab8fa9", "text": "gradient to zero is also a neces sary conditio n of optimalit y: nd � dθ0 (−l(θ,θ0)= − yt[1 − P (yt|xt,θ,θ0)]=0 (19) t=1 nd � dθ (−l(θ,θ0)) = − ytxt[1 − P (yt|xt,θ,θ0)]=0 (20) t=1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� \n� \n� � \n� \n� � �� 6 6.867 Mac hine learning , lectur e 4 (Jaakkola) \nThe sum in Eq.(1 9) is the diﬀerence between mistake probabiliti es associated with positiv ely \nand negatively labeled examples. The optimalit y of θ0 therefore ensures that the mistakes \nare balanced in this (soft) sense. Another way of unde rstanding this is that the vector of \nmistak e probabilities is orthogo nal to the vector of labels. Simila rly, the optima l settin g \nof θ is characterized by mistake proba bilities that are orthogonal to all rows of the label-\nexample matrix X˜=[y1x1,...,ynxn]. In other words, for each dimension j of the example \nvectors, [y1x1j ,...,ynxnj ] is orthogonal to the mista ke proba bilities. Taken together, these \northogonality conditions ensure that there’s no further linearly available informa tion in \nthe examples to impro ve the predicted probabilities (or mista ke proba bilities). This is \nperhaps a bit easier to see if we ﬁrst map ±1 labels into 0/1 labels: ˜yt = (1+ yt)/2 so that \ny˜t ∈{0, 1}. Then the above optima lity conditio ns can be rewritten in terms of predictio n \nerrors [˜yt − P (y =1|xt,θ,θ0)] rather than mistake probabilities as \nn\n[˜yt − P (y =1|xt,θ,θ0)]=0 (21) \nt=1 \nn\nxt[˜yt − P (y =1|xt,θ,θ0)]=0 (22) \nt=1 \nand \nn n\nθ0�[˜yt − P (y =1|xt,θ,θ0)] + θ�T xt[˜yt − P (y =1|xt,θ,θ0)] (23) \nt=1 t=1\nn\n= (θ�T xt + θ0)[˜yt − P (y =1|xt,θ,θ0)]=0 (24) \nt=1 \nmeaning that the prediction errors are orthogonal to any linear function of the inputs. \nLet’s try to brieﬂy understand the type of predictio ns we could obtain via maxim um like­\nlihood estimatio n of the logistic regression model. Supp ose the training examples are \nlinearly separa ble. In this case we can ﬁnd parameter values such that yt(θT xt + θ0) are \npositiv e for all training examples . By scaling up the parameter s, we make these values \nlarger and larger. This is beneﬁc ial as far as the likeliho od model is concerned since \nthe log of the logistic functio n is strictly increasing as a function of yt(θT xt + θ0) (the loss \nlog 1+exp −yt(θT xt + θ0) is strictly decreasing). Thus, as a result, the maxim um like­\nlihood parameter values would become unbounded, and inﬁnite scaling of any parameters \ncorrespondin g to a perfect linear classiﬁer would attain the highest likeliho od (likelihood \nof exactly one or the loss exactly zero). The resulting probabilit y values, predicting each \ntraining label correctly with probabilit y one, are hardly accurat e in the sense of reﬂecting \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "matrix", "section_heading": "gradient to zero is also a neces sary conditio n of optimalit y: nd � dθ0 (−l(θ,θ0)= − yt[1 − P (yt|xt,θ,θ0)]=0 (19) t=1", "source_title": "59a63d2efbe8aa01041937ff539a449a lec4", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "db8ca47b-f3bd-4b88-ba1e-fedf0752abad", "text": "the highest likeliho od (likelihood of exactly one or the loss exactly zero). The resulting probabilit y values, predicting each training label correctly with probabilit y one, are hardly accurat e in the sense of reﬂecting Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n7 6.867 Mac hine learning , lectur e 4 (Jaakkola) \nour uncerta inty about what the labels migh t be. So, when the number of training ex­\namples is small we would need to add the regula rizer �θ�2/2 just as in the SVM model. \nThe regularizer helps selec t reasonable parameters when the available training data fails to \nsuﬃcie ntly constra in the linear classiﬁer. \nTo estima te the parameters of the logistic regression model with regularization we would \nminimize instea d \nn1 � � � ��\n2�θ�2 + C log 1+exp −yt(θT xt + θ0) (25)\nt=1 \nwhere the constan t C again speciﬁes the trade-oﬀ between correct classiﬁcatio n (the ob­\njective) and the regular ization penalty. The regulariza tion problem is typically written \n(equiv alently) as \nnλ \n2 �θ�2 + � \nlog � \n1 + exp � \n−yt(θT xt + θ0) �� \n(26) \nt=1 \nsince it seems more natural to vary the strength of regula rizatio n with λ while keeping the \nobjective the same. \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "regularization", "section_heading": "the highest likeliho od (likelihood of exactly one or the loss exactly zero). The resulting probabilit y values, predict", "source_title": "59a63d2efbe8aa01041937ff539a449a lec4", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "24271ab7-8d37-4f44-a11e-d2ec0c3389a9", "text": "6.867 Machine learning \nFinal exam (Fall 2003) \nDecem ber 14, 2003 \nProblem 1: your information \n1.1. Your name and MIT ID: \n1.2. The grade you would give to yourself + brief justiﬁcati on (if you feel that \nthere’s no question your grade should be an A, then just say A): \nA ... why not?\n1\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "if ", "section_heading": "6.867 Machine learning ", "source_title": "0da3e361419509f57ef59bb4345fa2d5 final f03soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 0}
{"id": "0a63158e-333f-42fc-8f04-54bb6b9025b7", "text": "MIT ID: 1.2. The grade you would give to yourself + brief justiﬁcati on (if you feel that there’s no question your grade should be an A, then just say A): A ... why not? 1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 2 \n2.1. (3 points) Let F be a set of classiﬁers whose VC-dime nsion is 5. Suppose we have \nfour training examples and labels, {(x1,y1),..., (x4,y4)}, and select a classiﬁer fˆ \nfrom F by minimizing classiﬁcat ion error on the training set. In the absenc e of any \nother informat ion about the set of classiﬁers F, can we say that the predic tion fˆ(x5) \nfor a new example x5 has any relation to the training set? Brieﬂy justify your answer. \nSince VC-dimension is 5, F can shatter (some) ﬁve points. These points could be \nx1,...,x5. Thus we can ﬁnd f1 ∈F consistent with the four training examples and \nf1(x5)=1, as well as another classiﬁer f∈F also consistent with the training −1 \nexamples for which f−1(x5)= −1. The training set therefore doesn’t constr ain the \nprediction at x5. \n2.2. (T/F – 2 points) Consider a set of classiﬁers that includes all linear \nclassiﬁers that use diﬀerent choices of strict subsets of the components \nof the input vectors x ∈ Rd . Claim: the VC-dimens ion of this combined \nset cannot be more than d + 1. T \nA linear classiﬁer based on a subset of features can be represente d as a \nlinear classiﬁer based on all the features but we have simply set some \nof the parameters to zero. The set of classiﬁers here is therefore simply \nlinear classiﬁer s in Rd . \n2.3. (T/F – 2 points) Structu ral risk minimiz ation is based on compa ring \nupper bounds on the genera lizatio n error, where the bounds hold with \nprobabilit y 1 − δ over the choice of the training set. Claim: the value \nof the conﬁdenc e parameter δ canno t aﬀect model selectio n decisions. F \nThe δ parameter changes the complexity penalty in a manner that de­\npends on the VC-dimensio n and the numb er of training examples. It \ncan therefore aﬀect the model selection results. \n2.4. (6 points) Supp ose we use class- conditiona l Gaussians to solve a binary classiﬁcatio n \ntask. The covariance matr ices of the two Gaussians are constrained to be σ2 I, where \nthe value of σ2 is ﬁxed and I is the identity matr ix. The only adjustable parame­\nters are theref ore the means of the class conditiona l Gaussians, and the prior class \nfrequencies . We use the maxim um likeliho od criterion to train the model. Chec k all \nthat apply. \n2 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "MIT ID: 1.2. The grade you would give to yourself + brief justiﬁcati on (if you feel that there’s no question your grade", "source_title": "0da3e361419509f57ef59bb4345fa2d5 final f03soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "52c9472c-7b46-47c6-b95a-fb8a15e036fd", "text": "ters are theref ore the means of the class conditiona l Gaussians, and the prior class frequencies . We use the maxim um likeliho od criterion to train the model. Chec k all that apply. 2 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n( ) For any three distinct training points and suﬃc iently small σ2 , the classiﬁer \nwould have zero classiﬁcat ion error on the training set \n( ) For any three training points and suﬃcien tly large σ2, the classiﬁer would always \nmake one classiﬁcatio n error on the training set \n( ) The classiﬁc ation error of this classiﬁer on the training set is always at least that \nof a linear SVM, whether the points are linearly separable or not \nNone is the correct answer. The classiﬁer is trained as a gener ative \nmodel, thus it places each Gaussian at the sample mean of the points in \nthe class. The prior class frequencies will reﬂect the numb er of points \nin each class. \n• Since points in one class can be further away from their mean than\nfrom points in the other class, the ﬁrst option cannot be correct.\n• For the second option we can assume that the variance σ2 is much\nlarger than the distanc es between the points. In this case the Gaus­\nsians from each class assign roughly the same probability to any\ntraining point. The prior class frequencies favor the dominant\nclass, therefore resulting in one error. This does not hold, how­\never, when all the training points come from the same class. Thus\nyou got 2pts for either answer to the second.\n• The last option is incorrect since SVM does not minimize the\nnumb er of misclassiﬁe d points when the points are not linearly\nseparable; the accuracy could go either way, albeit typic ally in\nfavor of SVMs.\n3\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "svm", "section_heading": "ters are theref ore the means of the class conditiona l Gaussians, and the prior class frequencies . We use the maxim um", "source_title": "0da3e361419509f57ef59bb4345fa2d5 final f03soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "d995dc45-2d33-440c-a28b-7607e1b9487f", "text": "option is incorrect since SVM does not minimize the numb er of misclassiﬁe d points when the points are not linearly separable; the accuracy could go either way, albeit typic ally in favor of SVMs. 3 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 3 \n3.1. (T/F – 2 points) In the AdaBo ost algorithm, the weights on all the T \nmisclassiﬁe d points will go up by the same multiplica tive factor . \nYou can verify this by inspecting the weight update. Note that the ad­\naboost algorithm was formulate d for weak classiﬁer s that predict ±1 \nlabels. The weights of all misclassiﬁe d points will be multiplie d by \nexp(−αyˆih(x i; θˆk)) = exp(ˆα) before normalization. \n3.2. (3 points) Provide a brief rationa le for the following observ ation about AdaBo ost. \nThe weighted error of the kth weak classiﬁer (measured relativ e to the weights at the \nbeginning of the kth iteration) tends to increase as a function of the iteratio n k. \nThe weighting on the training examples focuses on examples that are hard to classify \ncorrectly (few weak classiﬁers have classiﬁe d such examples correctly). After a few \niterations most of the weight will be on these hard examples and the weighte d error \non the next weak classiﬁer will be closer to chanc e. \nConsider a text classiﬁcat ion problem, where documen ts are represe nted by binar y (0/1) \nfeature vectors φ =[φ1,...,φm]T ; here φi indicat es whether word i appears in the documen t. \nWe deﬁne a set of weak classiﬁers, h(φ; θ)= yφi, parameterized by θ = {i,y} (the choice \nof the comp onent, i ∈{1,...,m}, and the class label, y ∈ {−1, 1}, that the component \nshould be associated with). There are exactly 2m possible weak learners of this type. \nWe use this boosting algorithm for feature selection. The idea is to simply run the boosting \nalgorithm and select the features or comp onents in the order in which they were identiﬁed \nby the weak learners. We assume that the boosting algorithm ﬁnds the best available weak \nclassiﬁer at each iteratio n. \n3.3. (T/F – 2 points) The boosting algorithm described here can select T \nthe exact same weak classiﬁer more than once. \nThe boosting algorithm optimizes each new α by assuming that all the \nprevious votes remain ﬁxed. It therefore does not optimize these coef­\nﬁcients jointly. The only way to correct the votes assigne d to a weak \nlearner later on is to introduce the same weak learner again. Since we \nonly have a discr ete set of possible weak learner s here, it also makes \nsense to talk about selecting the exact same weak learner again. \n4 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "norm", "section_heading": "option is incorrect since SVM does not minimize the numb er of misclassiﬁe d points when the points are not linearly sep", "source_title": "0da3e361419509f57ef59bb4345fa2d5 final f03soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "cccb3381-88c5-4c38-bab6-bca47a5df96a", "text": "to introduce the same weak learner again. Since we only have a discr ete set of possible weak learner s here, it also makes sense to talk about selecting the exact same weak learner again. 4 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n3.4. (4 points) Is the ranking of features generated by the boosting algorithm likely to \nbe more useful for a linear classiﬁer than the ranking from simple mutual informatio n \ncalculat ions (estimates Iˆ(y; φi)). Brieﬂy justify your answ er. \nThe boosting algor ithm generates a linear combina tion of weak classiﬁers (here fea­\ntures). The algorithm therefore evaluates each new weak classiﬁer (feature) relative \nto a linear prediction based on those already include d. The mutual information cri­\nterion considers each feature individual ly and is therefore unable to recognize how \nmultiple features might inter act to beneﬁt linear predictio n. \n5\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "for ", "section_heading": "to introduce the same weak learner again. Since we only have a discr ete set of possible weak learner s here, it also ma", "source_title": "0da3e361419509f57ef59bb4345fa2d5 final f03soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 4}
{"id": "f0d05755-ca78-40a7-ab37-bf0585332481", "text": "linear prediction based on those already include d. The mutual information cri­ terion considers each feature individual ly and is therefore unable to recognize how multiple features might inter act to beneﬁt linear predictio n. 5 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 4\n0 1 2 3 4 5 600.511.522.533.544.55\ntime step tobservation x\n0 1 2 3 4 5 600.511.522.533.544.55\ntime step tobservation xExpert 1\nExpert 2Gating\nFigur e 1a) Figur e 1b) \nFigur e 1: Time dependen t observations. The data points in the ﬁgure are generated as \nsets of ﬁve consecutiv e time dependen t observations, x1,...,x5. The clusters come from \nrepeatedly generating ﬁve consecutiv e samples . Each visible cluster consists of 20 points, \nand has appro xima tely the same variance. The mean of each cluster is shown with a large \nX. \nConsider the data in Figure 1 (see the captio n for details). We begin by modeling this data \nwith a three state HMM, where each state has a Gaussian output distribut ion with some \nmean and variance (means and variances can be set indep enden tly for each state). \n4.1. (4 points) Draw the state transitio n diagram and the initial state distribution for \na three state HMM that models the data in Figure 1 in the maximum likelihood \nsense. Indic ate the possible transitions and their proba bilities in the ﬁgure below \n(whether or not the state is reachable after the ﬁrst two steps). In order words, your \ndrawing should characterize the 1st order homo geneous Markov chain govering the \nevolution of the states. Also indic ate the means of the corresp onding Gaussian output \ndistributio ns (please use the boxes). \nBegin\nt=1 t=2321\n.5.5\n2\n31\n1\n11\n6\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "hmm", "section_heading": "linear prediction based on those already include d. The mutual information cri­ terion considers each feature individual", "source_title": "0da3e361419509f57ef59bb4345fa2d5 final f03soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 5}
{"id": "5d13c70e-1b1e-411f-bceb-4cbe6a874bb7", "text": "homo geneous Markov chain govering the evolution of the states. Also indic ate the means of the corresp onding Gaussian output distributio ns (please use the boxes). Begin t=1 t=2321 .5.5 2 31 1 11 6 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n4.2. (4 points) In Figure 1a draw as ovals the clusters of outputs that would form if \nwe repeatedly generated samples from your HMM over time steps t =1,..., 5. The \nheigh t of the ovals should reﬂec t the variance of the clusters. \n4.3. (4 points) Supp ose at time t = 2 we observe x2 =1.5 but don’t see 2 \nthe observations for other time points. What is the most likely state at \nt = 2 according to the marg inal posterior proba bility γ2(s) deﬁned as \nP (s2 = sx2 =1.5). |\nWe have only two possible paths for the ﬁrst three states, 1,2,3, or 1,3,1. \nThe marginal poster ior probability comes from averaging the state oc­\ncupancies across these possible paths, weighte d by the corresponding \nprobabilities. Given the observation at t =2 (mean of the output dis­\ntribution from state 2), the ﬁrst path is more likely . \n4.4. (2 points) What would be the most likely state at t = 2 if we also saw 3 \nx3 =0 at t = 3? In this case γ2(s)= P (s2 = sx2 =1.5,x3 = 0). |\nThe new observation at t =3 is very unlikely to have come from state \n3, thus we switch to state sequenc e 1,3,1. \n4.5. (4 points) We can also try to model the data with conditio nal mixtures (mixtures \nof experts), where the conditioning is based on the time step. Supp ose we only use \ntwo experts which are linea r regression models with additiv e Gaussian noise, i.e., \n1 � 1 � \nP (x|t,θi)= � \n2πσi 2 exp − 2σi 2 (x − θi0 − θi1t)2 \nfor i =1, 2. The gating network is a logistic regression model from t to binary \nselection of the experts. Assuming your estima tion of the conditio nal mixture model \nis succe ssfully in the maximum likeliho od sense, draw the resulting mean predic tions \nof the two linear regression models as a function of time t in Figur e 1b). Also, with \na vertical line, indicat e where the gating network would chang e it’s preference from \none expert to the other. \n4.6. (T/F – 2 points) Claim: by repeatedly sampling from your con- T \nditional mixture model at succes sive time points t =1, 2, 3, 4, 5, the \nresulting samples would resemble the data in Figure 1 \n. \nSee the ﬁgure. \n7\n \n Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "logistic regression", "section_heading": "homo geneous Markov chain govering the evolution of the states. Also indic ate the means of the corresp onding Gaussian ", "source_title": "0da3e361419509f57ef59bb4345fa2d5 final f03soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "aa593487-d750-430c-894d-2be06768765a", "text": "by repeatedly sampling from your con- T ditional mixture model at succes sive time points t =1, 2, 3, 4, 5, the resulting samples would resemble the data in Figure 1 . See the ﬁgure. 7 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n4.7. (4 points) Having two comp eting models for the same data, the HMM and the \nmixtur e of experts model, we’d like to select the better one. We think that any \nreaso nable model selection criterio n would be able to select the better model in this \ncase. Whic h model would we choose? Provide a brief justiﬁcatio n. \nThe mixtur e of experts model assigns a higher probability to the available data since \nthe HMM puts some of the probability mass wher e there are no points. The HMM \nalso has more parameter s so any reasonable model selection criterion should select \nthe mixtur e of experts model. \n8\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 5\nx1 x2 x3 \ny1 y2 y3 \nx1 x2 x3 \ny1 y2 y3 \na) Bayesian network (directed) b) Markov rando m ﬁeld (undirected) \nFigur e 2: Graphical models \n5.1. (2 points) List two diﬀerent types of indep endence properties satisﬁed by the Bayesian \nnetwork model in Figur e 2a. \n1) x1 and x2 are marginal ly indep endent.\n2) y1 and y2 are conditional ly indep endent given x1 and x2.\nLots of other possibilities.\n5.2. (2 points) Write the facto rizatio n of the joint distribut ion implied by the directed \ngraph in Figure 2a. \nP (x1)P (x2)P (x3)P (y1|x1,x2)P (y2|x1,x2,x3)P (y3|x2,x3). \n5.3. (2 points) Provide an alternativ e facto rization of the joint distributi on, diﬀerent \nfrom the previous one. Your factorizat ion should be consisten t with all the properties \nof the directed graph in Figur e 2a. Consistency here means: what ever is implie d by \nthe graph should hold for the associated distributio n. \nAny factor ization that incorporates all the indep endencies from the graph and a few \nmore would be possible. For example, P (x1)P (x2)P (x3)P (y1)P (y2)P (y3). In this \ncase all the variables are indep endent, so any indep endenc e statement that we can \nderive from the graph clearly holds for this distr ibution as well. \n9\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "margin", "section_heading": "by repeatedly sampling from your con- T ditional mixture model at succes sive time points t =1, 2, 3, 4, 5, the resultin", "source_title": "0da3e361419509f57ef59bb4345fa2d5 final f03soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 7}
{"id": "3a0f334c-d699-4a67-9487-00cd17585fe6", "text": "(x2)P (x3)P (y1)P (y2)P (y3). In this case all the variables are indep endent, so any indep endenc e statement that we can derive from the graph clearly holds for this distr ibution as well. 9 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n5.4. (4 points) Provide an indep endenc e statemen t that holds for the undirected model \nin Figure 2b but does NOT hold for the Bayesian network. Whic h edge(s) should \nwe add to the undirected model so that it would be consisten t with (wouldn’t imply \nanything that is not true for) the Bayesian network? \nx1 is indep endent of x3 given x2 and y2. In the bayesian network any know ledge \nof y2 would make x1 and x3 dependent. Adding an edge between x1 and x3 would \nsuﬃc e (cf. moralization). \n5.5. (2 points) Is your resulting undirected graph triangula ted (Y/N)? Y \n5.6. (4 points) Provide two directed graphs represen ting 1) a mixture of two experts \nmodel for classiﬁcat ion, and 2) a mixture of Gaussians classiﬁers with two mixture \ncomp onents per class. Please use the following notation: x for the input observation, \ny for the class, and i for any selection of comp onents. \ni\nyX\niy\nX\nA mixtur e of Gaussians model, A mixtur e of experts classiﬁer , wher e i =1, 2 selects the Gaussian wher e i =1, 2 selects the expert. component within each class. \n10\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "bayesian network", "section_heading": "(x2)P (x3)P (y1)P (y2)P (y3). In this case all the variables are indep endent, so any indep endenc e statement that we c", "source_title": "0da3e361419509f57ef59bb4345fa2d5 final f03soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 8}
{"id": "083f8646-f190-4b5a-aea3-d917b8b0b98d", "text": "6.867 Machine learning \nFinal exam \nDecem ber 5, 2002 \n(2 points) Your name and MIT ID: \nJ Doe, #000\n(4 points) The grade you would give to yourself + a brief justiﬁcat ion:\nA or perhaps A-if there are any typos or other errors in the solutions...\nProblem 1 \nWe wish to estima te a mixture of two experts model for the data displa yed in Figure 1. \nThe experts we can use here are linear regression models of the form \np(y|x, w)= N( y; w1x + w0,σ2 ) \nwhere N(y; µ,σ2) denotes a Gaussian distr ibution over y with mean µ and variance σ2 . \nEach expert i can choose its parameter s wi =[wi0,wi1]T and σi 2 indep endently from other \nexperts. Note that the ﬁrst subindex i in wij refers to the expert. \nThe gating network in the case of two experts is given by a logistic regression model \nP (expert = 1x, v)= g( v1x + v0 ) |\nwhere g(z) = (1+exp(−z))−1 and v =[v0,v1]T . \n1 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "logistic regression", "section_heading": "6.867 Machine learning ", "source_title": "eb184a21ce698a4ea5c87ad77b3ea454 final f02soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 0}
{"id": "1bd86e2c-8b05-4c1f-a5ae-109ad8567476", "text": "The gating network in the case of two experts is given by a logistic regression model P (expert = 1x, v)= g( v1x + v0 ) | where g(z) = (1+exp(−z))−1 and v =[v0,v1]T . 1 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n0 1 2 3 4 5 6 7−1.5−1−0.500.511.522.533.5\nxy(1) (2)gate\n0 1 2 3 4 5 6 7−1.5−1−0.500.511.522.533.5\nxy(1)\n(2)(assigns 0.5 to each expert almost anywhere)gate a) unreg ularized case b) regula rized case \nFigur e 1: Data for mixt ures of experts \n1. (4 points) Supp ose we estimat e a mixt ure of two experts model based on the data in \nFigur e 1. You can assume that the estima tion is succes sful in the sense that we will \nﬁnd a setting of the parameters that maximizes the log-likeliho od of the data. Please \nindicate (appro xima tely) in Figure 1a) the mean predictio ns from the two experts as \nwell as the decision bounda ry for the gating network. Label the mean predic tions \n– functio ns of x – with “(1)” and “(2)” corresponding to the two experts, and the \ndecis ion boundary with “gate”. \n2. (4 points) We now switc h to a regula rized maximum likeliho od objective by incor­\nporating the following regular ization penalt y \nc 2 2−2(w11 + w21) \ninto the log-likeliho od objectiv e. Note that the penalty includes only one parameter \nfrom each of the experts. By increasing c, we impose a stronger penalty. Simila rly to \nthe previo us questio n, please indicate in Figure 1b) the optima l regularized solutio n \nfor the mixture of two experts model when the regulariza tion parameter c isset to a \nvery large value. \n3. (3 points) Are the variances in the predictive Gaussian distributio ns of the experts \n( ) larger,\n( ) smaller,\n(x) about the same \nafter the regula rizatio n? \n2 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "logistic regression", "section_heading": "The gating network in the case of two experts is given by a logistic regression model P (expert = 1x, v)= g( v1x + v0 ) ", "source_title": "eb184a21ce698a4ea5c87ad77b3ea454 final f02soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 1}
{"id": "973d4423-7bba-4332-9d6b-979c0b890bb1", "text": "to a very large value. 3. (3 points) Are the variances in the predictive Gaussian distributio ns of the experts ( ) larger, ( ) smaller, (x) about the same after the regula rizatio n? 2 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n1 2P(x=1)\nP(x=2)\nP(x=3)0s12\n0\n0.8\nP(x=4)0.1\n0.199\n0.001.01\n1 .990.20.7Figur e 2: A two-state HMM for Problem 2\n1 2 3 4\n.5.5\n.5.5 .5.5\n.5.01.49\nFigur e 3: An alter nativ e, four-stat e HMM for Problem 2 \nProblem 2 \nFigur e 2 shows a two-state HMM. The transition proba bilities of the Markov chain are \ngiven in the transition diagram. The output distributio n corresponding to each state is \ndeﬁned over {1, 2, 3, 4} and is given in the table next to the diagram. The HMM is equally \nlikely to start from either of the two states. \n1. (3 points) Give an example of an output sequence of length 2 which \ncan not be generated by the HMM in Figure 2. 1,2 \n2. (2 points) We generat ed a sequence of 6, 8672002 observ ations from the \nHMM, and found that the last observation in the sequence was 3. Wha t \nis the most likely hidden state corresp onding to that last observ ation? 2 \n3. (2 points) Consider an output sequence 3 3. What is the most likely \nsequenc e of hidden states corresponding to these observ ations? 1,1 \n4. (2 points) Now, consider an output sequence 3 3 4. What are the ﬁrst \ntwo states of the most likely hidde n state sequence ? 2,2 \n3\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "hmm", "section_heading": "to a very large value. 3. (3 points) Are the variances in the predictive Gaussian distributio ns of the experts ( ) larg", "source_title": "eb184a21ce698a4ea5c87ad77b3ea454 final f02soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 2}
{"id": "0d22f26c-6269-4852-8f1b-b43d62e434f1", "text": "hidden states corresponding to these observ ations? 1,1 4. (2 points) Now, consider an output sequence 3 3 4. What are the ﬁrst two states of the most likely hidde n state sequence ? 2,2 3 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n5. (4 points) We can try to increase the modeling capacit y of the HMM a bit by \nbreak ing each state into two states. Following this idea, we created the diagr am in \nFigur e 3. Can we set the initia l state distribut ion and the output distribut ions so \nthat this 4-state model, with the transition probabilities indic ated in the diagram, \nwould be equiv alent to the origina l 2-state model? If yes, how? If no, why not? \nNo we cannot. First note that we have to associate the ﬁrst two states in the 4-state \nmodel with state 1 of the 2-state model. The probability of leaving the ﬁrst two states \nin the 4-state model, however, depends on time (whether the chain happ ens to be in \nstate 1 or 2). In contrast, in the 2-state model the probability of transitioning to 2 \nis always 0.01. \n6. (T/F – 2 points) The Markov chain in Figure 3 is ergodic\n F\n4\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Probabilistic Models", "subtopic": "hmm", "section_heading": "hidden states corresponding to these observ ations? 1,1 4. (2 points) Now, consider an output sequence 3 3 4. What are t", "source_title": "eb184a21ce698a4ea5c87ad77b3ea454 final f02soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 3}
{"id": "dc3323e6-cfea-4ae3-a992-365322e5ddc6", "text": "be in state 1 or 2). In contrast, in the 2-state model the probability of transitioning to 2 is always 0.01. 6. (T/F – 2 points) The Markov chain in Figure 3 is ergodic F 4 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 3 \nFigur e 4 shows a graphica l model over four binary valued variables, x1,...,x4. We do not \nknow the parameters of the probabilit y distribu tion associated with the graph. \nx1 x2 \nx3 x4 \nFigur e 4: A graphical model \n1. (2 points) Would it typically help to know the value of x3 so as to no \ngain more informat ion about x2? (please answer yes or no) \n2. (2 points) Assume we already know the value of x4. Would it help in yes \nthis case to know the value of x3 to gain more information about x2? \n(please answ er yes or no) \n3. (3 points) List three diﬀerent conditio nal indep endence statemen ts between the four \nvariables that can be inferred from the graph. You can include margina l indep endence \nby saying “given nothing” . \n(a) (x1) is indep enden t of (x2) given (nothing) \n(b) (x3) is indep enden t of (x2) given (nothing) \n(c) (x3) is indep enden t of (x4) given (x1) \n4. (2 points) The following table gives a possible partial speciﬁc ation of the conditiona l \nprobabilit y P (x4x1,x2) associated with the graph. Fill in the missing values so that |\nwe could omit the arrow x1 → x4 in the graph and the graph would still adequately \nrepresen t the probabilit y distributio n. \nP (x4 =1x1 =0,x2 = 0) 0.8 |\nP (x4 =1x1 =0,x2 = 1) 0.4 |\nP (x4 =1x1 =1,x2 = 0) 0.8 |\nP (x4 =1x1 =1,x2 = 1) 0.4 |\n5\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Supervised Learning", "subtopic": "margin", "section_heading": "be in state 1 or 2). In contrast, in the 2-state model the probability of transitioning to 2 is always 0.01. 6. (T/F – 2", "source_title": "eb184a21ce698a4ea5c87ad77b3ea454 final f02soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 4}
{"id": "622ff9aa-6e6a-4148-9cd4-ff52baeb2965", "text": "y distributio n. P (x4 =1x1 =0,x2 = 0) 0.8 | P (x4 =1x1 =0,x2 = 1) 0.4 | P (x4 =1x1 =1,x2 = 0) 0.8 | P (x4 =1x1 =1,x2 = 1) 0.4 | 5 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n5. (4 points) Let’s again focus on the original graph in Figure 4. Since we don’t know \nthe unde rlying proba bility distributio n, we need to estima te it from observ ed data. \nUnfort unately , the dataset we have is incomplet e and contains only observ ations for \nx2 and x3. In other words, the dataset is D = {(x2,xt t \n3),t =1,...,n}. In the joint\ndistributio n\nP (x1,x2,x3,x4)= P (x1)P (x2)P (x3|x1)P (x4|x1,x2) \nwe have a number of comp onents (smaller probabilit y tables) that need to be esti­\nmated. Please indicate which components we can hope to estimate (adjust) on the \nbasis of the available data? \n(x) P (x1) \n(x) P (x2) \n(x) P (x3|x1)\n() P (x4|x1,x2)\n6. (4 points) If we use the EM algorithm to carry out the estima tion task, what \nposterio r probabilities do we have to evaluat e in the E-step? Please provide the \nnecessary posterior probabilities in the form P (\n···\nt|x2,xt \n3).\ntWe only need to evaluate P (x1|x).3\nSince x4 is never obser ved, x1 and x2 are always indep endent. We thus have to \nestimate only two indep endent parts x1 x3 and x2, wher e x1 is unobserve d. In . →\nthe EM-algorithm we need to ﬁll-in the missing values for x1, and thus evaluate \nP (x1|x)t \n3\n6\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "if ", "section_heading": "y distributio n. P (x4 =1x1 =0,x2 = 0) 0.8 | P (x4 =1x1 =0,x2 = 1) 0.4 | P (x4 =1x1 =1,x2 = 0) 0.8 | P (x4 =1x1 =1,x2 = ", "source_title": "eb184a21ce698a4ea5c87ad77b3ea454 final f02soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 5}
{"id": "41bc94a6-4896-4c7d-b2f1-6d60b15995c1", "text": "only two indep endent parts x1 x3 and x2, wher e x1 is unobserve d. In . → the EM-algorithm we need to ﬁll-in the missing values for x1, and thus evaluate P (x1|x)t 3 6 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem 4 \nWe try to select here between two models. Both models are logistic regression models \nbut diﬀer in terms of the type of featur es that are used in making the predictions. More \nspeciﬁcally , the models have the commo n squashed additiv e form \nP (y =1|x, w)= g( w0 + w1φ1(x)+ ... + φm(x)) \nwhere the input is a real number x ∈R. The models diﬀer in terms of the number and the \ntype of basis functio ns used: \nmodel 1 : m =1,φ1(x)= x\nmodel 2 : m =2,φ1(x)= x, φ2(x) = sin(x)\n1. (4 points) Supp ose we have n training examples (xt,yt), t =1,...,n, and we eval­\nuate structur al risk minimizatio n scores (bounds on the generalizat ion error ) for the \ntwo classiﬁers. Whic h of the following statements are valid in genera l for our two \nmodels: \n( ) Score for model 1 ≥ Score for model 2 \n( ) Score for model 1 ≤ Score for model 2 \n( ) Score for model 1 = Score for model 2 \n(x) Each of the above three cases may be correct depending on the data \n( ) None of the above \n7\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "only two indep endent parts x1 x3 and x2, wher e x1 is unobserve d. In . → the EM-algorithm we need to ﬁll-in the missin", "source_title": "eb184a21ce698a4ea5c87ad77b3ea454 final f02soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 6}
{"id": "296516ec-e05d-49e6-8c6d-66e5db47ab61", "text": "Score for model 2 ( ) Score for model 1 = Score for model 2 (x) Each of the above three cases may be correct depending on the data ( ) None of the above 7 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2. (4 points) We will now switch to the Bayesian infor matio n criterion (BIC) for select­\ning amon g the two models. Let L1(n) be the log-pro babilit y of the labels that model \n1 assigns to n training labels, where the probabilities are evaluat ed at the maxim um \nlikeliho od setting of the parameters. Let L2(n) be the corresp onding log-probabilit y \nfor model 2. We imagine here that L1(n) and L2(n) are evaluated on the basis of the \nﬁrst n training examples from a much larger set. \nNow, in our empirica l studies , we found that these log-probabilit ies are related in a \nsimple way: \nL2(n) − L1(n) ≈ 0.01 n· \nHow will we end up selecting between the two models as a function of the number of \ntraining examples? Please choose one of the following cases. \n( ) Always selec t 1\n( ) Always selec t 2\n(x) First select 1, then 2 for larger n\n( ) First select 2, then 1 for larger n\n3. (4 points) Provide a brief justiﬁcation for your answ er to the previous question. \nFor large n we would select model 2 since it has a consistent (albeit smal l) advantage. \nInitial ly, however, we would choose model 1 due to smal ler comple xity penalty. \nTo see this a bit more precisely, let’s recall the form of the BIC score: for model 1 \nit is deﬁne d as BIC1 = L1(n) − d\n2 1 log(n), where d1 is the numb er of parameter s in \nmodel 1. The diﬀer ence between the BIC scores for the two models is therefore \n3−2 \n0.01 n � �� � � ��·� d2 − d1BIC2 − BIC1 = L2(n) − L1(n) − 2 log(n) \n1 = 0.01 · n − 2 log(n) \nWhen n is smal l the complexity term dominates and BIC2 < BIC1 (the diﬀer ence \nis negative). For large n the linear increase of the log-likeliho od diﬀer ence overcomes \nthe logarithmic penalty and BIC2 > BIC1. \n8\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "erm", "section_heading": "Score for model 2 ( ) Score for model 1 = Score for model 2 (x) Each of the above three cases may be correct depending o", "source_title": "eb184a21ce698a4ea5c87ad77b3ea454 final f02soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 7}
{"id": "ab04b793-386c-4d66-8c77-cb915d0ec5ee", "text": "smal l the complexity term dominates and BIC2 < BIC1 (the diﬀer ence is negative). For large n the linear increase of the log-likeliho od diﬀer ence overcomes the logarithmic penalty and BIC2 > BIC1. 8 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� Problem 5 \nConsider a simple two-class documen t (text) classiﬁca tion problem. Each documen t is \nrepresen ted by a binary feature vector [φi \n1,...,φi ], where φi = 1 if keyword k is present in N k \nthe documen t, and zero otherwise. N is the number of keywords we have chosen to include. \nWe use a Naiv e Bayes model for this classiﬁcatio n task. The joint distributio n of the \nfeatures and the binary labels y ∈{0, 1} is in this case given by \nN\nP (φ1,...,φN ,y)= P (y) P (φky)|\nk=1 \nwhere, for example, \nP (φk =1|y = 0) = θk,0,P (φk =1|y = 1) = θk,1 \n1. (2 points) In the space below, draw the graphical model corresponding to Naiv e \nBayes generat ive model describ ed above. Assume that N = 3 (three keywords). \nφ3φ2φ1 y \n2. (4 points) To be able to make use of training examples with possibly missing labels, \nwe will have to resort to the EM algorithm. In the EM algorithm we need to evaluate \nthe posterior proba bility of the label y given the docume nt. We will use a message \npassing algorithm (belief propaga tion) to get this poster ior probabilit y. The problem \nhere is that we relied on a rather careless friend to evaluat e whether a docume nt \ncontains any of the keywords. In other words, we do not fully trust the “observ ed” \nvalues of the features. Let φˆk be the “observed” value for the kth feature in a given \ndocumen t. The evidence we now have about the actual value of φk is given by \nP (φˆk|φk), which is a table that models how we expect the friend to respond. \nGiven that we observe φˆk = 1, what is the message that φk needs to send to y? \n9\n \n Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "smal l the complexity term dominates and BIC2 < BIC1 (the diﬀer ence is negative). For large n the linear increase of th", "source_title": "eb184a21ce698a4ea5c87ad77b3ea454 final f02soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 8}
{"id": "582f1657-2326-4617-be51-2b254f82655a", "text": "given by P (φˆk|φk), which is a table that models how we expect the friend to respond. Given that we observe φˆk = 1, what is the message that φk needs to send to y? 9 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n� Ther e are a couple of options here depending on how you choose to incorporate \nthe evidenc e φˆk =1. Perhaps the simplest way is to put it directly in the single \nnode potential ψk(φk). In other words, we deﬁne ψk(φk)= P (φˆk =1φk), and |\nψky(φk,y)= P (φky). In this notation, the message is |\nmky(y)= ψk(φk)ψky(φk,y) →\nφk=0,1 \n(t)3. (3 points) We know that the EM algorithm is in some sense monotonic. Let θˆ\nk,y be \nthe estima te of the parameters θk,y in the beginning of iteration t of the EM algorithm, \n(t) ˆ(t) ˆ(t)and θ(t) be the vector of all parameter estima tes in that itera tion, [θˆ1,0,θ1,1,..., θN,y=1]. \nWhic h of the following quan tities increa ses monot onically with t? \n(t)() P (φk =1y,θk,y) for all k �N P (φ|\ni θ(t)) (x) ,...,φi \ni=1 1 N�N φi |\n() P (yi =1|1,...,φi ,θ(t))i=1 N \n4. (4 points) The class labels actua lly correspond to “relev ant” and “irrelevant” doc­\numen ts. In classifying any documen t as relevant or irrelevant, we have to take into \naccoun t that we might prefer to miss a few relev ant documen ts if we can avoid mis­\nclassifying a large number of irrelev ant documen ts as relevant. To express such a \npreference we deﬁne a utilit y U(y, yˆ), where y is the correct label and ˆy is how we \nclassify the documen t. Draw an inﬂuence diagr am that incorp orates the Naiv e Bayes \nmodel, our decisions, and the utilit y. Mark each node in the graph according to the \nvariables (or utility) that they represent. \nU y φ1 \nˆy φ2 \nφ3 \n5. (2 points) Let’s modify the Naiv e Bayes model a bit, to account for some of the \npossible dependencies between the keywords. For example, supp ose we order the \n10 \nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Math for ML", "subtopic": "vector", "section_heading": "given by P (φˆk|φk), which is a table that models how we expect the friend to respond. Given that we observe φˆk = 1, wh", "source_title": "eb184a21ce698a4ea5c87ad77b3ea454 final f02soln", "source_url": null, "difficulty_level": "Intermediate", "content_type": "Code", "chunk_index": 9}
{"id": "4f4032be-b1d0-44ab-a60c-58f712cee17b", "text": "y φ1 ˆy φ2 φ3 5. (2 points) Let’s modify the Naiv e Bayes model a bit, to account for some of the possible dependencies between the keywords. For example, supp ose we order the 10 Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nkeywords so that it would be useful to model the depende ncy of φk on φk−1, k = \n2,...,N (the keywords may, for example, repres ent nested categories). We expect \nthese dependencies to be the same for each class, but the parameters can be aﬀected \nby the class label. Draw the graphica l model for this – call it Sophistic ated Bayes – \nmodel. Please assume again that N = 3. \nφ3φ2φ1 y \n11\nCite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare\n(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].", "topic": "Python Foundations", "subtopic": "for ", "section_heading": "y φ1 ˆy φ2 φ3 5. (2 points) Let’s modify the Naiv e Bayes model a bit, to account for some of the possible dependencies ", "source_title": "eb184a21ce698a4ea5c87ad77b3ea454 final f02soln", "source_url": null, "difficulty_level": "Beginner", "content_type": "Code", "chunk_index": 10}
